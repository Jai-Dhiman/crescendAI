# Unified Experiment Config - Full Dataset (114K samples)
# Switch between audio-only, MIDI-only, and fusion modes
# Usage: python train.py --config configs/experiment_full.yaml --mode [audio|midi|fusion]

data:
  # LOCAL PATHS (after downloading from Hugging Face Hub)
  # Data is downloaded to /tmp/ (local SSD) for fast access
  # Download happens automatically in notebook Step 2
  train_path: /tmp/crescendai_data/data/annotations/synthetic_train.jsonl
  val_path: /tmp/crescendai_data/data/annotations/synthetic_val.jsonl
  test_path: /tmp/crescendai_data/data/annotations/synthetic_test.jsonl

  # Reduced to 3 core dimensions for faster convergence
  dimensions:
    - note_accuracy      # MIDI can predict this
    - rhythmic_precision # MIDI can predict this
    - tone_quality       # Only audio can predict this (fusion advantage)

  audio_sample_rate: 24000
  max_audio_length: 240000  # 10 seconds at 24kHz
  max_midi_events: 512

  # OPTIMIZED for LOCAL SSD (10-30x faster than Google Drive!)
  batch_size: 16           # Good balance of speed and memory
  num_workers: 4           # Parallel loading
  pin_memory: true         # Faster GPU transfer
  persistent_workers: true # Keep workers alive between epochs
  prefetch_factor: 2       # Pre-load batches

  # Stronger augmentation to bridge syntheticâ†’real domain gap
  augmentation:
    enabled: true
    pitch_shift:
      enabled: true
      probability: 0.4  # Increased from 0.2
      min_semitones: -1
      max_semitones: 1
    time_stretch:
      enabled: true
      probability: 0.4  # Increased from 0.2
      min_rate: 0.9
      max_rate: 1.1
    add_noise:
      enabled: true
      probability: 0.3  # Increased from 0.1
      min_snr_db: 30
      max_snr_db: 40
    gain_variation:
      enabled: true
      probability: 0.4  # Increased from 0.2
      min_db: -3
      max_db: 3
    # Enable room acoustics for more realistic audio
    room_acoustics:
      enabled: true
      probability: 0.3  # Adds reverberation/room effects
    compress_audio:
      enabled: false  # Keep disabled (slow)
    max_transforms: 3  # Increased from 2 for more diversity

  # Mixup augmentation (applied in collate_fn for efficiency)
  mixup:
    enabled: true
    alpha: 0.2          # Beta distribution parameter
    probability: 0.5    # Apply to 50% of batches

model:
  # These will be overridden by --mode flag
  audio_dim: 768
  midi_dim: 256
  fusion_dim: 1024
  aggregator_dim: 512
  num_dimensions: 3

  # MERT settings
  mert_model_name: m-a-p/MERT-v1-95M
  freeze_audio_encoder: false
  gradient_checkpointing: true  # Essential for memory
  use_layer_selection: true     # Use middle layers (5-7) instead of last layer
  selected_layers: [5, 6, 7]    # Optimal for music understanding (research-backed)
  freeze_bottom_layers: true    # Freeze bottom 6 layers to reduce overfitting
  num_freeze_layers: 6          # Freeze layers 0-5, train 6-11

  # MIDI encoder settings
  midi_hidden_size: 256
  midi_num_layers: 6
  midi_num_heads: 8

  # Fusion settings
  fusion_num_heads: 8
  fusion_dropout: 0.1

  # Aggregation
  lstm_hidden: 256
  lstm_layers: 2
  attention_heads: 4
  aggregator_dropout: 0.2

  # MTL head - expanded for better capacity
  shared_hidden: 256
  task_hidden: 128
  mtl_dropout: 0.2  # Increased from 0.1 for better regularization

training:
  max_epochs: 5            # Increased from 3 (model needs more time with new architecture)
  precision: 16            # Mixed precision (faster + less memory)
  optimizer: AdamW
  learning_rate: 3e-5      # Increased 3x from 1e-5 (scaled with effective batch size)
  backbone_lr: 3e-5        # Increased 3x - MERT can handle higher LR with partial freezing
  heads_lr: 3e-4           # Increased 3x - matches backbone scaling
  weight_decay: 0.01
  scheduler: cosine
  warmup_steps: 1000       # Increased from 500 for stability with higher LR
  min_lr: 1e-6
  gradient_clip_val: 0.5   # Reduced from 1.0 for tighter control with higher LR
  accumulate_grad_batches: 2  # Effective batch = 32

  # Validation
  val_check_interval: 0.25 # Validate 4x per epoch (increased from 2x for more feedback)
  limit_val_batches: 1.0   # Use full validation set
  track_correlations: true # Log Pearson/Spearman correlations

callbacks:
  checkpoint:
    monitor: val_loss
    mode: min
    save_top_k: 1          # Only save best (save Drive space)
    save_last: true
    dirpath: /content/drive/MyDrive/crescendai_checkpoints/{mode}_full
    filename: '{mode}-{epoch:02d}-{val_loss:.4f}'
  early_stopping:
    monitor: val_loss
    mode: min
    patience: 4            # More patience for larger dataset
    min_delta: 0.001
  lr_monitor:
    logging_interval: step

logging:
  log_every_n_steps: 50    # Log less frequently (more steps per epoch)
  use_wandb: false         # Disable for now (can enable later)
  use_tensorboard: true
  tensorboard_logdir: logs/{mode}_full

seed: 42

# Mode-specific overrides (applied by train.py)
modes:
  audio:
    use_midi: false
    use_fusion: false
    midi_dim: 0
    fusion_dim: 768        # Just audio features

  midi:
    use_midi: true
    use_fusion: false
    audio_dim: 0
    fusion_dim: 256        # Just MIDI features

  fusion:
    use_midi: true
    use_fusion: true
    # Use default dimensions (audio=768, midi=256, fusion=1024)
