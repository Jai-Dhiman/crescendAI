# Unified Experiment Config - Full Dataset (114K→450K with degradation)
# Updated for TRAINING_PLAN_v2.md Phase 2
# Switch between audio-only, MIDI-only, and fusion modes
# Usage: python train.py --config configs/experiment_full.yaml --mode [audio|midi|fusion]

data:
  # LOCAL PATHS (after downloading from Hugging Face Hub)
  # Data is downloaded to /tmp/ (local SSD) for fast access
  # Download happens automatically in notebook Step 2

  # Original synthetic labels (114K samples, 6 dimensions)
  train_path: /tmp/crescendai_data/data/annotations/synthetic_train.jsonl
  val_path: /tmp/crescendai_data/data/annotations/synthetic_val.jsonl
  test_path: /tmp/crescendai_data/data/annotations/synthetic_test.jsonl

  # Phase 1: Degraded dataset paths (450K samples, 8 dimensions, 4 quality tiers)
  # Uncomment these after running create_degraded_maestro.py
  # train_path: /tmp/crescendai_data/data/annotations_degraded/degraded_train.jsonl
  # val_path: /tmp/crescendai_data/data/annotations_degraded/degraded_val.jsonl
  # test_path: /tmp/crescendai_data/data/annotations_degraded/degraded_test.jsonl

  # All 8 dimensions (TRAINING_PLAN_v2.md - updated from 6)
  # Technical (4), Timbre/Dynamics (2), Interpretive (2)
  dimensions:
    # Technical (4)
    - note_accuracy          # MIDI note count/pitch complexity
    - rhythmic_stability     # MIDI timing variance and tempo stability
    - articulation_clarity   # MIDI duration variance + audio attack transients
    - pedal_technique        # MIDI CC64 sustain pedal coherence
    # Timbre/Dynamics (2)
    - tone_quality           # Audio spectral centroid + inharmonicity
    - dynamic_range          # MIDI velocity range + smoothing
    # Interpretive (2) - NEW in v2
    - musical_expression     # MIDI phrasing + audio dynamic contour
    - overall_interpretation # Combined performance quality metrics

  audio_sample_rate: 24000
  max_audio_length: 240000  # 10 seconds at 24kHz
  max_midi_events: 512

  # OPTIMIZED for LOCAL SSD (10-30x faster than Google Drive!)
  batch_size: 16           # Good balance of speed and memory
  num_workers: 4           # Parallel loading
  pin_memory: true         # Faster GPU transfer
  persistent_workers: true # Keep workers alive between epochs
  prefetch_factor: 2       # Pre-load batches

  # Stronger augmentation to bridge synthetic→real domain gap
  augmentation:
    enabled: true
    pitch_shift:
      enabled: true
      probability: 0.4  # Increased from 0.2
      min_semitones: -1
      max_semitones: 1
    time_stretch:
      enabled: true
      probability: 0.4  # Increased from 0.2
      min_rate: 0.9
      max_rate: 1.1
    add_noise:
      enabled: true
      probability: 0.3  # Increased from 0.1
      min_snr_db: 30
      max_snr_db: 40
    gain_variation:
      enabled: true
      probability: 0.4  # Increased from 0.2
      min_db: -3
      max_db: 3
    # Enable room acoustics for more realistic audio
    room_acoustics:
      enabled: true
      probability: 0.3  # Adds reverberation/room effects
    compress_audio:
      enabled: false  # Keep disabled (slow)
    max_transforms: 3  # Increased from 2 for more diversity

  # Mixup augmentation (applied in collate_fn for efficiency)
  mixup:
    enabled: true
    alpha: 0.2          # Beta distribution parameter
    probability: 0.5    # Apply to 50% of batches

  # Modality dropout (prevents modality collapse, ensures both encoders learn)
  modality_dropout:
    enabled: true
    audio_prob: 0.15    # Probability of dropping audio modality
    midi_prob: 0.15     # Probability of dropping MIDI modality

model:
  # These will be overridden by --mode flag
  audio_dim: 768
  midi_dim: 256
  fusion_dim: 1024
  aggregator_dim: 512
  num_dimensions: 8  # Updated from 6 (TRAINING_PLAN_v2.md)

  # MERT settings
  mert_model_name: m-a-p/MERT-v1-95M
  freeze_audio_encoder: false
  gradient_checkpointing: true  # Essential for memory
  use_layer_selection: true     # Use middle layers (5-7) instead of last layer
  selected_layers: [5, 6, 7]    # Optimal for music understanding (research-backed)
  freeze_bottom_layers: true    # Freeze bottom 6 layers to reduce overfitting
  num_freeze_layers: 6          # Freeze layers 0-5, train 6-11

  # MIDI encoder settings
  midi_hidden_size: 256
  midi_num_layers: 6
  midi_num_heads: 8
  midi_pretrained_checkpoint: null  # Path to pretrained MIDI encoder (optional)

  # Fusion settings
  fusion_num_heads: 8
  fusion_dropout: 0.1

  # Aggregation
  lstm_hidden: 256
  lstm_layers: 2
  attention_heads: 4
  aggregator_dropout: 0.2

  # MTL head - expanded for better capacity
  shared_hidden: 256
  task_hidden: 128
  mtl_dropout: 0.2  # Increased from 0.1 for better regularization

training:
  max_epochs: 20           # Increased for full training (diagnostic: 5 epochs)
  precision: 16            # Mixed precision (faster + less memory)
  optimizer: AdamW
  learning_rate: 3e-5      # Increased 3x from 1e-5 (scaled with effective batch size)
  backbone_lr: 3e-5        # Increased 3x - MERT can handle higher LR with partial freezing
  heads_lr: 3e-4           # Increased 3x - matches backbone scaling
  weight_decay: 0.01
  scheduler: cosine
  warmup_steps: 1000       # Increased from 500 for stability with higher LR
  min_lr: 1e-6
  gradient_clip_val: 0.5   # Reduced from 1.0 for tighter control with higher LR
  accumulate_grad_batches: 2  # Effective batch = 32

  # Staged unfreezing schedule (prevents catastrophic forgetting)
  staged_unfreezing:
    enabled: true
    schedule:
      - epoch: 0
        freeze: [audio_encoder, midi_encoder]
        unfreeze: [projection]
      - epoch: 5
        unfreeze: [audio_encoder.top_4, midi_encoder.top_2]
        lr_scale: 0.1
      - epoch: 10
        unfreeze: [audio_encoder.all, midi_encoder.all]
        lr_scale: 0.1

loss:
  # Base loss function (mse, huber, or mae)
  base_loss: huber
  huber_delta: 1.0

  # Loss component weights
  mse_weight: 1.0
  ranking_weight: 0.2
  contrastive_weight: 0.1
  ranking_margin: 5.0
  contrastive_temperature: 0.07

  # Label Distribution Smoothing (upweights rare label regions)
  lds:
    enabled: true
    num_bins: 100
    kernel_size: 5
    sigma: 2.0
    reweight_scale: 1.0

  # Bootstrap loss (handles noisy labels)
  bootstrap:
    enabled: true
    beta: 0.8              # Weight on original label (vs model prediction)
    warmup_epochs: 5       # Train with pure labels first

validation:
  val_check_interval: 0.25 # Validate 4x per epoch (increased from 2x for more feedback)
  limit_val_batches: 1.0   # Use full validation set
  track_correlations: true # Log Pearson/Spearman correlations

callbacks:
  checkpoint:
    monitor: val_loss
    mode: min
    save_top_k: 1          # Only save best (save Drive space)
    save_last: true
    dirpath: /content/drive/MyDrive/crescendai_checkpoints/{mode}_full
    filename: '{mode}-{epoch:02d}-{val_loss:.4f}'
  early_stopping:
    monitor: val_loss
    mode: min
    patience: 4            # More patience for larger dataset
    min_delta: 0.001
  lr_monitor:
    logging_interval: step

logging:
  log_every_n_steps: 50    # Log less frequently (more steps per epoch)
  use_wandb: false         # Disable for now (can enable later)
  use_tensorboard: true
  tensorboard_logdir: logs/{mode}_full

seed: 42

# Mode-specific overrides (applied by train.py)
modes:
  audio:
    use_midi: false
    use_fusion: false
    midi_dim: 0
    fusion_dim: 768        # Just audio features

  midi:
    use_midi: true
    use_fusion: false
    audio_dim: 0
    fusion_dim: 256        # Just MIDI features

  fusion:
    use_midi: true
    use_fusion: true
    # Use default dimensions (audio=768, midi=256, fusion=1024)
