{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Symbolic Encoder Training (S1, S2, S3)\n",
    "\n",
    "Train three symbolic encoder strategies on Thunder Compute:\n",
    "- **S1**: BERT-style Transformer on REMI tokens (TransformerSymbolicEncoder)\n",
    "- **S2**: GNN on score graph (GNNSymbolicEncoder)\n",
    "- **S3**: 1D-CNN + Transformer on continuous features (ContinuousSymbolicEncoder)\n",
    "\n",
    "Each model has a pretrain stage (self-supervised) and finetune stage (supervised ranking).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Jai-Dhiman/crescendAI.git /workspace/crescendai\n",
    "%cd /workspace/crescendai/model\n",
    "\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "!uv sync\n",
    "\n",
    "!rclone sync gdrive:crescendai_data/model_improvement/data ./data --progress\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "IS_REMOTE = os.environ.get('THUNDER_COMPUTE', False)\n",
    "if IS_REMOTE:\n",
    "    DATA_DIR = Path('/workspace/crescendai/model/data')\n",
    "    CHECKPOINT_DIR = Path('/workspace/crescendai/model/checkpoints/model_improvement')\n",
    "else:\n",
    "    DATA_DIR = Path('../data')\n",
    "    CHECKPOINT_DIR = Path('../checkpoints/model_improvement')\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from model_improvement.symbolic_encoders import (\n",
    "    TransformerSymbolicEncoder,\n",
    "    GNNSymbolicEncoder,\n",
    "    ContinuousSymbolicEncoder,\n",
    ")\n",
    "from model_improvement.tokenizer import PianoTokenizer, extract_continuous_features\n",
    "from model_improvement.data import MIDIPretrainingDataset, PairedPerformanceDataset, multi_task_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = DATA_DIR / 'percepiano_cache'\n",
    "\n",
    "with open(cache_dir / 'labels.json') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "with open(cache_dir / 'folds.json') as f:\n",
    "    folds = json.load(f)\n",
    "\n",
    "with open(cache_dir / 'piece_mapping.json') as f:\n",
    "    piece_to_keys = json.load(f)\n",
    "\n",
    "midi_dir = DATA_DIR / 'percepiano_midi'\n",
    "\n",
    "print(f'Loaded {len(labels)} labeled segments')\n",
    "print(f'Folds: {len(folds)}')\n",
    "for i, fold in enumerate(folds):\n",
    "    print(f'  Fold {i}: {len(fold[\"train\"])} train, {len(fold[\"val\"])} val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Symbolic Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenizing MIDI files for S1...')\n",
    "tokenizer = PianoTokenizer(max_seq_len=2048)\n",
    "\n",
    "token_sequences = {}\n",
    "for key in labels:\n",
    "    midi_path = midi_dir / f'{key}.mid'\n",
    "    if midi_path.exists():\n",
    "        tokens = tokenizer.encode(midi_path)\n",
    "        token_sequences[key] = tokens\n",
    "\n",
    "print(f'Tokenized {len(token_sequences)} MIDI files')\n",
    "print(f'Vocab size: {tokenizer.vocab_size}')\n",
    "print(f'Average sequence length: {np.mean([len(t) for t in token_sequences.values()]):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Extracting continuous features for S3...')\n",
    "\n",
    "continuous_features = {}\n",
    "for key in labels:\n",
    "    midi_path = midi_dir / f'{key}.mid'\n",
    "    if midi_path.exists():\n",
    "        features = extract_continuous_features(midi_path, frame_rate=50)\n",
    "        continuous_features[key] = torch.from_numpy(features).float()\n",
    "\n",
    "print(f'Extracted features for {len(continuous_features)} files')\n",
    "if continuous_features:\n",
    "    sample = next(iter(continuous_features.values()))\n",
    "    print(f'Feature shape: {sample.shape} (T, D={sample.shape[1]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_checkpoint(local_path, remote_subdir):\n",
    "    \"\"\"Sync checkpoint to Google Drive.\"\"\"\n",
    "    remote = f'gdrive:crescendai_data/model_improvement/checkpoints/{remote_subdir}'\n",
    "    subprocess.run(['rclone', 'copy', str(local_path), remote, '--progress'], check=True)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, model_name, fold_idx, max_epochs=200, monitor='val_loss'):\n",
    "    \"\"\"Train a model with standard callbacks.\"\"\"\n",
    "    ckpt_dir = CHECKPOINT_DIR / model_name / f'fold_{fold_idx}'\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=str(ckpt_dir),\n",
    "            filename='{epoch}-{' + monitor + ':.4f}',\n",
    "            monitor=monitor,\n",
    "            mode='min',\n",
    "            save_top_k=1,\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=monitor,\n",
    "            patience=20,\n",
    "            mode='min',\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        callbacks=callbacks,\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=10,\n",
    "        deterministic=True,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    upload_checkpoint(ckpt_dir, f'{model_name}/fold_{fold_idx}')\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train S1: Transformer on REMI Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1_CONFIG = {\n",
    "    'vocab_size': tokenizer.vocab_size + 1,  # +1 for mask token\n",
    "    'd_model': 512,\n",
    "    'nhead': 8,\n",
    "    'num_layers': 6,\n",
    "    'hidden_dim': 512,\n",
    "    'num_labels': 19,\n",
    "    'max_epochs': 200,\n",
    "}\n",
    "\n",
    "PRETRAIN_EPOCHS = 50\n",
    "FINETUNE_EPOCHS = 150\n",
    "\n",
    "print('Training S1: Transformer on REMI Tokens')\n",
    "print('=' * 50)\n",
    "\n",
    "s1_trainers = []\n",
    "for fold_idx, fold in enumerate(folds):\n",
    "    print(f'\\nFold {fold_idx}/{len(folds)-1}')\n",
    "\n",
    "    # --- Pretrain: Masked Language Modeling ---\n",
    "    print('  Pretrain: Masked token prediction')\n",
    "\n",
    "    train_tokens = [token_sequences[k] for k in fold['train'] if k in token_sequences]\n",
    "    val_tokens = [token_sequences[k] for k in fold['val'] if k in token_sequences]\n",
    "\n",
    "    pretrain_ds = MIDIPretrainingDataset(\n",
    "        token_sequences=train_tokens,\n",
    "        max_seq_len=2048,\n",
    "        mask_prob=0.15,\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "    )\n",
    "    pretrain_val_ds = MIDIPretrainingDataset(\n",
    "        token_sequences=val_tokens,\n",
    "        max_seq_len=2048,\n",
    "        mask_prob=0.15,\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "    )\n",
    "\n",
    "    pretrain_loader = DataLoader(pretrain_ds, batch_size=8, shuffle=True, num_workers=2)\n",
    "    pretrain_val_loader = DataLoader(pretrain_val_ds, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = TransformerSymbolicEncoder(**S1_CONFIG, stage='pretrain')\n",
    "    trainer_pt = train_model(\n",
    "        model, pretrain_loader, pretrain_val_loader,\n",
    "        'S1_pretrain', fold_idx,\n",
    "        max_epochs=PRETRAIN_EPOCHS,\n",
    "        monitor='val_mlm_loss',\n",
    "    )\n",
    "\n",
    "    # --- Finetune: Pairwise Ranking ---\n",
    "    print('  Finetune: Pairwise ranking')\n",
    "    model.stage = 'finetune'\n",
    "\n",
    "    finetune_ds = PairedPerformanceDataset(\n",
    "        cache_dir=cache_dir,\n",
    "        labels=labels,\n",
    "        piece_to_keys=piece_to_keys,\n",
    "        keys=[k for k in fold['train'] if k in token_sequences],\n",
    "    )\n",
    "    finetune_val_ds = PairedPerformanceDataset(\n",
    "        cache_dir=cache_dir,\n",
    "        labels=labels,\n",
    "        piece_to_keys=piece_to_keys,\n",
    "        keys=[k for k in fold['val'] if k in token_sequences],\n",
    "    )\n",
    "\n",
    "    def symbolic_collate(batch):\n",
    "        \"\"\"Custom collate that adds tokenized MIDI to paired data.\"\"\"\n",
    "        collated = {}\n",
    "        max_len = 2048\n",
    "\n",
    "        ids_a, ids_b, masks_a, masks_b = [], [], [], []\n",
    "        labels_a_list, labels_b_list = [], []\n",
    "        piece_ids_a, piece_ids_b = [], []\n",
    "\n",
    "        for item in batch:\n",
    "            key_a, key_b = item['key_a'], item['key_b']\n",
    "            if key_a not in token_sequences or key_b not in token_sequences:\n",
    "                continue\n",
    "\n",
    "            tok_a = token_sequences[key_a][:max_len]\n",
    "            tok_b = token_sequences[key_b][:max_len]\n",
    "\n",
    "            # Pad\n",
    "            pad_a = tok_a + [0] * (max_len - len(tok_a))\n",
    "            pad_b = tok_b + [0] * (max_len - len(tok_b))\n",
    "\n",
    "            ids_a.append(torch.tensor(pad_a, dtype=torch.long))\n",
    "            ids_b.append(torch.tensor(pad_b, dtype=torch.long))\n",
    "\n",
    "            mask_a = torch.zeros(max_len, dtype=torch.bool)\n",
    "            mask_a[:len(tok_a)] = True\n",
    "            masks_a.append(mask_a)\n",
    "\n",
    "            mask_b = torch.zeros(max_len, dtype=torch.bool)\n",
    "            mask_b[:len(tok_b)] = True\n",
    "            masks_b.append(mask_b)\n",
    "\n",
    "            labels_a_list.append(item['labels_a'])\n",
    "            labels_b_list.append(item['labels_b'])\n",
    "            piece_ids_a.append(item['piece_id'])\n",
    "            piece_ids_b.append(item['piece_id'])\n",
    "\n",
    "        if not ids_a:\n",
    "            return None\n",
    "\n",
    "        collated['input_ids_a'] = torch.stack(ids_a)\n",
    "        collated['input_ids_b'] = torch.stack(ids_b)\n",
    "        collated['mask_a'] = torch.stack(masks_a)\n",
    "        collated['mask_b'] = torch.stack(masks_b)\n",
    "        collated['labels_a'] = torch.stack(labels_a_list)\n",
    "        collated['labels_b'] = torch.stack(labels_b_list)\n",
    "        collated['piece_ids_a'] = torch.tensor(piece_ids_a)\n",
    "        collated['piece_ids_b'] = torch.tensor(piece_ids_b)\n",
    "        return collated\n",
    "\n",
    "    finetune_loader = DataLoader(finetune_ds, batch_size=8, shuffle=True, collate_fn=symbolic_collate, num_workers=2)\n",
    "    finetune_val_loader = DataLoader(finetune_val_ds, batch_size=8, shuffle=False, collate_fn=symbolic_collate, num_workers=2)\n",
    "\n",
    "    trainer_ft = train_model(model, finetune_loader, finetune_val_loader, 'S1', fold_idx, max_epochs=FINETUNE_EPOCHS)\n",
    "    s1_trainers.append(trainer_ft)\n",
    "\n",
    "    best_val = trainer_ft.callback_metrics.get('val_loss', float('inf'))\n",
    "    best_acc = trainer_ft.callback_metrics.get('val_pairwise_acc', 0.0)\n",
    "    print(f'  Best val_loss={best_val:.4f}, val_pairwise_acc={best_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train S2: GNN on Score Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_CONFIG = {\n",
    "    'node_features': 6,\n",
    "    'hidden_dim': 512,\n",
    "    'num_layers': 4,\n",
    "    'num_labels': 19,\n",
    "    'max_epochs': 200,\n",
    "}\n",
    "\n",
    "PRETRAIN_EPOCHS_S2 = 50\n",
    "FINETUNE_EPOCHS_S2 = 150\n",
    "\n",
    "print('Training S2: GNN on Score Graph')\n",
    "print('=' * 50)\n",
    "print('Note: S2 requires graph-structured MIDI data (node features + edge indices).')\n",
    "print('Graph construction from MIDI is dataset-specific and must be prepared separately.')\n",
    "print('See model_improvement/data.py for graph construction utilities.')\n",
    "\n",
    "# Graph data loading would look like:\n",
    "# from torch_geometric.data import Data, Batch\n",
    "# from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "#\n",
    "# For each MIDI file, build a graph:\n",
    "#   x = [pitch, velocity, onset, duration, pedal, voice] per note (node features)\n",
    "#   edge_index = temporal adjacency + harmonic intervals + voice grouping\n",
    "#   batch_vec = graph batch assignment\n",
    "#\n",
    "# S2 pretrain: link prediction (pos_edges, neg_edges)\n",
    "# S2 finetune: pairwise ranking on graph embeddings\n",
    "\n",
    "# Placeholder: S2 training follows same fold loop pattern as S1\n",
    "# but uses PyG DataLoader and graph-structured batches.\n",
    "# Uncomment and adapt once graph data is prepared.\n",
    "\n",
    "s2_trainers = []\n",
    "print('\\nS2 training requires graph-structured data preparation.')\n",
    "print('Skipping until MIDI-to-graph pipeline is implemented.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train S3: Continuous Feature Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_CONFIG = {\n",
    "    'input_channels': 5,  # pitch, velocity, density, pedal, IOI\n",
    "    'hidden_dim': 512,\n",
    "    'num_labels': 19,\n",
    "    'max_epochs': 200,\n",
    "}\n",
    "\n",
    "PRETRAIN_EPOCHS_S3 = 50\n",
    "FINETUNE_EPOCHS_S3 = 150\n",
    "\n",
    "print('Training S3: Continuous Feature Encoder')\n",
    "print('=' * 50)\n",
    "\n",
    "s3_trainers = []\n",
    "for fold_idx, fold in enumerate(folds):\n",
    "    print(f'\\nFold {fold_idx}/{len(folds)-1}')\n",
    "\n",
    "    # --- Pretrain: Contrastive with Gumbel codebook ---\n",
    "    print('  Pretrain: Contrastive + codebook quantization')\n",
    "\n",
    "    train_keys = [k for k in fold['train'] if k in continuous_features]\n",
    "    val_keys = [k for k in fold['val'] if k in continuous_features]\n",
    "\n",
    "    # Build pretrain datasets with masked features\n",
    "    class ContinuousPretrainDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, keys, features_dict, max_len=2000, mask_prob=0.15):\n",
    "            self.keys = keys\n",
    "            self.features = features_dict\n",
    "            self.max_len = max_len\n",
    "            self.mask_prob = mask_prob\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.keys)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            key = self.keys[idx]\n",
    "            feat = self.features[key]  # [T, C]\n",
    "            T, C = feat.shape\n",
    "\n",
    "            # Truncate/pad to max_len\n",
    "            if T > self.max_len:\n",
    "                feat = feat[:self.max_len]\n",
    "                T = self.max_len\n",
    "\n",
    "            mask = torch.ones(self.max_len, dtype=torch.bool)\n",
    "            if T < self.max_len:\n",
    "                padding = torch.zeros(self.max_len - T, C)\n",
    "                feat = torch.cat([feat, padding], dim=0)\n",
    "                mask[T:] = False\n",
    "\n",
    "            # Create masked version\n",
    "            masked_feat = feat.clone()\n",
    "            masked_positions = torch.zeros(self.max_len, dtype=torch.bool)\n",
    "            rand = torch.rand(self.max_len)\n",
    "            mask_candidates = mask.clone()\n",
    "            masked_positions = mask_candidates & (rand < self.mask_prob)\n",
    "            masked_feat[masked_positions] = 0.0\n",
    "\n",
    "            return {\n",
    "                'features': feat,\n",
    "                'mask': mask,\n",
    "                'masked_features': masked_feat,\n",
    "                'masked_positions': masked_positions,\n",
    "            }\n",
    "\n",
    "    pretrain_ds = ContinuousPretrainDataset(train_keys, continuous_features)\n",
    "    pretrain_val_ds = ContinuousPretrainDataset(val_keys, continuous_features)\n",
    "\n",
    "    pretrain_loader = DataLoader(pretrain_ds, batch_size=8, shuffle=True, num_workers=2)\n",
    "    pretrain_val_loader = DataLoader(pretrain_val_ds, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = ContinuousSymbolicEncoder(**S3_CONFIG, stage='pretrain')\n",
    "    trainer_pt = train_model(\n",
    "        model, pretrain_loader, pretrain_val_loader,\n",
    "        'S3_pretrain', fold_idx,\n",
    "        max_epochs=PRETRAIN_EPOCHS_S3,\n",
    "        monitor='val_contrastive_loss',\n",
    "    )\n",
    "\n",
    "    # --- Finetune: Pairwise ranking ---\n",
    "    print('  Finetune: Pairwise ranking')\n",
    "    model.stage = 'finetune'\n",
    "\n",
    "    def continuous_collate(batch):\n",
    "        \"\"\"Custom collate for continuous feature pairwise data.\"\"\"\n",
    "        max_len = 2000\n",
    "        feats_a, feats_b = [], []\n",
    "        masks_a, masks_b = [], []\n",
    "        labels_a_list, labels_b_list = [], []\n",
    "        piece_ids_a, piece_ids_b = [], []\n",
    "\n",
    "        for item in batch:\n",
    "            key_a, key_b = item['key_a'], item['key_b']\n",
    "            if key_a not in continuous_features or key_b not in continuous_features:\n",
    "                continue\n",
    "\n",
    "            def pad_feat(feat):\n",
    "                T, C = feat.shape\n",
    "                if T > max_len:\n",
    "                    feat = feat[:max_len]\n",
    "                    T = max_len\n",
    "                m = torch.ones(max_len, dtype=torch.bool)\n",
    "                if T < max_len:\n",
    "                    padding = torch.zeros(max_len - T, C)\n",
    "                    feat = torch.cat([feat, padding], dim=0)\n",
    "                    m[T:] = False\n",
    "                return feat, m\n",
    "\n",
    "            fa, ma = pad_feat(continuous_features[key_a])\n",
    "            fb, mb = pad_feat(continuous_features[key_b])\n",
    "\n",
    "            feats_a.append(fa)\n",
    "            feats_b.append(fb)\n",
    "            masks_a.append(ma)\n",
    "            masks_b.append(mb)\n",
    "            labels_a_list.append(item['labels_a'])\n",
    "            labels_b_list.append(item['labels_b'])\n",
    "            piece_ids_a.append(item['piece_id'])\n",
    "            piece_ids_b.append(item['piece_id'])\n",
    "\n",
    "        if not feats_a:\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            'features_a': torch.stack(feats_a),\n",
    "            'features_b': torch.stack(feats_b),\n",
    "            'mask_a': torch.stack(masks_a),\n",
    "            'mask_b': torch.stack(masks_b),\n",
    "            'labels_a': torch.stack(labels_a_list),\n",
    "            'labels_b': torch.stack(labels_b_list),\n",
    "            'piece_ids_a': torch.tensor(piece_ids_a),\n",
    "            'piece_ids_b': torch.tensor(piece_ids_b),\n",
    "        }\n",
    "\n",
    "    finetune_ds = PairedPerformanceDataset(\n",
    "        cache_dir=cache_dir,\n",
    "        labels=labels,\n",
    "        piece_to_keys=piece_to_keys,\n",
    "        keys=[k for k in fold['train'] if k in continuous_features],\n",
    "    )\n",
    "    finetune_val_ds = PairedPerformanceDataset(\n",
    "        cache_dir=cache_dir,\n",
    "        labels=labels,\n",
    "        piece_to_keys=piece_to_keys,\n",
    "        keys=[k for k in fold['val'] if k in continuous_features],\n",
    "    )\n",
    "\n",
    "    finetune_loader = DataLoader(finetune_ds, batch_size=8, shuffle=True, collate_fn=continuous_collate, num_workers=2)\n",
    "    finetune_val_loader = DataLoader(finetune_val_ds, batch_size=8, shuffle=False, collate_fn=continuous_collate, num_workers=2)\n",
    "\n",
    "    trainer_ft = train_model(model, finetune_loader, finetune_val_loader, 'S3', fold_idx, max_epochs=FINETUNE_EPOCHS_S3)\n",
    "    s3_trainers.append(trainer_ft)\n",
    "\n",
    "    best_val = trainer_ft.callback_metrics.get('val_loss', float('inf'))\n",
    "    best_acc = trainer_ft.callback_metrics.get('val_pairwise_acc', 0.0)\n",
    "    print(f'  Best val_loss={best_val:.4f}, val_pairwise_acc={best_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Symbolic Encoder Training Summary')\n",
    "print('=' * 60)\n",
    "print(f'{\"Model\":<10} {\"Fold\":<6} {\"Val Loss\":<12} {\"Pairwise Acc\":<14}')\n",
    "print('-' * 60)\n",
    "\n",
    "for name, trainers in [('S1', s1_trainers), ('S3', s3_trainers)]:\n",
    "    for fold_idx, trainer in enumerate(trainers):\n",
    "        val_loss = trainer.callback_metrics.get('val_loss', float('nan'))\n",
    "        val_acc = trainer.callback_metrics.get('val_pairwise_acc', float('nan'))\n",
    "        print(f'{name:<10} {fold_idx:<6} {val_loss:<12.4f} {val_acc:<14.4f}')\n",
    "\n",
    "if not s2_trainers:\n",
    "    print('S2       (skipped - graph data not yet prepared)')\n",
    "\n",
    "print('\\nCheckpoints saved to:', CHECKPOINT_DIR)\n",
    "print('Checkpoints synced to Google Drive via rclone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Upload Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['S1', 'S1_pretrain', 'S2', 'S3', 'S3_pretrain']:\n",
    "    local = CHECKPOINT_DIR / model_name\n",
    "    if local.exists():\n",
    "        upload_checkpoint(local, model_name)\n",
    "        print(f'Uploaded {model_name} checkpoints')\n",
    "\n",
    "print('\\nAll symbolic encoder training complete.')\n",
    "print('Run 04_symbolic_comparison.ipynb to evaluate and compare results.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}