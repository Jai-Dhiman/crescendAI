{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08: Symbolic Encoder Comparison (S1 vs S2 vs S3)\n",
    "\n",
    "Compare three symbolic encoder architectures:\n",
    "- **S1**: Transformer on REMI-tokenized MIDI (TransformerSymbolicEncoder)\n",
    "- **S2**: GNN on score graph with GATConv (GNNSymbolicEncoder)\n",
    "- **S3**: 1D-CNN + Transformer on continuous MIDI features (ContinuousSymbolicEncoder)\n",
    "\n",
    "Selection criteria (same as audio track):\n",
    "1. Primary: Pairwise ranking accuracy\n",
    "2. Tiebreak: R-squared on regression\n",
    "3. Veto: Robustness drop > 15%\n",
    "\n",
    "Additional symbolic-specific assessment:\n",
    "- Score alignment on ASAP (performance-score MIDI cosine similarity)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Thunder Compute Setup --\n",
    "# 1. Clone repo\n",
    "# !git clone <repo-url> /workspace/crescendai\n",
    "# %cd /workspace/crescendai/model\n",
    "\n",
    "# 2. Install dependencies\n",
    "# !uv sync\n",
    "\n",
    "# 3. Pull cached data from Google Drive via rclone\n",
    "# !rclone sync gdrive:crescendai/model/data ./data --progress\n",
    "\n",
    "# 4. Configure paths\n",
    "from pathlib import Path\n",
    "\n",
    "IS_REMOTE = os.environ.get('THUNDER_COMPUTE', False)\n",
    "if IS_REMOTE:\n",
    "    DATA_DIR = Path('/workspace/crescendai/model/data')\n",
    "    CHECKPOINT_DIR = Path('/workspace/crescendai/model/checkpoints/model_improvement')\n",
    "else:\n",
    "    DATA_DIR = Path('../data')\n",
    "    CHECKPOINT_DIR = Path('../checkpoints/model_improvement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from model_improvement.symbolic_encoders import (\n",
    "    TransformerSymbolicEncoder,\n",
    "    GNNSymbolicEncoder,\n",
    "    ContinuousSymbolicEncoder,\n",
    ")\n",
    "from model_improvement.tokenizer import PianoTokenizer, extract_continuous_features\n",
    "from model_improvement.metrics import MetricsSuite, compute_robustness_metrics, format_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Validation Data\n",
    "\n",
    "Load PercePiano MIDI paths/labels and ASAP for score alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = DATA_DIR / 'percepiano_cache'\n",
    "\n",
    "with open(cache_dir / 'labels.json') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "with open(cache_dir / 'folds.json') as f:\n",
    "    folds = json.load(f)\n",
    "\n",
    "print(f'Loaded {len(labels)} labeled segments')\n",
    "print(f'Number of folds: {len(folds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MIDI paths for symbolic encoders\n",
    "midi_dir = DATA_DIR / 'percepiano_midi'\n",
    "if midi_dir.exists():\n",
    "    midi_paths = {p.stem: p for p in midi_dir.glob('*.mid')}\n",
    "    print(f'Found {len(midi_paths)} MIDI files')\n",
    "else:\n",
    "    midi_paths = {}\n",
    "    print(f'MIDI directory not found: {midi_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ASAP data for score alignment assessment\n",
    "asap_dir = DATA_DIR / 'asap_cache'\n",
    "if asap_dir.exists():\n",
    "    print(f'ASAP data found at {asap_dir}')\n",
    "    asap_pairs = []\n",
    "    for perf_midi in sorted(asap_dir.glob('**/performance*.mid')):\n",
    "        score_midi = perf_midi.parent / perf_midi.name.replace('performance', 'score')\n",
    "        if score_midi.exists():\n",
    "            asap_pairs.append((perf_midi, score_midi))\n",
    "    print(f'Found {len(asap_pairs)} performance-score MIDI pairs')\n",
    "else:\n",
    "    asap_pairs = []\n",
    "    print(f'ASAP data not found at {asap_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Inputs for Each Encoder Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S1: Tokenize MIDIs\n",
    "tokenizer = PianoTokenizer(max_seq_len=2048)\n",
    "print(f'Tokenizer vocab size: {tokenizer.vocab_size}')\n",
    "\n",
    "s1_tokens = {}\n",
    "for key, midi_path in midi_paths.items():\n",
    "    try:\n",
    "        tokens = tokenizer.encode(midi_path)\n",
    "        s1_tokens[key] = tokens\n",
    "    except Exception as e:\n",
    "        print(f'  Failed to tokenize {key}: {e}')\n",
    "\n",
    "print(f'Tokenized {len(s1_tokens)} MIDI files for S1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3: Extract continuous features\n",
    "s3_features = {}\n",
    "for key, midi_path in midi_paths.items():\n",
    "    try:\n",
    "        features = extract_continuous_features(midi_path, frame_rate=50)\n",
    "        s3_features[key] = torch.tensor(features, dtype=torch.float32)\n",
    "    except Exception as e:\n",
    "        print(f'  Failed to extract features for {key}: {e}')\n",
    "\n",
    "print(f'Extracted continuous features for {len(s3_features)} files (S3)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_checkpoint(model_class, model_dir, **model_kwargs):\n",
    "    ckpt_dir = CHECKPOINT_DIR / model_dir\n",
    "    ckpts = sorted(ckpt_dir.glob('*.ckpt'))\n",
    "    if not ckpts:\n",
    "        raise FileNotFoundError(f'No checkpoints found in {ckpt_dir}')\n",
    "    best_ckpt = ckpts[-1]\n",
    "    print(f'Loading {model_class.__name__} from {best_ckpt.name}')\n",
    "    model = model_class.load_from_checkpoint(str(best_ckpt), **model_kwargs)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "models = {}\n",
    "for name, cls, kwargs in [\n",
    "    ('S1', TransformerSymbolicEncoder, {'stage': 'finetune'}),\n",
    "    ('S2', GNNSymbolicEncoder, {'stage': 'finetune'}),\n",
    "    ('S3', ContinuousSymbolicEncoder, {'stage': 'finetune'}),\n",
    "]:\n",
    "    try:\n",
    "        models[name] = load_best_checkpoint(cls, name, **kwargs)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'{name}: {e}')\n",
    "\n",
    "print(f'\\nLoaded models: {list(models.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run MetricsSuite on Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = MetricsSuite(ambiguous_threshold=0.05)\n",
    "\n",
    "def evaluate_s1(model, val_keys, labels_dict, tokens_dict):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    valid_keys = [k for k in val_keys if k in tokens_dict and k in labels_dict]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_logits, all_la, all_lb = [], [], []\n",
    "        for i, key_a in enumerate(valid_keys):\n",
    "            for key_b in valid_keys[i+1:]:\n",
    "                ids_a = torch.tensor(tokens_dict[key_a]).unsqueeze(0)\n",
    "                ids_b = torch.tensor(tokens_dict[key_b]).unsqueeze(0)\n",
    "                mask_a = torch.ones(1, ids_a.size(1), dtype=torch.bool)\n",
    "                mask_b = torch.ones(1, ids_b.size(1), dtype=torch.bool)\n",
    "                z_a = model.encode(ids_a, mask_a)\n",
    "                z_b = model.encode(ids_b, mask_b)\n",
    "                logits = model.compare(z_a, z_b)\n",
    "                lab_a = torch.tensor(labels_dict[key_a][:19], dtype=torch.float32)\n",
    "                lab_b = torch.tensor(labels_dict[key_b][:19], dtype=torch.float32)\n",
    "                all_logits.append(logits)\n",
    "                all_la.append(lab_a.unsqueeze(0))\n",
    "                all_lb.append(lab_b.unsqueeze(0))\n",
    "        \n",
    "        if all_logits:\n",
    "            pw = suite.pairwise_accuracy(torch.cat(all_logits), torch.cat(all_la), torch.cat(all_lb))\n",
    "            results['pairwise'] = pw['overall']\n",
    "            results['pairwise_detail'] = pw\n",
    "        \n",
    "        all_preds, all_targets = [], []\n",
    "        for key in valid_keys:\n",
    "            ids = torch.tensor(tokens_dict[key]).unsqueeze(0)\n",
    "            mask = torch.ones(1, ids.size(1), dtype=torch.bool)\n",
    "            out = model(ids, mask)\n",
    "            all_preds.append(out['scores'])\n",
    "            all_targets.append(torch.tensor(labels_dict[key][:19], dtype=torch.float32).unsqueeze(0))\n",
    "        if all_preds:\n",
    "            results['r2'] = suite.regression_r2(torch.cat(all_preds), torch.cat(all_targets))\n",
    "    return results\n",
    "\n",
    "def evaluate_s3(model, val_keys, labels_dict, features_dict):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    valid_keys = [k for k in val_keys if k in features_dict and k in labels_dict]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_logits, all_la, all_lb = [], [], []\n",
    "        for i, key_a in enumerate(valid_keys):\n",
    "            for key_b in valid_keys[i+1:]:\n",
    "                fa = features_dict[key_a].unsqueeze(0)\n",
    "                fb = features_dict[key_b].unsqueeze(0)\n",
    "                ma = torch.ones(1, fa.size(1), dtype=torch.bool)\n",
    "                mb = torch.ones(1, fb.size(1), dtype=torch.bool)\n",
    "                z_a = model.encode(fa, ma)\n",
    "                z_b = model.encode(fb, mb)\n",
    "                logits = model.compare(z_a, z_b)\n",
    "                lab_a = torch.tensor(labels_dict[key_a][:19], dtype=torch.float32)\n",
    "                lab_b = torch.tensor(labels_dict[key_b][:19], dtype=torch.float32)\n",
    "                all_logits.append(logits)\n",
    "                all_la.append(lab_a.unsqueeze(0))\n",
    "                all_lb.append(lab_b.unsqueeze(0))\n",
    "        \n",
    "        if all_logits:\n",
    "            pw = suite.pairwise_accuracy(torch.cat(all_logits), torch.cat(all_la), torch.cat(all_lb))\n",
    "            results['pairwise'] = pw['overall']\n",
    "            results['pairwise_detail'] = pw\n",
    "        \n",
    "        all_preds, all_targets = [], []\n",
    "        for key in valid_keys:\n",
    "            feat = features_dict[key].unsqueeze(0)\n",
    "            mask = torch.ones(1, feat.size(1), dtype=torch.bool)\n",
    "            out = model(feat, mask)\n",
    "            all_preds.append(out['scores'])\n",
    "            all_targets.append(torch.tensor(labels_dict[key][:19], dtype=torch.float32).unsqueeze(0))\n",
    "        if all_preds:\n",
    "            results['r2'] = suite.regression_r2(torch.cat(all_preds), torch.cat(all_targets))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\nEvaluating {name}...')\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for fold_idx, fold in enumerate(folds):\n",
    "        val_keys = fold['val']\n",
    "        if name == 'S1':\n",
    "            fold_res = evaluate_s1(model, val_keys, labels, s1_tokens)\n",
    "        elif name == 'S2':\n",
    "            fold_res = {'pairwise': 0.0, 'r2': 0.0}\n",
    "            print(f'  Fold {fold_idx}: S2 requires graph construction pipeline (skipped)')\n",
    "        elif name == 'S3':\n",
    "            fold_res = evaluate_s3(model, val_keys, labels, s3_features)\n",
    "        else:\n",
    "            fold_res = {}\n",
    "        fold_metrics.append(fold_res)\n",
    "        pw = fold_res.get('pairwise', 'N/A')\n",
    "        r2 = fold_res.get('r2', 'N/A')\n",
    "        if isinstance(pw, float) and pw > 0:\n",
    "            print(f'  Fold {fold_idx}: pairwise={pw:.4f}, r2={r2:.4f}')\n",
    "    \n",
    "    avg = {}\n",
    "    for mk in ['pairwise', 'r2']:\n",
    "        vals = [fm[mk] for fm in fold_metrics if mk in fm and isinstance(fm[mk], float)]\n",
    "        if vals:\n",
    "            avg[mk] = sum(vals) / len(vals)\n",
    "    if 'pairwise_detail' in fold_metrics[-1]:\n",
    "        avg['pairwise_detail'] = fold_metrics[-1]['pairwise_detail']\n",
    "    all_results[name] = avg\n",
    "    print(f'  Average: pairwise={avg.get(\"pairwise\", \"N/A\")}, r2={avg.get(\"r2\", \"N/A\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Score Alignment Assessment (ASAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_alignment(model, model_type, asap_pairs, tok=None, max_pairs=50):\n",
    "    \"\"\"Measure embedding similarity between performance and score MIDI.\"\"\"\n",
    "    similarities = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for perf_path, score_path in asap_pairs[:max_pairs]:\n",
    "            try:\n",
    "                if model_type == 'S1' and tok is not None:\n",
    "                    perf_ids = torch.tensor(tok.encode(perf_path)).unsqueeze(0)\n",
    "                    score_ids = torch.tensor(tok.encode(score_path)).unsqueeze(0)\n",
    "                    z_perf = model.encode(perf_ids, torch.ones(1, perf_ids.size(1), dtype=torch.bool))\n",
    "                    z_score = model.encode(score_ids, torch.ones(1, score_ids.size(1), dtype=torch.bool))\n",
    "                elif model_type == 'S3':\n",
    "                    pf = torch.tensor(extract_continuous_features(perf_path), dtype=torch.float32).unsqueeze(0)\n",
    "                    sf = torch.tensor(extract_continuous_features(score_path), dtype=torch.float32).unsqueeze(0)\n",
    "                    z_perf = model.encode(pf, torch.ones(1, pf.size(1), dtype=torch.bool))\n",
    "                    z_score = model.encode(sf, torch.ones(1, sf.size(1), dtype=torch.bool))\n",
    "                else:\n",
    "                    continue\n",
    "                cos_sim = torch.nn.functional.cosine_similarity(z_perf, z_score).item()\n",
    "                similarities.append(cos_sim)\n",
    "            except Exception:\n",
    "                continue\n",
    "    if similarities:\n",
    "        return {'mean_cosine_sim': float(np.mean(similarities)), 'std': float(np.std(similarities)), 'n': len(similarities)}\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_results = {}\n",
    "if asap_pairs:\n",
    "    for name, model in models.items():\n",
    "        if name == 'S2':\n",
    "            print(f'{name}: Alignment requires graph builder (skipped)')\n",
    "            continue\n",
    "        print(f'Score alignment for {name}...')\n",
    "        result = evaluate_alignment(model, name, asap_pairs, tok=tokenizer if name == 'S1' else None)\n",
    "        if result:\n",
    "            alignment_results[name] = result\n",
    "            print(f'  cosine_sim={result[\"mean_cosine_sim\"]:.4f} +/- {result[\"std\"]:.4f} (n={result[\"n\"]})')\n",
    "else:\n",
    "    print('No ASAP data available for alignment assessment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison Table + Per-Dimension Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = {}\n",
    "for name in models:\n",
    "    comparison[name] = {}\n",
    "    if name in all_results:\n",
    "        comparison[name]['pairwise'] = all_results[name].get('pairwise', 0.0)\n",
    "        comparison[name]['r2'] = all_results[name].get('r2', 0.0)\n",
    "    if name in alignment_results:\n",
    "        comparison[name]['alignment'] = alignment_results[name]['mean_cosine_sim']\n",
    "\n",
    "if comparison:\n",
    "    print(format_comparison_table(comparison))\n",
    "else:\n",
    "    print('No results to compare yet.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION_NAMES = [\n",
    "    'Correctness of pitch', 'Correctness of rhythm', 'Correctness of tempo',\n",
    "    'Tone quality', 'Dynamic range', 'Articulation', 'Balance',\n",
    "    'Rhythm. stability', 'Tempo stability', 'Phrasing', 'Stylistic accuracy',\n",
    "    'Ornamentation', 'Pedaling', 'Expressiveness', 'Technical proficiency',\n",
    "    'Memorization', 'Stage presence', 'Communication', 'Overall'\n",
    "]\n",
    "\n",
    "def plot_per_dimension_comparison(results_dict):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "    n_dims = 19\n",
    "    x = np.arange(n_dims)\n",
    "    width = 0.25\n",
    "    for i, (name, res) in enumerate(results_dict.items()):\n",
    "        if 'pairwise_detail' not in res:\n",
    "            continue\n",
    "        per_dim = res['pairwise_detail']['per_dimension']\n",
    "        values = [per_dim.get(d, 0.5) for d in range(n_dims)]\n",
    "        ax.bar(x + i * width, values, width, label=name, alpha=0.8)\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(DIMENSION_NAMES, rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_ylabel('Pairwise Accuracy')\n",
    "    ax.set_title('Per-Dimension Pairwise Ranking Accuracy (Symbolic Encoders)')\n",
    "    ax.legend()\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_ylim(0.3, 1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if all_results:\n",
    "    plot_per_dimension_comparison(all_results)\n",
    "else:\n",
    "    print('No results to plot yet.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Winner Selection\n",
    "\n",
    "1. **Primary**: Highest pairwise ranking accuracy\n",
    "2. **Tiebreak**: Highest R-squared\n",
    "3. **Veto**: Robustness score_drop_pct > 15%\n",
    "4. **Bonus**: Score alignment (informational)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBUSTNESS_VETO_THRESHOLD = 15.0\n",
    "\n",
    "def select_winner(comp):\n",
    "    candidates = []\n",
    "    for name, metrics in comp.items():\n",
    "        drop_pct = metrics.get('score_drop_pct', 0.0)\n",
    "        if drop_pct > ROBUSTNESS_VETO_THRESHOLD:\n",
    "            print(f'{name}: VETOED (score_drop_pct={drop_pct:.1f}%)')\n",
    "            continue\n",
    "        candidates.append((name, metrics.get('pairwise', 0.0), metrics.get('r2', 0.0), metrics.get('alignment', 0.0)))\n",
    "    if not candidates:\n",
    "        print('All models vetoed!')\n",
    "        return None\n",
    "    candidates.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "    winner = candidates[0][0]\n",
    "    print(f'\\nWinner: {winner}')\n",
    "    for name, pw, r2, align in candidates:\n",
    "        marker = ' <-- WINNER' if name == winner else ''\n",
    "        print(f'  {name}: pairwise={pw:.4f}, r2={r2:.4f}, alignment={align:.4f}{marker}')\n",
    "    return winner\n",
    "\n",
    "if comparison:\n",
    "    winner = select_winner(comparison)\n",
    "else:\n",
    "    print('Run training and evaluation first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_checkpoint(local_path, remote_subdir):\n",
    "    remote = f'gdrive:crescendai/model/checkpoints/model_improvement/{remote_subdir}'\n",
    "    subprocess.run(['rclone', 'copy', str(local_path), remote, '--progress'], check=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
