{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 00: Multi-Dataset Data Preparation\n",
    "\n",
    "One-time setup: download external MIDI datasets, validate, preprocess, and cache for pretraining.\n",
    "\n",
    "**Datasets:**\n",
    "- MAESTRO v3 (~1,276 MIDI files, ~81MB)\n",
    "- ATEPP v1.2 (~11,674 MIDI files, ~212MB)\n",
    "- ASAP (~1,067 performances, if already cached)\n",
    "- PercePiano (~1,202 MIDI files, already in data/)\n",
    "\n",
    "**Output:** `data/pretrain_cache/` containing tokenized sequences, score graphs, and continuous features.\n",
    "\n",
    "Run this notebook once locally (M4 Mac) or on cloud, then sync to GDrive.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IS_REMOTE = os.environ.get('THUNDER_COMPUTE', False)\n",
    "if IS_REMOTE:\n",
    "    subprocess.run(['bash', '-c', 'curl -fsSL https://rclone.org/install.sh | sudo bash'], check=True)\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/Jai-Dhiman/crescendAI.git', '/workspace/crescendai'], check=True)\n",
    "    os.chdir('/workspace/crescendai/model')\n",
    "    subprocess.run(['bash', '-c', 'curl -LsSf https://astral.sh/uv/install.sh | sh'], check=True)\n",
    "    subprocess.run(['uv', 'sync'], check=True)\n",
    "    subprocess.run(['rclone', 'sync', 'gdrive:crescendai_data/model_improvement/data', './data', '--progress'], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\n\nif IS_REMOTE:\n    DATA_DIR = Path('/workspace/crescendai/model/data')\nelse:\n    DATA_DIR = Path('../data')\n\nsys.path.insert(0, 'src' if IS_REMOTE else '../../model/src')\n\nfrom model_improvement.datasets import (\n    load_all_midi_files,\n    load_maestro_midi_files,\n    load_atepp_midi_files,\n    load_asap_midi_files,\n    load_percepiano_midi_files,\n)\nfrom model_improvement.preprocessing import (\n    preprocess_all,\n    preprocess_tokens,\n    preprocess_graphs,\n    preprocess_continuous_features,\n    merge_graph_shards,\n    merge_feature_shards,\n)\n\nprint(f'DATA_DIR: {DATA_DIR.resolve()}')\nprint(f'Exists: {DATA_DIR.exists()}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Download External Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "maestro_dir = DATA_DIR / 'maestro_cache'\n",
    "\n",
    "if maestro_dir.exists() and any(maestro_dir.glob('**/*.midi')):\n",
    "    print(f'MAESTRO already downloaded at {maestro_dir}')\n",
    "    print(f'  MIDI files: {len(list(maestro_dir.glob(\"**/*.midi\")))}')\n",
    "else:\n",
    "    print('Downloading MAESTRO v3 MIDI-only (~81MB)...')\n",
    "    maestro_zip = DATA_DIR / 'maestro-v3.0.0-midi.zip'\n",
    "    subprocess.run([\n",
    "        'curl', '-L', '-o', str(maestro_zip),\n",
    "        'https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip'\n",
    "    ], check=True)\n",
    "\n",
    "    print('Extracting...')\n",
    "    with zipfile.ZipFile(maestro_zip, 'r') as zf:\n",
    "        zf.extractall(DATA_DIR)\n",
    "\n",
    "    # MAESTRO extracts to maestro-v3.0.0/; rename to maestro_cache\n",
    "    extracted_dir = DATA_DIR / 'maestro-v3.0.0'\n",
    "    if extracted_dir.exists():\n",
    "        if maestro_dir.exists():\n",
    "            import shutil\n",
    "            shutil.rmtree(maestro_dir)\n",
    "        extracted_dir.rename(maestro_dir)\n",
    "\n",
    "    maestro_zip.unlink()\n",
    "    print(f'MAESTRO extracted to {maestro_dir}')\n",
    "    print(f'  MIDI files: {len(list(maestro_dir.glob(\"**/*.midi\")))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "atepp_dir = DATA_DIR / 'atepp_cache'\n",
    "\n",
    "if atepp_dir.exists() and any(atepp_dir.glob('**/*.mid')):\n",
    "    print(f'ATEPP already downloaded at {atepp_dir}')\n",
    "    print(f'  MIDI files: {len(list(atepp_dir.glob(\"**/*.mid\")))}')\n",
    "else:\n",
    "    print('Downloading ATEPP v1.2 (~212MB)...')\n",
    "    atepp_zip = DATA_DIR / 'ATEPP-1.2.zip'\n",
    "    subprocess.run([\n",
    "        'curl', '-L', '-o', str(atepp_zip),\n",
    "        'https://zenodo.org/records/14997880/files/ATEPP-1.2.zip'\n",
    "    ], check=True)\n",
    "\n",
    "    print('Extracting...')\n",
    "    with zipfile.ZipFile(atepp_zip, 'r') as zf:\n",
    "        zf.extractall(DATA_DIR)\n",
    "\n",
    "    # ATEPP extracts to ATEPP-1.2/ or similar; rename to atepp_cache\n",
    "    for candidate in ['ATEPP-1.2', 'ATEPP']:\n",
    "        extracted_dir = DATA_DIR / candidate\n",
    "        if extracted_dir.exists():\n",
    "            if atepp_dir.exists():\n",
    "                import shutil\n",
    "                shutil.rmtree(atepp_dir)\n",
    "            extracted_dir.rename(atepp_dir)\n",
    "            break\n",
    "    else:\n",
    "        # If extraction created the target directly\n",
    "        if not atepp_dir.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f'ATEPP extraction did not produce expected directory. '\n",
    "                f'Contents of {DATA_DIR}: {list(DATA_DIR.iterdir())}'\n",
    "            )\n",
    "\n",
    "    atepp_zip.unlink()\n",
    "    print(f'ATEPP extracted to {atepp_dir}')\n",
    "    print(f'  MIDI files: {len(list(atepp_dir.glob(\"**/*.mid\")))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Validate Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = load_all_midi_files(DATA_DIR)\n",
    "\n",
    "# Count by source\n",
    "from collections import Counter\n",
    "source_counts = Counter(e.source for e in entries)\n",
    "\n",
    "print(f'\\nValidation:')\n",
    "for source, count in sorted(source_counts.items()):\n",
    "    print(f'  {source}: {count}')\n",
    "print(f'  Total: {len(entries)}')\n",
    "\n",
    "# Sanity checks\n",
    "if 'maestro' in source_counts:\n",
    "    assert source_counts['maestro'] > 1000, f'Expected >1000 MAESTRO files, got {source_counts[\"maestro\"]}'\n",
    "if 'atepp' in source_counts:\n",
    "    assert source_counts['atepp'] > 5000, f'Expected >5000 ATEPP files, got {source_counts[\"atepp\"]}'\n",
    "\n",
    "print('\\nAll source validations passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## 4. Preprocess\n\nEach stage skips automatically if the final output file already exists.\nGraph and feature pipelines use shard-based processing to keep memory bounded (~200 entries per shard).\nLegacy `.partial` checkpoints are migrated to shard format automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "pretrain_cache = DATA_DIR / 'pretrain_cache'\npretrain_cache.mkdir(parents=True, exist_ok=True)\n\n# 4a. Tokenization (skips if all_tokens.pt exists)\ntokens = preprocess_tokens(entries, pretrain_cache / 'tokens' / 'all_tokens.pt')\n\n# 4b. Graph building -- shard-based, resumes from partial shards\ngraphs, hetero = preprocess_graphs(\n    entries,\n    pretrain_cache / 'graphs' / 'all_graphs.pt',\n    pretrain_cache / 'graphs' / 'all_hetero_graphs.pt',\n)\n\n# 4c. Continuous features -- shard-based\nfeatures = preprocess_continuous_features(\n    entries, pretrain_cache / 'features' / 'all_features.pt'\n)\n\nprint(f'\\nTokens: {len(tokens)}, Graphs: {len(graphs)}, Hetero: {len(hetero)}, Features: {len(features)}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Upload to GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Uploading pretrain_cache to GDrive...')\n",
    "subprocess.run([\n",
    "    'rclone', 'sync',\n",
    "    str(pretrain_cache),\n",
    "    'gdrive:crescendai_data/model_improvement/data/pretrain_cache',\n",
    "    '--progress',\n",
    "], check=True)\n",
    "\n",
    "# Also sync the raw MIDI dirs so other machines can skip downloads\n",
    "for subdir in ['maestro_cache', 'atepp_cache']:\n",
    "    src = DATA_DIR / subdir\n",
    "    if src.exists():\n",
    "        print(f'Uploading {subdir}...')\n",
    "        subprocess.run([\n",
    "            'rclone', 'sync',\n",
    "            str(src),\n",
    "            f'gdrive:crescendai_data/model_improvement/data/{subdir}',\n",
    "            '--progress',\n",
    "        ], check=True)\n",
    "\n",
    "print('Upload complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Spot-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "tokens = torch.load(pretrain_cache / 'tokens' / 'all_tokens.pt', map_location='cpu', weights_only=False)\n",
    "graphs = torch.load(pretrain_cache / 'graphs' / 'all_graphs.pt', map_location='cpu', weights_only=False)\n",
    "features = torch.load(pretrain_cache / 'features' / 'all_features.pt', map_location='cpu', weights_only=False)\n",
    "\n",
    "print(f'Tokens: {len(tokens)} entries')\n",
    "print(f'Graphs: {len(graphs)} entries')\n",
    "print(f'Features: {len(features)} entries')\n",
    "\n",
    "# Sample 5 per source\n",
    "for source in ['asap', 'maestro', 'atepp', 'percepiano']:\n",
    "    source_keys = [k for k in tokens if k.startswith(f'{source}__')]\n",
    "    if not source_keys:\n",
    "        print(f'\\n{source}: no entries')\n",
    "        continue\n",
    "\n",
    "    sample_keys = random.sample(source_keys, min(5, len(source_keys)))\n",
    "    print(f'\\n{source} samples ({len(source_keys)} total):')\n",
    "    for key in sample_keys:\n",
    "        tok_len = len(tokens[key]) if key in tokens else 'MISSING'\n",
    "        node_count = graphs[key].x.shape[0] if key in graphs else 'MISSING'\n",
    "        feat_shape = tuple(features[key].shape) if key in features else 'MISSING'\n",
    "        print(f'  {key}: tokens={tok_len}, nodes={node_count}, features={feat_shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}