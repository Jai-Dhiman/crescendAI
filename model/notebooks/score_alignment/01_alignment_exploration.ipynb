{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Alignment Exploration\n",
    "\n",
    "This notebook explores aligning student piano performances to rendered MIDI scores\n",
    "in MuQ embedding space using DTW-based algorithms.\n",
    "\n",
    "**Experiments:**\n",
    "- A: DTW Baseline on raw MuQ embeddings\n",
    "- B: Learned projection MLP with soft-DTW loss\n",
    "- C: Measure-level alignment (coarser granularity)\n",
    "\n",
    "**Success metric:** Mean onset error < 30ms (human perception threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DTW baseline on 214 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DTW baseline:   0%|          | 0/214 [00:00<?, ?it/s]/Users/jdhiman/Documents/crescendai/model/.venv/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:122: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "DTW baseline:   0%|          | 1/214 [00:17<1:02:53, 17.71s/it]/Users/jdhiman/Documents/crescendai/model/.venv/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:122: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "DTW baseline:   9%|â–‰         | 19/214 [32:51<4:31:30, 83.54s/it] "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"src\")\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from score_alignment.data.asap import parse_asap_metadata, get_performance_key\n",
    "from score_alignment.training.runner import run_dtw_baseline\n",
    "\n",
    "DATA_ROOT = Path(\"../../data\").resolve()\n",
    "ASAP_ROOT = DATA_ROOT / \"asap-dataset\"\n",
    "SCORE_CACHE_DIR = DATA_ROOT / \"muq_cache\" / \"scores\"\n",
    "PERF_CACHE_DIR = DATA_ROOT / \"muq_cache\" / \"performances\"\n",
    "RESULTS_DIR = DATA_ROOT / \"results\" / \"score_alignment\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parse ASAP and get validation split\n",
    "asap_index = parse_asap_metadata(ASAP_ROOT)\n",
    "aligned_perfs = asap_index.filter_with_alignments()\n",
    "all_keys = [get_performance_key(p) for p in aligned_perfs]\n",
    "_, val_keys = train_test_split(all_keys, test_size=0.2, random_state=42)\n",
    "\n",
    "# Run DTW baseline (CPU-only, no GPU needed)\n",
    "baseline_metrics = run_dtw_baseline(\n",
    "    performances=aligned_perfs,\n",
    "    score_cache_dir=SCORE_CACHE_DIR,\n",
    "    perf_cache_dir=PERF_CACHE_DIR,\n",
    "    asap_root=ASAP_ROOT,\n",
    "    keys=val_keys,\n",
    "    distance_metric=\"cosine\",\n",
    "    results_dir=RESULTS_DIR,\n",
    "    results_key=\"A_dtw_baseline\",\n",
    ")\n",
    "\n",
    "print(f\"Mean onset error: {baseline_metrics['weighted_mean_error_ms']:.1f} ms\")\n",
    "print(f\"Within 30ms: {baseline_metrics['weighted_percent_within_threshold']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "REPO_ROOT = \"/workspace/crescendai\"\n",
    "MODEL_ROOT = f\"{REPO_ROOT}/model\"\n",
    "\n",
    "# Clone repo if not present, otherwise pull latest changes\n",
    "if not os.path.exists(MODEL_ROOT):\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"https://github.com/Jai-Dhiman/crescendAI.git\", REPO_ROOT],\n",
    "        check=True,\n",
    "    )\n",
    "else:\n",
    "    subprocess.run([\"git\", \"pull\"], cwd=REPO_ROOT, check=True)\n",
    "\n",
    "# Install uv if not available\n",
    "if subprocess.run([\"which\", \"uv\"], capture_output=True).returncode != 0:\n",
    "    subprocess.run([\"sh\", \"-c\", \"curl -LsSf https://astral.sh/uv/install.sh | sh\"], check=True)\n",
    "    os.environ[\"PATH\"] = os.path.expanduser(\"~/.local/bin\") + \":\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Sync dependencies and install project\n",
    "subprocess.run([\"uv\", \"sync\"], cwd=MODEL_ROOT, check=True)\n",
    "\n",
    "# Install rclone\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "MODEL_ROOT = Path(\"/workspace/crescendai/model\")\n",
    "SRC_DIR = MODEL_ROOT / \"src\"\n",
    "VENV_SITE = MODEL_ROOT / \".venv\" / \"lib\"\n",
    "\n",
    "# Add uv venv site-packages so imports resolve\n",
    "for sp in VENV_SITE.glob(\"python*/site-packages\"):\n",
    "    if str(sp) not in sys.path:\n",
    "        sys.path.insert(0, str(sp))\n",
    "\n",
    "# Fallback: add src directly\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"Model root: {MODEL_ROOT}\")\n",
    "print(f\"Source dir: {SRC_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\n# Use TF32 on A100 for ~2x speedup on matmuls\ntorch.set_float32_matmul_precision(\"high\")\n\n# Score alignment imports\nfrom score_alignment.config import (\n    MUQ_FRAME_RATE,\n    ProjectionConfig,\n    TrainingConfig,\n    ASAP_REPO_URL,\n)\nfrom score_alignment.data.asap import (\n    parse_asap_metadata,\n    load_note_alignments,\n    extract_onset_pairs,\n    get_performance_key,\n)\nfrom score_alignment.data.alignment_dataset import (\n    FrameAlignmentDataset,\n    MeasureAlignmentDataset,\n    frame_alignment_collate_fn,\n)\nfrom score_alignment.alignment.dtw import align_embeddings, compute_cost_matrix\nfrom score_alignment.alignment.metrics import (\n    onset_error,\n    evaluate_dtw_alignment,\n    compute_alignment_summary,\n)\nfrom score_alignment.models.projection import AlignmentProjectionModel\nfrom score_alignment.training.runner import (\n    run_alignment_experiment,\n    run_dtw_baseline,\n)\n\nDEVICE = \"cuda\"\n\nprint(f\"MuQ frame rate: {MUQ_FRAME_RATE} fps\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Device: {DEVICE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data lives under model/data/\n",
    "DATA_ROOT = MODEL_ROOT / \"data\"\n",
    "ASAP_ROOT = DATA_ROOT / \"asap-dataset\"\n",
    "SCORE_CACHE_DIR = DATA_ROOT / \"muq_cache\" / \"scores\"\n",
    "PERF_CACHE_DIR = DATA_ROOT / \"muq_cache\" / \"performances\"\n",
    "CHECKPOINT_DIR = DATA_ROOT / \"checkpoints\" / \"score_alignment\"\n",
    "RESULTS_DIR = DATA_ROOT / \"results\" / \"score_alignment\"\n",
    "LOG_DIR = DATA_ROOT / \"logs\" / \"score_alignment\"\n",
    "\n",
    "# Create directories\n",
    "for d in [DATA_ROOT, SCORE_CACHE_DIR, PERF_CACHE_DIR, CHECKPOINT_DIR, RESULTS_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"ASAP root: {ASAP_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remote storage configuration (Google Drive via rclone)\n",
    "RCLONE_REMOTE = \"gdrive\"\n",
    "RCLONE_BASE = \"crescendai_data\"\n",
    "\n",
    "REMOTE_PATHS = {\n",
    "    \"muq_scores\": f\"{RCLONE_BASE}/score_alignment/muq_cache/scores\",\n",
    "    \"muq_performances\": f\"{RCLONE_BASE}/score_alignment/muq_cache/performances\",\n",
    "    \"checkpoints\": f\"{RCLONE_BASE}/checkpoints/score_alignment\",\n",
    "    \"results\": f\"{RCLONE_BASE}/results/score_alignment\",\n",
    "    \"logs\": f\"{RCLONE_BASE}/logs/score_alignment\",\n",
    "}\n",
    "\n",
    "# Common rclone flags for Google Drive downloads\n",
    "_RCLONE_DRIVE_FLAGS = [\"--drive-acknowledge-abuse\", \"--progress\"]\n",
    "\n",
    "def rclone_sync(remote_path: str, local_path: str, direction: str = \"download\") -> None:\n",
    "    local_dir = Path(local_path)\n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    full_remote = f\"{RCLONE_REMOTE}:{remote_path}\"\n",
    "\n",
    "    if direction == \"download\":\n",
    "        cmd = [\"rclone\", \"sync\", full_remote, str(local_dir)] + _RCLONE_DRIVE_FLAGS\n",
    "    else:\n",
    "        cmd = [\"rclone\", \"sync\", str(local_dir), full_remote, \"--progress\"]\n",
    "\n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"rclone sync failed: {result.stderr}\")\n",
    "    print(f\"Sync complete: {local_dir}\")\n",
    "\n",
    "\n",
    "def rclone_copy_file(remote_path: str, local_path: str) -> None:\n",
    "    local_file = Path(local_path)\n",
    "    local_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    full_remote = f\"{RCLONE_REMOTE}:{remote_path}\"\n",
    "\n",
    "    cmd = [\"rclone\", \"copyto\", full_remote, str(local_file)] + _RCLONE_DRIVE_FLAGS\n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"rclone copy failed: {result.stderr}\")\n",
    "    print(f\"Copied: {local_file}\")\n",
    "\n",
    "\n",
    "def upload_experiment(exp_id: str) -> None:\n",
    "    \"\"\"Upload checkpoint dir + results JSON for a given experiment.\"\"\"\n",
    "    exp_ckpt_dir = CHECKPOINT_DIR / exp_id\n",
    "    if exp_ckpt_dir.exists():\n",
    "        remote_ckpt = f\"{REMOTE_PATHS['checkpoints']}/{exp_id}\"\n",
    "        rclone_sync(remote_ckpt, str(exp_ckpt_dir), direction=\"upload\")\n",
    "\n",
    "    results_file = RESULTS_DIR / f\"{exp_id}.json\"\n",
    "    if results_file.exists():\n",
    "        remote_results = f\"{REMOTE_PATHS['results']}/{exp_id}.json\"\n",
    "        full_remote = f\"{RCLONE_REMOTE}:{remote_results}\"\n",
    "        cmd = [\"rclone\", \"copyto\", str(results_file), full_remote, \"--progress\"]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(f\"Failed to upload results: {result.stderr}\")\n",
    "        print(f\"Uploaded results: {exp_id}.json\")\n",
    "\n",
    "\n",
    "# Verify gdrive remote is configured\n",
    "check = subprocess.run([\"rclone\", \"listremotes\"], capture_output=True, text=True)\n",
    "if check.returncode != 0:\n",
    "    raise RuntimeError(\"rclone not installed or not working\")\n",
    "remotes = check.stdout.strip().split(\"\\n\")\n",
    "if f\"{RCLONE_REMOTE}:\" not in remotes:\n",
    "    raise RuntimeError(\n",
    "        f\"rclone remote '{RCLONE_REMOTE}' not configured. \"\n",
    "        f\"Available remotes: {remotes}. Run 'rclone config' to add it.\"\n",
    "    )\n",
    "print(f\"rclone remote '{RCLONE_REMOTE}' verified\")\n",
    "print(f\"Remote base: {RCLONE_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone ASAP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Check for actual ASAP content, not just the directory\n",
    "asap_json = ASAP_ROOT / \"asap_annotations.json\"\n",
    "if not asap_json.exists():\n",
    "    # Remove empty/broken directory if it exists\n",
    "    if ASAP_ROOT.exists():\n",
    "        shutil.rmtree(ASAP_ROOT)\n",
    "    print(f\"Cloning ASAP dataset from {ASAP_REPO_URL}...\")\n",
    "    subprocess.run([\"git\", \"clone\", ASAP_REPO_URL, str(ASAP_ROOT)], check=True)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(f\"ASAP dataset already exists at {ASAP_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse ASAP metadata\n",
    "asap_index = parse_asap_metadata(ASAP_ROOT)\n",
    "\n",
    "print(f\"Total performances: {len(asap_index)}\")\n",
    "print(f\"Composers: {len(asap_index.get_composers())}\")\n",
    "print(f\"Pieces: {len(asap_index.get_pieces())}\")\n",
    "\n",
    "# Filter to performances with alignments\n",
    "aligned_perfs = asap_index.filter_with_alignments()\n",
    "print(f\"Performances with alignments: {len(aligned_perfs)}\")\n",
    "\n",
    "if len(aligned_perfs) == 0:\n",
    "    raise RuntimeError(\n",
    "        f\"No performances with alignments found. \"\n",
    "        f\"Check that ASAP was cloned correctly at {ASAP_ROOT}. \"\n",
    "        f\"Total parsed: {len(asap_index)}\"\n",
    "    )\n",
    "\n",
    "# Get multi-performer pieces (for disentanglement later)\n",
    "multi_performer = asap_index.get_multi_performer_pieces(min_performers=3)\n",
    "print(f\"Pieces with 3+ performers: {len(multi_performer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download cached MuQ embeddings from gdrive (for Thunder Compute sessions)\n",
    "# Only .pt embeddings are needed at runtime -- audio files are not required.\n",
    "if not SCORE_CACHE_DIR.exists() or len(list(SCORE_CACHE_DIR.glob(\"*.pt\"))) == 0:\n",
    "    print(\"Downloading MuQ score embeddings...\")\n",
    "    rclone_sync(REMOTE_PATHS[\"muq_scores\"], str(SCORE_CACHE_DIR))\n",
    "else:\n",
    "    print(f\"MuQ score cache exists with {len(list(SCORE_CACHE_DIR.glob('*.pt')))} files\")\n",
    "\n",
    "if not PERF_CACHE_DIR.exists() or len(list(PERF_CACHE_DIR.glob(\"*.pt\"))) == 0:\n",
    "    print(\"Downloading MuQ performance embeddings...\")\n",
    "    rclone_sync(REMOTE_PATHS[\"muq_performances\"], str(PERF_CACHE_DIR))\n",
    "else:\n",
    "    print(f\"MuQ perf cache exists with {len(list(PERF_CACHE_DIR.glob('*.pt')))} files\")\n",
    "\n",
    "# Download existing checkpoints + results (for experiment skip logic)\n",
    "print(\"\\nDownloading existing checkpoints...\")\n",
    "try:\n",
    "    rclone_sync(REMOTE_PATHS[\"checkpoints\"], str(CHECKPOINT_DIR))\n",
    "    n_ckpts = len(list(CHECKPOINT_DIR.glob(\"**/*.ckpt\")))\n",
    "    print(f\"Found {n_ckpts} existing checkpoints\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing checkpoints (fine for fresh start): {e}\")\n",
    "\n",
    "print(\"\\nDownloading existing results...\")\n",
    "try:\n",
    "    rclone_sync(REMOTE_PATHS[\"results\"], str(RESULTS_DIR))\n",
    "    n_results = len(list(RESULTS_DIR.glob(\"*.json\")))\n",
    "    print(f\"Found {n_results} existing result files\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing results (fine for fresh start): {e}\")\n",
    "\n",
    "print(\"\\nData sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get all performance keys\n",
    "all_keys = [get_performance_key(p) for p in aligned_perfs]\n",
    "\n",
    "# Split 80/20\n",
    "train_keys, val_keys = train_test_split(all_keys, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_keys)} performances\")\n",
    "print(f\"Val: {len(val_keys)} performances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment A: DTW Baseline\n",
    "\n",
    "Run standard DTW on raw MuQ embeddings without any learned projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DTW baseline on validation set (results are cached to disk)\n",
    "baseline_metrics = run_dtw_baseline(\n",
    "    performances=aligned_perfs,\n",
    "    score_cache_dir=SCORE_CACHE_DIR,\n",
    "    perf_cache_dir=PERF_CACHE_DIR,\n",
    "    asap_root=ASAP_ROOT,\n",
    "    keys=val_keys,\n",
    "    distance_metric=\"cosine\",\n",
    "    results_dir=RESULTS_DIR,\n",
    "    results_key=\"A_dtw_baseline\",\n",
    ")\n",
    "\n",
    "print(\"\\nDTW Baseline Results:\")\n",
    "print(f\"  Mean onset error: {baseline_metrics['weighted_mean_error_ms']:.1f} ms\")\n",
    "print(f\"  Within 30ms: {baseline_metrics['weighted_percent_within_threshold']:.1f}%\")\n",
    "print(f\"  Num performances: {baseline_metrics['num_performances']}\")\n",
    "print(f\"  Total notes evaluated: {baseline_metrics['total_notes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_experiment(\"A_dtw_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alignment(sample_idx=0):\n",
    "    \"\"\"Visualize DTW alignment for a single sample.\"\"\"\n",
    "    dataset = FrameAlignmentDataset(\n",
    "        [p for p in aligned_perfs if get_performance_key(p) in val_keys][:10],\n",
    "        SCORE_CACHE_DIR,\n",
    "        PERF_CACHE_DIR,\n",
    "        ASAP_ROOT,\n",
    "    )\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"No samples available for visualization\")\n",
    "        return\n",
    "\n",
    "    sample = dataset[sample_idx]\n",
    "\n",
    "    score_emb = sample[\"score_embeddings\"].numpy()\n",
    "    perf_emb = sample[\"perf_embeddings\"].numpy()\n",
    "\n",
    "    cost_matrix = compute_cost_matrix(score_emb, perf_emb, metric=\"cosine\")\n",
    "    path_score, path_perf, cost, _ = align_embeddings(score_emb, perf_emb)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].imshow(cost_matrix, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[0].plot(path_perf, path_score, 'r-', linewidth=2, label='DTW path')\n",
    "    axes[0].set_xlabel('Performance frames')\n",
    "    axes[0].set_ylabel('Score frames')\n",
    "    axes[0].set_title(f'DTW Alignment - {sample[\"key\"]}')\n",
    "    axes[0].legend()\n",
    "\n",
    "    gt_score = sample[\"score_onsets\"].numpy()\n",
    "    gt_perf = sample[\"perf_onsets\"].numpy()\n",
    "\n",
    "    axes[1].scatter(gt_score, gt_perf, alpha=0.6, label='Ground truth')\n",
    "    axes[1].plot([0, max(gt_score)], [0, max(gt_perf)], 'k--', alpha=0.3, label='Diagonal')\n",
    "    axes[1].set_xlabel('Score onset (sec)')\n",
    "    axes[1].set_ylabel('Performance onset (sec)')\n",
    "    axes[1].set_title('Note-level Alignment')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_alignment(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment B: Learned Projection\n",
    "\n",
    "Train a projection MLP with soft-DTW divergence loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "projection_config = ProjectionConfig(\n    input_dim=1024,\n    hidden_dim=512,\n    output_dim=256,\n    num_layers=3,\n    dropout=0.1,\n)\n\ntraining_config = TrainingConfig(\n    learning_rate=1e-4,\n    weight_decay=1e-5,\n    batch_size=4,  # Keep small: soft-DTW divergence allocates T*T matrices per sample\n    max_epochs=50,\n    patience=10,\n    soft_dtw_gamma=1.0,\n    num_workers=4,  # Thunder Compute has 4 vCPU\n    max_frames=750,  # 30s at 25fps; soft-DTW is O(T^2) so keep this bounded\n)\n\nprint(\"Projection config:\")\nprint(f\"  {projection_config.input_dim} -> {projection_config.hidden_dim} -> {projection_config.output_dim}\")\nprint(f\"  Layers: {projection_config.num_layers}\")\nprint(f\"  max_frames: {training_config.max_frames} ({training_config.max_frames / 25:.0f}s)\")\nprint(f\"  Batch size: {training_config.batch_size}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_b_results = run_alignment_experiment(\n",
    "    exp_id=\"B_learned_projection\",\n",
    "    description=\"Learned projection MLP with soft-DTW loss\",\n",
    "    performances=aligned_perfs,\n",
    "    score_cache_dir=SCORE_CACHE_DIR,\n",
    "    perf_cache_dir=PERF_CACHE_DIR,\n",
    "    asap_root=ASAP_ROOT,\n",
    "    train_keys=train_keys,\n",
    "    val_keys=val_keys,\n",
    "    projection_config=projection_config,\n",
    "    training_config=training_config,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_experiment(\"B_learned_projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment C: Measure-Level Alignment\n",
    "\n",
    "Pool embeddings by measure for coarser alignment (faster, less memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_c_results = run_alignment_experiment(\n",
    "    exp_id=\"C_measure_level\",\n",
    "    description=\"Measure-level alignment with pooled embeddings\",\n",
    "    performances=aligned_perfs,\n",
    "    score_cache_dir=SCORE_CACHE_DIR,\n",
    "    perf_cache_dir=PERF_CACHE_DIR,\n",
    "    asap_root=ASAP_ROOT,\n",
    "    train_keys=train_keys,\n",
    "    val_keys=val_keys,\n",
    "    projection_config=projection_config,\n",
    "    training_config=training_config,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    use_measures=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_experiment(\"C_measure_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final sync: upload all results and checkpoints to gdrive\n",
    "print(\"Final sync to remote storage...\")\n",
    "rclone_sync(REMOTE_PATHS[\"checkpoints\"], str(CHECKPOINT_DIR), direction=\"upload\")\n",
    "rclone_sync(REMOTE_PATHS[\"results\"], str(RESULTS_DIR), direction=\"upload\")\n",
    "print(\"Final sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_data = [\n",
    "    {\n",
    "        \"Experiment\": \"A: DTW Baseline\",\n",
    "        \"Mean Error (ms)\": baseline_metrics.get(\"weighted_mean_error_ms\", np.nan),\n",
    "        \"Within 30ms (%)\": baseline_metrics.get(\"weighted_percent_within_threshold\", np.nan),\n",
    "        \"Notes Evaluated\": baseline_metrics.get(\"total_notes\", 0),\n",
    "    },\n",
    "]\n",
    "\n",
    "if \"metrics\" in exp_b_results:\n",
    "    results_data.append({\n",
    "        \"Experiment\": \"B: Learned Projection\",\n",
    "        \"Mean Error (ms)\": exp_b_results[\"metrics\"].get(\"weighted_mean_error_ms\", np.nan),\n",
    "        \"Within 30ms (%)\": exp_b_results[\"metrics\"].get(\"weighted_percent_within_threshold\", np.nan),\n",
    "        \"Notes Evaluated\": exp_b_results[\"metrics\"].get(\"total_notes\", 0),\n",
    "    })\n",
    "\n",
    "if \"metrics\" in exp_c_results:\n",
    "    results_data.append({\n",
    "        \"Experiment\": \"C: Measure-Level\",\n",
    "        \"Mean Error (ms)\": exp_c_results[\"metrics\"].get(\"weighted_mean_error_ms\", np.nan),\n",
    "        \"Within 30ms (%)\": exp_c_results[\"metrics\"].get(\"weighted_percent_within_threshold\", np.nan),\n",
    "        \"Notes Evaluated\": exp_c_results[\"metrics\"].get(\"total_notes\", 0),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"\\nResults Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "experiments = results_df[\"Experiment\"].values\n",
    "mean_errors = results_df[\"Mean Error (ms)\"].values\n",
    "within_30ms = results_df[\"Within 30ms (%)\"].values\n",
    "\n",
    "colors = ['#2ecc71' if e < 30 else '#e74c3c' for e in mean_errors]\n",
    "axes[0].bar(experiments, mean_errors, color=colors)\n",
    "axes[0].axhline(y=30, color='red', linestyle='--', label='30ms threshold')\n",
    "axes[0].set_ylabel('Mean Onset Error (ms)')\n",
    "axes[0].set_title('Mean Onset Error by Experiment')\n",
    "axes[0].legend()\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "axes[1].bar(experiments, within_30ms, color='steelblue')\n",
    "axes[1].set_ylabel('Onsets within 30ms (%)')\n",
    "axes[1].set_title('Alignment Accuracy by Experiment')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'alignment_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = results_df[\"Mean Error (ms)\"].idxmin()\n",
    "best_exp = results_df.loc[best_idx]\n",
    "\n",
    "print(\"Best performing experiment:\")\n",
    "print(f\"  {best_exp['Experiment']}\")\n",
    "print(f\"  Mean error: {best_exp['Mean Error (ms)']:.1f} ms\")\n",
    "print(f\"  Within 30ms: {best_exp['Within 30ms (%)']:.1f}%\")\n",
    "\n",
    "if best_exp[\"Mean Error (ms)\"] < 30:\n",
    "    print(\"\\nSuccess: Mean error below 30ms human perception threshold\")\n",
    "else:\n",
    "    print(\"\\nNeeds improvement: Mean error above 30ms threshold\")\n",
    "    print(\"Consider:\")\n",
    "    print(\"  - Larger projection network\")\n",
    "    print(\"  - Different soft-DTW gamma\")\n",
    "    print(\"  - More training data\")\n",
    "    print(\"  - Sakoe-Chiba band constraint for DTW\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}