{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Alignment Exploration\n",
    "\n",
    "This notebook explores aligning student piano performances to rendered MIDI scores\n",
    "in MuQ embedding space using DTW-based algorithms.\n",
    "\n",
    "**Experiments:**\n",
    "- A: DTW Baseline on raw MuQ embeddings\n",
    "- B: Learned projection MLP with soft-DTW loss\n",
    "- C: Measure-level alignment (coarser granularity)\n",
    "\n",
    "**Success metric:** Mean onset error < 30ms (human perception threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model root: /Users/jdhiman/Documents/crescendai/model\n",
      "Source dir: /Users/jdhiman/Documents/crescendai/model/src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook is at: model/notebooks/score_alignment/\n",
    "# src is at: model/src/\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "MODEL_ROOT = NOTEBOOK_DIR.parent.parent\n",
    "SRC_DIR = MODEL_ROOT / \"src\"\n",
    "\n",
    "# Add src to path\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"Model root: {MODEL_ROOT}\")\n",
    "print(f\"Source dir: {SRC_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuQ frame rate: 75 fps\n",
      "PyTorch version: 2.10.0\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Score alignment imports\n",
    "from score_alignment.config import (\n",
    "    MUQ_FRAME_RATE,\n",
    "    ProjectionConfig,\n",
    "    TrainingConfig,\n",
    "    ASAP_REPO_URL,\n",
    ")\n",
    "from score_alignment.data.asap import (\n",
    "    parse_asap_metadata,\n",
    "    load_note_alignments,\n",
    "    extract_onset_pairs,\n",
    "    get_performance_key,\n",
    ")\n",
    "from score_alignment.data.alignment_dataset import (\n",
    "    FrameAlignmentDataset,\n",
    "    MeasureAlignmentDataset,\n",
    "    frame_alignment_collate_fn,\n",
    ")\n",
    "from score_alignment.alignment.dtw import align_embeddings, compute_cost_matrix\n",
    "from score_alignment.alignment.metrics import (\n",
    "    onset_error,\n",
    "    evaluate_dtw_alignment,\n",
    "    compute_alignment_summary,\n",
    ")\n",
    "from score_alignment.models.projection import AlignmentProjectionModel\n",
    "from score_alignment.training.runner import (\n",
    "    run_alignment_experiment,\n",
    "    run_dtw_baseline,\n",
    ")\n",
    "\n",
    "# Check device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"MuQ frame rate: {MUQ_FRAME_RATE} fps\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: /Users/jdhiman/Documents/crescendai/model/data\n",
      "ASAP root: /Users/jdhiman/Documents/crescendai/model/data/asap-dataset\n"
     ]
    }
   ],
   "source": [
    "# All data lives under model/data/\n",
    "DATA_ROOT = MODEL_ROOT / \"data\"\n",
    "ASAP_ROOT = DATA_ROOT / \"asap-dataset\"\n",
    "SCORE_CACHE_DIR = DATA_ROOT / \"muq_cache\" / \"scores\"\n",
    "PERF_CACHE_DIR = DATA_ROOT / \"muq_cache\" / \"performances\"\n",
    "CHECKPOINT_DIR = DATA_ROOT / \"checkpoints\" / \"score_alignment\"\n",
    "RESULTS_DIR = DATA_ROOT / \"results\" / \"score_alignment\"\n",
    "LOG_DIR = DATA_ROOT / \"logs\" / \"score_alignment\"\n",
    "\n",
    "# Create directories\n",
    "for d in [DATA_ROOT, SCORE_CACHE_DIR, PERF_CACHE_DIR, CHECKPOINT_DIR, RESULTS_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"ASAP root: {ASAP_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone ASAP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASAP dataset already exists at /Users/jdhiman/Documents/crescendai/model/data/asap-dataset\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "if not ASAP_ROOT.exists():\n",
    "    print(f\"Cloning ASAP dataset from {ASAP_REPO_URL}...\")\n",
    "    subprocess.run([\"git\", \"clone\", ASAP_REPO_URL, str(ASAP_ROOT)], check=True)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(f\"ASAP dataset already exists at {ASAP_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total performances: 1066\n",
      "Composers: 16\n",
      "Pieces: 242\n",
      "Performances with alignments: 1063\n",
      "Pieces with 3+ performers: 126\n"
     ]
    }
   ],
   "source": [
    "# Parse ASAP metadata\n",
    "asap_index = parse_asap_metadata(ASAP_ROOT)\n",
    "\n",
    "print(f\"Total performances: {len(asap_index)}\")\n",
    "print(f\"Composers: {len(asap_index.get_composers())}\")\n",
    "print(f\"Pieces: {len(asap_index.get_pieces())}\")\n",
    "\n",
    "# Filter to performances with alignments\n",
    "aligned_perfs = asap_index.filter_with_alignments()\n",
    "print(f\"Performances with alignments: {len(aligned_perfs)}\")\n",
    "\n",
    "# Get multi-performer pieces (for disentanglement later)\n",
    "multi_performer = asap_index.get_multi_performer_pieces(min_performers=3)\n",
    "print(f\"Pieces with 3+ performers: {len(multi_performer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding Extraction\n",
    "\n",
    "Extract MuQ embeddings for scores (rendered MIDI) and performances.\n",
    "\n",
    "**Note:** MuQ extraction works best with GPU. On M4 Mac, MPS can be used but may be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached score embeddings: 0\n",
      "Cached performance embeddings: 0\n"
     ]
    }
   ],
   "source": [
    "# Check existing cache\n",
    "score_cached = list(SCORE_CACHE_DIR.glob(\"*.pt\"))\n",
    "perf_cached = list(PERF_CACHE_DIR.glob(\"*.pt\"))\n",
    "\n",
    "print(f\"Cached score embeddings: {len(score_cached)}\")\n",
    "print(f\"Cached performance embeddings: {len(perf_cached)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m218 packages\u001b[0m \u001b[2min 1.06s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m piano-eval-mvp\u001b[2m @ file:///Users/jdhiman/Documents/crescendai/model\u001b[0m\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/0)                                                   \n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m piano-eval-mvp\u001b[2m @ file:///Users/jdhiman/Documents/crescendai/model\u001b[0m\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m piano-eval-mvp\u001b[2m @ file:///Users/jdhiman/Documents/crescendai/model\u001b[0m\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m piano-eval-mvp\u001b[2m @ file:///Users/jdhiman/Documents/crescendai/model\u001b[0m\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m piano-eval-mvp\u001b[2m @ file:///Users/jdhiman/Documents/crescendai/model\u001b[0m\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m piano-eval-mvp\u001b[2m @ file:///Users/jdhiman/Documents/crescendai/model\u001b[0m\n",
      "\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m piano-eval-mvp\u001b[2m @ file:///Users/jdhiman/Documents/crescendai/model\u001b[0m\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m piano-eval-mvp\u001b[2m @ file:///Users/jdhiman/Documents/crescendai/model\u001b[0m\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m piano-eval-mvp\u001b[2m @ file:///Users/jdhiman/Documents/crescendai/model\u001b[0m\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m piano-eval-mvp\u001b[2m @ file:///Users/jdhiman/Documents/crescendai/model\u001b[0m\n",
      "\u001b[2K\u001b[1A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m piano-eval-mvp\u001b[2m @ file:///Users/jdhiman/Documents/crescendai/model\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 771ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m0 (from file:///Users/jdhiman/Doc\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnnaudio\u001b[0m\u001b[2m==0.3.3\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1mpiano-eval-mvp\u001b[0m\u001b[2m==0.1.0 (from file:///Users/jdhiman/Documents/crescendai/model)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add nnAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All embeddings already cached in /Users/jdhiman/Documents/crescendai/model/data/muq_cache/performances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_embeddings_if_needed(performances, cache_dir, asap_root, is_score=False):\n",
    "    \"\"\"Extract MuQ embeddings for audio files not yet cached.\"\"\"\n",
    "    from audio_experiments.extractors.muq import MuQExtractor\n",
    "\n",
    "    cache_dir = Path(cache_dir)\n",
    "    cached = {p.stem for p in cache_dir.glob(\"*.pt\")}\n",
    "\n",
    "    to_extract = []\n",
    "    for perf in performances:\n",
    "        key = get_performance_key(perf)\n",
    "        if is_score:\n",
    "            if perf.midi_score_path:\n",
    "                key = perf.midi_score_path.stem\n",
    "\n",
    "        if key not in cached:\n",
    "            audio_path = perf.audio_path\n",
    "            if audio_path:\n",
    "                full_path = asap_root / audio_path\n",
    "                if full_path.exists():\n",
    "                    to_extract.append((key, full_path))\n",
    "\n",
    "    if not to_extract:\n",
    "        print(f\"All embeddings already cached in {cache_dir}\")\n",
    "        return 0\n",
    "\n",
    "    print(f\"Extracting {len(to_extract)} MuQ embeddings...\")\n",
    "    extractor = MuQExtractor(cache_dir=cache_dir)\n",
    "\n",
    "    for key, audio_path in to_extract:\n",
    "        extractor.extract_from_file(audio_path)\n",
    "\n",
    "    return len(to_extract)\n",
    "\n",
    "extract_embeddings_if_needed(aligned_perfs, PERF_CACHE_DIR, ASAP_ROOT, is_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 850 performances\n",
      "Val: 213 performances\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get all performance keys\n",
    "all_keys = [get_performance_key(p) for p in aligned_perfs]\n",
    "\n",
    "# Split 80/20\n",
    "train_keys, val_keys = train_test_split(all_keys, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_keys)} performances\")\n",
    "print(f\"Val: {len(val_keys)} performances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment A: DTW Baseline\n",
    "\n",
    "Run standard DTW on raw MuQ embeddings without any learned projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DTW baseline on 0 samples...\n",
      "\n",
      "DTW Baseline Results:\n",
      "  Mean onset error: 0.0 ms\n",
      "  Within 30ms: 0.0%\n",
      "  Num performances: 0\n",
      "  Total notes evaluated: 0\n"
     ]
    }
   ],
   "source": [
    "# Run DTW baseline on validation set\n",
    "baseline_metrics = run_dtw_baseline(\n",
    "    performances=aligned_perfs,\n",
    "    score_cache_dir=SCORE_CACHE_DIR,\n",
    "    perf_cache_dir=PERF_CACHE_DIR,\n",
    "    asap_root=ASAP_ROOT,\n",
    "    keys=val_keys,\n",
    "    distance_metric=\"cosine\",\n",
    ")\n",
    "\n",
    "print(\"\\nDTW Baseline Results:\")\n",
    "print(f\"  Mean onset error: {baseline_metrics['weighted_mean_error_ms']:.1f} ms\")\n",
    "print(f\"  Within 30ms: {baseline_metrics['weighted_percent_within_threshold']:.1f}%\")\n",
    "print(f\"  Num performances: {baseline_metrics['num_performances']}\")\n",
    "print(f\"  Total notes evaluated: {baseline_metrics['total_notes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alignment(sample_idx=0):\n",
    "    \"\"\"Visualize DTW alignment for a single sample.\"\"\"\n",
    "    dataset = FrameAlignmentDataset(\n",
    "        [p for p in aligned_perfs if get_performance_key(p) in val_keys][:10],\n",
    "        SCORE_CACHE_DIR,\n",
    "        PERF_CACHE_DIR,\n",
    "        ASAP_ROOT,\n",
    "    )\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"No samples available for visualization\")\n",
    "        return\n",
    "    \n",
    "    sample = dataset[sample_idx]\n",
    "    \n",
    "    score_emb = sample[\"score_embeddings\"].numpy()\n",
    "    perf_emb = sample[\"perf_embeddings\"].numpy()\n",
    "    \n",
    "    cost_matrix = compute_cost_matrix(score_emb, perf_emb, metric=\"cosine\")\n",
    "    path_score, path_perf, cost, _ = align_embeddings(score_emb, perf_emb)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].imshow(cost_matrix, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[0].plot(path_perf, path_score, 'r-', linewidth=2, label='DTW path')\n",
    "    axes[0].set_xlabel('Performance frames')\n",
    "    axes[0].set_ylabel('Score frames')\n",
    "    axes[0].set_title(f'DTW Alignment - {sample[\"key\"]}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    gt_score = sample[\"score_onsets\"].numpy()\n",
    "    gt_perf = sample[\"perf_onsets\"].numpy()\n",
    "    \n",
    "    axes[1].scatter(gt_score, gt_perf, alpha=0.6, label='Ground truth')\n",
    "    axes[1].plot([0, max(gt_score)], [0, max(gt_perf)], 'k--', alpha=0.3, label='Diagonal')\n",
    "    axes[1].set_xlabel('Score onset (sec)')\n",
    "    axes[1].set_ylabel('Performance onset (sec)')\n",
    "    axes[1].set_title('Note-level Alignment')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# visualize_alignment(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment B: Learned Projection\n",
    "\n",
    "Train a projection MLP with soft-DTW divergence loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_config = ProjectionConfig(\n",
    "    input_dim=1024,\n",
    "    hidden_dim=512,\n",
    "    output_dim=256,\n",
    "    num_layers=3,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    batch_size=8,\n",
    "    max_epochs=50,\n",
    "    patience=10,\n",
    "    soft_dtw_gamma=1.0,\n",
    "    num_workers=0,  # Use 0 on Mac to avoid multiprocessing issues\n",
    ")\n",
    "\n",
    "print(\"Projection config:\")\n",
    "print(f\"  {projection_config.input_dim} -> {projection_config.hidden_dim} -> {projection_config.output_dim}\")\n",
    "print(f\"  Layers: {projection_config.num_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_b_results = run_alignment_experiment(\n",
    "    exp_id=\"B_learned_projection\",\n",
    "    description=\"Learned projection MLP with soft-DTW loss\",\n",
    "    performances=aligned_perfs,\n",
    "    score_cache_dir=SCORE_CACHE_DIR,\n",
    "    perf_cache_dir=PERF_CACHE_DIR,\n",
    "    asap_root=ASAP_ROOT,\n",
    "    train_keys=train_keys,\n",
    "    val_keys=val_keys,\n",
    "    projection_config=projection_config,\n",
    "    training_config=training_config,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment C: Measure-Level Alignment\n",
    "\n",
    "Pool embeddings by measure for coarser alignment (faster, less memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_c_results = run_alignment_experiment(\n",
    "    exp_id=\"C_measure_level\",\n",
    "    description=\"Measure-level alignment with pooled embeddings\",\n",
    "    performances=aligned_perfs,\n",
    "    score_cache_dir=SCORE_CACHE_DIR,\n",
    "    perf_cache_dir=PERF_CACHE_DIR,\n",
    "    asap_root=ASAP_ROOT,\n",
    "    train_keys=train_keys,\n",
    "    val_keys=val_keys,\n",
    "    projection_config=projection_config,\n",
    "    training_config=training_config,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    log_dir=LOG_DIR,\n",
    "    use_measures=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_data = [\n",
    "    {\n",
    "        \"Experiment\": \"A: DTW Baseline\",\n",
    "        \"Mean Error (ms)\": baseline_metrics.get(\"weighted_mean_error_ms\", np.nan),\n",
    "        \"Within 30ms (%)\": baseline_metrics.get(\"weighted_percent_within_threshold\", np.nan),\n",
    "        \"Notes Evaluated\": baseline_metrics.get(\"total_notes\", 0),\n",
    "    },\n",
    "]\n",
    "\n",
    "if \"metrics\" in exp_b_results:\n",
    "    results_data.append({\n",
    "        \"Experiment\": \"B: Learned Projection\",\n",
    "        \"Mean Error (ms)\": exp_b_results[\"metrics\"].get(\"weighted_mean_error_ms\", np.nan),\n",
    "        \"Within 30ms (%)\": exp_b_results[\"metrics\"].get(\"weighted_percent_within_threshold\", np.nan),\n",
    "        \"Notes Evaluated\": exp_b_results[\"metrics\"].get(\"total_notes\", 0),\n",
    "    })\n",
    "\n",
    "if \"metrics\" in exp_c_results:\n",
    "    results_data.append({\n",
    "        \"Experiment\": \"C: Measure-Level\",\n",
    "        \"Mean Error (ms)\": exp_c_results[\"metrics\"].get(\"weighted_mean_error_ms\", np.nan),\n",
    "        \"Within 30ms (%)\": exp_c_results[\"metrics\"].get(\"weighted_percent_within_threshold\", np.nan),\n",
    "        \"Notes Evaluated\": exp_c_results[\"metrics\"].get(\"total_notes\", 0),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"\\nResults Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "experiments = results_df[\"Experiment\"].values\n",
    "mean_errors = results_df[\"Mean Error (ms)\"].values\n",
    "within_30ms = results_df[\"Within 30ms (%)\"].values\n",
    "\n",
    "colors = ['#2ecc71' if e < 30 else '#e74c3c' for e in mean_errors]\n",
    "axes[0].bar(experiments, mean_errors, color=colors)\n",
    "axes[0].axhline(y=30, color='red', linestyle='--', label='30ms threshold')\n",
    "axes[0].set_ylabel('Mean Onset Error (ms)')\n",
    "axes[0].set_title('Mean Onset Error by Experiment')\n",
    "axes[0].legend()\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "axes[1].bar(experiments, within_30ms, color='steelblue')\n",
    "axes[1].set_ylabel('Onsets within 30ms (%)')\n",
    "axes[1].set_title('Alignment Accuracy by Experiment')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'alignment_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = results_df[\"Mean Error (ms)\"].idxmin()\n",
    "best_exp = results_df.loc[best_idx]\n",
    "\n",
    "print(\"Best performing experiment:\")\n",
    "print(f\"  {best_exp['Experiment']}\")\n",
    "print(f\"  Mean error: {best_exp['Mean Error (ms)']:.1f} ms\")\n",
    "print(f\"  Within 30ms: {best_exp['Within 30ms (%)']:.1f}%\")\n",
    "\n",
    "if best_exp[\"Mean Error (ms)\"] < 30:\n",
    "    print(\"\\nSuccess: Mean error below 30ms human perception threshold\")\n",
    "else:\n",
    "    print(\"\\nNeeds improvement: Mean error above 30ms threshold\")\n",
    "    print(\"Consider:\")\n",
    "    print(\"  - Larger projection network\")\n",
    "    print(\"  - Different soft-DTW gamma\")\n",
    "    print(\"  - More training data\")\n",
    "    print(\"  - Sakoe-Chiba band constraint for DTW\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
