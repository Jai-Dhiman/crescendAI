{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piano Performance Evaluation - Model Training\n",
    "\n",
    "## Loss Functions Enabled\n",
    "\n",
    "- **CORAL**: Ordinal regression for rank-consistent predictions (0-100 scale -> 20 bins)\n",
    "- **FDS**: Feature Distribution Smoothing for imbalanced targets\n",
    "- **LDS**: Label Distribution Smoothing\n",
    "- **Bootstrap Loss**: Handles noisy labels\n",
    "- **Huber Loss**: Robust to outliers\n",
    "- **Ranking Loss**: Pairwise ranking consistency\n",
    "- **Contrastive Loss**: Cross-modal alignment (InfoNCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\"\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repository (if not already present)\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Install package (torchaudio included for GPU-accelerated audio loading)\n",
    "!uv pip install --system -e .\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#\n",
    "# Reminder: Run this in terminal: rclone config\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_ROOT = '/tmp/checkpoints'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/fusion_comparison'\n",
    "GDRIVE_MERT_PATH = 'gdrive:MERT-v1-95M'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SETUP: CHECKPOINTS AND MERT MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Create checkpoint directories\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "print(f\"\\nCheckpoint directory: {CHECKPOINT_ROOT}\")\n",
    "\n",
    "# 2. Check if rclone is configured for Google Drive\n",
    "print(\"\\nChecking rclone configuration...\")\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "\n",
    "if 'gdrive:' in result.stdout:\n",
    "    print(\"  rclone 'gdrive' remote: CONFIGURED\")\n",
    "    RCLONE_AVAILABLE = True\n",
    "    \n",
    "    # Try to restore existing checkpoints\n",
    "    print(\"\\nRestoring checkpoints from Google Drive (if any)...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_CHECKPOINT_PATH, CHECKPOINT_ROOT, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    \n",
    "    # List restored checkpoints\n",
    "    for fusion_type in ['crossattn', 'gated', 'concat', 'audio_only', 'midi_only']:\n",
    "        ckpt_dir = Path(f\"{CHECKPOINT_ROOT}/{fusion_type}\")\n",
    "        if ckpt_dir.exists():\n",
    "            ckpts = list(ckpt_dir.glob('*.ckpt'))\n",
    "            if ckpts:\n",
    "                print(f\"  {fusion_type}: Restored {len(ckpts)} checkpoint(s)\")\n",
    "else:\n",
    "    print(\"  rclone 'gdrive' remote: NOT CONFIGURED\")\n",
    "    print(\"  WARNING: Checkpoints will NOT be backed up!\")\n",
    "    RCLONE_AVAILABLE = False\n",
    "\n",
    "# 3. MERT-95M model - download from Google Drive to HuggingFace cache\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MERT-95M MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "HF_CACHE = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "MERT_CACHE = HF_CACHE / \"models--m-a-p--MERT-v1-95M\"\n",
    "MERT_SNAPSHOT = MERT_CACHE / \"snapshots\" / \"main\"\n",
    "\n",
    "if MERT_SNAPSHOT.exists() and (MERT_SNAPSHOT / \"model.safetensors\").exists():\n",
    "    print(f\"MERT-95M already cached at: {MERT_SNAPSHOT}\")\n",
    "else:\n",
    "    print(\"Downloading MERT-95M from Google Drive...\")\n",
    "    \n",
    "    # Create directory structure\n",
    "    MERT_SNAPSHOT.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy from Google Drive\n",
    "    result = subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_MERT_PATH, str(MERT_SNAPSHOT), '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    \n",
    "    # Verify download\n",
    "    if (MERT_SNAPSHOT / \"model.safetensors\").exists():\n",
    "        print(f\"MERT-95M downloaded to: {MERT_SNAPSHOT}\")\n",
    "        model_size = (MERT_SNAPSHOT / \"model.safetensors\").stat().st_size / (1024**2)\n",
    "        print(f\"Model size: {model_size:.1f} MB\")\n",
    "    else:\n",
    "        print(\"ERROR: Failed to download MERT-95M from Google Drive!\")\n",
    "        print(f\"Make sure {GDRIVE_MERT_PATH} exists and contains model.safetensors\")\n",
    "\n",
    "print(f\"\\nGoogle Drive checkpoint path: {GDRIVE_CHECKPOINT_PATH}\")\n",
    "print(f\"rclone available: {RCLONE_AVAILABLE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Get the notebook's directory as the base for finding files\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "DATA_ROOT = Path(\"/tmp/maestro_data\")  # Extract to /tmp for speed\n",
    "\n",
    "# Google Drive paths (files in root directory)\n",
    "GDRIVE_DATASET_PATH = \"gdrive:maestro_with_variance.tar.gz\"\n",
    "LOCAL_TARBALL_PATH = Path(\"/tmp/maestro_with_variance.tar.gz\")\n",
    "\n",
    "print(f\"Working directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Data will be extracted to: {DATA_ROOT}\")\n",
    "\n",
    "# Check if already extracted\n",
    "if (DATA_ROOT / \"audio\").exists() and len(list((DATA_ROOT / \"audio\").glob(\"*.wav\"))) > 100:\n",
    "    print(f\"\\nDataset already extracted at {DATA_ROOT}\")\n",
    "    audio_files = list((DATA_ROOT / \"audio\").glob(\"*.wav\"))\n",
    "    midi_files = list((DATA_ROOT / \"midi\").glob(\"*.mid\"))\n",
    "    print(f\"  Audio files: {len(audio_files):,}\")\n",
    "    print(f\"  MIDI files: {len(midi_files):,}\")\n",
    "else:\n",
    "    # Download from Google Drive (root directory)\n",
    "    print(\"\\nDownloading dataset from Google Drive...\")\n",
    "    print(f\"Source: {GDRIVE_DATASET_PATH}\")\n",
    "    print(f\"Destination: {LOCAL_TARBALL_PATH}\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_DATASET_PATH, '/tmp/', '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    \n",
    "    if LOCAL_TARBALL_PATH.exists():\n",
    "        print(f\"\\nDownload complete! Size: {LOCAL_TARBALL_PATH.stat().st_size / (1024**3):.2f} GB\")\n",
    "        tarball_path = LOCAL_TARBALL_PATH\n",
    "    else:\n",
    "        # Fallback to local search\n",
    "        print(\"\\nGoogle Drive download failed, searching locally...\")\n",
    "        TARBALL_SEARCH_PATHS = [\n",
    "            NOTEBOOK_DIR / \"maestro_with_variance.tar.gz\",\n",
    "            Path(\"maestro_with_variance.tar.gz\"),\n",
    "            Path.home() / \"maestro_with_variance.tar.gz\",\n",
    "            NOTEBOOK_DIR.parent / \"maestro_with_variance.tar.gz\",\n",
    "        ]\n",
    "        \n",
    "        tarball_path = None\n",
    "        for p in TARBALL_SEARCH_PATHS:\n",
    "            if p.exists():\n",
    "                tarball_path = p\n",
    "                break\n",
    "        \n",
    "        if tarball_path is None:\n",
    "            print(\"=\"*60)\n",
    "            print(\"ERROR: maestro_with_variance.tar.gz not found!\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"\\nTo fix:\")\n",
    "            print(\"  1. Upload to Google Drive root: gdrive:maestro_with_variance.tar.gz\")\n",
    "            print(\"  2. Or place in notebook directory\")\n",
    "            raise FileNotFoundError(\"Dataset not found\")\n",
    "    \n",
    "    # Extract dataset\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"\\nExtracting to {DATA_ROOT}...\")\n",
    "    with tarfile.open(tarball_path, \"r:gz\") as tar:\n",
    "        tar.extractall(DATA_ROOT)\n",
    "    print(\"Extraction complete!\")\n",
    "    \n",
    "    # Clean up tarball to save space\n",
    "    if LOCAL_TARBALL_PATH.exists():\n",
    "        print(f\"Removing tarball to save space...\")\n",
    "        LOCAL_TARBALL_PATH.unlink()\n",
    "\n",
    "# Verify extraction\n",
    "audio_files = list((DATA_ROOT / \"audio\").glob(\"*.wav\"))\n",
    "midi_files = list((DATA_ROOT / \"midi\").glob(\"*.mid\"))\n",
    "annotation_files = list((DATA_ROOT / \"annotations\").glob(\"*.jsonl\"))\n",
    "\n",
    "print(f\"\\nDataset contents:\")\n",
    "print(f\"  Audio files: {len(audio_files):,}\")\n",
    "print(f\"  MIDI files: {len(midi_files):,}\")\n",
    "print(f\"  Annotation files: {len(annotation_files)}\")\n",
    "\n",
    "# Verify annotation structure\n",
    "if annotation_files:\n",
    "    import json as json_module\n",
    "    with open(annotation_files[0]) as f:\n",
    "        sample = json_module.loads(f.readline())\n",
    "    print(f\"\\nSample annotation:\")\n",
    "    print(f\"  Dimensions: {list(sample['labels'].keys())}\")\n",
    "    print(f\"  Quality tier: {sample.get('quality_tier', 'N/A')}\")\n",
    "    print(f\"  Quality score: {sample.get('quality_score', 'N/A')}\")\n",
    "\n",
    "# Store paths for later cells\n",
    "DATASET_PATHS = {\n",
    "    'train_path': str(DATA_ROOT / \"annotations\" / \"train.jsonl\"),\n",
    "    'val_path': str(DATA_ROOT / \"annotations\" / \"validation.jsonl\"),\n",
    "    'test_path': str(DATA_ROOT / \"annotations\" / \"test.jsonl\"),\n",
    "    'audio_dir': str(DATA_ROOT / \"audio\"),\n",
    "    'midi_dir': str(DATA_ROOT / \"midi\"),\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET PATHS (for training)\")\n",
    "print(f\"{'='*60}\")\n",
    "for k, v in DATASET_PATHS.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# ======================================================\n",
    "# Load pre-trained MIDIBert checkpoint from Google Drive\n",
    "# ======================================================\n",
    "\n",
    "MIDI_PRETRAIN_CONFIG = {\n",
    "    'output_dir': '/tmp/checkpoints/midi_pretrain',\n",
    "    'gdrive_output': 'gdrive:crescendai_checkpoints/midi_pretrain',\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 6,\n",
    "    'max_seq_length': 512,\n",
    "}\n",
    "\n",
    "output_dir = Path(MIDI_PRETRAIN_CONFIG['output_dir'])\n",
    "encoder_checkpoint = output_dir / 'encoder_pretrained.pt'\n",
    "best_checkpoint = output_dir / 'best.pt'\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"LOADING PRE-TRAINED MIDIBERT CHECKPOINT\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Check if already downloaded\n",
    "if encoder_checkpoint.exists():\n",
    "    print(f\"Checkpoint already exists: {encoder_checkpoint}\")\n",
    "else:\n",
    "    # Download from Google Drive\n",
    "    print(f\"Downloading from: {MIDI_PRETRAIN_CONFIG['gdrive_output']}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['rclone', 'copy', MIDI_PRETRAIN_CONFIG['gdrive_output'], str(output_dir), '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    \n",
    "    if not encoder_checkpoint.exists() and not best_checkpoint.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Failed to download MIDIBert checkpoint from Google Drive.\\n\"\n",
    "            f\"Expected: {MIDI_PRETRAIN_CONFIG['gdrive_output']}/encoder_pretrained.pt\"\n",
    "        )\n",
    "    \n",
    "    print(\"Download complete!\")\n",
    "\n",
    "# Set checkpoint path\n",
    "if encoder_checkpoint.exists():\n",
    "    MIDI_PRETRAINED_CHECKPOINT_PATH = str(encoder_checkpoint)\n",
    "elif best_checkpoint.exists():\n",
    "    MIDI_PRETRAINED_CHECKPOINT_PATH = str(best_checkpoint)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No MIDIBert checkpoint found\")\n",
    "\n",
    "# Show training stats if available\n",
    "log_path = output_dir / 'training_log.json'\n",
    "if log_path.exists():\n",
    "    import json as json_module\n",
    "    with open(log_path) as f:\n",
    "        log = json_module.load(f)\n",
    "    final_epoch = log[-1]\n",
    "    print(f\"\\nPre-training stats (from GiantMIDI-Piano):\")\n",
    "    print(f\"  Epochs: {final_epoch['epoch']}\")\n",
    "    print(f\"  Final val loss: {final_epoch['val_loss']:.4f}\")\n",
    "    print(f\"  Pitch val loss: {final_epoch['val_losses']['pitch']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MIDIBERT CHECKPOINT: {MIDI_PRETRAINED_CHECKPOINT_PATH}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "\n",
    "# Enable Tensor Core optimization for A100\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "CONFIG = {\n",
    "    # =========================================================================\n",
    "    # DATA PATHS (from Step 2 extraction)\n",
    "    # =========================================================================\n",
    "    'train_path': DATASET_PATHS['train_path'],\n",
    "    'val_path': DATASET_PATHS['val_path'],\n",
    "    'test_path': DATASET_PATHS['test_path'],\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DIMENSIONS (8 total)\n",
    "    # =========================================================================\n",
    "    'dimensions': [\n",
    "        'note_accuracy', 'rhythmic_stability', 'articulation_clarity', 'pedal_technique',\n",
    "        'tone_quality', 'dynamic_range', 'musical_expression', 'overall_interpretation'\n",
    "    ],\n",
    "    \n",
    "    # =========================================================================\n",
    "    # MODEL ARCHITECTURE\n",
    "    # =========================================================================\n",
    "    'audio_dim': 768,       # MERT-95M output dimension\n",
    "    'midi_dim': 256,        # MIDIBert output dimension\n",
    "    'shared_dim': 512,      # Projection space dimension\n",
    "    'use_projection': True, # Use projection heads before fusion\n",
    "    \n",
    "    # MIDIBert pretraining - use checkpoint from Step 2b\n",
    "    'midi_pretrained_checkpoint': MIDI_PRETRAINED_CHECKPOINT_PATH if 'MIDI_PRETRAINED_CHECKPOINT_PATH' in dir() else None,\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LOSS WEIGHTS\n",
    "    # =========================================================================\n",
    "    'mse_weight': 1.0,\n",
    "    'ranking_weight': 0.2,\n",
    "    'contrastive_weight': 0.1,\n",
    "    \n",
    "    # =========================================================================\n",
    "    # BASE LOSS FUNCTION\n",
    "    # =========================================================================\n",
    "    'base_loss': 'huber',      # 'mse', 'huber', or 'mae' - Huber is robust to outliers\n",
    "    'huber_delta': 1.0,\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LABEL DISTRIBUTION SMOOTHING (LDS)\n",
    "    # Handles imbalanced label distribution\n",
    "    # =========================================================================\n",
    "    'lds_enabled': True,\n",
    "    'lds_num_bins': 100,\n",
    "    'lds_sigma': 2.0,\n",
    "    'lds_reweight_scale': 1.0,\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FEATURE DISTRIBUTION SMOOTHING (FDS) - NEW\n",
    "    # Calibrates features across target bins for better generalization\n",
    "    # =========================================================================\n",
    "    'fds_enabled': False,  # Disabled to reduce CPU memory\n",
    "    'fds_num_bins': 100,\n",
    "    'fds_momentum': 0.9,\n",
    "    'fds_kernel_sigma': 2.0,\n",
    "    'fds_start_epoch': 2,      # Start after model warms up\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CORAL ORDINAL REGRESSION - NEW\n",
    "    # Converts regression to ordinal classification for rank consistency\n",
    "    # =========================================================================\n",
    "    'coral_enabled': True,\n",
    "    'coral_num_classes': 20,   # 20 bins = 5-point resolution on 0-100 scale\n",
    "    'coral_weight': 0.3,       # Weight for CORAL loss component\n",
    "    \n",
    "    # =========================================================================\n",
    "    # BOOTSTRAP LOSS (handles noisy labels)\n",
    "    # =========================================================================\n",
    "    'bootstrap_enabled': True,\n",
    "    'bootstrap_beta': 0.8,          # 80% label, 20% model prediction\n",
    "    'bootstrap_warmup_epochs': 2,   # Train with pure labels first\n",
    "    \n",
    "    # =========================================================================\n",
    "    # MODALITY DROPOUT (prevents modality collapse)\n",
    "    # =========================================================================\n",
    "    'modality_dropout': {\n",
    "        'enabled': True,\n",
    "        'audio_prob': 0.15,\n",
    "        'midi_prob': 0.15,\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TRAINING HYPERPARAMETERS\n",
    "    # =========================================================================\n",
    "    'epochs': 5,\n",
    "    'batch_size': 8,\n",
    "    'backbone_lr': 5e-6,\n",
    "    'heads_lr': 1e-4,\n",
    "    'warmup_steps': 500,\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STAGED UNFREEZING\n",
    "    # =========================================================================\n",
    "    'staged_unfreezing': {\n",
    "        'enabled': True,\n",
    "        'schedule': [\n",
    "            {'epoch': 0, 'freeze': ['audio_encoder', 'midi_encoder'], 'unfreeze': ['projection']},\n",
    "            {'epoch': 3, 'unfreeze': ['audio_encoder.top_4', 'midi_encoder.top_2'], 'lr_scale': 0.1},\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FUSION TYPES TO COMPARE\n",
    "    # =========================================================================\n",
    "    'fusion_types': ['crossattn', 'gated', 'concat'],\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PHASE 2 SUCCESS CRITERIA\n",
    "    # =========================================================================\n",
    "    'fusion_improvement_target': 10.0,  # Fusion must beat single-modal by >= 10%\n",
    "}\n",
    "\n",
    "# Print configuration\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nDATA PATHS:\")\n",
    "print(f\"  Train: {CONFIG['train_path']}\")\n",
    "print(f\"  Val: {CONFIG['val_path']}\")\n",
    "print(f\"  Test: {CONFIG['test_path']}\")\n",
    "\n",
    "print(\"\\nMODEL:\")\n",
    "print(f\"  Audio dim: {CONFIG['audio_dim']}\")\n",
    "print(f\"  MIDI dim: {CONFIG['midi_dim']}\")\n",
    "print(f\"  Shared dim: {CONFIG['shared_dim']}\")\n",
    "print(f\"  MIDIBert checkpoint: {CONFIG['midi_pretrained_checkpoint']}\")\n",
    "\n",
    "print(\"\\nLOSS FUNCTIONS:\")\n",
    "print(f\"  Base loss: {CONFIG['base_loss']}\")\n",
    "print(f\"  LDS: {'enabled' if CONFIG['lds_enabled'] else 'disabled'}\")\n",
    "print(f\"  FDS: {'enabled' if CONFIG['fds_enabled'] else 'disabled'}\")\n",
    "print(f\"  CORAL: {'enabled' if CONFIG['coral_enabled'] else 'disabled'}\")\n",
    "print(f\"  Bootstrap: {'enabled' if CONFIG['bootstrap_enabled'] else 'disabled'}\")\n",
    "\n",
    "print(\"\\nTRAINING:\")\n",
    "print(f\"  Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Modality dropout: {CONFIG['modality_dropout']['audio_prob']:.0%} audio, {CONFIG['modality_dropout']['midi_prob']:.0%} MIDI\")\n",
    "\n",
    "print(\"\\nFUSION TYPES TO COMPARE:\")\n",
    "for ft in CONFIG['fusion_types']:\n",
    "    print(f\"  - {ft}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataset import create_dataloaders\n",
    "\n",
    "# Prepare modality dropout config\n",
    "modality_dropout_config = None\n",
    "if CONFIG['modality_dropout']['enabled']:\n",
    "    modality_dropout_config = {\n",
    "        'audio_prob': CONFIG['modality_dropout']['audio_prob'],\n",
    "        'midi_prob': CONFIG['modality_dropout']['midi_prob'],\n",
    "    }\n",
    "    print(f\"Modality dropout enabled: audio={modality_dropout_config['audio_prob']}, midi={modality_dropout_config['midi_prob']}\")\n",
    "\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    train_annotation_path=CONFIG['train_path'],\n",
    "    val_annotation_path=CONFIG['val_path'],\n",
    "    test_annotation_path=CONFIG['test_path'],\n",
    "    dimension_names=CONFIG['dimensions'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=4,\n",
    "    augmentation_config=None,  # Disable augmentation for clean comparison\n",
    "    modality_dropout_config=modality_dropout_config,\n",
    "    audio_sample_rate=24000,\n",
    "    max_audio_length=240000,\n",
    "    max_midi_events=512,\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_loader.dataset):,}\")\n",
    "print(f\"Val samples: {len(val_loader.dataset):,}\")\n",
    "print(f\"Test samples: {len(test_loader.dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "from src.models.lightning_module import PerformanceEvaluationModel\n",
    "from src.utils.memory_profiler import MemoryProfilerCallback, log_memory\n",
    "from src.callbacks.unfreezing import StagedUnfreezingCallback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# Collect training labels for LDS fitting (if enabled)\n",
    "if CONFIG['lds_enabled']:\n",
    "    print(\"Collecting training labels for LDS fitting...\")\n",
    "    all_labels = []\n",
    "    for batch in train_loader:\n",
    "        all_labels.append(batch['labels'])\n",
    "    all_train_labels = torch.cat(all_labels, dim=0)\n",
    "    print(f\"Collected {len(all_train_labels):,} training labels\")\n",
    "\n",
    "# Handle MIDIBert pretrained checkpoint (download from Google Drive if needed)\n",
    "midi_pretrained_local = None\n",
    "if CONFIG.get('midi_pretrained_checkpoint'):\n",
    "    midi_ckpt_path = CONFIG['midi_pretrained_checkpoint']\n",
    "    if midi_ckpt_path.startswith('gdrive:'):\n",
    "        # Copy from Google Drive to local\n",
    "        local_path = '/tmp/midi_pretrained.pt'\n",
    "        print(f\"Copying MIDIBert checkpoint from Google Drive: {midi_ckpt_path}\")\n",
    "        !rclone copy {midi_ckpt_path} /tmp/ --progress\n",
    "        ckpt_name = midi_ckpt_path.split('/')[-1]\n",
    "        midi_pretrained_local = f'/tmp/{ckpt_name}'\n",
    "        if Path(midi_pretrained_local).exists():\n",
    "            print(f\"MIDIBert checkpoint loaded: {midi_pretrained_local}\")\n",
    "        else:\n",
    "            print(f\"WARNING: MIDIBert checkpoint not found at {midi_pretrained_local}\")\n",
    "            midi_pretrained_local = None\n",
    "    else:\n",
    "        midi_pretrained_local = midi_ckpt_path\n",
    "        if not Path(midi_pretrained_local).exists():\n",
    "            print(f\"WARNING: MIDIBert checkpoint not found at {midi_pretrained_local}\")\n",
    "            midi_pretrained_local = None\n",
    "\n",
    "# Store trained models (will be populated by individual training cells)\n",
    "trained_models = {}\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Find the latest checkpoint in a directory.\"\"\"\n",
    "    ckpt_dir = Path(checkpoint_dir)\n",
    "    if not ckpt_dir.exists():\n",
    "        return None\n",
    "    ckpts = list(ckpt_dir.glob('*.ckpt'))\n",
    "    if not ckpts:\n",
    "        return None\n",
    "    last_ckpt = ckpt_dir / 'last.ckpt'\n",
    "    if last_ckpt.exists():\n",
    "        return str(last_ckpt)\n",
    "    return str(sorted(ckpts, key=lambda x: x.stat().st_mtime)[-1])\n",
    "\n",
    "def train_single_modal_model(modality):\n",
    "    \"\"\"Train a single-modal model (audio-only or MIDI-only) for Phase 2 baseline comparison.\"\"\"\n",
    "    model_name = f\"{modality}_only\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"TRAINING: {model_name.upper()} BASELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check for existing checkpoint to resume\n",
    "    checkpoint_dir = Path(f'{CHECKPOINT_ROOT}/{model_name}')\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    resume_ckpt = find_latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    if resume_ckpt:\n",
    "        print(f\"Found checkpoint to resume: {resume_ckpt}\")\n",
    "    \n",
    "    # Set dimensions based on modality\n",
    "    if modality == 'audio':\n",
    "        audio_dim = CONFIG['audio_dim']\n",
    "        midi_dim = 0\n",
    "        print(\"Audio-only mode: MERT encoder only\")\n",
    "    elif modality == 'midi':\n",
    "        audio_dim = 0\n",
    "        midi_dim = CONFIG['midi_dim']\n",
    "        print(\"MIDI-only mode: MIDIBert encoder only\")\n",
    "        if midi_pretrained_local:\n",
    "            print(f\"Using pretrained MIDIBert: {midi_pretrained_local}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown modality: {modality}. Use 'audio' or 'midi'.\")\n",
    "    \n",
    "    # Create model with CORAL and FDS support\n",
    "    model = PerformanceEvaluationModel(\n",
    "        audio_dim=audio_dim,\n",
    "        midi_dim=midi_dim,\n",
    "        shared_dim=CONFIG['shared_dim'],\n",
    "        aggregator_dim=512,\n",
    "        num_dimensions=len(CONFIG['dimensions']),\n",
    "        dimension_names=CONFIG['dimensions'],\n",
    "        modality=modality,\n",
    "        fusion_type='gated',\n",
    "        use_projection=CONFIG['use_projection'],\n",
    "        midi_pretrained_checkpoint=midi_pretrained_local if modality == 'midi' else None,\n",
    "        # Loss weights\n",
    "        mse_weight=CONFIG['mse_weight'],\n",
    "        ranking_weight=CONFIG['ranking_weight'],\n",
    "        contrastive_weight=0,  # No contrastive loss for single-modal\n",
    "        # Base loss\n",
    "        base_loss=CONFIG['base_loss'],\n",
    "        huber_delta=CONFIG['huber_delta'],\n",
    "        # LDS\n",
    "        lds_enabled=CONFIG['lds_enabled'],\n",
    "        lds_num_bins=CONFIG['lds_num_bins'],\n",
    "        lds_sigma=CONFIG['lds_sigma'],\n",
    "        lds_reweight_scale=CONFIG['lds_reweight_scale'],\n",
    "        # FDS (NEW)\n",
    "        fds_enabled=CONFIG['fds_enabled'],\n",
    "        fds_num_bins=CONFIG['fds_num_bins'],\n",
    "        fds_momentum=CONFIG['fds_momentum'],\n",
    "        fds_kernel_sigma=CONFIG['fds_kernel_sigma'],\n",
    "        fds_start_epoch=CONFIG['fds_start_epoch'],\n",
    "        # CORAL (NEW)\n",
    "        coral_enabled=CONFIG['coral_enabled'],\n",
    "        coral_num_classes=CONFIG['coral_num_classes'],\n",
    "        coral_weight=CONFIG['coral_weight'],\n",
    "        # Bootstrap\n",
    "        bootstrap_enabled=CONFIG['bootstrap_enabled'],\n",
    "        bootstrap_beta=CONFIG['bootstrap_beta'],\n",
    "        bootstrap_warmup_epochs=CONFIG['bootstrap_warmup_epochs'],\n",
    "        # Training\n",
    "        backbone_lr=CONFIG['backbone_lr'],\n",
    "        heads_lr=CONFIG['heads_lr'],\n",
    "        warmup_steps=CONFIG['warmup_steps'],\n",
    "        max_epochs=CONFIG['epochs'],\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "    \n",
    "    # Fit LDS if enabled\n",
    "    if CONFIG['lds_enabled']:\n",
    "        model.fit_lds(all_train_labels)\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=str(checkpoint_dir),\n",
    "            filename=f'{model_name}-{{epoch:02d}}-{{val_loss:.4f}}',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_top_k=1,\n",
    "            save_last=True,\n",
    "        ),\n",
    "        EarlyStopping(monitor='val_loss', patience=3, mode='min'),\n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "        MemoryProfilerCallback(log_every_n_steps=100, log_to_file=f'/tmp/{model_name}_memory.log'),\n",
    "    ]\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=CONFIG['epochs'],\n",
    "        precision='16-mixed',\n",
    "        accelerator='auto',\n",
    "        devices='auto',\n",
    "        callbacks=callbacks,\n",
    "        logger=TensorBoardLogger(save_dir='logs', name=model_name),\n",
    "        log_every_n_steps=50,\n",
    "        gradient_clip_val=1.0,\n",
    "        accumulate_grad_batches=2,\n",
    "        val_check_interval=0.5,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.fit(model, train_loader, val_loader, ckpt_path=resume_ckpt)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'best_checkpoint': callbacks[0].best_model_path,\n",
    "        'best_val_loss': float(callbacks[0].best_model_score) if callbacks[0].best_model_score else None,\n",
    "    }\n",
    "    trained_models[model_name] = result\n",
    "    \n",
    "    print(f\"\\nBest checkpoint: {result['best_checkpoint']}\")\n",
    "    print(f\"Best val loss: {result['best_val_loss']:.4f}\" if result['best_val_loss'] else \"N/A\")\n",
    "    \n",
    "    # Sync to Google Drive\n",
    "    print(f\"\\nSyncing to Google Drive...\")\n",
    "    !rclone copy {checkpoint_dir} {GDRIVE_CHECKPOINT_PATH}/{model_name} --progress\n",
    "    print(\"Sync complete!\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def train_fusion_model(fusion_type):\n",
    "    \"\"\"Train a single fusion model with CORAL and FDS support.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"TRAINING: {fusion_type.upper()} FUSION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check for existing checkpoint to resume\n",
    "    checkpoint_dir = Path(f'{CHECKPOINT_ROOT}/{fusion_type}')\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    resume_ckpt = find_latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    if resume_ckpt:\n",
    "        print(f\"Found checkpoint to resume: {resume_ckpt}\")\n",
    "    \n",
    "    # Report pretrained MIDIBert status\n",
    "    if midi_pretrained_local:\n",
    "        print(f\"Using pretrained MIDIBert: {midi_pretrained_local}\")\n",
    "    else:\n",
    "        print(\"Training MIDIBert from scratch (no pretrained checkpoint)\")\n",
    "    \n",
    "    # Report CORAL and FDS status\n",
    "    print(f\"CORAL: {'enabled' if CONFIG['coral_enabled'] else 'disabled'}\")\n",
    "    print(f\"FDS: {'enabled' if CONFIG['fds_enabled'] else 'disabled'}\")\n",
    "    \n",
    "    # Create model with CORAL and FDS support\n",
    "    model = PerformanceEvaluationModel(\n",
    "        audio_dim=CONFIG['audio_dim'],\n",
    "        midi_dim=CONFIG['midi_dim'],\n",
    "        shared_dim=CONFIG['shared_dim'],\n",
    "        aggregator_dim=512,\n",
    "        num_dimensions=len(CONFIG['dimensions']),\n",
    "        dimension_names=CONFIG['dimensions'],\n",
    "        fusion_type=fusion_type,\n",
    "        use_projection=CONFIG['use_projection'],\n",
    "        midi_pretrained_checkpoint=midi_pretrained_local,\n",
    "        # Loss weights\n",
    "        mse_weight=CONFIG['mse_weight'],\n",
    "        ranking_weight=CONFIG['ranking_weight'],\n",
    "        contrastive_weight=CONFIG['contrastive_weight'],\n",
    "        # Base loss\n",
    "        base_loss=CONFIG['base_loss'],\n",
    "        huber_delta=CONFIG['huber_delta'],\n",
    "        # LDS\n",
    "        lds_enabled=CONFIG['lds_enabled'],\n",
    "        lds_num_bins=CONFIG['lds_num_bins'],\n",
    "        lds_sigma=CONFIG['lds_sigma'],\n",
    "        lds_reweight_scale=CONFIG['lds_reweight_scale'],\n",
    "        # FDS (NEW)\n",
    "        fds_enabled=CONFIG['fds_enabled'],\n",
    "        fds_num_bins=CONFIG['fds_num_bins'],\n",
    "        fds_momentum=CONFIG['fds_momentum'],\n",
    "        fds_kernel_sigma=CONFIG['fds_kernel_sigma'],\n",
    "        fds_start_epoch=CONFIG['fds_start_epoch'],\n",
    "        # CORAL (NEW)\n",
    "        coral_enabled=CONFIG['coral_enabled'],\n",
    "        coral_num_classes=CONFIG['coral_num_classes'],\n",
    "        coral_weight=CONFIG['coral_weight'],\n",
    "        # Bootstrap\n",
    "        bootstrap_enabled=CONFIG['bootstrap_enabled'],\n",
    "        bootstrap_beta=CONFIG['bootstrap_beta'],\n",
    "        bootstrap_warmup_epochs=CONFIG['bootstrap_warmup_epochs'],\n",
    "        # Training\n",
    "        backbone_lr=CONFIG['backbone_lr'],\n",
    "        heads_lr=CONFIG['heads_lr'],\n",
    "        warmup_steps=CONFIG['warmup_steps'],\n",
    "        max_epochs=CONFIG['epochs'],\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "    \n",
    "    # Fit LDS if enabled\n",
    "    if CONFIG['lds_enabled']:\n",
    "        model.fit_lds(all_train_labels)\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=str(checkpoint_dir),\n",
    "            filename=f'{fusion_type}-{{epoch:02d}}-{{val_loss:.4f}}',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_top_k=1,\n",
    "            save_last=True,\n",
    "        ),\n",
    "        EarlyStopping(monitor='val_loss', patience=3, mode='min'),\n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "        MemoryProfilerCallback(log_every_n_steps=100, log_to_file=f'/tmp/{fusion_type}_memory.log'),\n",
    "    ]\n",
    "    \n",
    "    if CONFIG['staged_unfreezing']['enabled']:\n",
    "        callbacks.append(StagedUnfreezingCallback(\n",
    "            schedule=CONFIG['staged_unfreezing']['schedule'],\n",
    "            verbose=True,\n",
    "        ))\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=CONFIG['epochs'],\n",
    "        precision='16-mixed',\n",
    "        accelerator='auto',\n",
    "        devices='auto',\n",
    "        callbacks=callbacks,\n",
    "        logger=TensorBoardLogger(save_dir='logs', name=fusion_type),\n",
    "        log_every_n_steps=50,\n",
    "        gradient_clip_val=1.0,\n",
    "        accumulate_grad_batches=2,\n",
    "        val_check_interval=0.5,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.fit(model, train_loader, val_loader, ckpt_path=resume_ckpt)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'best_checkpoint': callbacks[0].best_model_path,\n",
    "        'best_val_loss': float(callbacks[0].best_model_score) if callbacks[0].best_model_score else None,\n",
    "    }\n",
    "    trained_models[fusion_type] = result\n",
    "    \n",
    "    print(f\"\\nBest checkpoint: {result['best_checkpoint']}\")\n",
    "    print(f\"Best val loss: {result['best_val_loss']:.4f}\" if result['best_val_loss'] else \"N/A\")\n",
    "    \n",
    "    # Sync to Google Drive\n",
    "    print(f\"\\nSyncing to Google Drive...\")\n",
    "    !rclone copy {checkpoint_dir} {GDRIVE_CHECKPOINT_PATH}/{fusion_type} --progress\n",
    "    print(\"Sync complete!\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING FUNCTIONS READY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  - train_single_modal_model(modality): Train audio-only or MIDI-only baseline\")\n",
    "print(\"  - train_fusion_model(fusion_type): Train fusion model (crossattn, gated, concat)\")\n",
    "\n",
    "print(\"\\nNew features enabled:\")\n",
    "print(f\"  - CORAL ordinal regression: {CONFIG['coral_enabled']}\")\n",
    "print(f\"  - FDS feature smoothing: {CONFIG['fds_enabled']}\")\n",
    "print(f\"  - LDS label smoothing: {CONFIG['lds_enabled']}\")\n",
    "print(f\"  - Bootstrap loss: {CONFIG['bootstrap_enabled']}\")\n",
    "\n",
    "if midi_pretrained_local:\n",
    "    print(f\"\\nMIDIBert pretrained checkpoint: {midi_pretrained_local}\")\n",
    "else:\n",
    "    print(\"\\nMIDIBert will train from scratch (no pretrained checkpoint)\")\n",
    "\n",
    "print(\"\\nRun the cells below to train each model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREFLIGHT CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run fast dev run\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        sys.executable, \"train.py\",\n",
    "        \"--config\", \"configs/experiment.yaml\",\n",
    "        \"--fast-dev-run\"\n",
    "    ],\n",
    "    cwd=\"/tmp/crescendai/model\",\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Print output\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Check result\n",
    "if result.returncode == 0:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PREFLIGHT CHECK PASSED - Ready for full training\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PREFLIGHT CHECK FAILED - Fix errors before training\")\n",
    "    print(\"=\" * 70)\n",
    "    raise RuntimeError(\"Preflight check failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Train Audio-Only Baseline (Phase 2)\n",
    "\n",
    "Train audio-only model using MERT encoder only. This establishes a baseline for comparing fusion performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_modal_model('audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Train MIDI-Only Baseline (Phase 2)\n",
    "\n",
    "Train MIDI-only model using MIDIBert encoder only. This establishes a baseline for comparing fusion performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_single_modal_model('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Train CrossAttention Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_model('crossattn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_model('gated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fusion_model('concat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5f: Phase 2 Gate Check\n",
    "\n",
    "Evaluate all 5 models and check if fusion beats single-modal baselines by >= 10%.\n",
    "\n",
    "**GO Criteria:**\n",
    "- Best fusion model r > best single-modal r by >= 10%\n",
    "- Models learn quality (higher scores for pristine vs degraded)\n",
    "\n",
    "**NO-GO:** Debug fusion architecture before proceeding to Phase 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 2 GATE CHECK: EVALUATING ALL 5 MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define all model types\n",
    "single_modal_types = ['audio_only', 'midi_only']\n",
    "fusion_types = ['crossattn', 'gated', 'concat']\n",
    "all_model_types = single_modal_types + fusion_types\n",
    "\n",
    "# Load models and evaluate\n",
    "phase2_results = {}\n",
    "\n",
    "for model_type in all_model_types:\n",
    "    print(f\"\\nEvaluating {model_type}...\")\n",
    "    \n",
    "    # Find checkpoint\n",
    "    if model_type in trained_models:\n",
    "        ckpt_path = trained_models[model_type]['best_checkpoint']\n",
    "    else:\n",
    "        ckpt_dir = Path(f'{CHECKPOINT_ROOT}/{model_type}')\n",
    "        ckpts = list(ckpt_dir.glob('*.ckpt'))\n",
    "        ckpts = [c for c in ckpts if c.name != 'last.ckpt']\n",
    "        if ckpts:\n",
    "            ckpt_path = str(sorted(ckpts)[-1])\n",
    "        else:\n",
    "            print(f\"  No checkpoint found, skipping...\")\n",
    "            continue\n",
    "    \n",
    "    if not ckpt_path or not Path(ckpt_path).exists():\n",
    "        print(f\"  Checkpoint not found: {ckpt_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Loading: {ckpt_path}\")\n",
    "    model = PerformanceEvaluationModel.load_from_checkpoint(ckpt_path)\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    # Evaluate\n",
    "    trainer = pl.Trainer(accelerator='auto', devices='auto', precision=16, logger=False)\n",
    "    test_results = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    phase2_results[model_type] = test_results[0]\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Calculate mean Pearson r for each model\n",
    "mean_rs = {}\n",
    "for model_type in all_model_types:\n",
    "    if model_type in phase2_results:\n",
    "        mean_r = np.mean([phase2_results[model_type].get(f'test_pearson_{d}', 0) \n",
    "                         for d in CONFIG['dimensions']])\n",
    "        mean_rs[model_type] = mean_r\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2 RESULTS: MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Model':<15} {'Mean Pearson r':<15} {'Type'}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for model_type in all_model_types:\n",
    "    if model_type in mean_rs:\n",
    "        model_kind = \"Single-modal\" if model_type in single_modal_types else \"Fusion\"\n",
    "        print(f\"{model_type:<15} {mean_rs[model_type]:>14.4f} {model_kind}\")\n",
    "\n",
    "# Phase 2 Gate Check\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2 GATE CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best single-modal\n",
    "single_modal_rs = {k: v for k, v in mean_rs.items() if k in single_modal_types}\n",
    "best_single_modal = max(single_modal_rs, key=single_modal_rs.get) if single_modal_rs else None\n",
    "best_single_r = single_modal_rs[best_single_modal] if best_single_modal else 0\n",
    "\n",
    "# Best fusion\n",
    "fusion_rs = {k: v for k, v in mean_rs.items() if k in fusion_types}\n",
    "best_fusion = max(fusion_rs, key=fusion_rs.get) if fusion_rs else None\n",
    "best_fusion_r = fusion_rs[best_fusion] if best_fusion else 0\n",
    "\n",
    "# Calculate improvement\n",
    "if best_single_r > 0:\n",
    "    improvement = ((best_fusion_r - best_single_r) / best_single_r) * 100\n",
    "else:\n",
    "    improvement = 0\n",
    "\n",
    "print(f\"\\nBest single-modal: {best_single_modal} (r = {best_single_r:.4f})\")\n",
    "print(f\"Best fusion: {best_fusion} (r = {best_fusion_r:.4f})\")\n",
    "print(f\"Improvement: {improvement:+.1f}%\")\n",
    "print(f\"Target: >= {CONFIG['fusion_improvement_target']:.0f}%\")\n",
    "\n",
    "# Gate decision\n",
    "PHASE2_PASSED = improvement >= CONFIG['fusion_improvement_target']\n",
    "\n",
    "if PHASE2_PASSED:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"PHASE 2 GATE: PASS\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"{best_fusion} fusion beats {best_single_modal} by {improvement:.1f}%\")\n",
    "    print(\"-> Proceed to Phase 3: Contrastive Pre-training\")\n",
    "else:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"PHASE 2 GATE: FAIL\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"Fusion improvement ({improvement:.1f}%) < target ({CONFIG['fusion_improvement_target']:.0f}%)\")\n",
    "    print(\"-> Debug fusion architecture before proceeding\")\n",
    "    print(\"Suggestions:\")\n",
    "    print(\"  1. Check cross-modal alignment scores\")\n",
    "    print(\"  2. Try different fusion types\")\n",
    "    print(\"  3. Adjust learning rates\")\n",
    "\n",
    "# Store for later use\n",
    "phase2_gate_result = {\n",
    "    'passed': PHASE2_PASSED,\n",
    "    'best_single_modal': best_single_modal,\n",
    "    'best_single_r': best_single_r,\n",
    "    'best_fusion': best_fusion,\n",
    "    'best_fusion_r': best_fusion_r,\n",
    "    'improvement': improvement,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Detailed Analysis of Phase 2 Models\n",
    "\n",
    "Analyze the 5 Phase 2 models in detail. This section:\n",
    "- Uses `phase2_results` from the gate check (no redundant evaluation)\n",
    "- Provides per-dimension breakdown for all 5 models\n",
    "- Shows fusion diagnostics (alignment, gate values)\n",
    "- Runs statistical significance tests\n",
    "\n",
    "**Run this AFTER Phase 2 Gate Check (Step 5f).**\n",
    "Can be run while Phase 3/4 are training or after everything completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6a: Per-Dimension Analysis (All 5 Models)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"PER-DIMENSION PEARSON CORRELATION (ALL 5 MODELS)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Check that phase2_results exists\n",
    "if 'phase2_results' not in dir() or not phase2_results:\n",
    "    raise RuntimeError(\"phase2_results not found! Run Phase 2 Gate Check (cell 26) first.\")\n",
    "\n",
    "# All model types\n",
    "single_modal_types = ['audio_only', 'midi_only']\n",
    "fusion_types = CONFIG['fusion_types']\n",
    "all_model_types = single_modal_types + fusion_types\n",
    "\n",
    "# Build header\n",
    "header = f\"{'Dimension':<25}\"\n",
    "for mt in all_model_types:\n",
    "    if mt in phase2_results:\n",
    "        header += f\" {mt[:10]:>10}\"\n",
    "header += \" {'Best':>10}\"\n",
    "print(header)\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Per-dimension results\n",
    "dimension_winners = {}\n",
    "for dim in CONFIG['dimensions']:\n",
    "    row = f\"{dim:<25}\"\n",
    "    dim_scores = {}\n",
    "    \n",
    "    for mt in all_model_types:\n",
    "        if mt in phase2_results:\n",
    "            r = phase2_results[mt].get(f'test_pearson_{dim}', 0)\n",
    "            dim_scores[mt] = r\n",
    "            row += f\" {r:>10.3f}\"\n",
    "    \n",
    "    # Find best for this dimension\n",
    "    if dim_scores:\n",
    "        best_mt = max(dim_scores, key=dim_scores.get)\n",
    "        dimension_winners[dim] = best_mt\n",
    "        row += f\" {best_mt[:10]:>10}\"\n",
    "    \n",
    "    print(row)\n",
    "\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Summary: which model wins most dimensions\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"DIMENSION WINNERS SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "from collections import Counter\n",
    "winner_counts = Counter(dimension_winners.values())\n",
    "for mt, count in winner_counts.most_common():\n",
    "    pct = 100 * count / len(CONFIG['dimensions'])\n",
    "    print(f\"  {mt:<15}: {count}/{len(CONFIG['dimensions'])} dimensions ({pct:.0f}%)\")\n",
    "\n",
    "# Overall mean scores (same as gate check, for reference)\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"OVERALL PERFORMANCE (from Phase 2 Gate Check)\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Model':<15} {'Mean r':>10} {'Mean MAE':>12} {'Type':<12}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for mt in all_model_types:\n",
    "    if mt in phase2_results:\n",
    "        mean_r = np.mean([phase2_results[mt].get(f'test_pearson_{d}', 0) for d in CONFIG['dimensions']])\n",
    "        mean_mae = np.mean([phase2_results[mt].get(f'test_mae_{d}', 0) for d in CONFIG['dimensions']])\n",
    "        model_kind = \"Single-modal\" if mt in single_modal_types else \"Fusion\"\n",
    "        print(f\"{mt:<15} {mean_r:>10.4f} {mean_mae:>12.2f} {model_kind:<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6b: Fusion Diagnostics\n",
    "# Check cross-modal alignment, gate values, feature diversity\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"FUSION MODEL DIAGNOSTICS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "for ft in CONFIG['fusion_types']:\n",
    "    if ft not in phase2_results:\n",
    "        print(f\"\\n{ft.upper()}: No results available\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{ft.upper()}:\")\n",
    "    \n",
    "    # Cross-modal alignment (how well audio and MIDI embeddings align)\n",
    "    align = phase2_results[ft].get('test_cross_modal_alignment', None)\n",
    "    if align is not None:\n",
    "        align_status = \"GOOD\" if align > 0.5 else \"LOW - consider more contrastive training\"\n",
    "        print(f\"  Cross-modal alignment: {align:.4f} ({align_status})\")\n",
    "    else:\n",
    "        print(f\"  Cross-modal alignment: Not logged\")\n",
    "    \n",
    "    # Gate values (for gated fusion - shows audio vs MIDI contribution)\n",
    "    if ft == 'gated':\n",
    "        gate_mean = phase2_results[ft].get('test_gate_mean', None)\n",
    "        if gate_mean is not None:\n",
    "            if gate_mean > 0.6:\n",
    "                gate_status = \"Audio-dominant\"\n",
    "            elif gate_mean < 0.4:\n",
    "                gate_status = \"MIDI-dominant\"\n",
    "            else:\n",
    "                gate_status = \"Balanced\"\n",
    "            print(f\"  Gate mean: {gate_mean:.4f} ({gate_status})\")\n",
    "        else:\n",
    "            print(f\"  Gate mean: Not logged\")\n",
    "    \n",
    "    # Feature diversity (prevents mode collapse)\n",
    "    audio_div = phase2_results[ft].get('test_audio_diversity', None)\n",
    "    midi_div = phase2_results[ft].get('test_midi_diversity', None)\n",
    "    if audio_div is not None:\n",
    "        print(f\"  Audio feature diversity: {audio_div:.4f}\")\n",
    "    if midi_div is not None:\n",
    "        print(f\"  MIDI feature diversity: {midi_div:.4f}\")\n",
    "    \n",
    "    # Loss components (if logged)\n",
    "    mse_loss = phase2_results[ft].get('test_mse_loss', None)\n",
    "    ranking_loss = phase2_results[ft].get('test_ranking_loss', None)\n",
    "    coral_loss = phase2_results[ft].get('test_coral_loss', None)\n",
    "    \n",
    "    if any([mse_loss, ranking_loss, coral_loss]):\n",
    "        print(f\"  Loss breakdown:\")\n",
    "        if mse_loss is not None:\n",
    "            print(f\"    MSE/Huber: {mse_loss:.4f}\")\n",
    "        if ranking_loss is not None:\n",
    "            print(f\"    Ranking: {ranking_loss:.4f}\")\n",
    "        if coral_loss is not None:\n",
    "            print(f\"    CORAL: {coral_loss:.4f}\")\n",
    "\n",
    "# Compare audio-only vs MIDI-only to understand modality contribution\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"MODALITY CONTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "audio_only_r = np.mean([phase2_results.get('audio_only', {}).get(f'test_pearson_{d}', 0) for d in CONFIG['dimensions']])\n",
    "midi_only_r = np.mean([phase2_results.get('midi_only', {}).get(f'test_pearson_{d}', 0) for d in CONFIG['dimensions']])\n",
    "\n",
    "print(f\"\\nAudio-only mean r: {audio_only_r:.4f}\")\n",
    "print(f\"MIDI-only mean r:  {midi_only_r:.4f}\")\n",
    "\n",
    "if audio_only_r > 0 and midi_only_r > 0:\n",
    "    ratio = audio_only_r / midi_only_r\n",
    "    if ratio > 1.5:\n",
    "        print(f\"\\nAudio is {ratio:.1f}x stronger than MIDI\")\n",
    "        print(\"  -> Audio encoder (MERT) is primary contributor\")\n",
    "        print(\"  -> MIDI encoder may need more pre-training or different architecture\")\n",
    "    elif ratio < 0.67:\n",
    "        print(f\"\\nMIDI is {1/ratio:.1f}x stronger than audio\")\n",
    "        print(\"  -> MIDI encoder is primary contributor\")\n",
    "        print(\"  -> This is unusual - check audio data quality\")\n",
    "    else:\n",
    "        print(f\"\\nModalities are balanced (ratio: {ratio:.2f})\")\n",
    "        print(\"  -> Both encoders contribute meaningfully\")\n",
    "        print(\"  -> Fusion should provide complementary information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6c: Statistical Significance Tests\n",
    "# Compare best fusion vs best single-modal with paired t-test\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Get best models from Phase 2 gate check\n",
    "if 'phase2_gate_result' not in dir():\n",
    "    raise RuntimeError(\"phase2_gate_result not found! Run Phase 2 Gate Check first.\")\n",
    "\n",
    "best_fusion = phase2_gate_result['best_fusion']\n",
    "best_single = phase2_gate_result['best_single_modal']\n",
    "\n",
    "print(f\"\\nComparing: {best_fusion} (fusion) vs {best_single} (single-modal)\")\n",
    "print(\"Loading models to collect predictions for paired t-test...\")\n",
    "\n",
    "# Load both models and collect predictions\n",
    "comparison_preds = {}\n",
    "\n",
    "for model_type in [best_fusion, best_single]:\n",
    "    # Find checkpoint\n",
    "    if model_type in trained_models:\n",
    "        ckpt_path = trained_models[model_type]['best_checkpoint']\n",
    "    else:\n",
    "        ckpt_dir = Path(f'{CHECKPOINT_ROOT}/{model_type}')\n",
    "        ckpts = [c for c in ckpt_dir.glob('*.ckpt') if c.name != 'last.ckpt']\n",
    "        ckpt_path = str(sorted(ckpts)[-1]) if ckpts else None\n",
    "    \n",
    "    if not ckpt_path or not Path(ckpt_path).exists():\n",
    "        print(f\"  Checkpoint not found for {model_type}, skipping significance test\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Loading {model_type}...\")\n",
    "    model = PerformanceEvaluationModel.load_from_checkpoint(ckpt_path)\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            audio = batch['audio_waveform'].cuda()\n",
    "            midi = batch.get('midi_tokens')\n",
    "            if midi is not None:\n",
    "                midi = midi.cuda()\n",
    "            \n",
    "            output = model(audio_waveform=audio, midi_tokens=midi)\n",
    "            if output is not None:\n",
    "                all_preds.append(output['scores'].cpu().numpy())\n",
    "                all_targets.append(batch['labels'].numpy())\n",
    "    \n",
    "    comparison_preds[model_type] = {\n",
    "        'preds': np.concatenate(all_preds, axis=0),\n",
    "        'targets': np.concatenate(all_targets, axis=0),\n",
    "    }\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Perform paired t-test for each dimension\n",
    "if len(comparison_preds) == 2:\n",
    "    print(f\"\\n{'Dimension':<25} {'p-value':>12} {'Winner':>15} {'Significant?':>12}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    n_samples = min(len(comparison_preds[best_fusion]['preds']), \n",
    "                    len(comparison_preds[best_single]['preds']))\n",
    "    \n",
    "    significant_wins = {best_fusion: 0, best_single: 0}\n",
    "    \n",
    "    for dim_idx, dim in enumerate(CONFIG['dimensions']):\n",
    "        # Calculate absolute errors for each model\n",
    "        fusion_errors = np.abs(\n",
    "            comparison_preds[best_fusion]['preds'][:n_samples, dim_idx] - \n",
    "            comparison_preds[best_fusion]['targets'][:n_samples, dim_idx]\n",
    "        )\n",
    "        single_errors = np.abs(\n",
    "            comparison_preds[best_single]['preds'][:n_samples, dim_idx] - \n",
    "            comparison_preds[best_single]['targets'][:n_samples, dim_idx]\n",
    "        )\n",
    "        \n",
    "        # Paired t-test (lower error is better)\n",
    "        t_stat, p_value = stats.ttest_rel(fusion_errors, single_errors)\n",
    "        \n",
    "        fusion_mean = np.mean(fusion_errors)\n",
    "        single_mean = np.mean(single_errors)\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            if fusion_mean < single_mean:\n",
    "                winner = best_fusion\n",
    "                significant_wins[best_fusion] += 1\n",
    "            else:\n",
    "                winner = best_single\n",
    "                significant_wins[best_single] += 1\n",
    "            sig_str = \"Yes\"\n",
    "        else:\n",
    "            winner = \"Tie\"\n",
    "            sig_str = \"No\"\n",
    "        \n",
    "        print(f\"{dim:<25} {p_value:>12.4f} {winner:>15} {sig_str:>12}\")\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    print(f\"\\nSignificant wins: {best_fusion}: {significant_wins[best_fusion]}, {best_single}: {significant_wins[best_single]}\")\n",
    "    \n",
    "    # Overall conclusion\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"STATISTICAL CONCLUSION\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    if significant_wins[best_fusion] > significant_wins[best_single]:\n",
    "        print(f\"\\n{best_fusion} is STATISTICALLY BETTER on {significant_wins[best_fusion]}/{len(CONFIG['dimensions'])} dimensions\")\n",
    "    elif significant_wins[best_single] > significant_wins[best_fusion]:\n",
    "        print(f\"\\n{best_single} is STATISTICALLY BETTER on {significant_wins[best_single]}/{len(CONFIG['dimensions'])} dimensions\")\n",
    "        print(\"WARNING: Single-modal beats fusion - fusion architecture may need debugging\")\n",
    "    else:\n",
    "        print(f\"\\nNo clear statistical winner - models perform similarly\")\n",
    "else:\n",
    "    print(\"\\nCould not load both models for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6d: Ablation Study (OPTIONAL)\n",
    "\n",
    "**Skip this section if Phase 2 passed and you want to proceed directly to Phase 3.**\n",
    "\n",
    "Run ablation experiments to measure the impact of each training component:\n",
    "- Huber vs MSE loss\n",
    "- LDS enabled vs disabled\n",
    "- FDS enabled vs disabled\n",
    "- CORAL enabled vs disabled\n",
    "- Bootstrap enabled vs disabled\n",
    "- Modality dropout vs no dropout\n",
    "\n",
    "This helps quantify the contribution of each training improvement.\n",
    "\n",
    "**Note:** This trains 6 NEW model configurations (3 epochs each) = ~18 GPU-hours on A100.\n",
    "Only run if debugging or for research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6d: Ablation Study (OPTIONAL)\n",
    "# Skip this cell if you want to proceed directly to Phase 3\n",
    "\n",
    "# Set to False to skip ablation study\n",
    "RUN_ABLATION = False  # <-- Set to True to run ablation experiments\n",
    "\n",
    "if not RUN_ABLATION:\n",
    "    print(\"=\"*70)\n",
    "    print(\"ABLATION STUDY: SKIPPED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nSet RUN_ABLATION = True to run ablation experiments\")\n",
    "    print(\"This trains 7 model configurations (3 epochs each)\")\n",
    "    ablation_results = None\n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"ABLATION STUDY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Use the best fusion type from Phase 2\n",
    "    ABLATION_FUSION_TYPE = phase2_gate_result['best_fusion']\n",
    "    print(f\"Using fusion type: {ABLATION_FUSION_TYPE}\")\n",
    "    \n",
    "    # Define ablation configurations\n",
    "    # Each config modifies ONE variable from the full config\n",
    "    ablation_configs = {\n",
    "        'full': {\n",
    "            'base_loss': 'huber',\n",
    "            'lds_enabled': True,\n",
    "            'fds_enabled': True,\n",
    "            'coral_enabled': True,\n",
    "            'bootstrap_enabled': True,\n",
    "            'modality_dropout_enabled': True,\n",
    "        },\n",
    "        'mse_loss': {\n",
    "            'base_loss': 'mse',\n",
    "            'lds_enabled': True,\n",
    "            'fds_enabled': True,\n",
    "            'coral_enabled': True,\n",
    "            'bootstrap_enabled': True,\n",
    "            'modality_dropout_enabled': True,\n",
    "        },\n",
    "        'no_lds': {\n",
    "            'base_loss': 'huber',\n",
    "            'lds_enabled': False,\n",
    "            'fds_enabled': True,\n",
    "            'coral_enabled': True,\n",
    "            'bootstrap_enabled': True,\n",
    "            'modality_dropout_enabled': True,\n",
    "        },\n",
    "        'no_fds': {\n",
    "            'base_loss': 'huber',\n",
    "            'lds_enabled': True,\n",
    "            'fds_enabled': False,\n",
    "            'coral_enabled': True,\n",
    "            'bootstrap_enabled': True,\n",
    "            'modality_dropout_enabled': True,\n",
    "        },\n",
    "        'no_coral': {\n",
    "            'base_loss': 'huber',\n",
    "            'lds_enabled': True,\n",
    "            'fds_enabled': True,\n",
    "            'coral_enabled': False,\n",
    "            'bootstrap_enabled': True,\n",
    "            'modality_dropout_enabled': True,\n",
    "        },\n",
    "        'no_bootstrap': {\n",
    "            'base_loss': 'huber',\n",
    "            'lds_enabled': True,\n",
    "            'fds_enabled': True,\n",
    "            'coral_enabled': True,\n",
    "            'bootstrap_enabled': False,\n",
    "            'modality_dropout_enabled': True,\n",
    "        },\n",
    "        'no_modality_dropout': {\n",
    "            'base_loss': 'huber',\n",
    "            'lds_enabled': True,\n",
    "            'fds_enabled': True,\n",
    "            'coral_enabled': True,\n",
    "            'bootstrap_enabled': True,\n",
    "            'modality_dropout_enabled': False,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    ablation_results = {}\n",
    "    \n",
    "    for ablation_name, ablation_cfg in ablation_configs.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ABLATION: {ablation_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create checkpoint directory\n",
    "        ablation_ckpt_dir = Path(f'{CHECKPOINT_ROOT}/ablation/{ablation_name}')\n",
    "        ablation_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Check for existing checkpoint\n",
    "        resume_ckpt = find_latest_checkpoint(ablation_ckpt_dir)\n",
    "        if resume_ckpt:\n",
    "            print(f\"Resuming from: {resume_ckpt}\")\n",
    "        \n",
    "        # Create dataloader with appropriate modality dropout\n",
    "        if ablation_cfg['modality_dropout_enabled']:\n",
    "            abl_modality_dropout = {'audio_prob': 0.15, 'midi_prob': 0.15}\n",
    "        else:\n",
    "            abl_modality_dropout = None\n",
    "        \n",
    "        abl_train_loader, abl_val_loader, _ = create_dataloaders(\n",
    "            train_annotation_path=CONFIG['train_path'],\n",
    "            val_annotation_path=CONFIG['val_path'],\n",
    "            test_annotation_path=CONFIG['test_path'],\n",
    "            dimension_names=CONFIG['dimensions'],\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            num_workers=4,\n",
    "            modality_dropout_config=abl_modality_dropout,\n",
    "        )\n",
    "        \n",
    "        # Create model with ablation config\n",
    "        model = PerformanceEvaluationModel(\n",
    "            audio_dim=CONFIG['audio_dim'],\n",
    "            midi_dim=CONFIG['midi_dim'],\n",
    "            shared_dim=CONFIG['shared_dim'],\n",
    "            aggregator_dim=512,\n",
    "            num_dimensions=len(CONFIG['dimensions']),\n",
    "            dimension_names=CONFIG['dimensions'],\n",
    "            fusion_type=ABLATION_FUSION_TYPE,\n",
    "            use_projection=CONFIG['use_projection'],\n",
    "            mse_weight=CONFIG['mse_weight'],\n",
    "            ranking_weight=CONFIG['ranking_weight'],\n",
    "            contrastive_weight=CONFIG['contrastive_weight'],\n",
    "            base_loss=ablation_cfg['base_loss'],\n",
    "            huber_delta=CONFIG['huber_delta'],\n",
    "            lds_enabled=ablation_cfg['lds_enabled'],\n",
    "            lds_num_bins=CONFIG['lds_num_bins'],\n",
    "            lds_sigma=CONFIG['lds_sigma'],\n",
    "            lds_reweight_scale=CONFIG['lds_reweight_scale'],\n",
    "            fds_enabled=ablation_cfg['fds_enabled'],\n",
    "            fds_num_bins=CONFIG['fds_num_bins'],\n",
    "            fds_momentum=CONFIG['fds_momentum'],\n",
    "            fds_kernel_sigma=CONFIG['fds_kernel_sigma'],\n",
    "            fds_start_epoch=CONFIG['fds_start_epoch'],\n",
    "            coral_enabled=ablation_cfg['coral_enabled'],\n",
    "            coral_num_classes=CONFIG['coral_num_classes'],\n",
    "            coral_weight=CONFIG['coral_weight'],\n",
    "            bootstrap_enabled=ablation_cfg['bootstrap_enabled'],\n",
    "            bootstrap_beta=CONFIG['bootstrap_beta'],\n",
    "            bootstrap_warmup_epochs=CONFIG['bootstrap_warmup_epochs'],\n",
    "            backbone_lr=CONFIG['backbone_lr'],\n",
    "            heads_lr=CONFIG['heads_lr'],\n",
    "            warmup_steps=CONFIG['warmup_steps'],\n",
    "            max_epochs=3,  # Shorter for ablation\n",
    "            gradient_checkpointing=True,\n",
    "        )\n",
    "        \n",
    "        # Fit LDS if enabled\n",
    "        if ablation_cfg['lds_enabled']:\n",
    "            model.fit_lds(all_train_labels)\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(\n",
    "                dirpath=str(ablation_ckpt_dir),\n",
    "                filename=f'{ablation_name}-{{epoch:02d}}-{{val_loss:.4f}}',\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                save_top_k=1,\n",
    "                save_last=True,\n",
    "            ),\n",
    "            EarlyStopping(monitor='val_loss', patience=2, mode='min'),\n",
    "        ]\n",
    "        \n",
    "        # Train\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=3,\n",
    "            precision='16-mixed',\n",
    "            accelerator='auto',\n",
    "            devices='auto',\n",
    "            callbacks=callbacks,\n",
    "            logger=TensorBoardLogger(save_dir='logs', name=f'ablation_{ablation_name}'),\n",
    "            log_every_n_steps=50,\n",
    "            gradient_clip_val=1.0,\n",
    "            accumulate_grad_batches=2,\n",
    "            val_check_interval=0.5,\n",
    "        )\n",
    "        \n",
    "        trainer.fit(model, abl_train_loader, abl_val_loader, ckpt_path=resume_ckpt)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_results = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "        \n",
    "        # Store results\n",
    "        mean_r = np.mean([test_results[0].get(f'test_pearson_{d}', 0) for d in CONFIG['dimensions']])\n",
    "        mean_mae = np.mean([test_results[0].get(f'test_mae_{d}', 0) for d in CONFIG['dimensions']])\n",
    "        \n",
    "        ablation_results[ablation_name] = {\n",
    "            'config': ablation_cfg,\n",
    "            'mean_pearson': mean_r,\n",
    "            'mean_mae': mean_mae,\n",
    "            'val_loss': float(callbacks[0].best_model_score) if callbacks[0].best_model_score else None,\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nResults: r={mean_r:.3f}, MAE={mean_mae:.2f}\")\n",
    "        \n",
    "        # Sync to Google Drive\n",
    "        !rclone copy {ablation_ckpt_dir} {GDRIVE_CHECKPOINT_PATH}/ablation/{ablation_name} --progress\n",
    "        \n",
    "        # Cleanup\n",
    "        del model, trainer\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Print ablation summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ABLATION STUDY RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Config':<25} {'Pearson r':>12} {'MAE':>12} {'Delta r':>12}\")\n",
    "    print(\"-\"*65)\n",
    "    \n",
    "    full_r = ablation_results['full']['mean_pearson']\n",
    "    for name, res in ablation_results.items():\n",
    "        delta = res['mean_pearson'] - full_r\n",
    "        delta_str = f\"{delta:+.3f}\" if name != 'full' else \"baseline\"\n",
    "        print(f\"{name:<25} {res['mean_pearson']:>12.3f} {res['mean_mae']:>12.2f} {delta_str:>12}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*65)\n",
    "    print(\"Interpretation:\")\n",
    "    print(\"  - Negative delta = component HELPS (removing it hurts performance)\")\n",
    "    print(\"  - Positive delta = component HURTS (removing it improves performance)\")\n",
    "    \n",
    "    # Identify most impactful components\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPONENT IMPACT RANKING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    impacts = {name: full_r - res['mean_pearson'] \n",
    "               for name, res in ablation_results.items() if name != 'full'}\n",
    "    sorted_impacts = sorted(impacts.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'Component':<25} {'Impact':>12} {'Effect':<15}\")\n",
    "    print(\"-\"*55)\n",
    "    for name, impact in sorted_impacts:\n",
    "        effect = \"HELPS\" if impact > 0 else \"HURTS\"\n",
    "        print(f\"{name:<25} {abs(impact):>12.3f} {effect:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Phase 3 - Contrastive Pre-training\n",
    "\n",
    "**Prerequisite: Phase 2 must pass (fusion beats single-modal by >= 10%)**\n",
    "\n",
    "Align MERT and MIDIBert representation spaces using InfoNCE contrastive loss with hard negative mining.\n",
    "\n",
    "**Training Config:**\n",
    "- Freeze encoders, train only projection heads\n",
    "- Larger batch size (64) for more in-batch negatives\n",
    "- Hard negative mining: 25% of batch from same piece, different degradation\n",
    "- 15 epochs\n",
    "\n",
    "**Success Criteria:**\n",
    "- Cross-modal alignment score >= 0.6\n",
    "\n",
    "**Gate Logic:**\n",
    "- If PHASE2_PASSED is False, this cell will skip training\n",
    "- After training, Phase 3 gate check determines if we proceed to Phase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PHASE2_PASSED:\n",
    "    print(\"SKIPPING Phase 3: Phase 2 gate check failed\")\n",
    "    print(\"Debug fusion architecture before proceeding\")\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 3: CONTRASTIVE PRE-TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Configuration for contrastive pre-training\n",
    "    CONTRASTIVE_CONFIG = {\n",
    "        'epochs': 15,\n",
    "        'batch_size': 64,  # Larger batch for more in-batch negatives\n",
    "        'temperature': 0.07,\n",
    "        'learning_rate': 1e-4,\n",
    "        'warmup_epochs': 2,\n",
    "        'freeze_encoders': True,\n",
    "        'use_hard_negatives': True,\n",
    "        'hard_neg_ratio': 0.25,  # 25% of batch are hard negatives\n",
    "        'alignment_target': 0.6,  # Phase 3 gate criterion\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nContrastive Config:\")\n",
    "    for k, v in CONTRASTIVE_CONFIG.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Create contrastive dataloaders with hard negative mining\n",
    "    from src.data.dataset import create_contrastive_dataloader\n",
    "    \n",
    "    print(\"\\nCreating contrastive dataloaders with hard negative mining...\")\n",
    "    contrastive_train_loader = create_contrastive_dataloader(\n",
    "        annotation_path=CONFIG['train_path'],\n",
    "        dimension_names=CONFIG['dimensions'],\n",
    "        batch_size=CONTRASTIVE_CONFIG['batch_size'],\n",
    "        num_workers=4,\n",
    "        use_hard_negatives=CONTRASTIVE_CONFIG['use_hard_negatives'],\n",
    "        hard_neg_ratio=CONTRASTIVE_CONFIG['hard_neg_ratio'],\n",
    "    )\n",
    "    \n",
    "    contrastive_val_loader = create_contrastive_dataloader(\n",
    "        annotation_path=CONFIG['val_path'],\n",
    "        dimension_names=CONFIG['dimensions'],\n",
    "        batch_size=CONTRASTIVE_CONFIG['batch_size'],\n",
    "        num_workers=4,\n",
    "        use_hard_negatives=False,  # No hard negatives for validation\n",
    "    )\n",
    "    \n",
    "    print(f\"Train batches: {len(contrastive_train_loader)}\")\n",
    "    print(f\"Val batches: {len(contrastive_val_loader)}\")\n",
    "    \n",
    "    # Create model in contrastive training mode\n",
    "    print(\"\\nCreating model in contrastive-only mode...\")\n",
    "    contrastive_model = PerformanceEvaluationModel(\n",
    "        audio_dim=CONFIG['audio_dim'],\n",
    "        midi_dim=CONFIG['midi_dim'],\n",
    "        shared_dim=CONFIG['shared_dim'],\n",
    "        training_mode=\"contrastive\",  # Contrastive-only mode\n",
    "        modality=\"fusion\",\n",
    "        fusion_type=best_fusion,  # Use best fusion from Phase 2\n",
    "        use_projection=True,\n",
    "        freeze_audio_encoder=CONTRASTIVE_CONFIG['freeze_encoders'],\n",
    "        gradient_checkpointing=True,\n",
    "        midi_pretrained_checkpoint=midi_pretrained_local,\n",
    "        contrastive_temperature=CONTRASTIVE_CONFIG['temperature'],\n",
    "        contrastive_weight=1.0,\n",
    "        learning_rate=CONTRASTIVE_CONFIG['learning_rate'],\n",
    "        backbone_lr=0,  # Frozen encoders\n",
    "        heads_lr=CONTRASTIVE_CONFIG['learning_rate'],\n",
    "        warmup_steps=len(contrastive_train_loader) * CONTRASTIVE_CONFIG['warmup_epochs'],\n",
    "        max_epochs=CONTRASTIVE_CONFIG['epochs'],\n",
    "    )\n",
    "    \n",
    "    # Freeze encoders\n",
    "    if CONTRASTIVE_CONFIG['freeze_encoders']:\n",
    "        print(\"Freezing encoders, training only projection heads...\")\n",
    "        if contrastive_model.audio_encoder is not None:\n",
    "            for param in contrastive_model.audio_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        if contrastive_model.midi_encoder is not None:\n",
    "            for param in contrastive_model.midi_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable = sum(p.numel() for p in contrastive_model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in contrastive_model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\n",
    "    \n",
    "    # Checkpoint directory\n",
    "    contrastive_ckpt_dir = Path(f'{CHECKPOINT_ROOT}/contrastive')\n",
    "    contrastive_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Callbacks\n",
    "    contrastive_callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=str(contrastive_ckpt_dir),\n",
    "            filename='contrastive-{epoch:02d}-{val_alignment_score:.4f}',\n",
    "            monitor='val_alignment_score',\n",
    "            mode='max',  # Higher alignment is better\n",
    "            save_top_k=1,\n",
    "            save_last=True,\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_alignment_score',\n",
    "            patience=5,\n",
    "            mode='max',\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "    ]\n",
    "    \n",
    "    # Trainer\n",
    "    contrastive_trainer = pl.Trainer(\n",
    "        max_epochs=CONTRASTIVE_CONFIG['epochs'],\n",
    "        precision='16-mixed',\n",
    "        accelerator='auto',\n",
    "        devices='auto',\n",
    "        callbacks=contrastive_callbacks,\n",
    "        logger=TensorBoardLogger(save_dir='logs', name='contrastive'),\n",
    "        log_every_n_steps=50,\n",
    "        gradient_clip_val=1.0,\n",
    "        val_check_interval=0.5,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nStarting contrastive pre-training...\")\n",
    "    contrastive_trainer.fit(contrastive_model, contrastive_train_loader, contrastive_val_loader)\n",
    "    \n",
    "    # Get best results\n",
    "    best_alignment = contrastive_callbacks[0].best_model_score\n",
    "    best_contrastive_ckpt = contrastive_callbacks[0].best_model_path\n",
    "    \n",
    "    print(f\"\\nBest alignment score: {best_alignment:.4f}\")\n",
    "    print(f\"Best checkpoint: {best_contrastive_ckpt}\")\n",
    "    \n",
    "    # Sync to Google Drive\n",
    "    print(\"\\nSyncing to Google Drive...\")\n",
    "    !rclone copy {contrastive_ckpt_dir} {GDRIVE_CHECKPOINT_PATH}/contrastive --progress\n",
    "    \n",
    "    # Cleanup\n",
    "    del contrastive_model, contrastive_trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\nContrastive pre-training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3 Gate Check\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 3 GATE CHECK (TRAINING_PLAN_v2.md)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not PHASE2_PASSED:\n",
    "    print(\"Phase 2 did not pass - skipping Phase 3 gate check\")\n",
    "    PHASE3_PASSED = False\n",
    "else:\n",
    "    alignment_target = CONTRASTIVE_CONFIG['alignment_target']\n",
    "    \n",
    "    print(f\"\\nTarget alignment: >= {alignment_target}\")\n",
    "    print(f\"Achieved alignment: {best_alignment:.4f}\")\n",
    "    \n",
    "    PHASE3_PASSED = best_alignment >= alignment_target\n",
    "    \n",
    "    if PHASE3_PASSED:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(\"PHASE 3 GATE: PASS\")\n",
    "        print(f\"{'='*40}\")\n",
    "        print(f\"Cross-modal alignment ({best_alignment:.4f}) >= target ({alignment_target})\")\n",
    "        print(\"-> Proceed to Phase 4: Full training with aligned encoders\")\n",
    "        print(f\"\\nContrastive checkpoint to use: {best_contrastive_ckpt}\")\n",
    "    else:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(\"PHASE 3 GATE: FAIL\")\n",
    "        print(f\"{'='*40}\")\n",
    "        print(f\"Cross-modal alignment ({best_alignment:.4f}) < target ({alignment_target})\")\n",
    "        print(\"-> Consider:\")\n",
    "        print(\"  1. More epochs\")\n",
    "        print(\"  2. Different temperature (try 0.05 or 0.1)\")\n",
    "        print(\"  3. Increase hard negative ratio\")\n",
    "        print(\"  4. Check data quality\")\n",
    "\n",
    "# Store gate result\n",
    "phase3_gate_result = {\n",
    "    'passed': PHASE3_PASSED,\n",
    "    'alignment_score': float(best_alignment) if 'best_alignment' in dir() else None,\n",
    "    'target': alignment_target if 'alignment_target' in dir() else 0.6,\n",
    "    'checkpoint': best_contrastive_ckpt if 'best_contrastive_ckpt' in dir() else None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Phase 4 - Fine-tuning with Aligned Encoders\n",
    "\n",
    "**Prerequisite: Phase 3 must pass (alignment score >= 0.6)**\n",
    "\n",
    "Fine-tune the best fusion model using the contrastive-pretrained encoder weights.\n",
    "\n",
    "**Training Config:**\n",
    "- Load projection heads from contrastive checkpoint\n",
    "- Unfreeze top layers of encoders gradually\n",
    "- Full loss function (regression + ranking + contrastive)\n",
    "- 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PHASE3_PASSED:\n",
    "    print(\"SKIPPING Phase 4: Phase 3 gate check failed\")\n",
    "    print(\"Alignment score did not meet target. Debug contrastive training before proceeding.\")\n",
    "    phase4_results = None\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 4: FINE-TUNING WITH ALIGNED ENCODERS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Configuration for Phase 4\n",
    "    PHASE4_CONFIG = {\n",
    "        'epochs': 20,                # Full training: 20 epochs\n",
    "        'batch_size': 8,\n",
    "        'unfreeze_encoder_epoch': 5, # Start unfreezing encoders after epoch 5\n",
    "        'encoder_lr_scale': 0.1,     # 10x lower LR for encoders\n",
    "        'target_technical_r': 0.50,  # Phase 4 gate criterion\n",
    "        'target_interpretive_r': 0.35,\n",
    "        'target_fusion_improvement': 15.0,  # Must beat single-modal by 15%\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nPhase 4 Config:\")\n",
    "    for k, v in PHASE4_CONFIG.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    best_fusion = phase2_gate_result['best_fusion']\n",
    "    print(f\"\\nUsing best fusion type from Phase 2: {best_fusion}\")\n",
    "    print(f\"Loading contrastive-pretrained weights from: {best_contrastive_ckpt}\")\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    phase4_ckpt_dir = Path(f'{CHECKPOINT_ROOT}/phase4_{best_fusion}')\n",
    "    phase4_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check for existing checkpoint to resume\n",
    "    resume_ckpt = find_latest_checkpoint(phase4_ckpt_dir)\n",
    "    if resume_ckpt:\n",
    "        print(f\"Found checkpoint to resume: {resume_ckpt}\")\n",
    "    \n",
    "    # Load contrastive model to get projection head weights\n",
    "    print(\"\\nLoading contrastive-pretrained projection heads...\")\n",
    "    contrastive_state = torch.load(best_contrastive_ckpt, map_location='cpu')\n",
    "    \n",
    "    # Extract projection head weights\n",
    "    projection_weights = {}\n",
    "    for key, value in contrastive_state['state_dict'].items():\n",
    "        if 'audio_projection' in key or 'midi_projection' in key:\n",
    "            projection_weights[key] = value\n",
    "    print(f\"Loaded {len(projection_weights)} projection head parameters\")\n",
    "    \n",
    "    # Create model for Phase 4\n",
    "    phase4_model = PerformanceEvaluationModel(\n",
    "        audio_dim=CONFIG['audio_dim'],\n",
    "        midi_dim=CONFIG['midi_dim'],\n",
    "        shared_dim=CONFIG['shared_dim'],\n",
    "        aggregator_dim=512,\n",
    "        num_dimensions=len(CONFIG['dimensions']),\n",
    "        dimension_names=CONFIG['dimensions'],\n",
    "        modality=\"fusion\",\n",
    "        fusion_type=best_fusion,\n",
    "        use_projection=True,\n",
    "        freeze_audio_encoder=False,  # Will unfreeze gradually\n",
    "        gradient_checkpointing=True,\n",
    "        midi_pretrained_checkpoint=midi_pretrained_local,\n",
    "        # Loss weights - include contrastive to maintain alignment\n",
    "        mse_weight=CONFIG['mse_weight'],\n",
    "        ranking_weight=CONFIG['ranking_weight'],\n",
    "        contrastive_weight=CONFIG['contrastive_weight'],\n",
    "        # Base loss\n",
    "        base_loss=CONFIG['base_loss'],\n",
    "        huber_delta=CONFIG['huber_delta'],\n",
    "        # LDS\n",
    "        lds_enabled=CONFIG['lds_enabled'],\n",
    "        lds_num_bins=CONFIG['lds_num_bins'],\n",
    "        lds_sigma=CONFIG['lds_sigma'],\n",
    "        lds_reweight_scale=CONFIG['lds_reweight_scale'],\n",
    "        # FDS\n",
    "        fds_enabled=CONFIG['fds_enabled'],\n",
    "        fds_num_bins=CONFIG['fds_num_bins'],\n",
    "        fds_momentum=CONFIG['fds_momentum'],\n",
    "        fds_kernel_sigma=CONFIG['fds_kernel_sigma'],\n",
    "        fds_start_epoch=CONFIG['fds_start_epoch'],\n",
    "        # CORAL\n",
    "        coral_enabled=CONFIG['coral_enabled'],\n",
    "        coral_num_classes=CONFIG['coral_num_classes'],\n",
    "        coral_weight=CONFIG['coral_weight'],\n",
    "        # Bootstrap\n",
    "        bootstrap_enabled=CONFIG['bootstrap_enabled'],\n",
    "        bootstrap_beta=CONFIG['bootstrap_beta'],\n",
    "        bootstrap_warmup_epochs=CONFIG['bootstrap_warmup_epochs'],\n",
    "        # Training hyperparameters\n",
    "        backbone_lr=CONFIG['backbone_lr'] * PHASE4_CONFIG['encoder_lr_scale'],  # Lower LR for encoders\n",
    "        heads_lr=CONFIG['heads_lr'],\n",
    "        warmup_steps=len(train_loader) * 2,\n",
    "        max_epochs=PHASE4_CONFIG['epochs'],\n",
    "    )\n",
    "    \n",
    "    # Fit LDS if enabled\n",
    "    if CONFIG['lds_enabled']:\n",
    "        phase4_model.fit_lds(all_train_labels)\n",
    "    \n",
    "    # Load projection head weights from contrastive pre-training\n",
    "    missing, unexpected = phase4_model.load_state_dict(projection_weights, strict=False)\n",
    "    print(f\"Loaded projection heads: {len(projection_weights) - len(missing)} params\")\n",
    "    \n",
    "    # Freeze encoders initially\n",
    "    print(\"\\nFreezing encoders for initial training...\")\n",
    "    if phase4_model.audio_encoder is not None:\n",
    "        for param in phase4_model.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "    if phase4_model.midi_encoder is not None:\n",
    "        for param in phase4_model.midi_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable = sum(p.numel() for p in phase4_model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in phase4_model.parameters())\n",
    "    print(f\"Initial trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\n",
    "    \n",
    "    # Callbacks\n",
    "    phase4_callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=str(phase4_ckpt_dir),\n",
    "            filename=f'phase4_{best_fusion}-{{epoch:02d}}-{{val_loss:.4f}}',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_top_k=2,\n",
    "            save_last=True,\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,  # More patience for longer training\n",
    "            mode='min',\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "    ]\n",
    "    \n",
    "    # Add staged unfreezing callback\n",
    "    phase4_unfreezing_schedule = [\n",
    "        {'epoch': 0, 'freeze': ['audio_encoder', 'midi_encoder'], 'unfreeze': ['projection', 'fusion', 'heads']},\n",
    "        {'epoch': PHASE4_CONFIG['unfreeze_encoder_epoch'], 'unfreeze': ['audio_encoder.top_4', 'midi_encoder.top_2'], 'lr_scale': 0.1},\n",
    "        {'epoch': PHASE4_CONFIG['unfreeze_encoder_epoch'] + 5, 'unfreeze': ['audio_encoder', 'midi_encoder'], 'lr_scale': 0.05},\n",
    "    ]\n",
    "    phase4_callbacks.append(StagedUnfreezingCallback(\n",
    "        schedule=phase4_unfreezing_schedule,\n",
    "        verbose=True,\n",
    "    ))\n",
    "    \n",
    "    # Trainer\n",
    "    phase4_trainer = pl.Trainer(\n",
    "        max_epochs=PHASE4_CONFIG['epochs'],\n",
    "        precision='16-mixed',\n",
    "        accelerator='auto',\n",
    "        devices='auto',\n",
    "        callbacks=phase4_callbacks,\n",
    "        logger=TensorBoardLogger(save_dir='logs', name=f'phase4_{best_fusion}'),\n",
    "        log_every_n_steps=50,\n",
    "        gradient_clip_val=1.0,\n",
    "        accumulate_grad_batches=2,\n",
    "        val_check_interval=0.5,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nStarting Phase 4 training...\")\n",
    "    phase4_trainer.fit(phase4_model, train_loader, val_loader, ckpt_path=resume_ckpt)\n",
    "    \n",
    "    # Get best checkpoint\n",
    "    best_phase4_ckpt = phase4_callbacks[0].best_model_path\n",
    "    best_phase4_val_loss = float(phase4_callbacks[0].best_model_score) if phase4_callbacks[0].best_model_score else None\n",
    "    \n",
    "    print(f\"\\nBest Phase 4 checkpoint: {best_phase4_ckpt}\")\n",
    "    print(f\"Best val loss: {best_phase4_val_loss:.4f}\" if best_phase4_val_loss else \"N/A\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    phase4_test_results = phase4_trainer.test(phase4_model, test_loader, ckpt_path=best_phase4_ckpt)[0]\n",
    "    \n",
    "    # Calculate summary metrics\n",
    "    technical_dims = ['note_accuracy', 'rhythmic_stability', 'articulation_clarity', 'pedal_technique']\n",
    "    interpretive_dims = ['musical_expression', 'overall_interpretation']\n",
    "    \n",
    "    technical_r = np.mean([phase4_test_results.get(f'test_pearson_{d}', 0) for d in technical_dims])\n",
    "    interpretive_r = np.mean([phase4_test_results.get(f'test_pearson_{d}', 0) for d in interpretive_dims])\n",
    "    mean_r = np.mean([phase4_test_results.get(f'test_pearson_{d}', 0) for d in CONFIG['dimensions']])\n",
    "    mean_mae = np.mean([phase4_test_results.get(f'test_mae_{d}', 0) for d in CONFIG['dimensions']])\n",
    "    \n",
    "    # Sync to Google Drive\n",
    "    print(\"\\nSyncing Phase 4 checkpoint to Google Drive...\")\n",
    "    !rclone copy {phase4_ckpt_dir} {GDRIVE_CHECKPOINT_PATH}/phase4_{best_fusion} --progress\n",
    "    \n",
    "    # Cleanup\n",
    "    del phase4_model, phase4_trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Store results\n",
    "    phase4_results = {\n",
    "        'fusion_type': best_fusion,\n",
    "        'checkpoint': best_phase4_ckpt,\n",
    "        'val_loss': best_phase4_val_loss,\n",
    "        'test_metrics': phase4_test_results,\n",
    "        'technical_r': technical_r,\n",
    "        'interpretive_r': interpretive_r,\n",
    "        'mean_pearson': mean_r,\n",
    "        'mean_mae': mean_mae,\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPhase 4 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4 Gate Check\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 4 GATE CHECK (TRAINING_PLAN_v2.md)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not PHASE3_PASSED or phase4_results is None:\n",
    "    print(\"Phase 3 did not pass or Phase 4 was skipped\")\n",
    "    PHASE4_PASSED = False\n",
    "else:\n",
    "    technical_target = PHASE4_CONFIG['target_technical_r']\n",
    "    interpretive_target = PHASE4_CONFIG['target_interpretive_r']\n",
    "    fusion_improvement_target = PHASE4_CONFIG['target_fusion_improvement']\n",
    "    \n",
    "    # Get Phase 2 best single-modal for comparison\n",
    "    phase2_best_single_r = phase2_gate_result['best_single_r']\n",
    "    actual_improvement = ((phase4_results['mean_pearson'] - phase2_best_single_r) / phase2_best_single_r * 100)\n",
    "    \n",
    "    print(f\"\\nPhase 4 Results:\")\n",
    "    print(f\"  Technical dimensions: r = {phase4_results['technical_r']:.4f} (target: >= {technical_target})\")\n",
    "    print(f\"  Interpretive dimensions: r = {phase4_results['interpretive_r']:.4f} (target: >= {interpretive_target})\")\n",
    "    print(f\"  Improvement over single-modal: {actual_improvement:+.1f}% (target: >= {fusion_improvement_target}%)\")\n",
    "    \n",
    "    technical_passed = phase4_results['technical_r'] >= technical_target\n",
    "    interpretive_passed = phase4_results['interpretive_r'] >= interpretive_target\n",
    "    improvement_passed = actual_improvement >= fusion_improvement_target\n",
    "    \n",
    "    PHASE4_PASSED = technical_passed and interpretive_passed and improvement_passed\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    if PHASE4_PASSED:\n",
    "        print(\"PHASE 4 GATE: PASS\")\n",
    "        print(f\"{'='*40}\")\n",
    "        print(\"All targets met!\")\n",
    "        print(\"-> Ready for Phase 5: Expert Validation\")\n",
    "        print(f\"\\nRecommended checkpoint: {phase4_results['checkpoint']}\")\n",
    "    else:\n",
    "        print(\"PHASE 4 GATE: FAIL\")\n",
    "        print(f\"{'='*40}\")\n",
    "        print(\"Targets not met:\")\n",
    "        if not technical_passed:\n",
    "            print(f\"  - Technical r ({phase4_results['technical_r']:.3f}) < target ({technical_target})\")\n",
    "        if not interpretive_passed:\n",
    "            print(f\"  - Interpretive r ({phase4_results['interpretive_r']:.3f}) < target ({interpretive_target})\")\n",
    "        if not improvement_passed:\n",
    "            print(f\"  - Improvement ({actual_improvement:.1f}%) < target ({fusion_improvement_target}%)\")\n",
    "        print(\"\\n-> Consider:\")\n",
    "        print(\"  1. More training epochs\")\n",
    "        print(\"  2. Different fusion architecture\")\n",
    "        print(\"  3. Verify degradation labels quality\")\n",
    "        print(\"  4. Check data distribution\")\n",
    "\n",
    "# Store gate result\n",
    "phase4_gate_result = {\n",
    "    'passed': PHASE4_PASSED,\n",
    "    'technical_r': phase4_results['technical_r'] if phase4_results else None,\n",
    "    'interpretive_r': phase4_results['interpretive_r'] if phase4_results else None,\n",
    "    'improvement_over_single_modal': actual_improvement if phase4_results else None,\n",
    "    'targets': {\n",
    "        'technical': technical_target if 'technical_target' in dir() else 0.50,\n",
    "        'interpretive': interpretive_target if 'interpretive_target' in dir() else 0.35,\n",
    "        'improvement': fusion_improvement_target if 'fusion_improvement_target' in dir() else 15.0,\n",
    "    },\n",
    "    'checkpoint': phase4_results['checkpoint'] if phase4_results else None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save All Results\n",
    "\n",
    "Save comprehensive results including:\n",
    "- All 5 Phase 2 models (audio_only, midi_only, crossattn, gated, concat)\n",
    "- Phase 2 gate check results\n",
    "- Phase 3 contrastive pre-training results (if completed)\n",
    "- Phase 4 fine-tuning results (if completed)\n",
    "- Ablation study results (if completed)\n",
    "- Configuration and hyperparameters\n",
    "\n",
    "All results are saved to:\n",
    "- Local: `/tmp/checkpoints/`\n",
    "- Google Drive: `gdrive:crescendai_checkpoints/fusion_comparison/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Save All Results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING ALL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compile comprehensive results\n",
    "final_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    \n",
    "    # Training improvements applied\n",
    "    'training_improvements': {\n",
    "        'base_loss': CONFIG['base_loss'],\n",
    "        'huber_delta': CONFIG['huber_delta'],\n",
    "        'lds_enabled': CONFIG['lds_enabled'],\n",
    "        'fds_enabled': CONFIG['fds_enabled'],\n",
    "        'coral_enabled': CONFIG['coral_enabled'],\n",
    "        'bootstrap_enabled': CONFIG['bootstrap_enabled'],\n",
    "        'modality_dropout_enabled': CONFIG['modality_dropout']['enabled'],\n",
    "        'staged_unfreezing_enabled': CONFIG['staged_unfreezing']['enabled'],\n",
    "    },\n",
    "    \n",
    "    # Phase 2: All 5 models\n",
    "    'phase2_models': {},\n",
    "    'phase2_gate': phase2_gate_result if 'phase2_gate_result' in dir() else None,\n",
    "    \n",
    "    # Phase 3: Contrastive pre-training\n",
    "    'phase3_gate': phase3_gate_result if 'phase3_gate_result' in dir() else None,\n",
    "    \n",
    "    # Phase 4: Fine-tuning\n",
    "    'phase4_results': phase4_results if 'phase4_results' in dir() else None,\n",
    "    \n",
    "    # Ablation study\n",
    "    'ablation_results': ablation_results if 'ablation_results' in dir() and ablation_results else None,\n",
    "}\n",
    "\n",
    "# Add all Phase 2 model results\n",
    "single_modal_types = ['audio_only', 'midi_only']\n",
    "all_model_types = single_modal_types + CONFIG['fusion_types']\n",
    "\n",
    "for model_type in all_model_types:\n",
    "    if model_type in phase2_results:\n",
    "        # Get checkpoint path\n",
    "        if model_type in trained_models:\n",
    "            ckpt_path = trained_models[model_type].get('best_checkpoint')\n",
    "            val_loss = trained_models[model_type].get('best_val_loss')\n",
    "        else:\n",
    "            ckpt_path = None\n",
    "            val_loss = None\n",
    "        \n",
    "        # Calculate summary metrics\n",
    "        mean_r = np.mean([phase2_results[model_type].get(f'test_pearson_{d}', 0) for d in CONFIG['dimensions']])\n",
    "        mean_mae = np.mean([phase2_results[model_type].get(f'test_mae_{d}', 0) for d in CONFIG['dimensions']])\n",
    "        \n",
    "        final_results['phase2_models'][model_type] = {\n",
    "            'checkpoint': ckpt_path,\n",
    "            'val_loss': val_loss,\n",
    "            'metrics': phase2_results[model_type],\n",
    "            'mean_pearson': mean_r,\n",
    "            'mean_mae': mean_mae,\n",
    "            'type': 'single_modal' if model_type in single_modal_types else 'fusion',\n",
    "        }\n",
    "\n",
    "# Save to local file\n",
    "results_path = f'{CHECKPOINT_ROOT}/comprehensive_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "\n",
    "# Also save ablation results separately if they exist\n",
    "if ablation_results:\n",
    "    ablation_path = f'{CHECKPOINT_ROOT}/ablation_results.json'\n",
    "    with open(ablation_path, 'w') as f:\n",
    "        json.dump(ablation_results, f, indent=2, default=str)\n",
    "    print(f\"Ablation results saved to: {ablation_path}\")\n",
    "\n",
    "# Sync to Google Drive\n",
    "print(\"\\nSyncing all results to Google Drive...\")\n",
    "!rclone copy {CHECKPOINT_ROOT} {GDRIVE_CHECKPOINT_PATH} --progress\n",
    "print(\"Sync complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY TABLE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\n\" + \"-\"*90)\n",
    "print(\"PHASE 2: MODEL COMPARISON (5 Models)\")\n",
    "print(\"-\"*90)\n",
    "print(f\"{'Model':<15} {'Mean r':>10} {'Mean MAE':>12} {'Type':<15} {'Status'}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for model_type in all_model_types:\n",
    "    if model_type in final_results['phase2_models']:\n",
    "        m = final_results['phase2_models'][model_type]\n",
    "        status = \"BEST\" if model_type == phase2_gate_result.get('best_fusion') or model_type == phase2_gate_result.get('best_single_modal') else \"\"\n",
    "        print(f\"{model_type:<15} {m['mean_pearson']:>10.4f} {m['mean_mae']:>12.2f} {m['type']:<15} {status}\")\n",
    "\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Phase 2 gate result\n",
    "if phase2_gate_result:\n",
    "    print(f\"\\nPhase 2 Gate: {'PASS' if phase2_gate_result['passed'] else 'FAIL'}\")\n",
    "    print(f\"  Best fusion: {phase2_gate_result['best_fusion']} (r = {phase2_gate_result['best_fusion_r']:.4f})\")\n",
    "    print(f\"  Best single-modal: {phase2_gate_result['best_single_modal']} (r = {phase2_gate_result['best_single_r']:.4f})\")\n",
    "    print(f\"  Improvement: {phase2_gate_result['improvement']:+.1f}% (target: >= {CONFIG['fusion_improvement_target']:.0f}%)\")\n",
    "\n",
    "# Phase 3 results\n",
    "print(\"\\n\" + \"-\"*90)\n",
    "print(\"PHASE 3: CONTRASTIVE PRE-TRAINING\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "if 'phase3_gate_result' in dir() and phase3_gate_result and phase3_gate_result.get('alignment_score') is not None:\n",
    "    print(f\"Alignment score: {phase3_gate_result['alignment_score']:.4f} (target: >= {phase3_gate_result['target']})\")\n",
    "    print(f\"Gate: {'PASS' if phase3_gate_result['passed'] else 'FAIL'}\")\n",
    "    if phase3_gate_result.get('checkpoint'):\n",
    "        print(f\"Checkpoint: {phase3_gate_result['checkpoint']}\")\n",
    "else:\n",
    "    print(\"Not completed (Phase 2 did not pass or skipped)\")\n",
    "\n",
    "# Phase 4 results\n",
    "print(\"\\n\" + \"-\"*90)\n",
    "print(\"PHASE 4: FINE-TUNING WITH ALIGNED ENCODERS\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "if 'phase4_results' in dir() and phase4_results:\n",
    "    print(f\"Mean Pearson r: {phase4_results['mean_pearson']:.4f}\")\n",
    "    print(f\"Mean MAE: {phase4_results['mean_mae']:.2f}\")\n",
    "    \n",
    "    # Improvement over Phase 2\n",
    "    phase2_best = phase2_gate_result['best_fusion_r'] if phase2_gate_result else 0\n",
    "    if phase2_best > 0:\n",
    "        improvement = ((phase4_results['mean_pearson'] - phase2_best) / phase2_best * 100)\n",
    "        print(f\"Improvement over Phase 2: {improvement:+.1f}%\")\n",
    "    \n",
    "    print(f\"Checkpoint: {phase4_results['checkpoint']}\")\n",
    "else:\n",
    "    print(\"Not completed (Phase 3 did not pass or skipped)\")\n",
    "\n",
    "# Ablation summary\n",
    "if ablation_results:\n",
    "    print(\"\\n\" + \"-\"*90)\n",
    "    print(\"ABLATION STUDY SUMMARY\")\n",
    "    print(\"-\"*90)\n",
    "    \n",
    "    full_r = ablation_results['full']['mean_pearson']\n",
    "    impacts = [(name, full_r - res['mean_pearson']) \n",
    "               for name, res in ablation_results.items() if name != 'full']\n",
    "    sorted_impacts = sorted(impacts, key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    print(f\"{'Component':<25} {'Impact':>12} {'Effect'}\")\n",
    "    for name, impact in sorted_impacts[:5]:  # Top 5 impacts\n",
    "        effect = \"HELPS\" if impact > 0 else \"HURTS\"\n",
    "        print(f\"{name:<25} {abs(impact):>12.3f} {effect}\")\n",
    "\n",
    "# Best model recommendation\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "if 'phase4_results' in dir() and phase4_results:\n",
    "    print(f\"\\nBest model: Phase 4 fine-tuned ({phase2_gate_result['best_fusion']})\")\n",
    "    print(f\"  Checkpoint: {phase4_results['checkpoint']}\")\n",
    "    print(f\"  Performance: r = {phase4_results['mean_pearson']:.4f}\")\n",
    "elif phase2_gate_result and phase2_gate_result['passed']:\n",
    "    print(f\"\\nBest model: {phase2_gate_result['best_fusion']} fusion\")\n",
    "    if phase2_gate_result['best_fusion'] in trained_models:\n",
    "        print(f\"  Checkpoint: {trained_models[phase2_gate_result['best_fusion']]['best_checkpoint']}\")\n",
    "    print(f\"  Performance: r = {phase2_gate_result['best_fusion_r']:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nBest model: {phase2_gate_result['best_single_modal']} (single-modal)\")\n",
    "    print(\"  NOTE: Fusion did not beat single-modal - architecture needs debugging\")\n",
    "\n",
    "print(f\"\\nAll checkpoints synced to: {GDRIVE_CHECKPOINT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
