{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Audio Baseline Experiments\n",
    "\n",
    "Comprehensive experiments for the ISMIR paper.\n",
    "\n",
    "## Experiments\n",
    "- **B0**: Baseline re-run (MERT+MLP, L13-24, mean pool)\n",
    "- **A1-A3**: Baselines (linear probe, Mel-CNN, raw statistics)\n",
    "- **B1a-B1d**: Layer ablation (1-6, 7-12, 13-24, 1-24)\n",
    "- **B2a-B2c**: Pooling ablation (max, attention, LSTM)\n",
    "- **C1a-C1b**: Loss ablation (hybrid MSE+CCC, pure CCC)\n",
    "\n",
    "## Requirements\n",
    "- Compute: A100 (80GB VRAM)\n",
    "- rclone configured with `gdrive:` remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA deterministic mode (must be before any CUDA operations)\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"GPU required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies and clone repo\n",
    "!pip install transformers librosa soundfile pytorch_lightning nnAudio --quiet\n",
    "\n",
    "# Clone the repo\n",
    "import os\n",
    "REPO_DIR = '/tmp/crescendai'\n",
    "if os.path.exists(REPO_DIR):\n",
    "    !cd {REPO_DIR} && git pull origin main\n",
    "else:\n",
    "    !git clone https://github.com/jai-dhiman/crescendai.git {REPO_DIR}\n",
    "\n",
    "print(f\"Repo: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Setup imports\n",
    "import sys\n",
    "sys.path.insert(0, f'{REPO_DIR}/model/src')\n",
    "\n",
    "import json\n",
    "import subprocess\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Import from our package\n",
    "from audio_experiments import PERCEPIANO_DIMENSIONS, BASE_CONFIG, SEED\n",
    "from audio_experiments.extractors import (\n",
    "    extract_mert_for_layer_range,\n",
    "    extract_mel_spectrograms,\n",
    "    extract_statistics_for_all,\n",
    ")\n",
    "from audio_experiments.models import BaseMERTModel, LinearProbeModel, MelCNNModel, StatsMLPModel\n",
    "from audio_experiments.training import (\n",
    "    run_4fold_mert_experiment,\n",
    "    run_4fold_mel_experiment,\n",
    "    run_4fold_stats_experiment,\n",
    "    restore_all_from_gdrive,\n",
    "    should_run_experiment,\n",
    "    sync_experiment_to_gdrive,\n",
    "    print_experiment_status,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Imports: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Setup paths and download data\n",
    "DATA_ROOT = Path('/tmp/phase2')\n",
    "AUDIO_DIR = DATA_ROOT / 'audio'\n",
    "LABEL_DIR = DATA_ROOT / 'labels'\n",
    "MERT_CACHE_ROOT = DATA_ROOT / 'mert_cache'\n",
    "MEL_CACHE_DIR = DATA_ROOT / 'mel_cache'\n",
    "STATS_CACHE_DIR = DATA_ROOT / 'stats_cache'\n",
    "CHECKPOINT_ROOT = DATA_ROOT / 'checkpoints'\n",
    "RESULTS_DIR = DATA_ROOT / 'results'\n",
    "LOG_DIR = DATA_ROOT / 'logs'\n",
    "\n",
    "GDRIVE_AUDIO = 'gdrive:crescendai_data/audio_baseline/percepiano_rendered'\n",
    "GDRIVE_LABELS = 'gdrive:crescendai_data/percepiano_labels'\n",
    "GDRIVE_FOLDS = 'gdrive:crescendai_data/audio_baseline/audio_fold_assignments.json'\n",
    "GDRIVE_MERT_CACHE = 'gdrive:crescendai_data/audio_baseline/mert_embeddings'\n",
    "GDRIVE_RESULTS = 'gdrive:crescendai_data/checkpoints/audio_phase2'\n",
    "\n",
    "for d in [AUDIO_DIR, LABEL_DIR, MERT_CACHE_ROOT, MEL_CACHE_DIR, STATS_CACHE_DIR,\n",
    "          CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_rclone(cmd, desc):\n",
    "    print(f\"{desc}...\")\n",
    "    subprocess.run(cmd, capture_output=True)\n",
    "\n",
    "# Check rclone\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "if 'gdrive:' not in result.stdout:\n",
    "    raise RuntimeError(\"rclone 'gdrive' not configured\")\n",
    "\n",
    "# Download data\n",
    "run_rclone(['rclone', 'copy', GDRIVE_AUDIO, str(AUDIO_DIR), '--progress'], \"Downloading audio\")\n",
    "run_rclone(['rclone', 'copy', GDRIVE_LABELS, str(LABEL_DIR)], \"Downloading labels\")\n",
    "\n",
    "FOLD_FILE = DATA_ROOT / 'folds.json'\n",
    "run_rclone(['rclone', 'copyto', GDRIVE_FOLDS, str(FOLD_FILE)], \"Downloading folds\")\n",
    "\n",
    "# Load labels and folds\n",
    "LABEL_FILE = LABEL_DIR / 'label_2round_mean_reg_19_with0_rm_highstd0.json'\n",
    "with open(LABEL_FILE) as f:\n",
    "    LABELS = json.load(f)\n",
    "with open(FOLD_FILE) as f:\n",
    "    FOLD_ASSIGNMENTS = json.load(f)\n",
    "\n",
    "ALL_KEYS = list(LABELS.keys())\n",
    "print(f\"Audio: {len(list(AUDIO_DIR.glob('*.wav')))} files\")\n",
    "print(f\"Labels: {len(LABELS)} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Restore MERT cache and completed experiments from GDrive\n",
    "DEFAULT_MERT_DIR = MERT_CACHE_ROOT / 'L13-24'\n",
    "DEFAULT_MERT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "result = subprocess.run(['rclone', 'lsf', GDRIVE_MERT_CACHE], capture_output=True, text=True)\n",
    "if result.returncode == 0 and '.pt' in result.stdout:\n",
    "    print(\"Restoring MERT cache...\")\n",
    "    run_rclone(['rclone', 'copy', GDRIVE_MERT_CACHE, str(DEFAULT_MERT_DIR)], \"Restoring cache\")\n",
    "    print(f\"Restored: {len(list(DEFAULT_MERT_DIR.glob('*.pt')))} embeddings\")\n",
    "\n",
    "ALL_RESULTS = {}\n",
    "\n",
    "ALL_EXPERIMENT_IDS = [\n",
    "    'B0_baseline', 'A1_linear_probe', 'A2_mel_cnn', 'A3_raw_stats',\n",
    "    'B1a_layers_1-6', 'B1b_layers_7-12', 'B1c_layers_13-24', 'B1d_layers_1-24',\n",
    "    'B2a_max_pool', 'B2b_attention_pool', 'B2c_lstm_pool',\n",
    "    'C1a_hybrid_loss', 'C1b_pure_ccc',\n",
    "]\n",
    "\n",
    "print(\"\\nChecking GDrive for completed experiments...\")\n",
    "restored = restore_all_from_gdrive(\n",
    "    GDRIVE_RESULTS,\n",
    "    RESULTS_DIR,\n",
    "    CHECKPOINT_ROOT,\n",
    "    ALL_RESULTS,\n",
    ")\n",
    "\n",
    "# Cache completed experiments to avoid repeated GDrive calls\n",
    "from audio_experiments.training import get_completed_experiments\n",
    "COMPLETED_CACHE = get_completed_experiments(GDRIVE_RESULTS)\n",
    "\n",
    "print_experiment_status(ALL_EXPERIMENT_IDS, COMPLETED_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B0: Baseline\n",
    "if should_run_experiment('B0_baseline', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg.get('pooling', 'mean'),\n",
    "            loss_type=cfg.get('loss_type', 'mse'), max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B0_baseline'] = run_4fold_mert_experiment(\n",
    "        'B0_baseline', 'MERT+MLP, L13-24, mean pooling',\n",
    "        make_mert_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B0_baseline', ALL_RESULTS['B0_baseline'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1: Linear Probe\n",
    "if should_run_experiment('A1_linear_probe', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure embeddings exist (reuses B0's extraction if already done)\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    def make_linear_probe(cfg):\n",
    "        return LinearProbeModel(\n",
    "            input_dim=cfg['input_dim'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['A1_linear_probe'] = run_4fold_mert_experiment(\n",
    "        'A1_linear_probe', 'Linear probe on MERT',\n",
    "        make_linear_probe, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'A1_linear_probe', ALL_RESULTS['A1_linear_probe'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2: Mel-CNN\n",
    "if should_run_experiment('A2_mel_cnn', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    extract_mel_spectrograms(AUDIO_DIR, MEL_CACHE_DIR, ALL_KEYS)\n",
    "\n",
    "    ALL_RESULTS['A2_mel_cnn'] = run_4fold_mel_experiment(\n",
    "        'A2_mel_cnn', '4-layer CNN on mel spectrograms',\n",
    "        MEL_CACHE_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'A2_mel_cnn', ALL_RESULTS['A2_mel_cnn'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3: Raw Statistics\n",
    "if should_run_experiment('A3_raw_stats', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    extract_statistics_for_all(AUDIO_DIR, STATS_CACHE_DIR, ALL_KEYS)\n",
    "\n",
    "    ALL_RESULTS['A3_raw_stats'] = run_4fold_stats_experiment(\n",
    "        'A3_raw_stats', 'MLP on audio statistics (49-dim)',\n",
    "        STATS_CACHE_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'A3_raw_stats', ALL_RESULTS['A3_raw_stats'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1a: Layer Ablation - Early Layers (1-6)\n",
    "if should_run_experiment('B1a_layers_1-6', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    cache_dir = MERT_CACHE_ROOT / 'L1-6'\n",
    "    extract_mert_for_layer_range(1, 7, AUDIO_DIR, cache_dir, ALL_KEYS)\n",
    "\n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg.get('pooling', 'mean'),\n",
    "            loss_type=cfg.get('loss_type', 'mse'), max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B1a_layers_1-6'] = run_4fold_mert_experiment(\n",
    "        'B1a_layers_1-6', 'MERT layers 1-6 (early)',\n",
    "        make_mert_model, cache_dir, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B1a_layers_1-6', ALL_RESULTS['B1a_layers_1-6'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1b: Layer Ablation - Mid Layers (7-12)\n",
    "if should_run_experiment('B1b_layers_7-12', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    cache_dir = MERT_CACHE_ROOT / 'L7-12'\n",
    "    extract_mert_for_layer_range(7, 13, AUDIO_DIR, cache_dir, ALL_KEYS)\n",
    "\n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg.get('pooling', 'mean'),\n",
    "            loss_type=cfg.get('loss_type', 'mse'), max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B1b_layers_7-12'] = run_4fold_mert_experiment(\n",
    "        'B1b_layers_7-12', 'MERT layers 7-12 (mid)',\n",
    "        make_mert_model, cache_dir, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B1b_layers_7-12', ALL_RESULTS['B1b_layers_7-12'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1c: Layer Ablation - Late Layers (13-24)\n",
    "if should_run_experiment('B1c_layers_13-24', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    cache_dir = MERT_CACHE_ROOT / 'L13-24'\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, cache_dir, ALL_KEYS)\n",
    "\n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg.get('pooling', 'mean'),\n",
    "            loss_type=cfg.get('loss_type', 'mse'), max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B1c_layers_13-24'] = run_4fold_mert_experiment(\n",
    "        'B1c_layers_13-24', 'MERT layers 13-24 (late)',\n",
    "        make_mert_model, cache_dir, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B1c_layers_13-24', ALL_RESULTS['B1c_layers_13-24'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1d: Layer Ablation - All Layers (1-24)\n",
    "if should_run_experiment('B1d_layers_1-24', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    cache_dir = MERT_CACHE_ROOT / 'L1-24'\n",
    "    extract_mert_for_layer_range(1, 25, AUDIO_DIR, cache_dir, ALL_KEYS)\n",
    "\n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg.get('pooling', 'mean'),\n",
    "            loss_type=cfg.get('loss_type', 'mse'), max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B1d_layers_1-24'] = run_4fold_mert_experiment(\n",
    "        'B1d_layers_1-24', 'MERT all layers 1-24',\n",
    "        make_mert_model, cache_dir, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B1d_layers_1-24', ALL_RESULTS['B1d_layers_1-24'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2a: Pooling Ablation - Max Pooling\n",
    "if should_run_experiment('B2a_max_pool', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure L13-24 embeddings exist\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    cfg = BASE_CONFIG.copy()\n",
    "    cfg['pooling'] = 'max'\n",
    "\n",
    "    def make_max_pool_model(cfg=cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg['pooling'],\n",
    "            loss_type='mse', max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B2a_max_pool'] = run_4fold_mert_experiment(\n",
    "        'B2a_max_pool', 'MERT + max pooling',\n",
    "        make_max_pool_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B2a_max_pool', ALL_RESULTS['B2a_max_pool'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2b: Pooling Ablation - Attention Pooling\n",
    "if should_run_experiment('B2b_attention_pool', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure L13-24 embeddings exist\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    cfg = BASE_CONFIG.copy()\n",
    "    cfg['pooling'] = 'attention'\n",
    "\n",
    "    def make_attention_pool_model(cfg=cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg['pooling'],\n",
    "            loss_type='mse', max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B2b_attention_pool'] = run_4fold_mert_experiment(\n",
    "        'B2b_attention_pool', 'MERT + attention pooling',\n",
    "        make_attention_pool_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B2b_attention_pool', ALL_RESULTS['B2b_attention_pool'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2c: Pooling Ablation - Bi-LSTM Pooling\n",
    "if should_run_experiment('B2c_lstm_pool', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure L13-24 embeddings exist\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    cfg = BASE_CONFIG.copy()\n",
    "    cfg['pooling'] = 'lstm'\n",
    "\n",
    "    def make_lstm_pool_model(cfg=cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg['pooling'],\n",
    "            loss_type='mse', max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B2c_lstm_pool'] = run_4fold_mert_experiment(\n",
    "        'B2c_lstm_pool', 'MERT + Bi-LSTM pooling',\n",
    "        make_lstm_pool_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B2c_lstm_pool', ALL_RESULTS['B2c_lstm_pool'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1a: Loss Ablation - Hybrid MSE + CCC Loss\n",
    "if should_run_experiment('C1a_hybrid_loss', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure L13-24 embeddings exist\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    cfg = BASE_CONFIG.copy()\n",
    "    cfg['loss_type'] = 'hybrid'\n",
    "\n",
    "    def make_hybrid_loss_model(cfg=cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling='mean',\n",
    "            loss_type=cfg['loss_type'], max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['C1a_hybrid_loss'] = run_4fold_mert_experiment(\n",
    "        'C1a_hybrid_loss', 'MERT + MSE + 0.5*CCC loss',\n",
    "        make_hybrid_loss_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'C1a_hybrid_loss', ALL_RESULTS['C1a_hybrid_loss'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1b: Loss Ablation - Pure CCC Loss\n",
    "if should_run_experiment('C1b_pure_ccc', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure L13-24 embeddings exist\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    cfg = BASE_CONFIG.copy()\n",
    "    cfg['loss_type'] = 'ccc'\n",
    "\n",
    "    def make_ccc_loss_model(cfg=cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling='mean',\n",
    "            loss_type=cfg['loss_type'], max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['C1b_pure_ccc'] = run_4fold_mert_experiment(\n",
    "        'C1b_pure_ccc', 'MERT + pure CCC loss',\n",
    "        make_ccc_loss_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'C1b_pure_ccc', ALL_RESULTS['C1b_pure_ccc'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results table\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 2 RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_r2 = ALL_RESULTS.get('B0_baseline', {}).get('summary', {}).get('avg_r2', 0)\n",
    "\n",
    "print(f\"{'Experiment':<25} {'Avg R2':>10} {'95% CI':>20} {'vs B0':>10} {'Disp':>8}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "exp_order = [\n",
    "    'B0_baseline', None,\n",
    "    'A1_linear_probe', 'A2_mel_cnn', 'A3_raw_stats', None,\n",
    "    'B1a_layers_1-6', 'B1b_layers_7-12', 'B1c_layers_13-24', 'B1d_layers_1-24', None,\n",
    "    'B2a_max_pool', 'B2b_attention_pool', 'B2c_lstm_pool', None,\n",
    "    'C1a_hybrid_loss', 'C1b_pure_ccc',\n",
    "]\n",
    "\n",
    "for exp_id in exp_order:\n",
    "    if exp_id is None:\n",
    "        print(\"-\"*80)\n",
    "        continue\n",
    "    if exp_id not in ALL_RESULTS:\n",
    "        continue\n",
    "\n",
    "    r = ALL_RESULTS[exp_id]\n",
    "    s = r['summary']\n",
    "    ci = s.get('r2_ci_95', [0, 0])\n",
    "    diff = s['avg_r2'] - baseline_r2 if exp_id != 'B0_baseline' else 0\n",
    "    diff_str = f\"{diff:+.3f}\" if exp_id != 'B0_baseline' else '---'\n",
    "\n",
    "    print(f\"{exp_id:<25} {s['avg_r2']:>10.4f} [{ci[0]:.3f}, {ci[1]:.3f}] {diff_str:>10} {s.get('dispersion_ratio', 0):>8.2f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety sync\n",
    "with open(RESULTS_DIR / 'phase2_all_results.json', 'w') as f:\n",
    "    json.dump(ALL_RESULTS, f, indent=2)\n",
    "\n",
    "print(\"Final sync to Google Drive...\")\n",
    "run_rclone(['rclone', 'copy', str(RESULTS_DIR), GDRIVE_RESULTS], \"Syncing results\")\n",
    "run_rclone(['rclone', 'copy', str(CHECKPOINT_ROOT), f\"{GDRIVE_RESULTS}/checkpoints\"], \"Syncing checkpoints\")\n",
    "\n",
    "print_experiment_status(ALL_EXPERIMENT_IDS, {k: v['summary']['avg_r2'] for k, v in ALL_RESULTS.items()})\n",
    "print(\"Done! Results at:\", GDRIVE_RESULTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
