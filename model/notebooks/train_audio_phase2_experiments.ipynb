{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Audio Baseline Experiments\n",
    "\n",
    "Comprehensive experiments for the ISMIR paper.\n",
    "\n",
    "## Experiments\n",
    "- **B0**: Baseline re-run (MERT+MLP, L13-24, mean pool)\n",
    "- **A1-A3**: Baselines (linear probe, Mel-CNN, raw statistics)\n",
    "- **B1a-B1d**: Layer ablation (1-6, 7-12, 13-24, 1-24)\n",
    "- **B2a-B2c**: Pooling ablation (max, attention, LSTM)\n",
    "- **C1a-C1b**: Loss ablation (hybrid MSE+CCC, pure CCC)\n",
    "\n",
    "## Requirements\n",
    "- Compute: A100 (80GB VRAM)\n",
    "- rclone configured with `gdrive:` remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA deterministic mode (must be before any CUDA operations)\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"GPU required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies and clone repo\n",
    "!pip install transformers librosa soundfile pytorch_lightning nnAudio --quiet\n",
    "\n",
    "# Clone the repo\n",
    "import os\n",
    "REPO_DIR = '/tmp/crescendai'\n",
    "if os.path.exists(REPO_DIR):\n",
    "    !cd {REPO_DIR} && git pull origin main\n",
    "else:\n",
    "    !git clone https://github.com/jai-dhiman/crescendai.git {REPO_DIR}\n",
    "\n",
    "print(f\"Repo: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Setup imports\n",
    "import sys\n",
    "sys.path.insert(0, f'{REPO_DIR}/model/src')\n",
    "\n",
    "import json\n",
    "import subprocess\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Import from our package\n",
    "from audio_experiments import PERCEPIANO_DIMENSIONS, BASE_CONFIG, SEED\n",
    "from audio_experiments.extractors import (\n",
    "    extract_mert_for_layer_range,\n",
    "    extract_mel_spectrograms,\n",
    "    extract_statistics_for_all,\n",
    ")\n",
    "from audio_experiments.models import BaseMERTModel, LinearProbeModel, MelCNNModel, StatsMLPModel\n",
    "from audio_experiments.training import (\n",
    "    run_4fold_mert_experiment,\n",
    "    run_4fold_mel_experiment,\n",
    "    run_4fold_stats_experiment,\n",
    "    restore_all_from_gdrive,\n",
    "    should_run_experiment,\n",
    "    sync_experiment_to_gdrive,\n",
    "    print_experiment_status,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Imports: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Setup paths and download data\n",
    "DATA_ROOT = Path('/tmp/phase2')\n",
    "AUDIO_DIR = DATA_ROOT / 'audio'\n",
    "LABEL_DIR = DATA_ROOT / 'labels'\n",
    "MERT_CACHE_ROOT = DATA_ROOT / 'mert_cache'\n",
    "MEL_CACHE_DIR = DATA_ROOT / 'mel_cache'\n",
    "STATS_CACHE_DIR = DATA_ROOT / 'stats_cache'\n",
    "CHECKPOINT_ROOT = DATA_ROOT / 'checkpoints'\n",
    "RESULTS_DIR = DATA_ROOT / 'results'\n",
    "LOG_DIR = DATA_ROOT / 'logs'\n",
    "\n",
    "GDRIVE_AUDIO = 'gdrive:crescendai_data/audio_baseline/percepiano_rendered'\n",
    "GDRIVE_LABELS = 'gdrive:crescendai_data/percepiano_labels'\n",
    "GDRIVE_FOLDS = 'gdrive:crescendai_data/audio_baseline/audio_fold_assignments.json'\n",
    "GDRIVE_MERT_CACHE = 'gdrive:crescendai_data/audio_baseline/mert_embeddings'\n",
    "GDRIVE_RESULTS = 'gdrive:crescendai_data/checkpoints/audio_phase2'\n",
    "\n",
    "for d in [AUDIO_DIR, LABEL_DIR, MERT_CACHE_ROOT, MEL_CACHE_DIR, STATS_CACHE_DIR,\n",
    "          CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_rclone(cmd, desc):\n",
    "    print(f\"{desc}...\")\n",
    "    subprocess.run(cmd, capture_output=True)\n",
    "\n",
    "# Check rclone\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "if 'gdrive:' not in result.stdout:\n",
    "    raise RuntimeError(\"rclone 'gdrive' not configured\")\n",
    "\n",
    "# Download data\n",
    "run_rclone(['rclone', 'copy', GDRIVE_AUDIO, str(AUDIO_DIR), '--progress'], \"Downloading audio\")\n",
    "run_rclone(['rclone', 'copy', GDRIVE_LABELS, str(LABEL_DIR)], \"Downloading labels\")\n",
    "\n",
    "FOLD_FILE = DATA_ROOT / 'folds.json'\n",
    "run_rclone(['rclone', 'copyto', GDRIVE_FOLDS, str(FOLD_FILE)], \"Downloading folds\")\n",
    "\n",
    "# Load labels and folds\n",
    "LABEL_FILE = LABEL_DIR / 'label_2round_mean_reg_19_with0_rm_highstd0.json'\n",
    "with open(LABEL_FILE) as f:\n",
    "    LABELS = json.load(f)\n",
    "with open(FOLD_FILE) as f:\n",
    "    FOLD_ASSIGNMENTS = json.load(f)\n",
    "\n",
    "ALL_KEYS = list(LABELS.keys())\n",
    "print(f\"Audio: {len(list(AUDIO_DIR.glob('*.wav')))} files\")\n",
    "print(f\"Labels: {len(LABELS)} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Restore MERT cache and completed experiments from GDrive\n",
    "DEFAULT_MERT_DIR = MERT_CACHE_ROOT / 'L13-24'\n",
    "DEFAULT_MERT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "result = subprocess.run(['rclone', 'lsf', GDRIVE_MERT_CACHE], capture_output=True, text=True)\n",
    "if result.returncode == 0 and '.pt' in result.stdout:\n",
    "    print(\"Restoring MERT cache...\")\n",
    "    run_rclone(['rclone', 'copy', GDRIVE_MERT_CACHE, str(DEFAULT_MERT_DIR)], \"Restoring cache\")\n",
    "    print(f\"Restored: {len(list(DEFAULT_MERT_DIR.glob('*.pt')))} embeddings\")\n",
    "\n",
    "ALL_RESULTS = {}\n",
    "\n",
    "ALL_EXPERIMENT_IDS = [\n",
    "    'B0_baseline', 'A1_linear_probe', 'A2_mel_cnn', 'A3_raw_stats',\n",
    "    'B1a_layers_1-6', 'B1b_layers_7-12', 'B1c_layers_13-24', 'B1d_layers_1-24',\n",
    "    'B2a_max_pool', 'B2b_attention_pool', 'B2c_lstm_pool',\n",
    "    'C1a_hybrid_loss', 'C1b_pure_ccc',\n",
    "]\n",
    "\n",
    "print(\"\\nChecking GDrive for completed experiments...\")\n",
    "restored = restore_all_from_gdrive(\n",
    "    GDRIVE_RESULTS,\n",
    "    RESULTS_DIR,\n",
    "    CHECKPOINT_ROOT,\n",
    "    ALL_RESULTS,\n",
    ")\n",
    "\n",
    "# Cache completed experiments to avoid repeated GDrive calls\n",
    "from audio_experiments.training import get_completed_experiments\n",
    "COMPLETED_CACHE = get_completed_experiments(GDRIVE_RESULTS)\n",
    "\n",
    "print_experiment_status(ALL_EXPERIMENT_IDS, COMPLETED_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B0: Baseline\n",
    "if should_run_experiment('B0_baseline', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg.get('pooling', 'mean'),\n",
    "            loss_type=cfg.get('loss_type', 'mse'), max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B0_baseline'] = run_4fold_mert_experiment(\n",
    "        'B0_baseline', 'MERT+MLP, L13-24, mean pooling',\n",
    "        make_mert_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B0_baseline', ALL_RESULTS['B0_baseline'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1: Linear Probe\n",
    "if should_run_experiment('A1_linear_probe', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure embeddings exist (reuses B0's extraction if already done)\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    def make_linear_probe(cfg):\n",
    "        return LinearProbeModel(\n",
    "            input_dim=cfg['input_dim'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['A1_linear_probe'] = run_4fold_mert_experiment(\n",
    "        'A1_linear_probe', 'Linear probe on MERT',\n",
    "        make_linear_probe, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'A1_linear_probe', ALL_RESULTS['A1_linear_probe'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2: Mel-CNN\n",
    "if should_run_experiment('A2_mel_cnn', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    extract_mel_spectrograms(AUDIO_DIR, MEL_CACHE_DIR, ALL_KEYS)\n",
    "\n",
    "    ALL_RESULTS['A2_mel_cnn'] = run_4fold_mel_experiment(\n",
    "        'A2_mel_cnn', '4-layer CNN on mel spectrograms',\n",
    "        MEL_CACHE_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'A2_mel_cnn', ALL_RESULTS['A2_mel_cnn'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3: Raw Statistics\n",
    "if should_run_experiment('A3_raw_stats', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    extract_statistics_for_all(AUDIO_DIR, STATS_CACHE_DIR, ALL_KEYS)\n",
    "\n",
    "    ALL_RESULTS['A3_raw_stats'] = run_4fold_stats_experiment(\n",
    "        'A3_raw_stats', 'MLP on audio statistics (49-dim)',\n",
    "        STATS_CACHE_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'A3_raw_stats', ALL_RESULTS['A3_raw_stats'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1a: Layer Ablation - Early Layers (1-6)\n",
    "if should_run_experiment('B1a_layers_1-6', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    cache_dir = MERT_CACHE_ROOT / 'L1-6'\n",
    "    extract_mert_for_layer_range(1, 7, AUDIO_DIR, cache_dir, ALL_KEYS)\n",
    "\n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg.get('pooling', 'mean'),\n",
    "            loss_type=cfg.get('loss_type', 'mse'), max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B1a_layers_1-6'] = run_4fold_mert_experiment(\n",
    "        'B1a_layers_1-6', 'MERT layers 1-6 (early)',\n",
    "        make_mert_model, cache_dir, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B1a_layers_1-6', ALL_RESULTS['B1a_layers_1-6'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1b: Layer Ablation - Mid Layers (7-12)\n",
    "if should_run_experiment('B1b_layers_7-12', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    cache_dir = MERT_CACHE_ROOT / 'L7-12'\n",
    "    extract_mert_for_layer_range(7, 13, AUDIO_DIR, cache_dir, ALL_KEYS)\n",
    "\n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg.get('pooling', 'mean'),\n",
    "            loss_type=cfg.get('loss_type', 'mse'), max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B1b_layers_7-12'] = run_4fold_mert_experiment(\n",
    "        'B1b_layers_7-12', 'MERT layers 7-12 (mid)',\n",
    "        make_mert_model, cache_dir, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B1b_layers_7-12', ALL_RESULTS['B1b_layers_7-12'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1c: Layer Ablation - Late Layers (13-24)\n",
    "if should_run_experiment('B1c_layers_13-24', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    cache_dir = MERT_CACHE_ROOT / 'L13-24'\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, cache_dir, ALL_KEYS)\n",
    "\n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg.get('pooling', 'mean'),\n",
    "            loss_type=cfg.get('loss_type', 'mse'), max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B1c_layers_13-24'] = run_4fold_mert_experiment(\n",
    "        'B1c_layers_13-24', 'MERT layers 13-24 (late)',\n",
    "        make_mert_model, cache_dir, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B1c_layers_13-24', ALL_RESULTS['B1c_layers_13-24'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1d: Layer Ablation - All Layers (1-24)\n",
    "if should_run_experiment('B1d_layers_1-24', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    cache_dir = MERT_CACHE_ROOT / 'L1-24'\n",
    "    extract_mert_for_layer_range(1, 25, AUDIO_DIR, cache_dir, ALL_KEYS)\n",
    "\n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg.get('pooling', 'mean'),\n",
    "            loss_type=cfg.get('loss_type', 'mse'), max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B1d_layers_1-24'] = run_4fold_mert_experiment(\n",
    "        'B1d_layers_1-24', 'MERT all layers 1-24',\n",
    "        make_mert_model, cache_dir, LABELS, FOLD_ASSIGNMENTS,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B1d_layers_1-24', ALL_RESULTS['B1d_layers_1-24'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2a: Pooling Ablation - Max Pooling\n",
    "if should_run_experiment('B2a_max_pool', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure L13-24 embeddings exist\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    cfg = BASE_CONFIG.copy()\n",
    "    cfg['pooling'] = 'max'\n",
    "\n",
    "    def make_max_pool_model(cfg=cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg['pooling'],\n",
    "            loss_type='mse', max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B2a_max_pool'] = run_4fold_mert_experiment(\n",
    "        'B2a_max_pool', 'MERT + max pooling',\n",
    "        make_max_pool_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B2a_max_pool', ALL_RESULTS['B2a_max_pool'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2b: Pooling Ablation - Attention Pooling\n",
    "if should_run_experiment('B2b_attention_pool', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure L13-24 embeddings exist\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    cfg = BASE_CONFIG.copy()\n",
    "    cfg['pooling'] = 'attention'\n",
    "\n",
    "    def make_attention_pool_model(cfg=cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg['pooling'],\n",
    "            loss_type='mse', max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B2b_attention_pool'] = run_4fold_mert_experiment(\n",
    "        'B2b_attention_pool', 'MERT + attention pooling',\n",
    "        make_attention_pool_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B2b_attention_pool', ALL_RESULTS['B2b_attention_pool'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2c: Pooling Ablation - Bi-LSTM Pooling\n",
    "if should_run_experiment('B2c_lstm_pool', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure L13-24 embeddings exist\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    cfg = BASE_CONFIG.copy()\n",
    "    cfg['pooling'] = 'lstm'\n",
    "\n",
    "    def make_lstm_pool_model(cfg=cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling=cfg['pooling'],\n",
    "            loss_type='mse', max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['B2c_lstm_pool'] = run_4fold_mert_experiment(\n",
    "        'B2c_lstm_pool', 'MERT + Bi-LSTM pooling',\n",
    "        make_lstm_pool_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'B2c_lstm_pool', ALL_RESULTS['B2c_lstm_pool'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1a: Loss Ablation - Hybrid MSE + CCC Loss\n",
    "if should_run_experiment('C1a_hybrid_loss', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure L13-24 embeddings exist\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    cfg = BASE_CONFIG.copy()\n",
    "    cfg['loss_type'] = 'hybrid'\n",
    "\n",
    "    def make_hybrid_loss_model(cfg=cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling='mean',\n",
    "            loss_type=cfg['loss_type'], max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['C1a_hybrid_loss'] = run_4fold_mert_experiment(\n",
    "        'C1a_hybrid_loss', 'MERT + MSE + 0.5*CCC loss',\n",
    "        make_hybrid_loss_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'C1a_hybrid_loss', ALL_RESULTS['C1a_hybrid_loss'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1b: Loss Ablation - Pure CCC Loss\n",
    "if should_run_experiment('C1b_pure_ccc', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    # Ensure L13-24 embeddings exist\n",
    "    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n",
    "\n",
    "    cfg = BASE_CONFIG.copy()\n",
    "    cfg['loss_type'] = 'ccc'\n",
    "\n",
    "    def make_ccc_loss_model(cfg=cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling='mean',\n",
    "            loss_type=cfg['loss_type'], max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "\n",
    "    ALL_RESULTS['C1b_pure_ccc'] = run_4fold_mert_experiment(\n",
    "        'C1b_pure_ccc', 'MERT + pure CCC loss',\n",
    "        make_ccc_loss_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n",
    "        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        'C1b_pure_ccc', ALL_RESULTS['C1b_pure_ccc'],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results table\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 2 RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_r2 = ALL_RESULTS.get('B0_baseline', {}).get('summary', {}).get('avg_r2', 0)\n",
    "\n",
    "print(f\"{'Experiment':<25} {'Avg R2':>10} {'95% CI':>20} {'vs B0':>10} {'Disp':>8}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "exp_order = [\n",
    "    'B0_baseline', None,\n",
    "    'A1_linear_probe', 'A2_mel_cnn', 'A3_raw_stats', None,\n",
    "    'B1a_layers_1-6', 'B1b_layers_7-12', 'B1c_layers_13-24', 'B1d_layers_1-24', None,\n",
    "    'B2a_max_pool', 'B2b_attention_pool', 'B2c_lstm_pool', None,\n",
    "    'C1a_hybrid_loss', 'C1b_pure_ccc',\n",
    "]\n",
    "\n",
    "for exp_id in exp_order:\n",
    "    if exp_id is None:\n",
    "        print(\"-\"*80)\n",
    "        continue\n",
    "    if exp_id not in ALL_RESULTS:\n",
    "        continue\n",
    "\n",
    "    r = ALL_RESULTS[exp_id]\n",
    "    s = r['summary']\n",
    "    ci = s.get('r2_ci_95', [0, 0])\n",
    "    diff = s['avg_r2'] - baseline_r2 if exp_id != 'B0_baseline' else 0\n",
    "    diff_str = f\"{diff:+.3f}\" if exp_id != 'B0_baseline' else '---'\n",
    "\n",
    "    print(f\"{exp_id:<25} {s['avg_r2']:>10.4f} [{ci[0]:.3f}, {ci[1]:.3f}] {diff_str:>10} {s.get('dispersion_ratio', 0):>8.2f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety sync\n",
    "with open(RESULTS_DIR / 'phase2_all_results.json', 'w') as f:\n",
    "    json.dump(ALL_RESULTS, f, indent=2)\n",
    "\n",
    "print(\"Final sync to Google Drive...\")\n",
    "run_rclone(['rclone', 'copy', str(RESULTS_DIR), GDRIVE_RESULTS], \"Syncing results\")\n",
    "run_rclone(['rclone', 'copy', str(CHECKPOINT_ROOT), f\"{GDRIVE_RESULTS}/checkpoints\"], \"Syncing checkpoints\")\n",
    "\n",
    "print_experiment_status(ALL_EXPERIMENT_IDS, {k: v['summary']['avg_r2'] for k, v in ALL_RESULTS.items()})\n",
    "print(\"Done! Results at:\", GDRIVE_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Phase 3: Advanced Architecture Experiments\n\nBased on research recommendations for improving audio-only R2 toward 0.50+:\n- **D1a/D1b**: Statistical pooling (mean+std, mean+std+min+max)\n- **D2a/D2b**: Uncertainty-weighted loss (mean pool, attention pool)\n- **D3**: Dimension-specific heads (BiLSTM for timing, MLP for rest)\n- **D4**: Multi-layer MERT concat ([6,9,12])\n- **D5**: Transformer pooling (2-layer encoder before attention pool)\n- **D6**: Multi-scale temporal pooling\n\nExpected gains: +0.02-0.05 R2 cumulative from best configurations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Phase 3 model imports\nfrom audio_experiments.models import (\n    StatsPoolingModel,\n    UncertaintyWeightedModel,\n    DimensionSpecificModel,\n    TransformerPoolingModel,\n    MultiScalePoolingModel,\n    MultiLayerMERTModel,\n)\nfrom audio_experiments.extractors import extract_mert_multilayer_concat\n\n# Phase 3 experiment IDs\nPHASE3_EXPERIMENT_IDS = [\n    'D1a_stats_mean_std', 'D1b_stats_full',\n    'D2a_uncertainty_mean', 'D2b_uncertainty_attn',\n    'D3_dimension_heads',\n    'D4_multilayer_6_9_12',\n    'D5_transformer_pool',\n    'D6_multiscale_pool',\n]\n\n# Extend ALL_EXPERIMENT_IDS\nALL_EXPERIMENT_IDS.extend(PHASE3_EXPERIMENT_IDS)\n\n# Check for completed experiments\nPHASE3_COMPLETED = get_completed_experiments(GDRIVE_RESULTS)\nprint_experiment_status(PHASE3_EXPERIMENT_IDS, PHASE3_COMPLETED)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D1a: Statistical Pooling (mean + std)\nif should_run_experiment('D1a_stats_mean_std', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE3_COMPLETED):\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\n    def make_stats_model(cfg):\n        return StatsPoolingModel(\n            input_dim=1024,\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            pooling_stats='mean_std',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D1a_stats_mean_std'] = run_4fold_mert_experiment(\n        'D1a_stats_mean_std', 'MERT + stats pooling (mean+std)',\n        make_stats_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D1a_stats_mean_std', ALL_RESULTS['D1a_stats_mean_std'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D1b: Statistical Pooling (mean + std + min + max)\nif should_run_experiment('D1b_stats_full', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE3_COMPLETED):\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\n    def make_stats_full_model(cfg):\n        return StatsPoolingModel(\n            input_dim=1024,\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            pooling_stats='mean_std_min_max',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D1b_stats_full'] = run_4fold_mert_experiment(\n        'D1b_stats_full', 'MERT + stats pooling (mean+std+min+max)',\n        make_stats_full_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D1b_stats_full', ALL_RESULTS['D1b_stats_full'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D2a: Uncertainty-Weighted Loss (mean pooling)\nif should_run_experiment('D2a_uncertainty_mean', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE3_COMPLETED):\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\n    def make_uncertainty_model(cfg):\n        return UncertaintyWeightedModel(\n            input_dim=cfg['input_dim'],\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            pooling='mean',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D2a_uncertainty_mean'] = run_4fold_mert_experiment(\n        'D2a_uncertainty_mean', 'MERT + uncertainty-weighted loss (mean pool)',\n        make_uncertainty_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D2a_uncertainty_mean', ALL_RESULTS['D2a_uncertainty_mean'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D2b: Uncertainty-Weighted Loss (attention pooling)\nif should_run_experiment('D2b_uncertainty_attn', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE3_COMPLETED):\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\n    def make_uncertainty_attn_model(cfg):\n        return UncertaintyWeightedModel(\n            input_dim=cfg['input_dim'],\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            pooling='attention',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D2b_uncertainty_attn'] = run_4fold_mert_experiment(\n        'D2b_uncertainty_attn', 'MERT + uncertainty-weighted loss (attention pool)',\n        make_uncertainty_attn_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D2b_uncertainty_attn', ALL_RESULTS['D2b_uncertainty_attn'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D3: Dimension-Specific Heads (BiLSTM for timing, MLP for rest)\nif should_run_experiment('D3_dimension_heads', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE3_COMPLETED):\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\n    def make_dimension_heads_model(cfg):\n        return DimensionSpecificModel(\n            input_dim=cfg['input_dim'],\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            lstm_hidden=256,\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D3_dimension_heads'] = run_4fold_mert_experiment(\n        'D3_dimension_heads', 'MERT + dimension-specific heads (BiLSTM timing, MLP rest)',\n        make_dimension_heads_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D3_dimension_heads', ALL_RESULTS['D3_dimension_heads'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D4: Multi-Layer MERT Concat [6, 9, 12]\nif should_run_experiment('D4_multilayer_6_9_12', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE3_COMPLETED):\n    multilayer_cache = MERT_CACHE_ROOT / 'L6-9-12-concat'\n    extract_mert_multilayer_concat([6, 9, 12], AUDIO_DIR, multilayer_cache, ALL_KEYS)\n\n    cfg = BASE_CONFIG.copy()\n    cfg['input_dim'] = 1024 * 3  # 3 layers concatenated = 3072\n\n    def make_multilayer_model(cfg=cfg):\n        return MultiLayerMERTModel(\n            input_dim=cfg['input_dim'],\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            pooling='attention',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D4_multilayer_6_9_12'] = run_4fold_mert_experiment(\n        'D4_multilayer_6_9_12', 'MERT concat layers [6,9,12] + attention pool',\n        make_multilayer_model, multilayer_cache, LABELS, FOLD_ASSIGNMENTS,\n        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D4_multilayer_6_9_12', ALL_RESULTS['D4_multilayer_6_9_12'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D5: Transformer Pooling (2-layer encoder before attention pool)\nif should_run_experiment('D5_transformer_pool', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE3_COMPLETED):\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\n    def make_transformer_model(cfg):\n        return TransformerPoolingModel(\n            input_dim=cfg['input_dim'],\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            num_heads=8,\n            num_layers=2,\n            pooling='attention',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D5_transformer_pool'] = run_4fold_mert_experiment(\n        'D5_transformer_pool', 'MERT + 2-layer transformer + attention pool',\n        make_transformer_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D5_transformer_pool', ALL_RESULTS['D5_transformer_pool'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D6: Multi-Scale Temporal Pooling\nif should_run_experiment('D6_multiscale_pool', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE3_COMPLETED):\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\n    def make_multiscale_model(cfg):\n        return MultiScalePoolingModel(\n            input_dim=cfg['input_dim'],\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            scales=(4, 8, 16, 32),\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D6_multiscale_pool'] = run_4fold_mert_experiment(\n        'D6_multiscale_pool', 'MERT + multi-scale pooling (4,8,16,32 frames)',\n        make_multiscale_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D6_multiscale_pool', ALL_RESULTS['D6_multiscale_pool'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Phase 3 Results Summary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Phase 3 Results Summary\nprint(\"=\"*80)\nprint(\"PHASE 3 RESULTS SUMMARY\")\nprint(\"=\"*80)\n\n# Get Phase 2 baseline for comparison\nbaseline_r2 = ALL_RESULTS.get('B0_baseline', {}).get('summary', {}).get('avg_r2', 0)\nbest_phase2 = max(\n    ALL_RESULTS.get('B1b_layers_7-12', {}).get('summary', {}).get('avg_r2', 0),\n    ALL_RESULTS.get('B2b_attention_pool', {}).get('summary', {}).get('avg_r2', 0),\n    baseline_r2\n)\n\nprint(f\"\\nPhase 2 Baseline (B0): {baseline_r2:.4f}\")\nprint(f\"Best Phase 2: {best_phase2:.4f}\")\nprint()\n\nprint(f\"{'Experiment':<25} {'Avg R2':>10} {'95% CI':>20} {'vs Best P2':>12}\")\nprint(\"-\"*75)\n\nfor exp_id in PHASE3_EXPERIMENT_IDS:\n    if exp_id not in ALL_RESULTS:\n        result_file = RESULTS_DIR / f'{exp_id}.json'\n        if result_file.exists():\n            with open(result_file) as f:\n                ALL_RESULTS[exp_id] = json.load(f)\n        else:\n            continue\n\n    r = ALL_RESULTS[exp_id]\n    s = r['summary']\n    ci = s.get('r2_ci_95', [0, 0])\n    diff = s['avg_r2'] - best_phase2\n    print(f\"{exp_id:<25} {s['avg_r2']:>10.4f} [{ci[0]:.3f}, {ci[1]:.3f}] {diff:>+12.4f}\")\n\nprint(\"=\"*75)\n\n# Find best Phase 3 experiment\nphase3_results = [(ALL_RESULTS.get(exp_id, {}).get('summary', {}).get('avg_r2', 0), exp_id)\n                  for exp_id in PHASE3_EXPERIMENT_IDS if exp_id in ALL_RESULTS]\nif phase3_results:\n    best_p3 = max(phase3_results)\n    print(f\"\\nBest Phase 3: {best_p3[1]} (R2={best_p3[0]:.4f})\")\n    print(f\"Improvement over Phase 2: {best_p3[0] - best_phase2:+.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Per-Dimension Analysis for Best Phase 3 Model\nfrom audio_experiments import DIMENSION_CATEGORIES\n\nphase3_results = [(ALL_RESULTS.get(exp_id, {}).get('summary', {}).get('avg_r2', 0), exp_id)\n                  for exp_id in PHASE3_EXPERIMENT_IDS if exp_id in ALL_RESULTS]\n\nif phase3_results:\n    best_p3 = max(phase3_results)\n    best_exp = ALL_RESULTS[best_p3[1]]\n    per_dim = best_exp.get('per_dimension', {})\n\n    print(f\"\\nPer-Dimension R2 for {best_p3[1]}\")\n    print(\"-\"*50)\n\n    for category, dims in DIMENSION_CATEGORIES.items():\n        cat_r2s = [per_dim.get(d, {}).get('r2', 0) for d in dims]\n        cat_avg = np.mean(cat_r2s) if cat_r2s else 0\n        print(f\"\\n{category.upper()} (avg: {cat_avg:.3f})\")\n        for dim in dims:\n            r2 = per_dim.get(dim, {}).get('r2', 0)\n            print(f\"  {dim:<25} {r2:.4f}\")\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"TIMING DIMENSIONS (key target for improvement)\")\n    print(\"=\"*50)\n    timing_r2 = per_dim.get('timing', {}).get('r2', 0)\n    tempo_r2 = per_dim.get('tempo', {}).get('r2', 0)\n    print(f\"timing: {timing_r2:.4f}\")\n    print(f\"tempo:  {tempo_r2:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Final sync all results (Phase 2 + Phase 3)\nall_results_combined = {}\nfor exp_id in ALL_EXPERIMENT_IDS:\n    if exp_id in ALL_RESULTS:\n        all_results_combined[exp_id] = ALL_RESULTS[exp_id]\n\nwith open(RESULTS_DIR / 'all_results_combined.json', 'w') as f:\n    json.dump(all_results_combined, f, indent=2)\n\nprint(\"Final sync to Google Drive...\")\nrun_rclone(['rclone', 'copy', str(RESULTS_DIR), GDRIVE_RESULTS], \"Syncing results\")\nrun_rclone(['rclone', 'copy', str(CHECKPOINT_ROOT), f\"{GDRIVE_RESULTS}/checkpoints\"], \"Syncing checkpoints\")\n\nprint_experiment_status(ALL_EXPERIMENT_IDS, {k: v['summary']['avg_r2'] for k, v in ALL_RESULTS.items() if 'summary' in v})\nprint(\"\\nDone! Results at:\", GDRIVE_RESULTS)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Phase 3.5: MuQ Experiments\n\nMuQ (Music Understanding Quantized) is an alternative music representation model from ByteDance/OpenMuQ.\nSimilar to MERT but trained with different objectives, potentially capturing complementary features.\n\n- **D7**: MuQ baseline (mean pooling)\n- **D8**: MuQ with stats pooling (mean+std)\n- **D9**: MERT+MuQ ensemble (average predictions)\n- **D9b**: MERT+MuQ early fusion (concatenate embeddings)\n\nExpected gains: R2 += 0.03-0.05 from ensemble/fusion.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# MuQ imports and setup\nfrom audio_experiments.extractors import MuQExtractor, extract_muq_embeddings\nfrom audio_experiments.models import MuQBaseModel, MuQStatsModel, MERTMuQEnsemble, MERTMuQConcatModel\n\n# MuQ cache directory\nMUQ_CACHE_DIR = DATA_ROOT / 'muq_cache'\nMUQ_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n\n# Phase 3.5 experiment IDs\nPHASE35_EXPERIMENT_IDS = [\n    'D7_muq_baseline',\n    'D8_muq_stats',\n    'D9_mert_muq_ensemble',\n    'D9b_mert_muq_concat',\n]\n\nALL_EXPERIMENT_IDS.extend(PHASE35_EXPERIMENT_IDS)\nPHASE35_COMPLETED = get_completed_experiments(GDRIVE_RESULTS)\nprint_experiment_status(PHASE35_EXPERIMENT_IDS, PHASE35_COMPLETED)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D7: MuQ Baseline (mean pooling)\nif should_run_experiment('D7_muq_baseline', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE35_COMPLETED):\n    # Extract MuQ embeddings (using last hidden state)\n    extract_muq_embeddings(AUDIO_DIR, MUQ_CACHE_DIR, ALL_KEYS)\n\n    def make_muq_baseline(cfg):\n        return MuQBaseModel(\n            input_dim=1024,  # MuQ hidden size\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            pooling='mean',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D7_muq_baseline'] = run_4fold_mert_experiment(\n        'D7_muq_baseline', 'MuQ baseline with mean pooling',\n        make_muq_baseline, MUQ_CACHE_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D7_muq_baseline', ALL_RESULTS['D7_muq_baseline'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D8: MuQ with Stats Pooling (mean + std)\nif should_run_experiment('D8_muq_stats', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE35_COMPLETED):\n    # Ensure MuQ embeddings exist\n    extract_muq_embeddings(AUDIO_DIR, MUQ_CACHE_DIR, ALL_KEYS)\n\n    def make_muq_stats(cfg):\n        return MuQStatsModel(\n            input_dim=1024,\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            pooling_stats='mean_std',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D8_muq_stats'] = run_4fold_mert_experiment(\n        'D8_muq_stats', 'MuQ with stats pooling (mean+std)',\n        make_muq_stats, MUQ_CACHE_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D8_muq_stats', ALL_RESULTS['D8_muq_stats'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D9: MERT+MuQ Ensemble (late fusion - average predictions)\n# Note: This requires a custom training loop since we need both MERT and MuQ embeddings\n\nif should_run_experiment('D9_mert_muq_ensemble', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE35_COMPLETED):\n    from audio_experiments.data import DualEmbeddingDataset, dual_collate_fn\n    from torch.utils.data import DataLoader\n    \n    # Ensure both embeddings exist\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n    extract_muq_embeddings(AUDIO_DIR, MUQ_CACHE_DIR, ALL_KEYS)\n    \n    def run_ensemble_experiment():\n        from sklearn.metrics import r2_score\n        import numpy as np\n        \n        all_preds, all_labels = [], []\n        fold_metrics = []\n        \n        for fold in range(4):\n            print(f\"\\nFold {fold + 1}/4\")\n            val_keys = [k for k, f in FOLD_ASSIGNMENTS.items() if f == fold]\n            train_keys = [k for k, f in FOLD_ASSIGNMENTS.items() if f != fold]\n            \n            # Create dual-embedding datasets\n            train_ds = DualEmbeddingDataset(\n                DEFAULT_MERT_DIR, MUQ_CACHE_DIR, LABELS, train_keys,\n                max_frames=BASE_CONFIG['max_frames']\n            )\n            val_ds = DualEmbeddingDataset(\n                DEFAULT_MERT_DIR, MUQ_CACHE_DIR, LABELS, val_keys,\n                max_frames=BASE_CONFIG['max_frames']\n            )\n            \n            train_dl = DataLoader(\n                train_ds, batch_size=BASE_CONFIG['batch_size'],\n                shuffle=True, num_workers=2, collate_fn=dual_collate_fn\n            )\n            val_dl = DataLoader(\n                val_ds, batch_size=BASE_CONFIG['batch_size'],\n                shuffle=False, num_workers=2, collate_fn=dual_collate_fn\n            )\n            \n            # Create model\n            model = MERTMuQEnsemble(\n                input_dim=1024,\n                hidden_dim=BASE_CONFIG['hidden_dim'],\n                dropout=BASE_CONFIG['dropout'],\n                learning_rate=BASE_CONFIG['learning_rate'],\n                weight_decay=BASE_CONFIG['weight_decay'],\n                pooling='attention',\n                fusion_weight=0.5,\n                max_epochs=BASE_CONFIG['max_epochs'],\n            )\n            \n            # Setup trainer\n            ckpt_dir = CHECKPOINT_ROOT / 'D9_mert_muq_ensemble' / f'fold{fold}'\n            ckpt_dir.mkdir(parents=True, exist_ok=True)\n            \n            trainer = pl.Trainer(\n                max_epochs=BASE_CONFIG['max_epochs'],\n                callbacks=[\n                    pl.callbacks.ModelCheckpoint(\n                        dirpath=ckpt_dir, filename='best',\n                        monitor='val_r2', mode='max', save_top_k=1\n                    ),\n                    pl.callbacks.EarlyStopping(\n                        monitor='val_r2', mode='max',\n                        patience=BASE_CONFIG['patience']\n                    ),\n                ],\n                logger=pl.loggers.CSVLogger(LOG_DIR, name='D9_mert_muq_ensemble', version=f'fold{fold}'),\n                accelerator='auto', devices=1,\n                gradient_clip_val=BASE_CONFIG['gradient_clip_val'],\n                enable_progress_bar=True, deterministic=True,\n            )\n            \n            trainer.fit(model, train_dl, val_dl)\n            \n            # Load best and evaluate\n            best_path = list(ckpt_dir.glob('best*.ckpt'))[0]\n            model = MERTMuQEnsemble.load_from_checkpoint(best_path)\n            model.eval()\n            \n            preds, labels = [], []\n            for batch in val_dl:\n                batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                with torch.no_grad():\n                    pred = model(\n                        batch['mert_embeddings'], batch['muq_embeddings'],\n                        batch.get('mert_mask'), batch.get('muq_mask')\n                    )\n                preds.append(pred.cpu())\n                labels.append(batch['labels'].cpu())\n            \n            preds = torch.cat(preds).numpy()\n            labels = torch.cat(labels).numpy()\n            fold_r2 = r2_score(labels, preds)\n            \n            print(f\"Fold {fold + 1} R2: {fold_r2:.4f}\")\n            fold_metrics.append({'fold': fold, 'r2': fold_r2})\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n        \n        all_preds = np.array(all_preds)\n        all_labels = np.array(all_labels)\n        avg_r2 = r2_score(all_labels, all_preds)\n        \n        return {\n            'summary': {\n                'avg_r2': avg_r2,\n                'fold_r2s': [m['r2'] for m in fold_metrics],\n            },\n            'description': 'MERT+MuQ late fusion ensemble',\n        }\n    \n    ALL_RESULTS['D9_mert_muq_ensemble'] = run_ensemble_experiment()\n    sync_experiment_to_gdrive(\n        'D9_mert_muq_ensemble', ALL_RESULTS['D9_mert_muq_ensemble'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Phase 3.6: Contrastive Auxiliary Loss Experiments\n\nAdds a contrastive learning objective as auxiliary loss during training. \nThe idea is to learn representations where performances with similar ratings are closer together in embedding space.\n\n- **D10a**: Contrastive with lambda=0.05 (light regularization)\n- **D10b**: Contrastive with lambda=0.1 (moderate regularization)\n- **D10c**: Contrastive with lambda=0.2 (strong regularization)\n- **D10d**: Contrastive warmup (lambda decays from 0.5 to 0.05)\n\nExpected gains: R2 += 0.02-0.04 from improved representation structure.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Contrastive model imports and setup\nfrom audio_experiments.models import ContrastiveAuxiliaryModel, ContrastiveWarmupModel\n\n# Phase 3.6 experiment IDs\nPHASE36_EXPERIMENT_IDS = [\n    'D10a_contrastive_0.05',\n    'D10b_contrastive_0.1',\n    'D10c_contrastive_0.2',\n    'D10d_contrastive_warmup',\n]\n\nALL_EXPERIMENT_IDS.extend(PHASE36_EXPERIMENT_IDS)\nPHASE36_COMPLETED = get_completed_experiments(GDRIVE_RESULTS)\nprint_experiment_status(PHASE36_EXPERIMENT_IDS, PHASE36_COMPLETED)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D10a: Contrastive Auxiliary Loss (lambda=0.05)\nif should_run_experiment('D10a_contrastive_0.05', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE36_COMPLETED):\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\n    def make_contrastive_model_005(cfg):\n        return ContrastiveAuxiliaryModel(\n            input_dim=cfg['input_dim'],\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            contrastive_lambda=0.05,\n            temperature=0.07,\n            pooling='attention',\n            contrastive_type='supervised',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D10a_contrastive_0.05'] = run_4fold_mert_experiment(\n        'D10a_contrastive_0.05', 'MERT + contrastive auxiliary loss (lambda=0.05)',\n        make_contrastive_model_005, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D10a_contrastive_0.05', ALL_RESULTS['D10a_contrastive_0.05'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D10b: Contrastive Auxiliary Loss (lambda=0.1)\nif should_run_experiment('D10b_contrastive_0.1', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE36_COMPLETED):\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\n    def make_contrastive_model_01(cfg):\n        return ContrastiveAuxiliaryModel(\n            input_dim=cfg['input_dim'],\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            contrastive_lambda=0.1,\n            temperature=0.07,\n            pooling='attention',\n            contrastive_type='supervised',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D10b_contrastive_0.1'] = run_4fold_mert_experiment(\n        'D10b_contrastive_0.1', 'MERT + contrastive auxiliary loss (lambda=0.1)',\n        make_contrastive_model_01, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D10b_contrastive_0.1', ALL_RESULTS['D10b_contrastive_0.1'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D10c: Contrastive Auxiliary Loss (lambda=0.2)\nif should_run_experiment('D10c_contrastive_0.2', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE36_COMPLETED):\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\n    def make_contrastive_model_02(cfg):\n        return ContrastiveAuxiliaryModel(\n            input_dim=cfg['input_dim'],\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            contrastive_lambda=0.2,\n            temperature=0.07,\n            pooling='attention',\n            contrastive_type='supervised',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D10c_contrastive_0.2'] = run_4fold_mert_experiment(\n        'D10c_contrastive_0.2', 'MERT + contrastive auxiliary loss (lambda=0.2)',\n        make_contrastive_model_02, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D10c_contrastive_0.2', ALL_RESULTS['D10c_contrastive_0.2'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# D10d: Contrastive Warmup (lambda decays from 0.5 to 0.05)\nif should_run_experiment('D10d_contrastive_warmup', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, PHASE36_COMPLETED):\n    extract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\n    def make_contrastive_warmup_model(cfg):\n        return ContrastiveWarmupModel(\n            input_dim=cfg['input_dim'],\n            hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'],\n            contrastive_lambda_start=0.5,\n            contrastive_lambda_end=0.05,\n            temperature=0.07,\n            pooling='attention',\n            max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS['D10d_contrastive_warmup'] = run_4fold_mert_experiment(\n        'D10d_contrastive_warmup', 'MERT + contrastive warmup (0.5 -> 0.05)',\n        make_contrastive_warmup_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        'D10d_contrastive_warmup', ALL_RESULTS['D10d_contrastive_warmup'],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Phase 3.5 + 3.6 Results Summary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Phase 3.5 + 3.6 Results Summary\nprint(\"=\"*80)\nprint(\"PHASE 3.5 + 3.6 RESULTS SUMMARY (MuQ + Contrastive)\")\nprint(\"=\"*80)\n\n# Get best previous result for comparison\nbest_previous = max(\n    ALL_RESULTS.get('B0_baseline', {}).get('summary', {}).get('avg_r2', 0),\n    max([ALL_RESULTS.get(exp_id, {}).get('summary', {}).get('avg_r2', 0) \n         for exp_id in PHASE3_EXPERIMENT_IDS if exp_id in ALL_RESULTS], default=0)\n)\n\nprint(f\"\\nBest Previous R2: {best_previous:.4f}\")\nprint()\n\nprint(f\"{'Experiment':<30} {'Avg R2':>10} {'vs Best':>12}\")\nprint(\"-\"*55)\n\n# MuQ experiments\nprint(\"\\nMuQ Experiments:\")\nfor exp_id in PHASE35_EXPERIMENT_IDS:\n    if exp_id in ALL_RESULTS:\n        s = ALL_RESULTS[exp_id]['summary']\n        diff = s['avg_r2'] - best_previous\n        print(f\"  {exp_id:<28} {s['avg_r2']:>10.4f} {diff:>+12.4f}\")\n\n# Contrastive experiments\nprint(\"\\nContrastive Experiments:\")\nfor exp_id in PHASE36_EXPERIMENT_IDS:\n    if exp_id in ALL_RESULTS:\n        s = ALL_RESULTS[exp_id]['summary']\n        diff = s['avg_r2'] - best_previous\n        print(f\"  {exp_id:<28} {s['avg_r2']:>10.4f} {diff:>+12.4f}\")\n\nprint(\"=\"*55)\n\n# Find overall best\nall_new_exps = PHASE35_EXPERIMENT_IDS + PHASE36_EXPERIMENT_IDS\nnew_results = [(ALL_RESULTS.get(exp_id, {}).get('summary', {}).get('avg_r2', 0), exp_id)\n               for exp_id in all_new_exps if exp_id in ALL_RESULTS]\nif new_results:\n    best_new = max(new_results)\n    print(f\"\\nBest New Experiment: {best_new[1]} (R2={best_new[0]:.4f})\")\n    print(f\"Improvement over Previous Best: {best_new[0] - best_previous:+.4f}\")\n\n# Final sync\nwith open(RESULTS_DIR / 'all_results_with_muq_contrastive.json', 'w') as f:\n    json.dump(ALL_RESULTS, f, indent=2)\n\nprint(\"\\n\\nSyncing to Google Drive...\")\nrun_rclone(['rclone', 'copy', str(RESULTS_DIR), GDRIVE_RESULTS], \"Syncing results\")\nrun_rclone(['rclone', 'copy', str(CHECKPOINT_ROOT), f\"{GDRIVE_RESULTS}/checkpoints\"], \"Syncing checkpoints\")\nprint(\"Done!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}