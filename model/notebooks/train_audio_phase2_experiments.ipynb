{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Audio Baseline Experiments\n",
    "\n",
    "Comprehensive experiments for the ISMIR paper.\n",
    "\n",
    "## Experiments\n",
    "- **B0**: Baseline re-run (MERT+MLP, L13-24, mean pool)\n",
    "- **A1-A3**: Baselines (linear probe, Mel-CNN, raw statistics)\n",
    "- **B1a-B1d**: Layer ablation (1-6, 7-12, 13-24, 1-24)\n",
    "- **B2a-B2c**: Pooling ablation (max, attention, LSTM)\n",
    "- **C1a-C1b**: Loss ablation (hybrid MSE+CCC, pure CCC)\n",
    "\n",
    "## Requirements\n",
    "- Compute: A100 (80GB VRAM)\n",
    "- rclone configured with `gdrive:` remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"GPU required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies and clone repo\n",
    "!pip install transformers librosa soundfile pytorch_lightning nnAudio --quiet\n",
    "\n",
    "# Clone the repo\n",
    "import os\n",
    "REPO_DIR = '/tmp/crescendai'\n",
    "if os.path.exists(REPO_DIR):\n",
    "    !cd {REPO_DIR} && git pull origin main\n",
    "else:\n",
    "    !git clone https://github.com/jai-dhiman/crescendai.git {REPO_DIR}\n",
    "\n",
    "print(f\"Repo: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 3. Setup imports\nimport sys\nsys.path.insert(0, f'{REPO_DIR}/model/src')\n\nimport json\nimport subprocess\nimport warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pytorch_lightning as pl\n\n# Import from our package\nfrom audio_experiments import PERCEPIANO_DIMENSIONS, BASE_CONFIG, SEED\nfrom audio_experiments.extractors import (\n    extract_mert_for_layer_range,\n    extract_mel_spectrograms,\n    extract_statistics_for_all,\n)\nfrom audio_experiments.models import BaseMERTModel, LinearProbeModel, MelCNNModel, StatsMLPModel\nfrom audio_experiments.training import (\n    run_4fold_mert_experiment,\n    run_4fold_mel_experiment,\n    run_4fold_stats_experiment,\n    restore_all_from_gdrive,\n    sync_experiment_to_gdrive,\n    print_experiment_status,\n)\n\nwarnings.filterwarnings('ignore')\ntorch.set_float32_matmul_precision('medium')\npl.seed_everything(SEED, workers=True)\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Imports: OK\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Setup paths and download data\n",
    "DATA_ROOT = Path('/tmp/phase2')\n",
    "AUDIO_DIR = DATA_ROOT / 'audio'\n",
    "LABEL_DIR = DATA_ROOT / 'labels'\n",
    "MERT_CACHE_ROOT = DATA_ROOT / 'mert_cache'\n",
    "MEL_CACHE_DIR = DATA_ROOT / 'mel_cache'\n",
    "STATS_CACHE_DIR = DATA_ROOT / 'stats_cache'\n",
    "CHECKPOINT_ROOT = DATA_ROOT / 'checkpoints'\n",
    "RESULTS_DIR = DATA_ROOT / 'results'\n",
    "LOG_DIR = DATA_ROOT / 'logs'\n",
    "\n",
    "GDRIVE_AUDIO = 'gdrive:crescendai_data/audio_baseline/percepiano_rendered'\n",
    "GDRIVE_LABELS = 'gdrive:crescendai_data/percepiano_labels'\n",
    "GDRIVE_FOLDS = 'gdrive:crescendai_data/audio_baseline/audio_fold_assignments.json'\n",
    "GDRIVE_MERT_CACHE = 'gdrive:crescendai_data/audio_baseline/mert_embeddings'\n",
    "GDRIVE_RESULTS = 'gdrive:crescendai_data/checkpoints/audio_phase2'\n",
    "\n",
    "for d in [AUDIO_DIR, LABEL_DIR, MERT_CACHE_ROOT, MEL_CACHE_DIR, STATS_CACHE_DIR,\n",
    "          CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_rclone(cmd, desc):\n",
    "    print(f\"{desc}...\")\n",
    "    subprocess.run(cmd, capture_output=True)\n",
    "\n",
    "# Check rclone\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "if 'gdrive:' not in result.stdout:\n",
    "    raise RuntimeError(\"rclone 'gdrive' not configured\")\n",
    "\n",
    "# Download data\n",
    "run_rclone(['rclone', 'copy', GDRIVE_AUDIO, str(AUDIO_DIR), '--progress'], \"Downloading audio\")\n",
    "run_rclone(['rclone', 'copy', GDRIVE_LABELS, str(LABEL_DIR)], \"Downloading labels\")\n",
    "\n",
    "FOLD_FILE = DATA_ROOT / 'folds.json'\n",
    "run_rclone(['rclone', 'copyto', GDRIVE_FOLDS, str(FOLD_FILE)], \"Downloading folds\")\n",
    "\n",
    "# Load labels and folds\n",
    "LABEL_FILE = LABEL_DIR / 'label_2round_mean_reg_19_with0_rm_highstd0.json'\n",
    "with open(LABEL_FILE) as f:\n",
    "    LABELS = json.load(f)\n",
    "with open(FOLD_FILE) as f:\n",
    "    FOLD_ASSIGNMENTS = json.load(f)\n",
    "\n",
    "ALL_KEYS = list(LABELS.keys())\n",
    "print(f\"Audio: {len(list(AUDIO_DIR.glob('*.wav')))} files\")\n",
    "print(f\"Labels: {len(LABELS)} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 5. Restore MERT cache and completed experiments from GDrive\nDEFAULT_MERT_DIR = MERT_CACHE_ROOT / 'L13-24'\nDEFAULT_MERT_DIR.mkdir(parents=True, exist_ok=True)\n\nresult = subprocess.run(['rclone', 'lsf', GDRIVE_MERT_CACHE], capture_output=True, text=True)\nif result.returncode == 0 and '.pt' in result.stdout:\n    print(\"Restoring MERT cache...\")\n    run_rclone(['rclone', 'copy', GDRIVE_MERT_CACHE, str(DEFAULT_MERT_DIR)], \"Restoring cache\")\n    print(f\"Restored: {len(list(DEFAULT_MERT_DIR.glob('*.pt')))} embeddings\")\n\n# Results tracker\nALL_RESULTS = {}\n\n# All experiment IDs in execution order\nALL_EXPERIMENT_IDS = [\n    'B0_baseline', 'A1_linear_probe', 'A2_mel_cnn', 'A3_raw_stats',\n    'B1a_layers_1-6', 'B1b_layers_7-12', 'B1c_layers_13-24', 'B1d_layers_1-24',\n    'B2a_max_pool', 'B2b_attention_pool', 'B2c_lstm_pool',\n    'C1a_hybrid_loss', 'C1b_pure_ccc',\n]\n\n# Restore completed experiments from GDrive (enables automatic skipping)\nprint(\"\\nChecking GDrive for completed experiments...\")\nrestored = restore_all_from_gdrive(\n    GDRIVE_RESULTS,\n    RESULTS_DIR,\n    CHECKPOINT_ROOT,\n    ALL_RESULTS,\n)\n\n# Show status\nfrom audio_experiments.training import get_completed_experiments\ncompleted = get_completed_experiments(GDRIVE_RESULTS)\nprint_experiment_status(ALL_EXPERIMENT_IDS, completed)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# B0: Baseline\nextract_mert_for_layer_range(13, 25, AUDIO_DIR, DEFAULT_MERT_DIR, ALL_KEYS)\n\ndef make_mert_model(cfg):\n    return BaseMERTModel(\n        input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n        dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n        weight_decay=cfg['weight_decay'], pooling=cfg.get('pooling', 'mean'),\n        loss_type=cfg.get('loss_type', 'mse'), max_epochs=cfg['max_epochs'],\n    )\n\nALL_RESULTS['B0_baseline'] = run_4fold_mert_experiment(\n    'B0_baseline', 'MERT+MLP, L13-24, mean pooling',\n    make_mert_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n    BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n)\nsync_experiment_to_gdrive(\n    'B0_baseline', ALL_RESULTS['B0_baseline'],\n    RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# A1: Linear Probe\ndef make_linear_probe(cfg):\n    return LinearProbeModel(\n        input_dim=cfg['input_dim'], learning_rate=cfg['learning_rate'],\n        weight_decay=cfg['weight_decay'], max_epochs=cfg['max_epochs'],\n    )\n\nALL_RESULTS['A1_linear_probe'] = run_4fold_mert_experiment(\n    'A1_linear_probe', 'Linear probe on MERT',\n    make_linear_probe, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n    BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n)\nsync_experiment_to_gdrive(\n    'A1_linear_probe', ALL_RESULTS['A1_linear_probe'],\n    RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# A2: Mel-CNN\nextract_mel_spectrograms(AUDIO_DIR, MEL_CACHE_DIR, ALL_KEYS)\n\nALL_RESULTS['A2_mel_cnn'] = run_4fold_mel_experiment(\n    'A2_mel_cnn', '4-layer CNN on mel spectrograms',\n    MEL_CACHE_DIR, LABELS, FOLD_ASSIGNMENTS,\n    BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n)\nsync_experiment_to_gdrive(\n    'A2_mel_cnn', ALL_RESULTS['A2_mel_cnn'],\n    RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# A3: Raw Statistics\nextract_statistics_for_all(AUDIO_DIR, STATS_CACHE_DIR, ALL_KEYS)\n\nALL_RESULTS['A3_raw_stats'] = run_4fold_stats_experiment(\n    'A3_raw_stats', 'MLP on audio statistics (49-dim)',\n    STATS_CACHE_DIR, LABELS, FOLD_ASSIGNMENTS,\n    BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n)\nsync_experiment_to_gdrive(\n    'A3_raw_stats', ALL_RESULTS['A3_raw_stats'],\n    RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# B1a-d: Layer Ablation\nlayer_configs = [\n    ('B1a_layers_1-6', 1, 7, 'MERT layers 1-6 (early)'),\n    ('B1b_layers_7-12', 7, 13, 'MERT layers 7-12 (mid)'),\n    ('B1c_layers_13-24', 13, 25, 'MERT layers 13-24 (late)'),\n    ('B1d_layers_1-24', 1, 25, 'MERT all layers 1-24'),\n]\n\nfor exp_id, layer_start, layer_end, desc in layer_configs:\n    cache_dir = MERT_CACHE_ROOT / f'L{layer_start}-{layer_end-1}'\n    extract_mert_for_layer_range(layer_start, layer_end, AUDIO_DIR, cache_dir, ALL_KEYS)\n    ALL_RESULTS[exp_id] = run_4fold_mert_experiment(\n        exp_id, desc, make_mert_model, cache_dir, LABELS, FOLD_ASSIGNMENTS,\n        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        exp_id, ALL_RESULTS[exp_id],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# B2a-c: Pooling Ablation\npooling_configs = [\n    ('B2a_max_pool', 'max', 'MERT + max pooling'),\n    ('B2b_attention_pool', 'attention', 'MERT + attention pooling'),\n    ('B2c_lstm_pool', 'lstm', 'MERT + Bi-LSTM pooling'),\n]\n\nfor exp_id, pooling, desc in pooling_configs:\n    cfg = BASE_CONFIG.copy()\n    cfg['pooling'] = pooling\n\n    def make_pooling_model(cfg=cfg):\n        return BaseMERTModel(\n            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'], pooling=cfg['pooling'],\n            loss_type='mse', max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS[exp_id] = run_4fold_mert_experiment(\n        exp_id, desc, make_pooling_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        exp_id, ALL_RESULTS[exp_id],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# C1a-b: Loss Ablation\nloss_configs = [\n    ('C1a_hybrid_loss', 'hybrid', 'MERT + MSE + 0.5*CCC loss'),\n    ('C1b_pure_ccc', 'ccc', 'MERT + pure CCC loss'),\n]\n\nfor exp_id, loss_type, desc in loss_configs:\n    cfg = BASE_CONFIG.copy()\n    cfg['loss_type'] = loss_type\n\n    def make_loss_model(cfg=cfg):\n        return BaseMERTModel(\n            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n            weight_decay=cfg['weight_decay'], pooling='mean',\n            loss_type=cfg['loss_type'], max_epochs=cfg['max_epochs'],\n        )\n\n    ALL_RESULTS[exp_id] = run_4fold_mert_experiment(\n        exp_id, desc, make_loss_model, DEFAULT_MERT_DIR, LABELS, FOLD_ASSIGNMENTS,\n        cfg, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n    )\n    sync_experiment_to_gdrive(\n        exp_id, ALL_RESULTS[exp_id],\n        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results table\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 2 RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_r2 = ALL_RESULTS.get('B0_baseline', {}).get('summary', {}).get('avg_r2', 0)\n",
    "\n",
    "print(f\"{'Experiment':<25} {'Avg R2':>10} {'95% CI':>20} {'vs B0':>10} {'Disp':>8}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "exp_order = [\n",
    "    'B0_baseline', None,\n",
    "    'A1_linear_probe', 'A2_mel_cnn', 'A3_raw_stats', None,\n",
    "    'B1a_layers_1-6', 'B1b_layers_7-12', 'B1c_layers_13-24', 'B1d_layers_1-24', None,\n",
    "    'B2a_max_pool', 'B2b_attention_pool', 'B2c_lstm_pool', None,\n",
    "    'C1a_hybrid_loss', 'C1b_pure_ccc',\n",
    "]\n",
    "\n",
    "for exp_id in exp_order:\n",
    "    if exp_id is None:\n",
    "        print(\"-\"*80)\n",
    "        continue\n",
    "    if exp_id not in ALL_RESULTS:\n",
    "        continue\n",
    "\n",
    "    r = ALL_RESULTS[exp_id]\n",
    "    s = r['summary']\n",
    "    ci = s.get('r2_ci_95', [0, 0])\n",
    "    diff = s['avg_r2'] - baseline_r2 if exp_id != 'B0_baseline' else 0\n",
    "    diff_str = f\"{diff:+.3f}\" if exp_id != 'B0_baseline' else '---'\n",
    "\n",
    "    print(f\"{exp_id:<25} {s['avg_r2']:>10.4f} [{ci[0]:.3f}, {ci[1]:.3f}] {diff_str:>10} {s.get('dispersion_ratio', 0):>8.2f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final sync (redundant safety - individual experiments already synced)\nwith open(RESULTS_DIR / 'phase2_all_results.json', 'w') as f:\n    json.dump(ALL_RESULTS, f, indent=2)\n\nprint(\"Final sync to Google Drive...\")\nrun_rclone(['rclone', 'copy', str(RESULTS_DIR), GDRIVE_RESULTS], \"Syncing results\")\nrun_rclone(['rclone', 'copy', str(CHECKPOINT_ROOT), f\"{GDRIVE_RESULTS}/checkpoints\"], \"Syncing checkpoints\")\n\n# Print final status\nprint_experiment_status(ALL_EXPERIMENT_IDS, {k: v['summary']['avg_r2'] for k, v in ALL_RESULTS.items()})\nprint(\"Done! Results at:\", GDRIVE_RESULTS)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}