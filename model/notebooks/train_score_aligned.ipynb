{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Score-Aligned Piano Performance Evaluation (v2 - Hierarchical)\n\n**Goal**: Train model with score alignment features to improve R-squared from 0.10 to 0.30-0.40\n\n## Key Improvements in This Version\n\n1. **Hierarchical Encoder (HAN)**: Note -> Beat -> Measure hierarchy matching PercePiano architecture\n2. **Expanded Features**: 20 per-note features (was 6) including timing, articulation, dynamics, pitch\n3. **Note Locations**: Beat/measure/voice indices for proper hierarchical aggregation\n4. **Fixed Dimension Handling**: Proper 19 PercePiano dimensions\n5. **Pre-trained MIDI Encoder**: Loads encoder_pretrained.pt for better initialization\n6. **Pre-flight Validation**: Validates data, scores, and encoder before training\n\n## What You Need on Google Drive\n\n- `gdrive:percepiano_data/` containing:\n  - `percepiano_train.json`, `percepiano_val.json`, `percepiano_test.json`\n  - `PercePiano/virtuoso/data/all_2rounds/` (performance MIDI files)\n  - `PercePiano/virtuoso/data/score_xml/` (MusicXML score files)\n- `gdrive:crescendai_checkpoints/midi_pretrain/encoder_pretrained.pt` (pre-trained encoder)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rclone Reminder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Install dependencies\n",
    "!uv pip install --system -e .\n",
    "!pip install tensorboard rich\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_ROOT = '/tmp/checkpoints/score_aligned'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/score_aligned'\n",
    "GDRIVE_DATA_PATH = 'gdrive:percepiano_data'\n",
    "DATA_ROOT = Path('/tmp/percepiano_data')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SETUP: CHECKPOINTS AND DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check rclone\n",
    "print(\"\\nChecking rclone configuration...\")\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "\n",
    "if 'gdrive:' in result.stdout:\n",
    "    print(\"  rclone 'gdrive' remote: CONFIGURED\")\n",
    "    RCLONE_AVAILABLE = True\n",
    "    \n",
    "    # Restore existing checkpoints\n",
    "    print(\"\\nRestoring checkpoints from Google Drive (if any)...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_CHECKPOINT_PATH, CHECKPOINT_ROOT, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "else:\n",
    "    print(\"  rclone 'gdrive' remote: NOT CONFIGURED\")\n",
    "    print(\"  Run 'rclone config' in terminal to set up Google Drive\")\n",
    "    RCLONE_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nCheckpoint directory: {CHECKPOINT_ROOT}\")\n",
    "print(f\"rclone available: {RCLONE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Data with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport subprocess\nimport json\n\nDATA_ROOT = Path('/tmp/percepiano_data')\nDATA_ROOT.mkdir(parents=True, exist_ok=True)\n\nPRETRAIN_DIR = Path('/tmp/checkpoints/midi_pretrain')\nPRETRAIN_DIR.mkdir(parents=True, exist_ok=True)\n\n# Download PercePiano data\ntrain_file = DATA_ROOT / 'percepiano_train.json'\nif train_file.exists():\n    print(f\"Data already exists at {DATA_ROOT}\")\nelse:\n    print(\"Downloading PercePiano data from Google Drive...\")\n    result = subprocess.run(\n        ['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'],\n        capture_output=False\n    )\n\n# Download pre-trained encoder\npretrained_path = PRETRAIN_DIR / 'encoder_pretrained.pt'\nif not pretrained_path.exists():\n    print(\"\\nDownloading pre-trained MIDI encoder from Google Drive...\")\n    subprocess.run(\n        ['rclone', 'copy', 'gdrive:crescendai_checkpoints/midi_pretrain', str(PRETRAIN_DIR), '--progress'],\n        capture_output=False\n    )\n\n# Verify data\nprint(\"\\n\" + \"=\"*60)\nprint(\"DATA VERIFICATION\")\nprint(\"=\"*60)\n\nfor split in ['train', 'val', 'test']:\n    path = DATA_ROOT / f'percepiano_{split}.json'\n    if path.exists():\n        with open(path) as f:\n            data = json.load(f)\n        has_scores = sum(1 for s in data if s.get('score_path'))\n        print(f\"{split}: {len(data)} samples ({has_scores} with score paths)\")\n    else:\n        print(f\"ERROR: {path} not found!\")\n\n# Check MIDI and score files\nmidi_dir = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'all_2rounds'\nscore_dir = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'score_xml'\n\nif midi_dir.exists():\n    midi_files = list(midi_dir.glob('*.mid'))\n    print(f\"\\nMIDI files: {len(midi_files)}\")\nelse:\n    raise FileNotFoundError(f\"MIDI directory not found at {midi_dir}\")\n\nif score_dir.exists():\n    score_files = list(score_dir.glob('*.musicxml'))\n    print(f\"Score files: {len(score_files)}\")\n    if len(score_files) == 0:\n        raise FileNotFoundError(f\"No MusicXML files found in {score_dir}\")\nelse:\n    raise FileNotFoundError(\n        f\"Score directory not found at {score_dir}\\n\"\n        \"Run: rclone copy gdrive:percepiano_data/PercePiano/virtuoso/data/score_xml/ {score_dir}/\"\n    )\n\n# Verify pre-trained encoder\nif pretrained_path.exists():\n    size_mb = pretrained_path.stat().st_size / 1e6\n    print(f\"\\nPre-trained encoder: {pretrained_path} ({size_mb:.1f} MB)\")\nelse:\n    raise FileNotFoundError(\n        f\"Pre-trained encoder not found at {pretrained_path}\\n\"\n        \"Run: rclone copy gdrive:crescendai_checkpoints/midi_pretrain/ {PRETRAIN_DIR}/\"\n    )\n\nprint(\"\\n[OK] All required files present\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Update JSON Files for Thunder Compute Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path('/tmp/percepiano_data')\n",
    "MIDI_DIR = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'all_2rounds'\n",
    "SCORE_DIR = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'score_xml'\n",
    "\n",
    "# All 19 PercePiano dimensions\n",
    "PERCEPIANO_DIMENSIONS = [\n",
    "    \"timing\", \"articulation_length\", \"articulation_touch\",\n",
    "    \"pedal_amount\", \"pedal_clarity\", \"timbre_variety\",\n",
    "    \"timbre_depth\", \"timbre_brightness\", \"timbre_loudness\",\n",
    "    \"dynamic_range\", \"tempo\", \"space\", \"balance\", \"drama\",\n",
    "    \"mood_valence\", \"mood_energy\", \"mood_imagination\",\n",
    "    \"sophistication\", \"interpretation\",\n",
    "]\n",
    "\n",
    "def update_paths_for_thunder(data_root: Path):\n",
    "    \"\"\"Update paths in JSON files for Thunder Compute environment.\"\"\"\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        path = data_root / f'percepiano_{split}.json'\n",
    "        \n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for sample in data:\n",
    "            # Update MIDI path\n",
    "            filename = Path(sample['midi_path']).name\n",
    "            sample['midi_path'] = str(MIDI_DIR / filename)\n",
    "            \n",
    "            # Keep score_path as just filename (relative to SCORE_DIR)\n",
    "            # The dataset loader will combine with SCORE_DIR\n",
    "            \n",
    "            # Make sure scores dict uses all 19 dimensions\n",
    "            if 'percepiano_scores' in sample:\n",
    "                pp_scores = sample['percepiano_scores'][:19]\n",
    "                sample['scores'] = {\n",
    "                    dim: pp_scores[i]\n",
    "                    for i, dim in enumerate(PERCEPIANO_DIMENSIONS)\n",
    "                }\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"Updated {split}: {len(data)} samples\")\n",
    "\n",
    "update_paths_for_thunder(DATA_ROOT)\n",
    "\n",
    "# Verify\n",
    "with open(DATA_ROOT / 'percepiano_train.json') as f:\n",
    "    sample = json.load(f)[0]\n",
    "\n",
    "print(f\"\\nSample MIDI path: {sample['midi_path']}\")\n",
    "print(f\"Sample score path: {sample.get('score_path', 'N/A')}\")\n",
    "print(f\"Dimensions: {len(sample['scores'])}\")\n",
    "print(f\"MIDI exists: {Path(sample['midi_path']).exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Pre-flight validation - FAIL FAST if requirements not met\nfrom src.utils.preflight_validation import run_preflight_validation, PreflightValidationError\n\nprint(\"=\"*60)\nprint(\"PRE-FLIGHT VALIDATION\")\nprint(\"=\"*60)\n\ntry:\n    run_preflight_validation(\n        data_dir=DATA_ROOT,\n        score_dir=SCORE_DIR,\n        pretrained_checkpoint=PRETRAIN_DIR / 'encoder_pretrained.pt',\n        require_pretrained=True,  # Require pre-trained encoder\n        min_score_coverage=0.95,  # Require 95% score coverage\n    )\n    print(\"\\n[OK] Pre-flight validation PASSED - ready to train\")\nexcept PreflightValidationError as e:\n    print(f\"\\n[VALIDATION FAILED]\\n{e}\")\n    raise RuntimeError(\"Fix the issues above before training\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\ntorch.set_float32_matmul_precision('medium')\n\n# Paths (defined earlier)\nSCORE_DIR = Path('/tmp/percepiano_data/PercePiano/virtuoso/data/score_xml')\nPRETRAINED_CHECKPOINT = Path('/tmp/checkpoints/midi_pretrain/encoder_pretrained.pt')\n\nCONFIG = {\n    # Data\n    'data_dir': '/tmp/percepiano_data',\n    'score_dir': str(SCORE_DIR),\n    'pretrained_checkpoint': str(PRETRAINED_CHECKPOINT),\n    \n    # MIDI Encoder (768/12/12 to match MidiBERT)\n    'midi_hidden_dim': 768,\n    'midi_num_layers': 12,\n    'midi_num_heads': 12,\n    'max_seq_length': 512,\n    \n    # Score Encoder\n    'score_hidden_dim': 256,\n    'score_num_layers': 2,\n    'score_note_features': 20,  # Expanded from 6 to 20 features per note\n    'use_hierarchical_encoder': True,  # Use HAN-style note->beat->measure hierarchy\n    \n    # Fusion\n    'fusion_type': 'gated',  # Options: 'concat', 'crossattn', 'gated'\n    'fused_dim': 768,\n    \n    # Aggregation (PercePiano style)\n    'attention_da': 128,\n    'attention_r': 4,\n    'head_hidden_dim': 256,\n    'dropout': 0.1,\n    \n    # Training\n    'batch_size': 4,\n    'learning_rate': 1e-5,\n    'weight_decay': 0.01,\n    'max_epochs': 100,\n    'early_stopping_patience': 20,\n    'gradient_clip_val': 1.0,\n    'accumulate_grad_batches': 4,  # Effective batch size = 16\n    'precision': '16-mixed',\n    \n    # Sequence lengths\n    'max_score_notes': 1024,\n    'max_tempo_segments': 256,\n    \n    # Checkpoints\n    'checkpoint_dir': '/tmp/checkpoints/score_aligned',\n    'gdrive_checkpoint': 'gdrive:crescendai_checkpoints/score_aligned',\n    \n    # Options\n    'freeze_midi_encoder': False,\n}\n\nprint(\"Training Configuration (Score-Aligned Model with Hierarchical Encoder):\")\nprint(\"=\"*70)\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")\nprint(\"=\"*70)\nprint(\"\\nKey improvements in this version:\")\nprint(\"  - Pre-trained MIDI encoder loaded at initialization\")\nprint(\"  - Pre-flight validation ensures all data is present\")\nprint(\"  - Hierarchical encoder (HAN): note -> beat -> measure hierarchy\")\nprint(\"  - No fallback mode - fail fast if data missing\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create DataLoaders with Score Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nfrom src.data.percepiano_score_dataset import create_score_dataloaders\n\ntrain_loader, val_loader, test_loader = create_score_dataloaders(\n    data_dir=Path(CONFIG['data_dir']),\n    score_dir=Path(CONFIG['score_dir']) if CONFIG['score_dir'] else None,\n    batch_size=CONFIG['batch_size'],\n    max_midi_seq_length=CONFIG['max_seq_length'],\n    max_score_notes=CONFIG['max_score_notes'],\n    num_workers=4,\n)\n\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Val batches: {len(val_loader)}\")\nprint(f\"Test batches: {len(test_loader)}\")\n\n# Test a batch\nbatch = next(iter(train_loader))\nprint(f\"\\nBatch shapes:\")\nprint(f\"  midi_tokens: {batch['midi_tokens'].shape}\")\nprint(f\"  score_note_features: {batch['score_note_features'].shape}\")\nprint(f\"  score_global_features: {batch['score_global_features'].shape}\")\nprint(f\"  score_tempo_curve: {batch['score_tempo_curve'].shape}\")\nprint(f\"  scores: {batch['scores'].shape}\")\n\n# Check for note_locations (new hierarchical features)\nif 'note_locations_beat' in batch:\n    print(f\"\\nHierarchical features (note_locations):\")\n    print(f\"  note_locations_beat: {batch['note_locations_beat'].shape}\")\n    print(f\"  note_locations_measure: {batch['note_locations_measure'].shape}\")\n    print(f\"  note_locations_voice: {batch['note_locations_voice'].shape}\")\nelse:\n    print(\"\\nWARNING: note_locations not found in batch - hierarchical encoder may not work optimally\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Score-Aligned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.models.score_aligned_module import ScoreAlignedModule\n\n# Create model with pre-trained MIDI encoder\nmodel = ScoreAlignedModule(\n    # MIDI Encoder\n    midi_hidden_dim=CONFIG['midi_hidden_dim'],\n    midi_num_layers=CONFIG['midi_num_layers'],\n    midi_num_heads=CONFIG['midi_num_heads'],\n    max_seq_length=CONFIG['max_seq_length'],\n    # Score Encoder\n    score_hidden_dim=CONFIG['score_hidden_dim'],\n    score_num_layers=CONFIG['score_num_layers'],\n    score_note_features=CONFIG['score_note_features'],\n    use_hierarchical_encoder=CONFIG['use_hierarchical_encoder'],\n    # Fusion\n    fusion_type=CONFIG['fusion_type'],\n    fused_dim=CONFIG['fused_dim'],\n    # Aggregation\n    attention_da=CONFIG['attention_da'],\n    attention_r=CONFIG['attention_r'],\n    head_hidden_dim=CONFIG['head_hidden_dim'],\n    # Training\n    learning_rate=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay'],\n    dropout=CONFIG['dropout'],\n    freeze_midi_encoder=CONFIG['freeze_midi_encoder'],\n    # Pre-trained encoder - loaded at initialization!\n    midi_pretrained_checkpoint=CONFIG['pretrained_checkpoint'],\n)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(\"=\"*70)\nprint(\"SCORE-ALIGNED MODEL WITH HIERARCHICAL ENCODER\")\nprint(\"=\"*70)\nprint(f\"Model class: ScoreAlignedModule (no fallback - fail fast)\")\nprint(f\"Pre-trained encoder: {CONFIG['pretrained_checkpoint']}\")\nprint(f\"Score encoder: {'Hierarchical (HAN)' if CONFIG['use_hierarchical_encoder'] else 'Flat (Transformer)'}\")\nprint(f\"Note features: {CONFIG['score_note_features']} per note\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Fusion type: {CONFIG['fusion_type']}\")\nprint(f\"Dimensions: {len(model.dimensions)}\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Checkpoint callback - monitor mean R-squared\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CONFIG['checkpoint_dir'],\n",
    "    filename='score_aligned-{epoch:02d}-{val_mean_r2:.4f}',\n",
    "    monitor='val/mean_r2',\n",
    "    mode='max',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val/mean_r2',\n",
    "    patience=CONFIG['early_stopping_patience'],\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# LR monitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir='/tmp/logs',\n",
    "    name='score_aligned',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "# Note: deterministic=True removed because AdaptiveAvgPool1d in TempoCurveEncoder\n",
    "# doesn't have a deterministic CUDA backward implementation.\n",
    "# Reproducibility is still ensured via pl.seed_everything(42) in the training cell.\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['max_epochs'],\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    precision=CONFIG['precision'],\n",
    "    gradient_clip_val=CONFIG['gradient_clip_val'],\n",
    "    accumulate_grad_batches=CONFIG['accumulate_grad_batches'],\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.5,  # Validate twice per epoch\n",
    ")\n",
    "\n",
    "print(\"Trainer configured!\")\n",
    "print(f\"  Precision: {CONFIG['precision']}\")\n",
    "print(f\"  Max epochs: {CONFIG['max_epochs']}\")\n",
    "print(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['accumulate_grad_batches']}\")\n",
    "print(f\"  Early stopping patience: {CONFIG['early_stopping_patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "print(\"\\nKey metrics to watch:\")\n",
    "print(\"  - val/mean_r2: Overall R-squared (target: 0.30-0.40)\")\n",
    "print(\"  - val/tempo_r2: Tempo dimension (currently -0.15, should improve!)\")\n",
    "print()\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints to Google Drive\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"Syncing checkpoints to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Comprehensive Evaluation\n",
    "\n",
    "This section provides:\n",
    "- Per-dimension analysis with statistical tests\n",
    "- Comparison to SOTA baselines (PercePiano paper)\n",
    "- Category-level metrics (timing, dynamics, etc.)\n",
    "- Visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with best checkpoint\n",
    "print(\"Running test with best checkpoint...\")\n",
    "best_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best checkpoint: {best_path}\")\n",
    "\n",
    "if best_path:\n",
    "    test_results = trainer.test(model, test_loader, ckpt_path=best_path)\n",
    "    print(\"\\nTest Results:\")\n",
    "    for k, v in test_results[0].items():\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\n\n# Load best model\nfrom src.models.score_aligned_module import ScoreAlignedModuleWithFallback\nbest_model = ScoreAlignedModuleWithFallback.load_from_checkpoint(checkpoint_callback.best_model_path)\nbest_model.eval()\nbest_model.cuda()\n\n# Helper to get note_locations from batch\ndef get_note_locations(batch):\n    if 'note_locations_beat' in batch:\n        return {\n            'beat': batch['note_locations_beat'],\n            'measure': batch['note_locations_measure'],\n            'voice': batch['note_locations_voice'],\n        }\n    return None\n\n# Collect predictions on test set\nall_preds = []\nall_targets = []\n\nprint(\"Collecting predictions on test set...\")\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        note_locations = get_note_locations(batch)\n        outputs = best_model(\n            batch['midi_tokens'],\n            batch['score_note_features'],\n            batch['score_global_features'],\n            batch['score_tempo_curve'],\n            batch.get('midi_attention_mask'),\n            batch.get('score_attention_mask'),\n            note_locations=note_locations,\n        )\n        all_preds.append(outputs['predictions'].cpu())\n        all_targets.append(batch['scores'].cpu())\n\nall_preds = torch.cat(all_preds).numpy()\nall_targets = torch.cat(all_targets).numpy()\ndimensions = best_model.dimensions\n\nprint(f\"Collected {len(all_preds)} test samples across {len(dimensions)} dimensions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import torch\nimport numpy as np\n\n# Load best model (using ScoreAlignedModule, not fallback version)\nfrom src.models.score_aligned_module import ScoreAlignedModule\nbest_model = ScoreAlignedModule.load_from_checkpoint(checkpoint_callback.best_model_path)\nbest_model.eval()\nbest_model.cuda()\n\n# Helper to get note_locations from batch\ndef get_note_locations(batch):\n    if 'note_locations_beat' in batch:\n        return {\n            'beat': batch['note_locations_beat'],\n            'measure': batch['note_locations_measure'],\n            'voice': batch['note_locations_voice'],\n        }\n    return None\n\n# Collect predictions on test set\nall_preds = []\nall_targets = []\n\nprint(\"Collecting predictions on test set...\")\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        note_locations = get_note_locations(batch)\n        outputs = best_model(\n            batch['midi_tokens'],\n            batch['score_note_features'],\n            batch['score_global_features'],\n            batch['score_tempo_curve'],\n            batch.get('midi_attention_mask'),\n            batch.get('score_attention_mask'),\n            note_locations=note_locations,\n        )\n        all_preds.append(outputs['predictions'].cpu())\n        all_targets.append(batch['scores'].cpu())\n\nall_preds = torch.cat(all_preds).numpy()\nall_targets = torch.cat(all_targets).numpy()\ndimensions = best_model.dimensions\n\nprint(f\"Collected {len(all_preds)} test samples across {len(dimensions)} dimensions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import (\n",
    "    compute_all_metrics,\n",
    "    PerDimensionAnalysis,\n",
    "    compare_to_sota,\n",
    "    format_comparison_table,\n",
    "    create_results_table,\n",
    "    PERCEPIANO_BASELINES,\n",
    "    DIMENSION_BASELINES,\n",
    ")\n",
    "\n",
    "# Compute all metrics\n",
    "metrics = compute_all_metrics(\n",
    "    predictions=all_preds,\n",
    "    targets=all_targets,\n",
    "    dimension_names=list(dimensions),\n",
    ")\n",
    "\n",
    "# Print results table\n",
    "print(create_results_table(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Per-Dimension Analysis\n",
    "\n",
    "Detailed analysis of which dimensions are working well and which need improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive per-dimension analysis\n",
    "analysis = PerDimensionAnalysis.from_predictions(\n",
    "    predictions=all_preds,\n",
    "    targets=all_targets,\n",
    "    dimension_names=list(dimensions),\n",
    "    weak_threshold=0.15,   # R^2 below this = needs improvement\n",
    "    strong_threshold=0.30, # R^2 above this = working well\n",
    ")\n",
    "\n",
    "print(analysis.format_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 SOTA Comparison\n",
    "\n",
    "Compare our model to published baselines from the PercePiano paper:\n",
    "- **Bi-LSTM**: R^2 = 0.185 (baseline)\n",
    "- **MidiBERT**: R^2 = 0.313 (pretrained MIDI encoder)\n",
    "- **Bi-LSTM + SA + HAN**: R^2 = 0.397 (best published result with score alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our overall R^2\n",
    "our_r2 = metrics['r2'].value\n",
    "\n",
    "# Get per-dimension R^2 for detailed comparison\n",
    "per_dim_r2 = metrics['r2'].per_dimension\n",
    "\n",
    "# Compare to SOTA\n",
    "comparison = compare_to_sota(\n",
    "    model_r2=our_r2,\n",
    "    model_name=\"CrescendAI (Score-Aligned)\",\n",
    "    split_type=\"piece\",  # PercePiano uses piece-split by default\n",
    "    per_dimension_r2=per_dim_r2,\n",
    ")\n",
    "\n",
    "print(format_comparison_table(comparison))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Visualization\n",
    "\n",
    "Visualize per-dimension results and error distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from src.evaluation import (\n",
    "    plot_per_dimension_results,\n",
    "    plot_prediction_scatter,\n",
    "    plot_error_distribution,\n",
    ")\n",
    "\n",
    "# Per-dimension R^2 bar chart with SOTA baselines\n",
    "fig1 = plot_per_dimension_results(\n",
    "    dimension_results=per_dim_r2,\n",
    "    metric_name=\"R^2\",\n",
    "    baseline_values=DIMENSION_BASELINES,\n",
    "    title=\"Per-Dimension R^2 (vs SOTA Baselines)\",\n",
    "    figsize=(14, 6),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction scatter plots for key dimensions\n",
    "key_dims = ['tempo', 'timing', 'dynamic_range', 'interpretation', 'articulation_length', 'mood_energy']\n",
    "key_indices = [list(dimensions).index(d) for d in key_dims if d in dimensions]\n",
    "\n",
    "fig2 = plot_prediction_scatter(\n",
    "    predictions=all_preds[:, key_indices],\n",
    "    targets=all_targets[:, key_indices],\n",
    "    dimension_names=[dimensions[i] for i in key_indices],\n",
    "    n_cols=3,\n",
    "    figsize=(12, 8),\n",
    ")\n",
    "plt.suptitle(\"Prediction vs Target (Key Dimensions)\", y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution by dimension\n",
    "fig3 = plot_error_distribution(\n",
    "    predictions=all_preds,\n",
    "    targets=all_targets,\n",
    "    dimension_names=list(dimensions),\n",
    "    figsize=(16, 6),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"=\"*70)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Overall performance\n",
    "print(f\"\\n1. OVERALL PERFORMANCE\")\n",
    "print(f\"   Mean R^2: {our_r2:.4f}\")\n",
    "print(f\"   Target (0.30-0.40): {'ACHIEVED' if our_r2 >= 0.30 else 'NOT YET'}\")\n",
    "print(f\"   Ranking vs SOTA: {comparison['rank']}/{comparison['total_baselines']}\")\n",
    "\n",
    "# 2. Tempo dimension (key target)\n",
    "tempo_r2 = per_dim_r2.get('tempo', 0)\n",
    "print(f\"\\n2. TEMPO DIMENSION (Primary Target)\")\n",
    "print(f\"   Previous (MIDI-only): R^2 ~ -0.15\")\n",
    "print(f\"   Current (Score-aligned): R^2 = {tempo_r2:.4f}\")\n",
    "print(f\"   Improvement: {'YES' if tempo_r2 > 0 else 'NEEDS WORK'}\")\n",
    "\n",
    "# 3. Strong dimensions\n",
    "print(f\"\\n3. STRONG DIMENSIONS (R^2 > 0.30)\")\n",
    "strong = analysis.get_ranked_dimensions('r2')[:5]\n",
    "for dim, r2 in strong:\n",
    "    if r2 > 0.30:\n",
    "        print(f\"   {dim}: {r2:.4f}\")\n",
    "\n",
    "# 4. Weak dimensions needing improvement\n",
    "print(f\"\\n4. DIMENSIONS NEEDING IMPROVEMENT (R^2 < 0.15)\")\n",
    "weak = analysis.get_ranked_dimensions('r2', ascending=True)[:5]\n",
    "for dim, r2 in weak:\n",
    "    if r2 < 0.15:\n",
    "        print(f\"   {dim}: {r2:.4f}\")\n",
    "\n",
    "# 5. Score alignment benefit\n",
    "if 'improvement_vs_midi_only' in comparison:\n",
    "    print(f\"\\n5. SCORE ALIGNMENT BENEFIT\")\n",
    "    print(f\"   vs Best MIDI-only baseline: {comparison['improvement_vs_midi_only']:+.4f}\")\n",
    "\n",
    "# 6. Category-level performance\n",
    "print(f\"\\n6. CATEGORY PERFORMANCE\")\n",
    "for cat, cat_metrics in sorted(analysis.category_metrics.items(), \n",
    "                                key=lambda x: x[1]['r2'], reverse=True):\n",
    "    print(f\"   {cat}: R^2={cat_metrics['r2']:.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save and Sync Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Save final model with comprehensive evaluation results\n",
    "final_path = Path(CONFIG['checkpoint_dir']) / 'score_aligned_final.pt'\n",
    "torch.save({\n",
    "    'state_dict': best_model.state_dict(),\n",
    "    'hparams': dict(best_model.hparams),\n",
    "    'dimensions': list(dimensions),\n",
    "    'metrics': {\n",
    "        'r2': our_r2,\n",
    "        'per_dimension_r2': per_dim_r2,\n",
    "        'mse': metrics['mse'].value,\n",
    "        'mae': metrics['mae'].value,\n",
    "        'pearson_r': metrics['pearson_r'].value,\n",
    "        'spearman_rho': metrics['spearman_rho'].value,\n",
    "    },\n",
    "    'sota_comparison': {\n",
    "        'rank': comparison['rank'],\n",
    "        'total_baselines': comparison['total_baselines'],\n",
    "        'vs_best_baseline': comparison['improvement_vs_best'],\n",
    "    },\n",
    "    'category_metrics': analysis.category_metrics,\n",
    "    'strong_dimensions': analysis.strong_dimensions,\n",
    "    'weak_dimensions': analysis.weak_dimensions,\n",
    "}, final_path)\n",
    "print(f\"Saved final model to {final_path}\")\n",
    "\n",
    "# Final sync to Google Drive\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"\\nFinal sync to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")\n",
    "    print(f\"Checkpoints available at: {CONFIG['gdrive_checkpoint']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}