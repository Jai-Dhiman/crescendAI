{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Train with Original PercePiano Code\n",
    "\n",
    "Use the ORIGINAL PercePiano model implementation directly.\n",
    "This ensures we're using their exact architecture.\n",
    "\n",
    "## Goal\n",
    "- Use original `VirtuosoNetMultiLevel` from PercePiano\n",
    "- Use original `HanEncoder` from PercePiano  \n",
    "- Adapt data loading for our preprocessed data\n",
    "- Target: R2 = 0.397 (Paper SOTA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pathlib import Path\n\n# Check for local PercePiano code first, then cloud paths\nLOCAL_PERCEPIANO = Path('/Users/jdhiman/Documents/crescendai/model/data/raw/PercePiano/virtuoso/virtuoso')\nCLOUD_PERCEPIANO = Path('/tmp/crescendai/model/data/raw/PercePiano/virtuoso/virtuoso')\n\nif LOCAL_PERCEPIANO.exists():\n    PERCEPIANO_PATH = LOCAL_PERCEPIANO\nelif CLOUD_PERCEPIANO.exists():\n    PERCEPIANO_PATH = CLOUD_PERCEPIANO\nelse:\n    # Clone if needed (Thunder Compute)\n    print(\"Cloning PercePiano repository...\")\n    import subprocess\n    subprocess.run(['git', 'clone', 'https://github.com/JonghoKimSNU/PercePiano.git', '/tmp/PercePiano'], check=True)\n    PERCEPIANO_PATH = Path('/tmp/PercePiano/virtuoso/virtuoso')\n\n# Add to Python path\nsys.path.insert(0, str(PERCEPIANO_PATH))\nsys.path.insert(0, str(PERCEPIANO_PATH / 'pyScoreParser'))\n\nprint(f\"PercePiano path: {PERCEPIANO_PATH}\")\nprint(f\"Path exists: {PERCEPIANO_PATH.exists()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "!pip install omegaconf tqdm scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 2: Import Original PercePiano Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import original PercePiano components\n",
    "from model_m2pf import VirtuosoNetMultiLevel, VirtuosoNetSingle\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "\n",
    "print(\"Successfully imported original PercePiano models!\")\n",
    "print(f\"  VirtuosoNetMultiLevel: {VirtuosoNetMultiLevel}\")\n",
    "print(f\"  VirtuosoNetSingle: {VirtuosoNetSingle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Load SOTA config\nLOCAL_CONFIG = Path('/Users/jdhiman/Documents/crescendai/model/data/raw/PercePiano/virtuoso/ymls/shared/label19/han_measnote_nomask_bigger256.yml')\nCLOUD_CONFIG = Path('/tmp/PercePiano/virtuoso/ymls/shared/label19/han_measnote_nomask_bigger256.yml')\n\nif LOCAL_CONFIG.exists():\n    CONFIG_PATH = LOCAL_CONFIG\nelif CLOUD_CONFIG.exists():\n    CONFIG_PATH = CLOUD_CONFIG\nelse:\n    CONFIG_PATH = PERCEPIANO_PATH.parent / 'ymls' / 'shared' / 'label19' / 'han_measnote_nomask_bigger256.yml'\n\nwith open(CONFIG_PATH, 'r') as f:\n    config = yaml.safe_load(f)\n\nnet_param = OmegaConf.create(config['nn_params'])\n\n# Override input_size to match our 84-dimension features\nnet_param.input_size = 84\n\nprint(\"\\nSOTA Configuration loaded:\")\nprint(f\"  Config: {CONFIG_PATH.name}\")\nprint(f\"  score_encoder: {net_param.score_encoder_name}\")\nprint(f\"  hidden_size: {net_param.encoder.size}\")\nprint(f\"  note_layers: {net_param.note.layer}\")\nprint(f\"  voice_layers: {net_param.voice.layer}\")\nprint(f\"  beat_layers: {net_param.beat.layer}\")\nprint(f\"  measure_layers: {net_param.measure.layer}\")\nprint(f\"  attention_heads: {net_param.num_attention_head}\")\nprint(f\"  dropout: {net_param.drop_out}\")\nprint(f\"  input_size: {net_param.input_size} (our 84-dim features)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = Path('/tmp/percepiano_vnet_84dim')\n",
    "CHECKPOINT_ROOT = Path('/tmp/checkpoints/percepiano_original')\n",
    "GDRIVE_DATA_PATH = 'gdrive:crescendai_data/percepiano_vnet_84dim'\n",
    "\n",
    "# Create directories\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check rclone\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "RCLONE_AVAILABLE = 'gdrive:' in result.stdout\n",
    "print(f\"rclone available: {RCLONE_AVAILABLE}\")\n",
    "\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"\\nDownloading data from Google Drive...\")\n",
    "    subprocess.run(['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'])\n",
    "\n",
    "# Verify\n",
    "train_count = len(list((DATA_ROOT / 'train').glob('*.pkl'))) if (DATA_ROOT / 'train').exists() else 0\n",
    "val_count = len(list((DATA_ROOT / 'val').glob('*.pkl'))) if (DATA_ROOT / 'val').exists() else 0\n",
    "print(f\"\\nData: train={train_count}, val={val_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Create Data Stats (required by PercePiano model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "import pickle\nimport numpy as np\n\n# Load our stat file\nstat_path = DATA_ROOT / 'stat.pkl'\nif stat_path.exists():\n    with open(stat_path, 'rb') as f:\n        our_stats = pickle.load(f)\n    print(\"Loaded our stats\")\n    print(f\"Keys: {list(our_stats.keys())}\")\nelse:\n    print(\"No stat.pkl found - will create minimal stats\")\n    our_stats = {}\n\n# Create data_stats in format expected by PercePiano\n# The MixEmbedder only needs key_to_dim to exist (doesn't use it unless use_continuos_feature_only=True)\n# We provide an empty dict which is sufficient\ndata_stats = {\n    'key_to_dim': {'input': {}},  # Empty dict - MixEmbedder will use net_param.input_size directly\n    'stats': our_stats,\n    'graph_keys': [],\n}\n\nprint(f\"\\ndata_stats created with keys: {list(data_stats.keys())}\")\nprint(f\"key_to_dim: {data_stats['key_to_dim']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Create Dataset Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_sequence\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class PercePianoDataset(Dataset):\n",
    "    \"\"\"Adapter to load our preprocessed data for original PercePiano model.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', max_notes=5000):\n",
    "        self.data_dir = Path(data_dir) / split\n",
    "        self.max_notes = max_notes\n",
    "        \n",
    "        # Load all pkl files\n",
    "        self.files = sorted([f for f in self.data_dir.glob('*.pkl') if f.name != 'stat.pkl'])\n",
    "        print(f\"Loaded {len(self.files)} samples from {split}\")\n",
    "        \n",
    "        # Load label file\n",
    "        label_path = Path(data_dir).parent / 'label_2round_mean_reg_19_with0_rm_highstd0.json'\n",
    "        if not label_path.exists():\n",
    "            # Try PercePiano location\n",
    "            label_path = PERCEPIANO_PATH.parent.parent / 'label_2round_mean_reg_19_with0_rm_highstd0.json'\n",
    "        \n",
    "        if label_path.exists():\n",
    "            with open(label_path) as f:\n",
    "                self.label_map = json.load(f)\n",
    "            print(f\"Loaded {len(self.label_map)} labels\")\n",
    "        else:\n",
    "            self.label_map = None\n",
    "            print(\"No external label file - using embedded labels\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.files[idx], 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Get input features\n",
    "        x = torch.tensor(data['input'], dtype=torch.float32)\n",
    "        \n",
    "        # Truncate if needed\n",
    "        if len(x) > self.max_notes:\n",
    "            x = x[:self.max_notes]\n",
    "        \n",
    "        # Get note locations\n",
    "        note_locations = {\n",
    "            'beat': torch.tensor(data['note_location']['beat'][:len(x)], dtype=torch.long),\n",
    "            'measure': torch.tensor(data['note_location']['measure'][:len(x)], dtype=torch.long),\n",
    "            'voice': torch.tensor(data['note_location']['voice'][:len(x)], dtype=torch.long),\n",
    "            'section': torch.tensor(data['note_location']['section'][:len(x)], dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        # Get labels (19 dimensions)\n",
    "        if 'labels' in data:\n",
    "            labels = torch.tensor(data['labels'][:19], dtype=torch.float32)\n",
    "        else:\n",
    "            labels = torch.zeros(19, dtype=torch.float32)\n",
    "        \n",
    "        return x, note_locations, labels\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate batch for PercePiano model (pack sequences).\"\"\"\n",
    "    xs, note_locs, labels = zip(*batch)\n",
    "    \n",
    "    # Pack sequences (sorted by length, descending)\n",
    "    lengths = [len(x) for x in xs]\n",
    "    sorted_idx = sorted(range(len(lengths)), key=lambda i: lengths[i], reverse=True)\n",
    "    \n",
    "    xs_sorted = [xs[i] for i in sorted_idx]\n",
    "    batch_x = pack_sequence(xs_sorted, enforce_sorted=True)\n",
    "    \n",
    "    # Pad note locations\n",
    "    note_locations = {\n",
    "        'beat': pad_sequence([note_locs[i]['beat'] for i in sorted_idx], batch_first=True),\n",
    "        'measure': pad_sequence([note_locs[i]['measure'] for i in sorted_idx], batch_first=True),\n",
    "        'voice': pad_sequence([note_locs[i]['voice'] for i in sorted_idx], batch_first=True),\n",
    "        'section': pad_sequence([note_locs[i]['section'] for i in sorted_idx], batch_first=True),\n",
    "    }\n",
    "    \n",
    "    # Stack labels\n",
    "    labels_batch = torch.stack([labels[i] for i in sorted_idx])\n",
    "    \n",
    "    return batch_x, note_locations, labels_batch\n",
    "\n",
    "\n",
    "# Test dataset\n",
    "train_ds = PercePianoDataset(DATA_ROOT, 'train')\n",
    "val_ds = PercePianoDataset(DATA_ROOT, 'val')\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_ds)}\")\n",
    "print(f\"Val samples: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 6: Initialize Original PercePiano Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Verify input size matches our data\nsample_x, _, _ = train_ds[0]\nactual_input_size = sample_x.shape[1]\nprint(f\"Data input size: {actual_input_size}\")\nprint(f\"Config input size: {net_param.input_size}\")\n\nif actual_input_size != net_param.input_size:\n    print(f\"Updating config input_size from {net_param.input_size} to {actual_input_size}\")\n    net_param.input_size = actual_input_size\n\n# Set graph_keys (required by model)\nnet_param.graph_keys = []\n\n# Create the model using original PercePiano code\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = VirtuosoNetMultiLevel(net_param, data_stats, multi_level=\"total_note_cat\")\nmodel = model.to(device)\n\n# Count parameters\nn_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nModel: VirtuosoNetMultiLevel\")\nprint(f\"Parameters: {n_params:,}\")\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 7: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Hyperparameters (matching paper)\n",
    "BATCH_SIZE = 8\n",
    "LR = 2.5e-5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "MAX_EPOCHS = 200\n",
    "PATIENCE = 40\n",
    "GRAD_CLIP = 2.0\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          collate_fn=collate_fn, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3000, gamma=0.98)\n",
    "criterion = torch.nn.MSELoss()\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "print(f\"Training config:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  LR: {LR}\")\n",
    "print(f\"  Max epochs: {MAX_EPOCHS}\")\n",
    "print(f\"  Patience: {PATIENCE}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_x, note_locations, labels in loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        note_locations = {k: v.to(device) for k, v in note_locations.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (returns tuple for multi-level)\n",
    "        outputs = model(batch_x, None, None, note_locations)\n",
    "        logits = outputs[-1]  # Last level (total_note_cat)\n",
    "        preds = sigmoid(logits)\n",
    "        \n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, note_locations, labels in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            note_locations = {k: v.to(device) for k, v in note_locations.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_x, None, None, note_locations)\n",
    "            logits = outputs[-1]\n",
    "            preds = sigmoid(logits)\n",
    "            \n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    import numpy as np\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    r2 = r2_score(all_labels, all_preds)\n",
    "    \n",
    "    return total_loss / len(loader), r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING WITH ORIGINAL PERCEPIANO MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTarget: R2 = 0.397 (Paper SOTA)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_r2 = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Check for best\n",
    "    is_best = val_r2 > best_r2\n",
    "    if is_best:\n",
    "        best_r2 = val_r2\n",
    "        best_epoch = epoch\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'r2': val_r2,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, CHECKPOINT_ROOT / 'best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Log\n",
    "    best_marker = \" *best*\" if is_best else \"\"\n",
    "    print(f\"Epoch {epoch:3d} | train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | \"\n",
    "          f\"val_r2: {val_r2:+.4f} | time: {elapsed:.1f}s{best_marker}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch} (patience={PATIENCE})\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBest R2: {best_r2:+.4f} (epoch {best_epoch})\")\n",
    "print(f\"Target: R2 = 0.397 (Paper SOTA)\")\n",
    "\n",
    "if best_r2 >= 0.35:\n",
    "    print(\"[SUCCESS] Approaching SOTA!\")\n",
    "elif best_r2 >= 0.30:\n",
    "    print(\"[GOOD] Strong performance\")\n",
    "elif best_r2 >= 0.20:\n",
    "    print(\"[PARTIAL] Reasonable but below target\")\n",
    "else:\n",
    "    print(\"[ISSUE] Below expected - investigate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 8: Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-dimension R2 analysis\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "DIMENSIONS = [\n",
    "    'timing', 'articulation_length', 'articulation_touch',\n",
    "    'pedal_amount', 'pedal_clarity', 'timbre_variety', 'timbre_depth',\n",
    "    'timbre_brightness', 'timbre_loudness', 'sophistication',\n",
    "    'dynamic_range', 'tempo', 'space', 'balance', 'drama',\n",
    "    'mood_valence', 'mood_energy', 'mood_imagination', 'interpretation'\n",
    "]\n",
    "\n",
    "# Get all predictions\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, note_locations, labels in val_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        note_locations = {k: v.to(device) for k, v in note_locations.items()}\n",
    "        \n",
    "        outputs = model(batch_x, None, None, note_locations)\n",
    "        preds = sigmoid(outputs[-1])\n",
    "        \n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_labels = np.vstack(all_labels)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PER-DIMENSION R2\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Dimension':<25} {'R2':>10}\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "dim_r2s = []\n",
    "for i, dim in enumerate(DIMENSIONS):\n",
    "    if i < all_preds.shape[1]:\n",
    "        r2 = r2_score(all_labels[:, i], all_preds[:, i])\n",
    "        dim_r2s.append((dim, r2))\n",
    "\n",
    "# Sort by R2\n",
    "dim_r2s.sort(key=lambda x: x[1], reverse=True)\n",
    "for dim, r2 in dim_r2s:\n",
    "    status = \"[OK]\" if r2 >= 0.2 else \"[LOW]\" if r2 >= 0 else \"[NEG]\"\n",
    "    print(f\"{dim:<25} {r2:>+10.4f} {status}\")\n",
    "\n",
    "positive = sum(1 for _, r2 in dim_r2s if r2 > 0)\n",
    "print(f\"\\nPositive R2: {positive}/{len(dim_r2s)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}