{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A — End-to-End Colab Pipeline (with 1-item smoke test)\n",
    "\n",
    "This notebook runs the full pipeline in Colab:\n",
    "- Clone repo and install dependencies (uv)\n",
    "- 1-item smoke test (synthetic) to verify InfoNCE + model apply\n",
    "- MAESTRO subset acquisition (half train by duration) with selective extraction\n",
    "- Build MAESTRO manifest (and optional ASAP)\n",
    "- Build window-level proxies\n",
    "- SSL pretraining with InfoNCE (piece-aware sampling + temperature schedule)\n",
    "- Minimal local head training on proxies (few steps)\n",
    "- Export checkpoints/artifacts for app integration\n",
    "\n",
    "Tip: If you open a new Colab runtime, re-run this notebook top-to-bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement Plan Checklist\n",
    "- [x] SSL InfoNCE objective with temperature schedule\n",
    "- [x] Piece-aware sampling (unique_per_file) in iterators\n",
    "- [x] LocalWindowHead scaffold and minimal training\n",
    "- [x] Piece-aware CV splits (hash-based)\n",
    "- [ ] Probe and dedup reports\n",
    "- [ ] Linear calibration export\n",
    "\n",
    "New additions in this notebook:\n",
    "- [x] Proxy CV evaluation with bootstrap confidence intervals and baseline\n",
    "- [x] Mixed precision (bf16) toggle for pretraining\n",
    "- [x] Early stopping on InfoNCE loss + warmup+cosine LR (already present)\n",
    "- [x] Checkpoint resume support\n",
    "- [x] Gradient accumulation via optimizer multi-steps\n",
    "- [ ] Basic logging (TensorBoard/W&B) — optional cell provided below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Clone repo (idempotent), install uv, sync deps\n",
    "REPO_URL = 'https://github.com/Jai-Dhiman/crescendai.git'\n",
    "import os\n",
    "if not os.path.exists('/content/crescendai'):\n",
    "    !git clone --depth=1 $REPO_URL /content/crescendai\n",
    "else:\n",
    "    %cd /content/crescendai\n",
    "    !git pull --ff-only\n",
    "%cd /content/crescendai/model\n",
    "\n",
    "# Install uv and sync\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "!~/.local/bin/uv --version\n",
    "!~/.local/bin/uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Mount Google Drive to persist manifests/reports/checkpoints/exports\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive/crescendai'\n",
    "    os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "    print('Drive mounted at', DRIVE_ROOT)\n",
    "except Exception as e:\n",
    "    DRIVE_ROOT = None\n",
    "    print('Drive not mounted; continuing without persistence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-item Smoke Test (synthetic)\n",
    "Verifies JAX/Flax, SSAST apply, and InfoNCE train step work end-to-end without real data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, jax.numpy as jnp\n",
    "from src.models.ssast_pretraining import SSASTPreTrainingModel\n",
    "from src.training.masked_train_step import ssl_infonce_train_step\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Create tiny SSAST encoder (mask_prob=0 for clean embeddings)\n",
    "encoder = SSASTPreTrainingModel(patch_size=16, embed_dim=256, num_layers=3, num_heads=4, mask_prob=0.0)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "dummy = jnp.ones((2, 128, 128), dtype=jnp.float32)  # [B,T,F]\n",
    "params = encoder.init(rng, dummy, rng, training=False)\n",
    "\n",
    "# Wrap params into a simple TrainState\n",
    "class EncState(train_state.TrainState): pass\n",
    "tx = optax.adamw(learning_rate=1e-4, weight_decay=0.01)\n",
    "state = EncState(apply_fn=encoder.apply, params=params, tx=tx, opt_state=tx.init(params))\n",
    "\n",
    "# Pad masks zeros (no padding)\n",
    "pad_masks = jnp.zeros_like(dummy, dtype=bool)\n",
    "\n",
    "# One InfoNCE step with deterministic small temperature and no augmentation\n",
    "dropout_rng, stochastic_rng = jax.random.split(rng)\n",
    "state, metrics = ssl_infonce_train_step(\n",
    "    state, dummy, pad_masks, dropout_rng, stochastic_rng,\n",
    "    temperature=0.2, augment=False\n",
    ")\n",
    "print('Smoke test OK — InfoNCE loss:', float(metrics['info_nce_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation — MAESTRO subset (half train) with selective extraction\n",
    "This section downloads only the CSV, chooses ~half the train split by duration (capped by MAX_HOURS),\n",
    "downloads the single zip, extracts only the selected files, then deletes the zip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, urllib.request, json, shutil, subprocess, zipfile\n",
    "os.makedirs('/content/data/manifests', exist_ok=True)\n",
    "os.makedirs('/content/data/reports', exist_ok=True)\n",
    "os.makedirs('/content/tmp', exist_ok=True)\n",
    "\n",
    "CSV_PATH = '/content/data/maestro-v3.0.0.csv'\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    urllib.request.urlretrieve('https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0.csv', CSV_PATH)\n",
    "with open(CSV_PATH, newline='') as f:\n",
    "    rows = list(csv.DictReader(f))\n",
    "train = [r for r in rows if r.get('split','')=='train']\n",
    "def pf(x):\n",
    "    try: return float(x)\n",
    "    except: return 0.0\n",
    "for r in train: r['duration']=pf(r.get('duration',0.0))\n",
    "total_s = sum(r['duration'] for r in train)\n",
    "MAX_HOURS = 100.0  # adjust as needed\n",
    "target_s = min(total_s*0.5, MAX_HOURS*3600)\n",
    "train_sorted = sorted(train, key=lambda r: (r.get('year',''), r.get('audio_filename','')))\n",
    "sel, acc = [], 0.0\n",
    "for r in train_sorted:\n",
    "    if acc >= target_s: break\n",
    "    sel.append(r); acc += r['duration']\n",
    "with open('/content/data/manifests/maestro_train_half_files.txt','w') as f:\n",
    "    for r in sel: f.write(r['audio_filename'].strip()+'\n')\n",
    "print('Selected files:', len(sel), 'Duration(h)=', sum(r['duration'] for r in sel)/3600.0)\n",
    "\n",
    "# Download & selective extract\n",
    "MAE_ZIP_URL='https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0.zip'\n",
    "TMP_ZIP='/content/tmp/maestro-v3.0.0.zip'\n",
    "DEST_ROOT='/content/data/maestro-v3.0.0'\n",
    "os.makedirs(DEST_ROOT, exist_ok=True)\n",
    "out = subprocess.check_output(['bash','-lc', f\"curl -sI {MAE_ZIP_URL} | awk '/Content-Length/ {{print $2}}'\"])\n",
    "zip_size=int(out.decode().strip()); assert zip_size>0\n",
    "free=shutil.disk_usage('/content').free\n",
    "if not os.path.exists(TMP_ZIP):\n",
    "    subprocess.run(['bash','-lc', f\"curl -L {MAE_ZIP_URL} -o {TMP_ZIP}\"], check=True)\n",
    "members_needed=set('maestro-v3.0.0/'+r['audio_filename'].strip() for r in sel)\n",
    "with zipfile.ZipFile(TMP_ZIP) as z:\n",
    "    names=set(z.namelist())\n",
    "    missing=[m for m in members_needed if m not in names]\n",
    "    if missing: raise RuntimeError(f'Missing {len(missing)} expected files')\n",
    "    for i,m in enumerate(sorted(members_needed)):\n",
    "        z.extract(m, path='/content/data')\n",
    "        if (i+1)%50==0: print('Extracted',i+1,'/',len(members_needed))\n",
    "# Copy CSV next to data and remove zip\n",
    "_ = shutil.copy2(CSV_PATH, f'{DEST_ROOT}/maestro-v3.0.0.csv')\n",
    "try: os.remove(TMP_ZIP)\n",
    "except: pass\n",
    "print('MAESTRO subset ready at', DEST_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MAESTRO manifest\n",
    "%cd /content/crescendai/model\n",
    "!~/.local/bin/uv run python src/tools/build_manifest.py \\\n --dataset maestro \\\n --root /content/data/maestro-v3.0.0 \\\n --split train \\\n --file-list /content/data/manifests/maestro_train_half_files.txt \\\n --output /content/data/manifests/maestro_train_half.jsonl \\\n --license CC-BY-NC-4.0\n",
    "# Copy manifest to Drive if mounted\n",
    "import shutil, os\n",
    "if DRIVE_ROOT:\n",
    "    os.makedirs(f'{DRIVE_ROOT}/manifests', exist_ok=True)\n",
    "    shutil.copy2('/content/data/manifests/maestro_train_half.jsonl', f'{DRIVE_ROOT}/manifests/')\n",
    "    print('Manifest copied to Drive')\n",
    "else:\n",
    "    print('Drive not mounted; skipping manifest copy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: ASAP retrieval & manifest (uncomment and configure)\n",
    "If you want ASAP now, uncomment and set ASAP_URL or use Git LFS as in the data setup notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (disabled by default)\n",
    "# !mkdir -p /content/data/ASAP\n",
    "# ASAP_URL = 'https://REPLACE_WITH_OFFICIAL_ASAP_ARCHIVE_URL'\n",
    "# if ASAP_URL.startswith('http'):\n",
    "#     !curl -L "$ASAP_URL" -o /content/tmp/asap_download\n",
    "#     !unzip -q /content/tmp/asap_download -d /content/data/ASAP || tar -xzf /content/tmp/asap_download -C /content/data/ASAP\n",
    "#     !rm -f /content/tmp/asap_download\n",
    "# !~/.local/bin/uv run python src/tools/build_manifest.py --dataset asap --root /content/data/ASAP --output /content/data/manifests/asap.jsonl --license CC-BY-NC-SA-4.0\n",
    "# if DRIVE_ROOT:\n",
    "#     import shutil, os\n",
    "#     os.makedirs(f'{DRIVE_ROOT}/manifests', exist_ok=True)\n",
    "#     shutil.copy2('/content/data/manifests/asap.jsonl', f'{DRIVE_ROOT}/manifests/')\n",
    "#     print('ASAP manifest copied to Drive')\n",
    "# else:\n",
    "#     print('Drive not mounted; skipping ASAP manifest copy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build window-level proxies (MAESTRO; ASAP optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /content/data/proxies\n",
    "!~/.local/bin/uv run python -m src.tools.build_proxies \\\n --manifest /content/data/manifests/maestro_train_half.jsonl \\\n --out /content/data/proxies/maestro_train_half_windows.jsonl \\\n --max-files 200 --skip-errors\n",
    "# Optional: ASAP proxies\n",
    "import os\n",
    "if os.path.exists('/content/data/manifests/asap.jsonl'):\n",
    "    !~/.local/bin/uv run python -m src.tools.build_proxies \\\n --manifest /content/data/manifests/asap.jsonl \\\n --out /content/data/proxies/asap_windows.jsonl \\\n --max-files 200 --skip-errors\n",
    "# Copy to Drive if mounted\n",
    "import shutil\n",
    "if DRIVE_ROOT:\n",
    "    os.makedirs(f'{DRIVE_ROOT}/proxies', exist_ok=True)\n",
    "    if os.path.exists('/content/data/proxies/maestro_train_half_windows.jsonl'):\n",
    "        shutil.copy2('/content/data/proxies/maestro_train_half_windows.jsonl', f'{DRIVE_ROOT}/proxies/')\n",
    "    if os.path.exists('/content/data/proxies/asap_windows.jsonl'):\n",
    "        shutil.copy2('/content/data/proxies/asap_windows.jsonl', f'{DRIVE_ROOT}/proxies/')\n",
    "    print('Proxies copied to Drive')\n",
    "else:\n",
    "    print('Drive not mounted; skipping proxies copy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSL Pretraining (InfoNCE + piece-aware sampling + temperature schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.training import ASTTrainingPipeline\n",
    "from src.utils.seeding import set_seed\n",
    "set_seed(42, deterministic=True)\n",
    "cfg = {\n",
    "    'checkpoint_dir': 'checkpoints/ast_ssast',\n",
    "    'results_dir': 'results/ast_training',\n",
    "    'cache_dir': 'cache/maestro',\n",
    "    'maestro_path': '/content/data/maestro-v3.0.0',\n",
    "    'batch_size': 8,\n",
    "    'segment_length': 128,\n",
    "    'patch_size': 16,\n",
    "    'embed_dim': 256,\n",
    "    'num_layers': 3,\n",
    "    'num_heads': 4,\n",
    "    'pretrain_lr': 1e-4,\n",
    "    'pretrain_steps': 2000,          # adjust higher for real runs\n",
    "    'log_interval': 50,\n",
    "    'save_interval': 500,\n",
    "    'ssl_enable': True,\n",
    "    'ssl_objective': 'infonce',\n",
    "    'temperature_start': 0.2,\n",
    "    'temperature_end': 0.07,\n",
    "    'augment': True,\n",
    "    'augment_time_mask': 8,\n",
    "    'augment_freq_mask': 8,\n",
    "    'augment_db_jitter': 1.5,\n",
    "    'augment_db_noise_std': 0.2,\n",
    "    'augment_time_shift': 2,\n",
    "    # New toggles\n",
    "    'use_bf16': True,                # set False if incompatible\n",
    "    'resume': True,                  # resume from ssast_final if present\n",
    "    'early_stop_patience': 200,      # steps without improvement before stop\n",
    "    'early_stop_min_delta': 5e-4,    # require this improvement\n",
    "    'accumulate_steps': 4,           # gradient accumulation (via optimizer multi-steps)\n",
    "}\n",
    "pipeline = ASTTrainingPipeline(cfg)\n",
    "ckpt = pipeline.pretrain_ssast()\n",
    "print('Pretraining checkpoint at:', ckpt)\n",
    "# Copy checkpoints to Drive if mounted\n",
    "import shutil, os\n",
    "if DRIVE_ROOT and os.path.exists('checkpoints'):\n",
    "    shutil.make_archive('/content/ast_ssast_checkpoints', 'zip', 'checkpoints')\n",
    "    shutil.move('/content/ast_ssast_checkpoints.zip', f'{DRIVE_ROOT}/ast_ssast_checkpoints.zip')\n",
    "    print('Checkpoints zipped and copied to Drive')\n",
    "else:\n",
    "    print('Drive not mounted or no checkpoints found; skipping copy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Basic logging (Weights & Biases)\n",
    "Set WANDB_API_KEY as an environment variable before running to enable online mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional W&B logging — safe to skip if not configured\n",
    "try:\n",
    "    import os, wandb\n",
    "    wandb_mode = os.environ.get('WANDB_MODE', 'offline')\n",
    "    run = wandb.init(project='crescendai-ssl', config=cfg, mode=wandb_mode)\n",
    "    wandb.log({'pretrain_steps': cfg['pretrain_steps']})\n",
    "    if 'ckpt' in globals():\n",
    "        wandb.summary['checkpoint'] = ckpt\n",
    "    print('W&B logging initialized (mode=', wandb_mode, ')')\n",
    "except Exception as e:\n",
    "    print('W&B not configured:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Local Head Training on Proxies (sanity run)\n",
    "This section trains the LocalWindowHead for a few steps on window-level proxies (MAESTRO).\n",
    "It uses the SSAST encoder to get token embeddings and the local head to predict 5 proxies.\n"
   ]
  }]},
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np\n",
    "import jax, jax.numpy as jnp\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from src.models.ssast_pretraining import SSASTPreTrainingModel\n",
    "from src.models.local_heads import LocalWindowHead\n",
    "from src.data.audio_io import load_audio_mono_22050, mel_db_128x128\n",
    "\n",
    "# Load a few proxy records\n",
    "PROXY_JSONL = '/content/data/proxies/maestro_train_half_windows.jsonl'\n",
    "recs = []\n",
    "with open(PROXY_JSONL) as f:\n",
    "    for i,line in enumerate(f):\n",
    "        if i>=64: break\n",
    "        recs.append(json.loads(line))\n",
    "print('Loaded proxy records:', len(recs))\n",
    "\n",
    "# Prepare encoder and local head\n",
    "rng = jax.random.PRNGKey(123)\n",
    "encoder = SSASTPreTrainingModel(patch_size=16, embed_dim=256, num_layers=3, num_heads=4, mask_prob=0.0)\n",
    "dummy = jnp.ones((1,128,128), jnp.float32)\n",
    "enc_params = encoder.init(rng, dummy, rng, training=False)\n",
    "head = LocalWindowHead(out_dims=5, hidden_dim=128, dropout_rate=0.1)\n",
    "# Token count = (128/16)*(128/16) = 64; embed_dim=256\n",
    "tokens_dummy = jnp.ones((1,64,256), jnp.float32)\n",
    "head_params = head.init(rng, tokens_dummy, training=True)\n",
    "\n",
    "class PairState(train_state.TrainState): pass\n",
    "tx = optax.adamw(learning_rate=3e-4, weight_decay=1e-2)\n",
    "params_all = {'encoder': enc_params, 'head': head_params}\n",
    "opt_state = tx.init(params_all)\n",
    "state = PairState(apply_fn=None, params=params_all, tx=tx, opt_state=opt_state)\n",
    "\n",
    "def make_batch(records):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for r in records:\n",
    "        p = r['audio_path']\n",
    "        s0, s1 = int(r['start_sample']), int(r['end_sample'])\n",
    "        y = load_audio_mono_22050(p, target_sr=22050)[s0:s1]\n",
    "        if y.size < 2048:\n",
    "            continue\n",
    "        mel = mel_db_128x128(y, sr=22050)\n",
    "        xs.append(mel)\n",
    "        t = r['proxies']\n",
    "        ys.append([t['rms_dr_db'], t['onset_density'], t['centroid_mean'], t['rolloff_mean'], t['tempo_bpm']])\n",
    "    if not xs:\n",
    "        raise RuntimeError('No usable windows in batch')\n",
    "    X = jnp.array(xs, dtype=jnp.float32)\n",
    "    Y = jnp.array(ys, dtype=jnp.float32)\n",
    "    return X, Y\n",
    "\n",
    "@jax.jit\n",
    "def step(state, specs, targets, rng):\n",
    "    def loss_fn(params):\n",
    "        out = encoder.apply(params['encoder'], specs, rng, training=False)\n",
    "        feats = out.get('encoder_embeddings', out)  # [B,P,D]\n",
    "        preds = head.apply(params['head'], feats, training=True)  # [B,P,5]\n",
    "        # Simple pooling: mean over tokens\n",
    "        preds_g = jnp.mean(preds, axis=1)  # [B,5]\n",
    "        loss = jnp.mean((preds_g - targets)**2)\n",
    "        return loss, {'loss': loss}\n",
    "    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    updates, new_opt = state.tx.update(grads, state.opt_state, state.params)\n",
    "    new_params = optax.apply_updates(state.params, updates)\n",
    "    return state.replace(params=new_params, opt_state=new_opt), metrics\n",
    "\n",
    "# Train a few steps over small batches of proxy records\n",
    "BATCH = 8\n",
    "for it in range(5):\n",
    "    batch = recs[it*BATCH:(it+1)*BATCH]\n",
    "    if not batch: break\n",
    "    X, Y = make_batch(batch)\n",
    "    rng, sub = jax.random.split(rng)\n",
    "    state, metrics = step(state, X, Y, sub)\n",
    "    print(f'proxy step {it}: loss={float(metrics["loss"]):.4f}')\n",
    "\n",
    "# Save local head params to Drive\n",
    "import flax.serialization as serialization\n",
    "if DRIVE_ROOT:\n",
    "    os.makedirs(f'{DRIVE_ROOT}/exports', exist_ok=True)\n",
    "    with open(f'{DRIVE_ROOT}/exports/local_head_params.msgpack', 'wb') as f:\n",
    "        f.write(serialization.to_bytes(state.params['head']))\n",
    "    print('Local head params exported to Drive.')\n",
    "else:\n",
    "    print('Drive not mounted; skipping export')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export notes\n",
    "- Pretraining checkpoints are already saved under checkpoints/ (zipped and copied to Drive above).\n",
    "- Local head parameters are exported to Drive as msgpack.\n",
    "- Your app can load the encoder params from the pretraining checkpoint and the local head params for proxy predictions.\n",
    "\n",
    "If you want, we can add a consolidated export format (single file) in a follow-up.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
