{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# Audio Baseline for PercePiano (MERT-330M)\n",
    "\n",
    "Train audio baseline using MERT-330M embeddings on Thunder Compute.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. Download pre-rendered WAV files from Google Drive\n",
    "2. Extract MERT-330M embeddings (GPU required)\n",
    "3. Train 4-fold cross-validation\n",
    "4. Evaluate and analyze results\n",
    "\n",
    "## Target: R2 >= 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-header",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gpu-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. MERT extraction will be very slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-install-rclone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rclone\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Thunder Compute has PyTorch pre-installed)\n",
    "!pip install transformers librosa soundfile pytorch_lightning --quiet\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "if 'gdrive:' not in result.stdout:\n",
    "    raise RuntimeError(\"rclone not configured. Run 'rclone config' to set up gdrive remote.\")\n",
    "print(\"rclone 'gdrive' remote: CONFIGURED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import subprocess\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-paths-header",
   "metadata": {},
   "source": [
    "## Step 2: Download Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_ROOT = Path('/tmp/audio_baseline')\n",
    "AUDIO_DIR = DATA_ROOT / 'percepiano_rendered'\n",
    "MERT_CACHE_DIR = DATA_ROOT / 'mert_embeddings'\n",
    "CHECKPOINT_ROOT = DATA_ROOT / 'checkpoints'\n",
    "LABEL_DIR = DATA_ROOT / 'labels'\n",
    "\n",
    "# Google Drive paths\n",
    "GDRIVE_AUDIO = 'gdrive:crescendai_data/audio_baseline/percepiano_rendered'\n",
    "GDRIVE_LABELS = 'gdrive:crescendai_data/percepiano_labels'\n",
    "GDRIVE_FOLDS = 'gdrive:crescendai_data/audio_baseline/audio_fold_assignments.json'\n",
    "GDRIVE_CHECKPOINTS = 'gdrive:crescendai_data/checkpoints/audio_baseline'\n",
    "GDRIVE_MERT_CACHE = 'gdrive:crescendai_data/audio_baseline/mert_embeddings'\n",
    "\n",
    "# Create directories\n",
    "for d in [AUDIO_DIR, MERT_CACHE_DIR, CHECKPOINT_ROOT, LABEL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-download-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-rendered audio from Google Drive\n",
    "def run_rclone(cmd, description):\n",
    "    \"\"\"Run rclone command with error checking.\"\"\"\n",
    "    print(f\"{description}...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"rclone failed: {result.stderr}\")\n",
    "    return result\n",
    "\n",
    "print(f\"Source: {GDRIVE_AUDIO}\")\n",
    "print(f\"Destination: {AUDIO_DIR}\")\n",
    "\n",
    "run_rclone(\n",
    "    ['rclone', 'copy', GDRIVE_AUDIO, str(AUDIO_DIR), '--progress', '-v'],\n",
    "    \"Downloading pre-rendered audio files\"\n",
    ")\n",
    "\n",
    "wav_count = len(list(AUDIO_DIR.glob('*.wav')))\n",
    "print(f\"\\nDownloaded {wav_count} WAV files\")\n",
    "\n",
    "if wav_count == 0:\n",
    "    raise RuntimeError(\"No WAV files downloaded! Run prepare_audio_baseline.py locally first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-download-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download labels and fold assignments\n",
    "run_rclone(\n",
    "    ['rclone', 'copy', GDRIVE_LABELS, str(LABEL_DIR), '-v'],\n",
    "    \"Downloading labels\"\n",
    ")\n",
    "\n",
    "# Use copyto for single file (not copy which expects directory)\n",
    "FOLD_FILE = DATA_ROOT / 'audio_fold_assignments.json'\n",
    "run_rclone(\n",
    "    ['rclone', 'copyto', GDRIVE_FOLDS, str(FOLD_FILE), '-v'],\n",
    "    \"Downloading fold assignments\"\n",
    ")\n",
    "\n",
    "# Verify\n",
    "LABEL_FILE = LABEL_DIR / 'label_2round_mean_reg_19_with0_rm_highstd0.json'\n",
    "\n",
    "if not LABEL_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Label file not found: {LABEL_FILE}\")\n",
    "\n",
    "with open(LABEL_FILE) as f:\n",
    "    labels = json.load(f)\n",
    "print(f\"Labels: {len(labels)} segments\")\n",
    "\n",
    "if not FOLD_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Fold file not found: {FOLD_FILE}\")\n",
    "\n",
    "with open(FOLD_FILE) as f:\n",
    "    fold_assignments = json.load(f)\n",
    "print(f\"\\nFold statistics:\")\n",
    "for fold_name, keys in fold_assignments.items():\n",
    "    print(f\"  {fold_name}: {len(keys)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-restore-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing MERT cache (resume capability)\n",
    "print(\"Checking for existing MERT cache on Google Drive...\")\n",
    "result = subprocess.run(\n",
    "    ['rclone', 'lsf', GDRIVE_MERT_CACHE],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0 and result.stdout.strip():\n",
    "    remote_files = [f for f in result.stdout.strip().split('\\n') if f.endswith('.pt')]\n",
    "    if remote_files:\n",
    "        print(f\"Found {len(remote_files)} cached embeddings. Restoring...\")\n",
    "        run_rclone(\n",
    "            ['rclone', 'copy', GDRIVE_MERT_CACHE, str(MERT_CACHE_DIR), '-v'],\n",
    "            \"Restoring MERT cache\"\n",
    "        )\n",
    "        print(f\"Restored {len(list(MERT_CACHE_DIR.glob('*.pt')))} embeddings\")\n",
    "else:\n",
    "    print(\"No existing cache found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-mert-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: MERT Feature Extraction\n",
    "\n",
    "- Model: m-a-p/MERT-v1-330M (~8GB VRAM)\n",
    "- Layers: 12-24 averaged\n",
    "- Output: 1024-dim per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-mert-extractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "class MERT330MExtractor:\n",
    "    def __init__(self, cache_dir=None):\n",
    "        self.target_sr = 24000\n",
    "        self.use_layers = (12, 25)\n",
    "        self.cache_dir = Path(cache_dir) if cache_dir else None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        print(f\"Loading MERT-v1-330M on {self.device}...\")\n",
    "        self.processor = AutoProcessor.from_pretrained(\"m-a-p/MERT-v1-330M\", trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            \"m-a-p/MERT-v1-330M\",\n",
    "            output_hidden_states=True,\n",
    "            trust_remote_code=True,\n",
    "        ).to(self.device)\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded. Hidden size: {self.model.config.hidden_size}\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_from_file(self, audio_path, use_cache=True):\n",
    "        audio_path = Path(audio_path)\n",
    "        \n",
    "        if use_cache and self.cache_dir:\n",
    "            cache_path = self.cache_dir / f\"{audio_path.stem}.pt\"\n",
    "            if cache_path.exists():\n",
    "                return torch.load(cache_path, weights_only=True)\n",
    "        \n",
    "        audio, _ = librosa.load(audio_path, sr=self.target_sr, mono=True)\n",
    "        inputs = self.processor(audio, sampling_rate=self.target_sr, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = self.model(**inputs)\n",
    "        hidden_states = outputs.hidden_states[self.use_layers[0]:self.use_layers[1]]\n",
    "        embeddings = torch.stack(hidden_states, dim=0).mean(dim=0).squeeze(0).cpu()\n",
    "        \n",
    "        if use_cache and self.cache_dir:\n",
    "            self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(embeddings, cache_path)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-extract-mert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MERT embeddings\n",
    "print(\"=\"*60)\n",
    "print(\"MERT FEATURE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "audio_files = sorted(AUDIO_DIR.glob('*.wav'))\n",
    "cached_files = set(f.stem for f in MERT_CACHE_DIR.glob('*.pt'))\n",
    "to_extract = [f for f in audio_files if f.stem not in cached_files]\n",
    "\n",
    "print(f\"Audio files: {len(audio_files)}\")\n",
    "print(f\"Already cached: {len(cached_files)}\")\n",
    "print(f\"To extract: {len(to_extract)}\")\n",
    "\n",
    "if to_extract:\n",
    "    extractor = MERT330MExtractor(cache_dir=MERT_CACHE_DIR)\n",
    "    \n",
    "    failed = []\n",
    "    for audio_path in tqdm(to_extract, desc=\"Extracting\"):\n",
    "        try:\n",
    "            extractor.extract_from_file(audio_path)\n",
    "        except Exception as e:\n",
    "            failed.append((audio_path.stem, str(e)))\n",
    "    \n",
    "    del extractor\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nExtracted: {len(to_extract) - len(failed)}\")\n",
    "    if failed:\n",
    "        print(f\"Failed: {len(failed)}\")\n",
    "else:\n",
    "    print(\"\\nAll embeddings cached!\")\n",
    "\n",
    "print(f\"Total cached: {len(list(MERT_CACHE_DIR.glob('*.pt')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-sync-mert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync MERT cache to Google Drive\n",
    "run_rclone(\n",
    "    ['rclone', 'copy', str(MERT_CACHE_DIR), GDRIVE_MERT_CACHE, '-v'],\n",
    "    \"Syncing MERT cache to Google Drive\"\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-dataset-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCEPIANO_DIMENSIONS = [\n",
    "    \"timing\", \"articulation_length\", \"articulation_touch\",\n",
    "    \"pedal_amount\", \"pedal_clarity\",\n",
    "    \"timbre_variety\", \"timbre_depth\", \"timbre_brightness\", \"timbre_loudness\",\n",
    "    \"dynamic_range\", \"tempo\", \"space\", \"balance\", \"drama\",\n",
    "    \"mood_valence\", \"mood_energy\", \"mood_imagination\",\n",
    "    \"sophistication\", \"interpretation\",\n",
    "]\n",
    "\n",
    "DIMENSION_CATEGORIES = {\n",
    "    \"timing\": [\"timing\"],\n",
    "    \"articulation\": [\"articulation_length\", \"articulation_touch\"],\n",
    "    \"pedal\": [\"pedal_amount\", \"pedal_clarity\"],\n",
    "    \"timbre\": [\"timbre_variety\", \"timbre_depth\", \"timbre_brightness\", \"timbre_loudness\"],\n",
    "    \"dynamics\": [\"dynamic_range\"],\n",
    "    \"tempo_space\": [\"tempo\", \"space\", \"balance\", \"drama\"],\n",
    "    \"emotion\": [\"mood_valence\", \"mood_energy\", \"mood_imagination\"],\n",
    "    \"interpretation\": [\"sophistication\", \"interpretation\"],\n",
    "}\n",
    "\n",
    "\n",
    "class AudioPercePianoDataset(Dataset):\n",
    "    def __init__(self, mert_cache_dir, labels, fold_assignments, fold_id, mode, max_frames=1000):\n",
    "        self.mert_cache_dir = Path(mert_cache_dir)\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "        available = {p.stem for p in self.mert_cache_dir.glob('*.pt')}\n",
    "        \n",
    "        if mode == \"test\":\n",
    "            valid_keys = set(fold_assignments.get(\"test\", []))\n",
    "        elif mode == \"val\":\n",
    "            valid_keys = set(fold_assignments.get(f\"fold_{fold_id}\", []))\n",
    "        else:\n",
    "            valid_keys = set()\n",
    "            for i in range(4):\n",
    "                if i != fold_id:\n",
    "                    valid_keys.update(fold_assignments.get(f\"fold_{i}\", []))\n",
    "        \n",
    "        self.samples = [(k, torch.tensor(labels[k][:19], dtype=torch.float32))\n",
    "                        for k in valid_keys if k in available and k in labels]\n",
    "        print(f\"{mode} (fold {fold_id}): {len(self.samples)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        key, label = self.samples[idx]\n",
    "        emb = torch.load(self.mert_cache_dir / f\"{key}.pt\", weights_only=True)\n",
    "        if emb.shape[0] > self.max_frames:\n",
    "            emb = emb[:self.max_frames]\n",
    "        return {\"embeddings\": emb, \"labels\": label, \"key\": key, \"length\": emb.shape[0]}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    embs = [b[\"embeddings\"] for b in batch]\n",
    "    labels = torch.stack([b[\"labels\"] for b in batch])\n",
    "    lengths = torch.tensor([b[\"length\"] for b in batch])\n",
    "    padded = pad_sequence(embs, batch_first=True)\n",
    "    mask = torch.arange(padded.shape[1]).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "    return {\"embeddings\": padded, \"attention_mask\": mask, \"labels\": labels, \"keys\": [b[\"key\"] for b in batch]}\n",
    "\n",
    "\n",
    "print(f\"Dataset defined. {len(PERCEPIANO_DIMENSIONS)} dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPercePianoModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=512, num_labels=19, dropout=0.2,\n",
    "                 learning_rate=1e-4, weight_decay=1e-5, pooling=\"mean\", max_epochs=100):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = learning_rate\n",
    "        self.wd = weight_decay\n",
    "        self.pooling = pooling\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        if pooling == \"attention\":\n",
    "            self.attn = nn.Sequential(nn.Linear(input_dim, 256), nn.Tanh(), nn.Linear(256, 1))\n",
    "        \n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_labels), nn.Sigmoid(),\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.val_outputs = []\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        if self.pooling == \"mean\":\n",
    "            if mask is not None:\n",
    "                m = mask.unsqueeze(-1).float()\n",
    "                pooled = (x * m).sum(1) / m.sum(1).clamp(min=1)\n",
    "            else:\n",
    "                pooled = x.mean(1)\n",
    "        elif self.pooling == \"attention\":\n",
    "            scores = self.attn(x).squeeze(-1)\n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(~mask, float('-inf'))\n",
    "            w = torch.softmax(scores, dim=-1).unsqueeze(-1)\n",
    "            pooled = (x * w).sum(1)\n",
    "        else:\n",
    "            pooled = x.mean(1)\n",
    "        return self.clf(pooled)\n",
    "    \n",
    "    def training_step(self, batch, idx):\n",
    "        loss = self.loss_fn(self(batch[\"embeddings\"], batch[\"attention_mask\"]), batch[\"labels\"])\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, idx):\n",
    "        preds = self(batch[\"embeddings\"], batch[\"attention_mask\"])\n",
    "        self.log(\"val_loss\", self.loss_fn(preds, batch[\"labels\"]), prog_bar=True)\n",
    "        self.val_outputs.append({\"p\": preds.cpu(), \"l\": batch[\"labels\"].cpu()})\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.val_outputs:\n",
    "            p = torch.cat([x[\"p\"] for x in self.val_outputs]).numpy()\n",
    "            l = torch.cat([x[\"l\"] for x in self.val_outputs]).numpy()\n",
    "            self.log(\"val_r2\", r2_score(l, p), prog_bar=True)\n",
    "            self.val_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.max_epochs, eta_min=1e-6)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"interval\": \"epoch\"}}\n",
    "\n",
    "\n",
    "print(\"Model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-train-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'input_dim': 1024,\n",
    "    'hidden_dim': 512,\n",
    "    'num_labels': 19,\n",
    "    'dropout': 0.2,\n",
    "    'pooling': 'mean',\n",
    "    \n",
    "    # Optimizer\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'gradient_clip_val': 1.0,\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 64,  # A100 80GB can handle larger batches\n",
    "    'max_epochs': 200,\n",
    "    'patience': 15,\n",
    "    'max_frames': 1000,\n",
    "    'n_folds': 4,\n",
    "    'num_workers': 2,  # Safe for 4 vCPUs\n",
    "}\n",
    "\n",
    "print(\"Config:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-restore-ckpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints (resume capability)\n",
    "print(\"Checking for existing checkpoints on Google Drive...\")\n",
    "result = subprocess.run(['rclone', 'lsf', GDRIVE_CHECKPOINTS], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0 and result.stdout.strip():\n",
    "    print(f\"Found checkpoints. Restoring...\")\n",
    "    run_rclone(\n",
    "        ['rclone', 'copy', GDRIVE_CHECKPOINTS, str(CHECKPOINT_ROOT), '-v'],\n",
    "        \"Restoring checkpoints\"\n",
    "    )\n",
    "    restored = list(CHECKPOINT_ROOT.glob('*.ckpt'))\n",
    "    print(f\"Restored {len(restored)} checkpoints\")\n",
    "else:\n",
    "    print(\"No existing checkpoints found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "def extract_best_r2_from_checkpoint(ckpt_path):\n",
    "    \"\"\"Extract best R2 score from PyTorch Lightning checkpoint.\"\"\"\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "    \n",
    "    # PyTorch Lightning stores callbacks with full class path as key\n",
    "    callbacks = ckpt.get('callbacks', {})\n",
    "    for key, value in callbacks.items():\n",
    "        if 'ModelCheckpoint' in key:\n",
    "            score = value.get('best_model_score')\n",
    "            if score is not None:\n",
    "                return float(score)\n",
    "    return None\n",
    "\n",
    "# Create log directory\n",
    "LOG_DIR = CHECKPOINT_ROOT / 'logs'\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"4-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fold_results = {}\n",
    "\n",
    "for fold in range(CONFIG['n_folds']):\n",
    "    ckpt_path = CHECKPOINT_ROOT / f'fold{fold}_best.ckpt'\n",
    "    \n",
    "    if ckpt_path.exists():\n",
    "        r2 = extract_best_r2_from_checkpoint(ckpt_path)\n",
    "        if r2 is not None:\n",
    "            fold_results[fold] = r2\n",
    "            print(f\"Fold {fold}: SKIP (exists) R2={fold_results[fold]:+.4f}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Fold {fold}: Checkpoint exists but R2 not found, will retrain\")\n",
    "    \n",
    "    print(f\"\\nFold {fold}: Training...\")\n",
    "    \n",
    "    train_ds = AudioPercePianoDataset(MERT_CACHE_DIR, labels, fold_assignments, fold, \"train\", CONFIG['max_frames'])\n",
    "    val_ds = AudioPercePianoDataset(MERT_CACHE_DIR, labels, fold_assignments, fold, \"val\", CONFIG['max_frames'])\n",
    "    \n",
    "    train_dl = DataLoader(\n",
    "        train_ds, batch_size=CONFIG['batch_size'], shuffle=True, \n",
    "        collate_fn=collate_fn, num_workers=CONFIG['num_workers'], pin_memory=True\n",
    "    )\n",
    "    val_dl = DataLoader(\n",
    "        val_ds, batch_size=CONFIG['batch_size'], shuffle=False, \n",
    "        collate_fn=collate_fn, num_workers=CONFIG['num_workers'], pin_memory=True\n",
    "    )\n",
    "    \n",
    "    model = AudioPercePianoModel(\n",
    "        CONFIG['input_dim'], CONFIG['hidden_dim'], CONFIG['num_labels'],\n",
    "        CONFIG['dropout'], CONFIG['learning_rate'], CONFIG['weight_decay'], \n",
    "        CONFIG['pooling'], CONFIG['max_epochs']\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(dirpath=CHECKPOINT_ROOT, filename=f'fold{fold}_best', monitor='val_r2', mode='max', save_top_k=1),\n",
    "        EarlyStopping(monitor='val_r2', mode='max', patience=CONFIG['patience'], verbose=True),\n",
    "    ]\n",
    "    \n",
    "    logger = CSVLogger(save_dir=LOG_DIR, name=f'fold{fold}', version='')\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=CONFIG['max_epochs'],\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        gradient_clip_val=CONFIG['gradient_clip_val'],\n",
    "        enable_progress_bar=True,\n",
    "        deterministic=True,\n",
    "        log_every_n_steps=10,\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_dl, val_dl)\n",
    "    \n",
    "    fold_results[fold] = float(callbacks[0].best_model_score or 0)\n",
    "    print(f\"Fold {fold} Best R2: {fold_results[fold]:+.4f}\")\n",
    "    \n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for f, r2 in sorted(fold_results.items()):\n",
    "    print(f\"  Fold {f}: {r2:+.4f}\")\n",
    "avg = np.mean(list(fold_results.values()))\n",
    "std = np.std(list(fold_results.values()))\n",
    "print(f\"  Average: {avg:+.4f} +/- {std:.4f}\")\n",
    "print(f\"  Target: >= 0.25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-sync-ckpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints to Google Drive (critical for ephemeral storage)\n",
    "run_rclone(\n",
    "    ['rclone', 'copy', str(CHECKPOINT_ROOT), GDRIVE_CHECKPOINTS, '-v'],\n",
    "    \"Syncing checkpoints to Google Drive\"\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-eval-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# COLLECT PREDICTIONS\n",
    "# ============================================================================\n",
    "all_preds, all_labels, all_keys = [], [], []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for fold in range(CONFIG['n_folds']):\n",
    "    ckpt_path = CHECKPOINT_ROOT / f'fold{fold}_best.ckpt'\n",
    "    if not ckpt_path.exists():\n",
    "        print(f\"Warning: Fold {fold} checkpoint not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    model = AudioPercePianoModel.load_from_checkpoint(ckpt_path).to(device).eval()\n",
    "    val_ds = AudioPercePianoDataset(MERT_CACHE_DIR, labels, fold_assignments, fold, \"val\", CONFIG['max_frames'])\n",
    "    val_dl = DataLoader(val_ds, batch_size=CONFIG['batch_size'], collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dl:\n",
    "            preds = model(batch[\"embeddings\"].to(device), batch[\"attention_mask\"].to(device))\n",
    "            if torch.isnan(preds).any() or torch.isinf(preds).any():\n",
    "                print(f\"WARNING: NaN/Inf detected in fold {fold} predictions!\")\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(batch[\"labels\"].numpy())\n",
    "            all_keys.extend(batch[\"keys\"])\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if not all_preds:\n",
    "    raise RuntimeError(\"No predictions collected - no checkpoints found\")\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_labels = np.vstack(all_labels)\n",
    "all_keys = np.array(all_keys)\n",
    "\n",
    "print(f\"\\nTotal samples evaluated: {len(all_preds)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BASELINE COMPARISONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE COMPARISONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "mean_baseline_preds = np.tile(all_labels.mean(axis=0), (len(all_labels), 1))\n",
    "mean_baseline_r2 = r2_score(all_labels, mean_baseline_preds)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random_baseline_preds = np.random.rand(*all_labels.shape)\n",
    "random_baseline_r2 = r2_score(all_labels, random_baseline_preds)\n",
    "\n",
    "print(f\"Random Baseline R2:     {random_baseline_r2:+.4f}\")\n",
    "print(f\"Mean Predictor R2:      {mean_baseline_r2:+.4f} (always 0 by definition)\")\n",
    "print(f\"Our Model R2:           {r2_score(all_labels, all_preds):+.4f}\")\n",
    "print(f\"Improvement over mean:  {r2_score(all_labels, all_preds) - mean_baseline_r2:+.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# OVERALL METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERALL METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "overall_r2 = r2_score(all_labels, all_preds)\n",
    "overall_mae = mean_absolute_error(all_labels, all_preds)\n",
    "overall_rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
    "\n",
    "pearson_corrs = []\n",
    "for i in range(all_labels.shape[1]):\n",
    "    corr, _ = stats.pearsonr(all_labels[:, i], all_preds[:, i])\n",
    "    pearson_corrs.append(corr)\n",
    "avg_pearson = np.mean(pearson_corrs)\n",
    "\n",
    "print(f\"R2 Score:           {overall_r2:+.4f}\")\n",
    "print(f\"MAE:                {overall_mae:.4f}\")\n",
    "print(f\"RMSE:               {overall_rmse:.4f}\")\n",
    "print(f\"Avg Pearson Corr:   {avg_pearson:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BOOTSTRAP CONFIDENCE INTERVALS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BOOTSTRAP CONFIDENCE INTERVALS (95%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def bootstrap_r2(y_true, y_pred, n_bootstrap=1000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n_samples = len(y_true)\n",
    "    r2_scores = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        r2_scores.append(r2_score(y_true[idx], y_pred[idx]))\n",
    "    return np.percentile(r2_scores, [2.5, 50, 97.5])\n",
    "\n",
    "overall_ci = bootstrap_r2(all_labels, all_preds)\n",
    "print(f\"Overall R2: {overall_ci[1]:+.4f} [{overall_ci[0]:+.4f}, {overall_ci[2]:+.4f}]\")\n",
    "\n",
    "print(\"\\nPer-dimension (selected):\")\n",
    "for d in ['timing', 'timbre_brightness', 'dynamic_range', 'pedal_amount', 'interpretation']:\n",
    "    i = PERCEPIANO_DIMENSIONS.index(d)\n",
    "    ci = bootstrap_r2(all_labels[:, i], all_preds[:, i])\n",
    "    print(f\"  {d:<22} {ci[1]:+.4f} [{ci[0]:+.4f}, {ci[2]:+.4f}]\")\n",
    "\n",
    "# ============================================================================\n",
    "# CATEGORY ANALYSIS (PER-DIMENSION)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CATEGORY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dim_metrics = {}\n",
    "for i, d in enumerate(PERCEPIANO_DIMENSIONS):\n",
    "    y_true, y_pred = all_labels[:, i], all_preds[:, i]\n",
    "    pearson, p_val = stats.pearsonr(y_true, y_pred)\n",
    "    dim_metrics[d] = {\n",
    "        'r2': r2_score(y_true, y_pred),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'pearson': pearson, 'p_value': p_val,\n",
    "        'label_mean': y_true.mean(), 'label_std': y_true.std(),\n",
    "        'pred_mean': y_pred.mean(), 'pred_std': y_pred.std(),\n",
    "    }\n",
    "\n",
    "for cat, dims in DIMENSION_CATEGORIES.items():\n",
    "    cat_r2 = np.mean([dim_metrics[d]['r2'] for d in dims])\n",
    "    cat_pearson = np.mean([dim_metrics[d]['pearson'] for d in dims])\n",
    "    print(f\"\\n{cat.upper()}\")\n",
    "    print(f\"  Category R2: {cat_r2:+.4f}  |  Pearson: {cat_pearson:.4f}\")\n",
    "    for d in dims:\n",
    "        m = dim_metrics[d]\n",
    "        sig = \"***\" if m['p_value'] < 0.001 else \"**\" if m['p_value'] < 0.01 else \"*\" if m['p_value'] < 0.05 else \"\"\n",
    "        print(f\"    {d:<22} R2={m['r2']:+.4f}  MAE={m['mae']:.3f}  r={m['pearson']:.3f}{sig}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREDICTION DISTRIBUTION ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREDICTION DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Dimension':<25} {'Lbl Mean':>8} {'Lbl Std':>8} {'Prd Mean':>8} {'Prd Std':>8} {'Bias':>7}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "distribution_issues = []\n",
    "for i, d in enumerate(PERCEPIANO_DIMENSIONS):\n",
    "    lbl_mean, lbl_std = all_labels[:, i].mean(), all_labels[:, i].std()\n",
    "    pred_mean, pred_std = all_preds[:, i].mean(), all_preds[:, i].std()\n",
    "    bias = pred_mean - lbl_mean\n",
    "    std_ratio = pred_std / lbl_std if lbl_std > 0 else 0\n",
    "    \n",
    "    if std_ratio < 0.5:\n",
    "        distribution_issues.append(f\"{d}: under-dispersed (std ratio: {std_ratio:.2f})\")\n",
    "    elif std_ratio > 1.5:\n",
    "        distribution_issues.append(f\"{d}: over-dispersed (std ratio: {std_ratio:.2f})\")\n",
    "    if abs(bias) > 0.1:\n",
    "        distribution_issues.append(f\"{d}: significant bias ({bias:+.3f})\")\n",
    "    \n",
    "    print(f\"{d:<25} {lbl_mean:>8.3f} {lbl_std:>8.3f} {pred_mean:>8.3f} {pred_std:>8.3f} {bias:>+7.3f}\")\n",
    "\n",
    "avg_pred_std = np.mean([all_preds[:, i].std() for i in range(19)])\n",
    "avg_lbl_std = np.mean([all_labels[:, i].std() for i in range(19)])\n",
    "\n",
    "print(f\"\\nMean Regression Check:\")\n",
    "print(f\"  Avg label std: {avg_lbl_std:.4f} | Avg pred std: {avg_pred_std:.4f} | Ratio: {avg_pred_std/avg_lbl_std:.2%}\")\n",
    "if avg_pred_std / avg_lbl_std < 0.7:\n",
    "    print(\"  WARNING: Model may be suffering from mean regression\")\n",
    "\n",
    "if distribution_issues:\n",
    "    print(f\"\\nDistribution Issues ({len(distribution_issues)}):\")\n",
    "    for issue in distribution_issues[:5]:\n",
    "        print(f\"  - {issue}\")\n",
    "    if len(distribution_issues) > 5:\n",
    "        print(f\"  ... and {len(distribution_issues) - 5} more\")\n",
    "\n",
    "# ============================================================================\n",
    "# PER-SAMPLE ERROR ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-SAMPLE ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_mse = np.mean((all_preds - all_labels) ** 2, axis=1)\n",
    "sample_mae = np.mean(np.abs(all_preds - all_labels), axis=1)\n",
    "sorted_idx = np.argsort(sample_mse)\n",
    "\n",
    "print(\"\\nHARDEST SAMPLES (highest MSE):\")\n",
    "for i in sorted_idx[-5:][::-1]:\n",
    "    dim_errors = np.abs(all_preds[i] - all_labels[i])\n",
    "    worst_dims = np.argsort(dim_errors)[-3:][::-1]\n",
    "    worst_str = \", \".join([f\"{PERCEPIANO_DIMENSIONS[d]}({dim_errors[d]:.2f})\" for d in worst_dims])\n",
    "    print(f\"  {all_keys[i][:35]:<35} MSE={sample_mse[i]:.4f} | {worst_str}\")\n",
    "\n",
    "print(f\"\\nError Distribution: MSE mean={sample_mse.mean():.4f} std={sample_mse.std():.4f}\")\n",
    "print(f\"  Outliers: {(sample_mse > 2*sample_mse.mean()).sum()} samples > 2x mean, {(sample_mse > 3*sample_mse.mean()).sum()} > 3x mean\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING CURVE ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING CURVE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "training_summary = {}\n",
    "for fold in range(CONFIG['n_folds']):\n",
    "    log_file = LOG_DIR / f'fold{fold}' / 'metrics.csv'\n",
    "    if not log_file.exists():\n",
    "        print(f\"Fold {fold}: No log found\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(log_file)\n",
    "    val_r2 = df[df['val_r2'].notna()]['val_r2']\n",
    "    train_loss = df[df['train_loss'].notna()]['train_loss']\n",
    "    val_loss = df[df['val_loss'].notna()]['val_loss']\n",
    "    \n",
    "    if len(val_r2) > 0:\n",
    "        training_summary[fold] = {\n",
    "            'epochs': len(val_r2),\n",
    "            'best_epoch': int(val_r2.idxmax()) if pd.notna(val_r2.idxmax()) else 0,\n",
    "            'best_r2': float(val_r2.max()),\n",
    "            'final_r2': float(val_r2.iloc[-1]),\n",
    "            'final_train_loss': float(train_loss.iloc[-1]) if len(train_loss) > 0 else None,\n",
    "            'final_val_loss': float(val_loss.iloc[-1]) if len(val_loss) > 0 else None,\n",
    "        }\n",
    "        s = training_summary[fold]\n",
    "        gap_str = \"\"\n",
    "        if s['final_train_loss'] and s['final_val_loss']:\n",
    "            gap = s['final_val_loss'] - s['final_train_loss']\n",
    "            gap_str = f\" | gap={gap:.4f}\" if gap > 0.01 else \"\"\n",
    "        print(f\"Fold {fold}: {s['epochs']} epochs, best R2={s['best_r2']:+.4f} @ epoch {s['best_epoch']}{gap_str}\")\n",
    "\n",
    "if training_summary:\n",
    "    print(f\"\\nAvg epochs to convergence: {np.mean([s['epochs'] for s in training_summary.values()]):.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel: MERT-330M + MLP ({CONFIG['pooling']} pooling)\")\n",
    "print(f\"Training: {CONFIG['n_folds']}-fold CV, batch={CONFIG['batch_size']}, seed={SEED}\")\n",
    "\n",
    "print(f\"\\n--- PERFORMANCE ---\")\n",
    "print(f\"Average R2:     {avg:+.4f} +/- {std:.4f}\")\n",
    "print(f\"95% CI:         [{overall_ci[0]:+.4f}, {overall_ci[2]:+.4f}]\")\n",
    "print(f\"MAE:            {overall_mae:.4f}\")\n",
    "print(f\"Pearson:        {avg_pearson:.4f}\")\n",
    "\n",
    "print(f\"\\n--- VS BASELINES ---\")\n",
    "print(f\"Random:         {random_baseline_r2:+.4f}\")\n",
    "print(f\"Mean predictor: {mean_baseline_r2:+.4f}\")\n",
    "print(f\"Our model:      {overall_r2:+.4f} (+{overall_r2 - mean_baseline_r2:.4f})\")\n",
    "\n",
    "print(f\"\\n--- TARGET ---\")\n",
    "print(f\"Target: R2 >= 0.25\")\n",
    "print(f\"Status: {'PASS' if avg >= 0.25 else 'BELOW TARGET'}\")\n",
    "\n",
    "if avg < 0.25:\n",
    "    print(\"\\nSuggested improvements:\")\n",
    "    if avg_pred_std / avg_lbl_std < 0.7:\n",
    "        print(\"  - Mean regression detected: try correlation loss\")\n",
    "    print(\"  - Try attention pooling: CONFIG['pooling'] = 'attention'\")\n",
    "    print(\"  - Try LSTM/Transformer on MERT frames\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute bootstrap CI for overall\n",
    "overall_ci = bootstrap_r2(all_labels, all_preds)\n",
    "\n",
    "results = {\n",
    "    \"summary\": {\n",
    "        \"avg_r2\": float(avg),\n",
    "        \"std_r2\": float(std),\n",
    "        \"overall_r2\": float(overall_r2),\n",
    "        \"r2_ci_95\": [float(overall_ci[0]), float(overall_ci[2])],\n",
    "        \"overall_mae\": float(overall_mae),\n",
    "        \"overall_rmse\": float(overall_rmse),\n",
    "        \"avg_pearson\": float(avg_pearson),\n",
    "        \"n_samples\": int(len(all_preds)),\n",
    "        \"target\": 0.25,\n",
    "        \"target_met\": bool(avg >= 0.25),\n",
    "    },\n",
    "    \"fold_results\": {str(k): float(v) for k, v in fold_results.items()},\n",
    "    \"per_dimension\": {\n",
    "        d: {k: float(v) if isinstance(v, (np.floating, float)) else v \n",
    "            for k, v in m.items()}\n",
    "        for d, m in dim_metrics.items()\n",
    "    },\n",
    "    \"baselines\": {\n",
    "        \"random_r2\": float(random_baseline_r2),\n",
    "        \"mean_predictor_r2\": float(mean_baseline_r2),\n",
    "        \"improvement_over_mean\": float(overall_r2 - mean_baseline_r2),\n",
    "    },\n",
    "    \"distribution_analysis\": {\n",
    "        \"avg_label_std\": float(avg_lbl_std),\n",
    "        \"avg_pred_std\": float(avg_pred_std),\n",
    "        \"dispersion_ratio\": float(avg_pred_std / avg_lbl_std),\n",
    "        \"issues\": distribution_issues,\n",
    "    },\n",
    "    \"training_summary\": training_summary,\n",
    "    \"hardest_samples\": [\n",
    "        {\"key\": all_keys[i], \"mse\": float(sample_mse[i]), \"mae\": float(sample_mae[i])}\n",
    "        for i in sorted_idx[-10:][::-1]\n",
    "    ],\n",
    "    \"config\": CONFIG,\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "results_file = CHECKPOINT_ROOT / \"results_comprehensive.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Saved: {results_file}\")\n",
    "\n",
    "# Sync to Google Drive\n",
    "run_rclone(\n",
    "    ['rclone', 'copy', str(results_file), GDRIVE_CHECKPOINTS, '-v'],\n",
    "    \"Syncing comprehensive results\"\n",
    ")\n",
    "\n",
    "# Also sync training logs\n",
    "run_rclone(\n",
    "    ['rclone', 'copy', str(LOG_DIR), f\"{GDRIVE_CHECKPOINTS}/logs\", '-v'],\n",
    "    \"Syncing training logs\"\n",
    ")\n",
    "\n",
    "print(\"\\nAll results saved and synced!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel: MERT-330M + MLP ({CONFIG['pooling']} pooling)\")\n",
    "print(f\"Training: {CONFIG['n_folds']}-fold CV, {CONFIG['max_epochs']} max epochs, batch={CONFIG['batch_size']}\")\n",
    "\n",
    "print(f\"\\n--- PERFORMANCE ---\")\n",
    "print(f\"Average R2:     {avg:+.4f} +/- {std:.4f}\")\n",
    "print(f\"95% CI:         [{overall_ci[0]:+.4f}, {overall_ci[2]:+.4f}]\")\n",
    "print(f\"Overall R2:     {overall_r2:+.4f}\")\n",
    "print(f\"MAE:            {overall_mae:.4f}\")\n",
    "print(f\"Pearson:        {avg_pearson:.4f}\")\n",
    "\n",
    "print(f\"\\n--- VS BASELINES ---\")\n",
    "print(f\"Random:         {random_baseline_r2:+.4f}\")\n",
    "print(f\"Mean predictor: {mean_baseline_r2:+.4f}\")\n",
    "print(f\"Improvement:    {overall_r2 - mean_baseline_r2:+.4f}\")\n",
    "\n",
    "print(f\"\\n--- TARGET ---\")\n",
    "print(f\"Target R2:      >= 0.25\")\n",
    "print(f\"Status:         {'PASS' if avg >= 0.25 else 'BELOW TARGET'}\")\n",
    "\n",
    "if avg >= 0.25:\n",
    "    print(\"\\nAudio baseline validation PASSED!\")\n",
    "    print(\"Audio features provide meaningful signal for piano performance evaluation.\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Proceed to Phase B (Pianoteq rendering)\")\n",
    "    print(\"  2. Compare audio vs symbolic baseline performance\")\n",
    "    print(\"  3. Design fusion architecture\")\n",
    "else:\n",
    "    print(\"\\nBelow target. Analysis suggests:\")\n",
    "    if avg_pred_std / avg_lbl_std < 0.7:\n",
    "        print(\"  - Mean regression issue: try different loss (e.g., correlation loss)\")\n",
    "    if any('under-dispersed' in issue for issue in distribution_issues):\n",
    "        print(\"  - Predictions lack variance: try attention pooling or temporal modeling\")\n",
    "    print(\"\\nPotential improvements:\")\n",
    "    print(\"  - Try attention pooling (set CONFIG['pooling'] = 'attention')\")\n",
    "    print(\"  - Try LSTM/Transformer on MERT frames\")\n",
    "    print(\"  - Increase model capacity (hidden_dim)\")\n",
    "    print(\"  - Add multi-task auxiliary losses\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
