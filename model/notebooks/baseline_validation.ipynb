{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Validation - Synthetic Labels\n",
    "\n",
    "This notebook trains and evaluates baseline models to validate:\n",
    "1. Can the architecture learn patterns from synthetic labels?\n",
    "2. Does audio+MIDI outperform audio-only?\n",
    "3. How do different data sources (MAESTRO vs YouTube) compare?\n",
    "\n",
    "**Target**: Pearson r = 0.48-0.55 on technical dimensions OR evidence of clear learning\n",
    "\n",
    "**Requirements:**\n",
    "- Colab Pro (T4/V100 GPU recommended)\n",
    "- Google Drive for data and checkpoints\n",
    "- HuggingFace account for MERT model\n",
    "- Git repository access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "import os\n",
    "os.environ.pop(\"HF_TOKEN\", None)\n",
    "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "try:\n",
    "    import getpass as gp\n",
    "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
    "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
    "    if not isinstance(token, str):\n",
    "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
    "    token = token.strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"Empty token provided\")\n",
    "    login(token=token, add_to_git_credential=False)\n",
    "    who = HfApi().whoami(token=token)\n",
    "    print(f\"‚úì Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "except Exception as e:\n",
    "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
    "    print(\"Falling back to interactive login widget...\")\n",
    "    login()\n",
    "    try:\n",
    "        who = HfApi().whoami()\n",
    "        print(f\"‚úì Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"[HF Login] Verification skipped: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Setup paths\n",
    "data_dir = '/content/drive/MyDrive/piano_eval_data'\n",
    "checkpoint_dir = '/content/drive/MyDrive/piano_eval_checkpoints'\n",
    "\n",
    "print(f\"‚úì Data directory: {data_dir}\")\n",
    "print(f\"‚úì Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "REPO_URL = \"https://github.com/Jai-Dhiman/crescendai.git\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "!rm -rf /content/crescendai\n",
    "!git clone --branch {BRANCH} {REPO_URL} /content/crescendai\n",
    "%cd /content/crescendai/model\n",
    "\n",
    "!git log -1 --oneline\n",
    "print(f\"\\n‚úì Repository cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv and dependencies\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "!uv pip install --system -e .\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning\n",
    "print(f\"\\n‚úì Dependencies installed\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  Lightning: {pytorch_lightning.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"\\n‚úì GPU ready for training\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: NO GPU DETECTED!\")\n",
    "    print(\"Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "    raise RuntimeError(\"GPU required for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Data Files\n",
    "\n",
    "**Upload these files to Google Drive before running:**\n",
    "\n",
    "To `{data_dir}/annotations/`:\n",
    "- `synthetic_train.jsonl`\n",
    "- `synthetic_val.jsonl`\n",
    "- `synthetic_test.jsonl`\n",
    "\n",
    "To `{data_dir}/all_segments/`:\n",
    "- `maestro_001.wav` through `maestro_100.wav`\n",
    "- `youtube_*.wav` files (at least 96 files for test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data exists\n",
    "import os\n",
    "\n",
    "annotations_dir = f'{data_dir}/annotations'\n",
    "segments_dir = f'{data_dir}/all_segments'\n",
    "\n",
    "# Check annotations\n",
    "assert os.path.exists(f'{annotations_dir}/synthetic_train.jsonl'), \"synthetic_train.jsonl not found!\"\n",
    "assert os.path.exists(f'{annotations_dir}/synthetic_val.jsonl'), \"synthetic_val.jsonl not found!\"\n",
    "assert os.path.exists(f'{annotations_dir}/synthetic_test.jsonl'), \"synthetic_test.jsonl not found!\"\n",
    "\n",
    "print(f\"‚úì Annotation files found\")\n",
    "\n",
    "# Count audio files\n",
    "maestro_files = len([f for f in os.listdir(segments_dir) if f.startswith('maestro_')])\n",
    "youtube_files = len([f for f in os.listdir(segments_dir) if f.startswith('youtube_')])\n",
    "\n",
    "print(f\"‚úì Audio segments found:\")\n",
    "print(f\"  MAESTRO: {maestro_files}\")\n",
    "print(f\"  YouTube: {youtube_files}\")\n",
    "\n",
    "if maestro_files < 100:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Expected 100 MAESTRO files, found {maestro_files}\")\n",
    "if youtube_files < 96:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Expected ~100 YouTube files, found {youtube_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Audio-Only Baseline (Experiment A)\n",
    "\n",
    "**Start here**: Train audio-only model first to validate the approach.\n",
    "\n",
    "This takes ~2-3 GPU hours on T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update audio-only config with Google Drive paths\n",
    "audioonly_config_path = '/content/crescendai/model/configs/baseline_audioonly.yaml'\n",
    "\n",
    "import yaml\n",
    "with open(audioonly_config_path, 'r') as f:\n",
    "    config_audioonly = yaml.safe_load(f)\n",
    "\n",
    "# Update paths\n",
    "config_audioonly['data']['train_path'] = f'{annotations_dir}/synthetic_train.jsonl'\n",
    "config_audioonly['data']['val_path'] = f'{annotations_dir}/synthetic_val.jsonl'\n",
    "config_audioonly['data']['test_path'] = f'{annotations_dir}/synthetic_test.jsonl'\n",
    "config_audioonly['callbacks']['checkpoint']['dirpath'] = f'{checkpoint_dir}/baseline_audioonly'\n",
    "config_audioonly['logging']['tensorboard_logdir'] = f'{checkpoint_dir}/logs/baseline_audioonly'\n",
    "\n",
    "# Save updated config\n",
    "colab_audioonly_path = '/tmp/baseline_audioonly_colab.yaml'\n",
    "with open(colab_audioonly_path, 'w') as f:\n",
    "    yaml.dump(config_audioonly, f, default_flow_style=False)\n",
    "\n",
    "print(f\"‚úì Audio-only configuration updated\")\n",
    "print(f\"\\nExperiment: Audio-Only Baseline\")\n",
    "print(f\"  Epochs: {config_audioonly['training']['max_epochs']}\")\n",
    "print(f\"  MIDI: Disabled\")\n",
    "print(f\"  Fusion: Disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train audio-only model\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING AUDIO-ONLY MODEL (EXPERIMENT A)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Expected duration: ~2-3 GPU hours on T4\\n\")\n",
    "\n",
    "!python train.py --config {colab_audioonly_path}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Audio-only training complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Audio+MIDI Baseline (Experiment B - Optional)\n",
    "\n",
    "**Only run this if:**\n",
    "1. Audio-only training showed promising results (will check in evaluation)\n",
    "2. You have MIDI files uploaded to Google Drive\n",
    "\n",
    "This takes ~3-4 GPU hours on T4.\n",
    "\n",
    "**Skip this section if you don't have MIDI files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update audio+MIDI config with Google Drive paths\n",
    "config_path = '/content/crescendai/model/configs/baseline_synthetic.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update paths\n",
    "config['data']['train_path'] = f'{annotations_dir}/synthetic_train.jsonl'\n",
    "config['data']['val_path'] = f'{annotations_dir}/synthetic_val.jsonl'\n",
    "config['data']['test_path'] = f'{annotations_dir}/synthetic_test.jsonl'\n",
    "config['callbacks']['checkpoint']['dirpath'] = f'{checkpoint_dir}/baseline_synthetic'\n",
    "config['logging']['tensorboard_logdir'] = f'{checkpoint_dir}/logs/baseline_synthetic'\n",
    "\n",
    "# Save updated config\n",
    "colab_config_path = '/tmp/baseline_synthetic_colab.yaml'\n",
    "with open(colab_config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"‚úì Configuration updated for Colab\")\n",
    "print(f\"\\nExperiment: Audio+MIDI Baseline\")\n",
    "print(f\"  Epochs: {config['training']['max_epochs']}\")\n",
    "print(f\"  Batch size: {config['data']['batch_size']}\")\n",
    "print(f\"  Dimensions: {config['data']['dimensions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train audio+MIDI model\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING AUDIO+MIDI MODEL (EXPERIMENT B)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Expected duration: ~3-4 GPU hours on T4\\n\")\n",
    "\n",
    "!python train.py --config {colab_config_path}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Audio+MIDI training complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation - Audio-Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio-only model\n",
    "import sys\n",
    "sys.path.insert(0, '/content/crescendai/model')\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from src.models.lightning_module import PerformanceEvaluationModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_best_checkpoint(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.ckpt') and 'last' not in f]\n",
    "    if not checkpoints:\n",
    "        raise ValueError(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "    return os.path.join(checkpoint_dir, sorted(checkpoints)[0])\n",
    "\n",
    "audioonly_ckpt = get_best_checkpoint(f'{checkpoint_dir}/baseline_audioonly')\n",
    "\n",
    "print(f\"Loading audio-only model...\")\n",
    "print(f\"  Checkpoint: {os.path.basename(audioonly_ckpt)}\")\n",
    "\n",
    "model_audioonly = PerformanceEvaluationModel.load_from_checkpoint(audioonly_ckpt)\n",
    "model_audioonly.eval().cuda()\n",
    "\n",
    "print(f\"\\n‚úì Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataloader\n",
    "from src.data.dataset import PerformanceDataset, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataset = PerformanceDataset(\n",
    "    annotation_path=f'{annotations_dir}/synthetic_test.jsonl',\n",
    "    dimension_names=config_audioonly['data']['dimensions'],\n",
    "    audio_sample_rate=24000,\n",
    "    max_audio_length=240000,\n",
    "    max_midi_events=0,  # Audio-only\n",
    "    augmentation_config=None,\n",
    "    apply_augmentation=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Test dataloader created ({len(test_dataset)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate audio-only model\n",
    "def evaluate_model(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            audio = batch['audio_waveform'].to(device)\n",
    "            midi = batch.get('midi_tokens')\n",
    "            if midi is not None:\n",
    "                midi = midi.to(device)\n",
    "            targets = batch['labels'].to(device)\n",
    "\n",
    "            output = model(audio, midi)\n",
    "            preds = output['scores']\n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    return all_preds, all_targets\n",
    "\n",
    "print(\"Evaluating audio-only model on test set...\")\n",
    "preds_audioonly, targets = evaluate_model(model_audioonly, test_loader)\n",
    "\n",
    "print(f\"\\n‚úì Evaluation complete\")\n",
    "print(f\"  Test samples: {len(targets)}\")\n",
    "print(f\"  Dimensions: {len(config_audioonly['data']['dimensions'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations for audio-only\n",
    "dimension_names = config_audioonly['data']['dimensions']\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, dim_name in enumerate(dimension_names):\n",
    "    r_audioonly, p_audioonly = pearsonr(targets[:, i], preds_audioonly[:, i])\n",
    "    mae_audioonly = np.abs(targets[:, i] - preds_audioonly[:, i]).mean()\n",
    "\n",
    "    results.append({\n",
    "        'dimension': dim_name,\n",
    "        'audioonly_r': r_audioonly,\n",
    "        'audioonly_mae': mae_audioonly,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUDIO-ONLY RESULTS - Test Set\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "avg_r = results_df['audioonly_r'].mean()\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Mean Pearson r: {avg_r:.3f}\")\n",
    "print(f\"  Mean MAE: {results_df['audioonly_mae'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check success criteria\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"SUCCESS CRITERIA EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. Feasibility (Can model learn patterns?)\")\n",
    "print(f\"   {'‚úì' if avg_r > 0.2 else '‚úó'} Mean r = {avg_r:.3f} {'>= 0.2' if avg_r >= 0.2 else '< 0.2'}\")\n",
    "print(f\"   Status: {'PASS - Model is learning!' if avg_r > 0.2 else 'FAIL - Model not learning'}\")\n",
    "\n",
    "print(f\"\\n2. Architecture Validation (Good performance?)\")\n",
    "print(f\"   {'‚úì' if avg_r >= 0.35 else '‚úó'} Mean r = {avg_r:.3f} {'>= 0.35' if avg_r >= 0.35 else '< 0.35'}\")\n",
    "print(f\"   Status: {'PASS - Architecture validated!' if avg_r >= 0.35 else 'NOT MET'}\")\n",
    "\n",
    "print(f\"\\n3. MVP Target (Excellent performance?)\")\n",
    "print(f\"   {'‚úì' if avg_r >= 0.48 else '‚úó'} Mean r = {avg_r:.3f} {'>= 0.48' if avg_r >= 0.48 else '< 0.48'}\")\n",
    "print(f\"   Status: {'PASS - MVP TARGET HIT!' if avg_r >= 0.48 else 'NOT MET'}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Decision\n",
    "if avg_r >= 0.48:\n",
    "    print(\"\\nüéâ EXCELLENT! Proceed with real label collection.\")\n",
    "    print(\"   Audio-only performance already at MVP level.\")\n",
    "    print(\"   Consider adding MIDI for 15-20% boost.\")\n",
    "elif avg_r >= 0.35:\n",
    "    print(\"\\n‚úì GOOD! Architecture is validated.\")\n",
    "    print(\"  Synthetic labels work, but noisy.\")\n",
    "    print(\"  Proceed with real label collection.\")\n",
    "    print(\"  Consider training audio+MIDI model to compare.\")\n",
    "elif avg_r >= 0.2:\n",
    "    print(\"\\n‚ö†Ô∏è MARGINAL. Model learning but weak.\")\n",
    "    print(\"  Check synthetic label quality.\")\n",
    "    print(\"  Consider collecting small batch (~50) of real labels first.\")\n",
    "else:\n",
    "    print(\"\\n‚úó FAILURE. Model not learning.\")\n",
    "    print(\"  DO NOT collect real labels yet.\")\n",
    "    print(\"  Debug: Check training logs, label distributions, model outputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_path = f'{checkpoint_dir}/audioonly_results.csv'\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation - Compare Audio+MIDI vs Audio-Only (Optional)\n",
    "\n",
    "**Only run this section if:**\n",
    "1. You trained the audio+MIDI model above\n",
    "2. Audio-only results were promising (r >= 0.35)\n",
    "\n",
    "This will quantify the multi-modal advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio+MIDI model\n",
    "audioMIDI_ckpt = get_best_checkpoint(f'{checkpoint_dir}/baseline_synthetic')\n",
    "\n",
    "print(f\"Loading audio+MIDI model...\")\n",
    "print(f\"  Checkpoint: {os.path.basename(audioMIDI_ckpt)}\")\n",
    "\n",
    "model_audioMIDI = PerformanceEvaluationModel.load_from_checkpoint(audioMIDI_ckpt)\n",
    "model_audioMIDI.eval().cuda()\n",
    "\n",
    "print(f\"\\n‚úì Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataloader with MIDI\n",
    "test_dataset_midi = PerformanceDataset(\n",
    "    annotation_path=f'{annotations_dir}/synthetic_test.jsonl',\n",
    "    dimension_names=config['data']['dimensions'],\n",
    "    audio_sample_rate=24000,\n",
    "    max_audio_length=240000,\n",
    "    max_midi_events=512,\n",
    "    augmentation_config=None,\n",
    "    apply_augmentation=False,\n",
    ")\n",
    "\n",
    "test_loader_midi = DataLoader(\n",
    "    test_dataset_midi,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Test dataloader with MIDI created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate audio+MIDI model\n",
    "print(\"Evaluating audio+MIDI model on test set...\")\n",
    "preds_audioMIDI, targets_midi = evaluate_model(model_audioMIDI, test_loader_midi)\n",
    "\n",
    "print(f\"\\n‚úì Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare audio+MIDI vs audio-only\n",
    "comparison_results = []\n",
    "\n",
    "for i, dim_name in enumerate(dimension_names):\n",
    "    # Audio+MIDI correlations\n",
    "    r_audioMIDI, _ = pearsonr(targets_midi[:, i], preds_audioMIDI[:, i])\n",
    "    mae_audioMIDI = np.abs(targets_midi[:, i] - preds_audioMIDI[:, i]).mean()\n",
    "\n",
    "    # Audio-only correlations (recompute on same samples)\n",
    "    r_audioonly = results_df.loc[results_df['dimension'] == dim_name, 'audioonly_r'].values[0]\n",
    "    mae_audioonly = results_df.loc[results_df['dimension'] == dim_name, 'audioonly_mae'].values[0]\n",
    "\n",
    "    # Multi-modal advantage\n",
    "    advantage = ((r_audioMIDI - r_audioonly) / r_audioonly * 100) if r_audioonly != 0 else 0\n",
    "\n",
    "    comparison_results.append({\n",
    "        'dimension': dim_name,\n",
    "        'audioMIDI_r': r_audioMIDI,\n",
    "        'audioMIDI_mae': mae_audioMIDI,\n",
    "        'audioonly_r': r_audioonly,\n",
    "        'audioonly_mae': mae_audioonly,\n",
    "        'multimodal_advantage': advantage,\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: Audio+MIDI vs Audio-Only\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Audio+MIDI mean r: {comparison_df['audioMIDI_r'].mean():.3f}\")\n",
    "print(f\"  Audio-only mean r: {comparison_df['audioonly_r'].mean():.3f}\")\n",
    "print(f\"  Multi-modal advantage: {comparison_df['multimodal_advantage'].mean():.1f}%\")\n",
    "\n",
    "multimodal_gain = comparison_df['multimodal_advantage'].mean()\n",
    "\n",
    "print(f\"\\n4. Multi-modal Advantage\")\n",
    "print(f\"   {'‚úì' if multimodal_gain >= 10 else '‚úó'} Advantage = {multimodal_gain:.1f}% {'>= 10%' if multimodal_gain >= 10 else '< 10%'}\")\n",
    "print(f\"   Status: {'PASS - MIDI helps!' if multimodal_gain >= 10 else 'NOT MET - MIDI not helping much'}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_path = f'{checkpoint_dir}/comparison_results.csv'\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Comparison results saved to: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Check the results above to determine next steps:\n",
    "\n",
    "**If audio-only r >= 0.48:**\n",
    "- ‚úì MVP target hit with audio alone!\n",
    "- ‚Üí Proceed with real label collection\n",
    "- ‚Üí MIDI will provide extra 15-20% boost\n",
    "\n",
    "**If audio-only r >= 0.35:**\n",
    "- ‚úì Architecture validated\n",
    "- ‚Üí Collect real labels (200-300 segments)\n",
    "- ‚Üí Expect significant improvement\n",
    "\n",
    "**If audio-only r >= 0.20:**\n",
    "- ‚ö†Ô∏è Marginal learning\n",
    "- ‚Üí Collect small batch (50) of real labels first\n",
    "- ‚Üí Test if real labels improve performance\n",
    "\n",
    "**If audio-only r < 0.20:**\n",
    "- ‚úó Model not learning\n",
    "- ‚Üí Debug before collecting real labels\n",
    "- ‚Üí Check training logs and label quality\n",
    "\n",
    "All results are saved to Google Drive for future reference."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
