{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDI-Only Piano Performance Evaluation - PercePiano Training\n",
    "\n",
    "**Goal**: Train MIDI-only model on PercePiano expert labels to validate symbolic-only analysis.\n",
    "\n",
    "**Target**: R^2 >= 0.185 (match PercePiano Bi-LSTM baseline)\n",
    "\n",
    "**Stretch Goal**: R^2 >= 0.30 (near best published result of 0.397)\n",
    "\n",
    "## What You Need on Google Drive\n",
    "\n",
    "Upload this folder to your Google Drive root:\n",
    "- `gdrive:percepiano_data/` containing:\n",
    "  - `percepiano_train.json`\n",
    "  - `percepiano_val.json`\n",
    "  - `percepiano_test.json`\n",
    "  - `PercePiano/` (the cloned repository with MIDI files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv and rclone\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\"\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Install dependencies\n",
    "!uv pip install --system -e .\n",
    "!pip install tensorboard rich\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure rclone (Run Once)\n",
    "\n",
    "Run this in a terminal:\n",
    "```bash\n",
    "rclone config\n",
    "```\n",
    "\n",
    "Follow prompts to set up `gdrive` remote for Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_ROOT = '/tmp/checkpoints/midi_only_percepiano'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/midi_only_percepiano'\n",
    "GDRIVE_DATA_PATH = 'gdrive:percepiano_data'\n",
    "DATA_ROOT = Path('/tmp/percepiano_data')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SETUP: CHECKPOINTS AND DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check rclone\n",
    "print(\"\\nChecking rclone configuration...\")\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "\n",
    "if 'gdrive:' in result.stdout:\n",
    "    print(\"  rclone 'gdrive' remote: CONFIGURED\")\n",
    "    RCLONE_AVAILABLE = True\n",
    "    \n",
    "    # Restore existing checkpoints\n",
    "    print(\"\\nRestoring checkpoints from Google Drive (if any)...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_CHECKPOINT_PATH, CHECKPOINT_ROOT, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "else:\n",
    "    print(\"  rclone 'gdrive' remote: NOT CONFIGURED\")\n",
    "    print(\"  Run 'rclone config' in terminal to set up Google Drive\")\n",
    "    RCLONE_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nCheckpoint directory: {CHECKPOINT_ROOT}\")\n",
    "print(f\"rclone available: {RCLONE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download PercePiano Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "DATA_ROOT = Path('/tmp/percepiano_data')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if data already exists\n",
    "train_file = DATA_ROOT / 'percepiano_train.json'\n",
    "if train_file.exists():\n",
    "    print(f\"Data already exists at {DATA_ROOT}\")\n",
    "else:\n",
    "    print(\"Downloading PercePiano data from Google Drive...\")\n",
    "    result = subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "\n",
    "# Verify data\n",
    "for split in ['train', 'val', 'test']:\n",
    "    path = DATA_ROOT / f'percepiano_{split}.json'\n",
    "    if path.exists():\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"{split}: {len(data)} samples\")\n",
    "    else:\n",
    "        print(f\"ERROR: {path} not found!\")\n",
    "\n",
    "# Check MIDI files\n",
    "midi_dir = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'all_2rounds'\n",
    "if midi_dir.exists():\n",
    "    midi_files = list(midi_dir.glob('*.mid'))\n",
    "    print(f\"\\nMIDI files: {len(midi_files)}\")\n",
    "else:\n",
    "    print(f\"\\nERROR: MIDI directory not found at {midi_dir}\")\n",
    "    print(\"Make sure to upload the full PercePiano repository\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Regenerate JSON Files with 19 Dimensions\n\nThe dataset needs to be regenerated to use all 19 PercePiano dimensions (matching the reference implementation)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\nDATA_ROOT = Path('/tmp/percepiano_data')\n\n# All 19 PercePiano dimensions (matching reference implementation)\nPERCEPIANO_DIMENSIONS = [\n    \"timing\",              # 0: Stable <-> Unstable\n    \"articulation_length\", # 1: Short <-> Long\n    \"articulation_touch\",  # 2: Soft/Cushioned <-> Hard/Solid\n    \"pedal_amount\",        # 3: Sparse/Dry <-> Saturated/Wet\n    \"pedal_clarity\",       # 4: Clean <-> Blurred\n    \"timbre_variety\",      # 5: Even <-> Colorful\n    \"timbre_depth\",        # 6: Shallow <-> Rich\n    \"timbre_brightness\",   # 7: Bright <-> Dark\n    \"timbre_loudness\",     # 8: Soft <-> Loud\n    \"dynamic_range\",       # 9: Little Range <-> Large Range\n    \"tempo\",               # 10: Fast-paced <-> Slow-paced\n    \"space\",               # 11: Flat <-> Spacious\n    \"balance\",             # 12: Disproportioned <-> Balanced\n    \"drama\",               # 13: Pure <-> Dramatic\n    \"mood_valence\",        # 14: Optimistic <-> Dark\n    \"mood_energy\",         # 15: Low Energy <-> High Energy\n    \"mood_imagination\",    # 16: Honest <-> Imaginative\n    \"sophistication\",      # 17: Sophisticated/Mellow <-> Raw/Crude\n    \"interpretation\",      # 18: Unsatisfactory <-> Convincing\n]\n\ndef regenerate_with_19_dims(data_root: Path):\n    \"\"\"Regenerate JSON files with all 19 dimensions in 0-1 scale.\"\"\"\n    \n    for split in ['train', 'val', 'test']:\n        path = data_root / f'percepiano_{split}.json'\n        \n        with open(path) as f:\n            data = json.load(f)\n        \n        for sample in data:\n            # Get original PercePiano scores (first 19 values, 20th is performer ID)\n            pp_scores = sample['percepiano_scores'][:19]\n            \n            # Map to all 19 dimensions with original 0-1 scale\n            sample['scores'] = {\n                dim: pp_scores[i]\n                for i, dim in enumerate(PERCEPIANO_DIMENSIONS)\n            }\n            \n            # Update MIDI path to Thunder Compute location\n            filename = Path(sample['midi_path']).name\n            sample['midi_path'] = str(data_root / 'PercePiano' / 'virtuoso' / 'data' / 'all_2rounds' / filename)\n        \n        # Save updated file\n        with open(path, 'w') as f:\n            json.dump(data, f, indent=2)\n        \n        print(f\"Regenerated {split}: {len(data)} samples with 19 dimensions\")\n\n# Regenerate the JSON files\nregenerate_with_19_dims(DATA_ROOT)\n\n# Verify the new format\nwith open(DATA_ROOT / 'percepiano_train.json') as f:\n    data = json.load(f)\n\nsample = data[0]\nprint(f\"\\nSample dimensions: {len(sample['scores'])}\")\nprint(f\"Dimension names: {list(sample['scores'].keys())}\")\nprint(f\"Sample scores (first 5):\")\nfor i, (dim, score) in enumerate(sample['scores'].items()):\n    if i >= 5:\n        break\n    print(f\"  {dim}: {score:.4f}\")\n\n# Verify MIDI path\nsample_path = Path(sample['midi_path'])\nprint(f\"\\nMIDI path: {sample_path}\")\nprint(f\"Exists: {sample_path.exists()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Training Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\ntorch.set_float32_matmul_precision('medium')  # Tensor Core optimization\n\nCONFIG = {\n    # Data\n    'data_dir': '/tmp/percepiano_data',\n    \n    # Model Architecture - UPDATED TO MATCH MidiBERT (768/12/12)\n    'midi_hidden_dim': 768,    # Increased from 256 to match MidiBERT\n    'midi_num_layers': 12,     # Increased from 6 to match MidiBERT\n    'midi_num_heads': 12,      # Increased from 8 to match MidiBERT\n    'max_seq_length': 512,     # Match PercePiano reference\n    'attention_da': 128,       # PercePiano self-attention hidden dim\n    'attention_r': 4,          # PercePiano self-attention heads\n    'head_hidden_dim': 256,    # PercePiano classifier hidden dim\n    'dropout': 0.1,\n    'use_percepiano_architecture': True,  # Use new PercePiano-style components\n    \n    # Legacy params (only used if use_percepiano_architecture=False)\n    'lstm_hidden': 256,\n    'lstm_layers': 2,\n    'attention_heads': 4,\n    'shared_hidden': 256,\n    'task_hidden': 128,\n    \n    # Training - MATCHED TO PERCEPIANO REFERENCE\n    'batch_size': 8,           # Reduced for larger model (was 12)\n    'learning_rate': 1e-5,     # PercePiano uses 1e-5\n    'weight_decay': 0.01,\n    'max_epochs': 100,         # PercePiano uses 100\n    'early_stopping_patience': 20,  # PercePiano uses 20\n    'gradient_clip_val': 1.0,\n    'precision': '16-mixed',\n    \n    # Checkpoints\n    'checkpoint_dir': '/tmp/checkpoints/midi_only_percepiano',\n    'gdrive_checkpoint': 'gdrive:crescendai_checkpoints/midi_only_percepiano',\n}\n\n# Print config\nprint(\"Training Configuration (PercePiano Architecture Overhaul):\")\nprint(\"=\"*60)\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Create DataLoaders"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.data.percepiano_dataset import create_dataloaders\n",
    "\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    data_dir=Path(CONFIG['data_dir']),\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    max_seq_length=CONFIG['max_seq_length'],\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  midi_tokens: {batch['midi_tokens'].shape}\")\n",
    "print(f\"  attention_mask: {batch['attention_mask'].shape}\")\n",
    "print(f\"  scores: {batch['scores'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 6b: (Optional) MAESTRO Pretraining\n\n**Skip this section if you want to train from scratch** - just run the cells below.\n\nTo pretrain the encoder on MAESTRO for better performance:\n1. Download MAESTRO v3.0 MIDI files from https://magenta.tensorflow.org/datasets/maestro\n2. Upload to `gdrive:maestro_midi/` (just the MIDI folder, ~85MB)\n3. Run the cells below\n\nPretraining takes ~8-12 hours on A100 but can significantly improve results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Download pretrained encoder from Google Drive\nimport subprocess\nfrom pathlib import Path\nimport os\n\nPRETRAIN_CHECKPOINT_DIR = Path('/tmp/checkpoints/midi_pretrain')\nPRETRAIN_GDRIVE = 'gdrive:crescendai_checkpoints/midi_pretrain'\n\nPRETRAIN_CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"=\" * 70)\nprint(\"PRETRAINED ENCODER SETUP\")\nprint(\"=\" * 70)\n\n# Download pretrained encoder (this is the key file - 344MB)\nprint(\"\\nDownloading pretrained encoder from Google Drive...\")\nresult = subprocess.run(\n    ['rclone', 'copy', f'{PRETRAIN_GDRIVE}/encoder_pretrained.pt', str(PRETRAIN_CHECKPOINT_DIR), '--progress'],\n    capture_output=False\n)\n\n# Verify download\npretrained_path = PRETRAIN_CHECKPOINT_DIR / 'encoder_pretrained.pt'\nif pretrained_path.exists():\n    size_mb = pretrained_path.stat().st_size / (1024 * 1024)\n    print(f\"\\nPretrained encoder downloaded successfully!\")\n    print(f\"  Path: {pretrained_path}\")\n    print(f\"  Size: {size_mb:.1f} MB\")\n    \n    # Verify it's the right file (should be ~330MB for 768/12/12 encoder)\n    if size_mb < 100:\n        print(f\"\\n  WARNING: File seems too small ({size_mb:.1f} MB). Expected ~330 MB.\")\n        print(\"  The encoder may not have downloaded correctly.\")\n    else:\n        print(f\"  Status: OK (size looks correct for 768/12/12 encoder)\")\n    \n    PRETRAINED_EXISTS = True\nelse:\n    print(\"\\nERROR: Pretrained encoder not found after download attempt!\")\n    print(\"Check that gdrive:crescendai_checkpoints/midi_pretrain/encoder_pretrained.pt exists\")\n    PRETRAINED_EXISTS = False\n\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# This cell is now optional - pretraining has already been completed\n# The pretrained encoder (encoder_pretrained.pt) was downloaded in the previous cell\n\n# Only run this if you need to RE-pretrain from scratch (takes 8-12 hours)\nRUN_PRETRAINING = False  # Set to True only if you want to redo pretraining\n\nif RUN_PRETRAINING:\n    print(\"Pretraining is disabled. Set RUN_PRETRAINING=True to re-pretrain.\")\n    print(\"Note: Pretraining takes 8-12 hours on A100.\")\nelse:\n    if PRETRAINED_EXISTS:\n        print(\"Using existing pretrained encoder from GDrive (recommended)\")\n        print(f\"  File: {pretrained_path}\")\n    else:\n        print(\"WARNING: No pretrained encoder available.\")\n        print(\"The model will train from random initialization (lower performance expected).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.models.midi_only_module import MIDIOnlyModule\n\n# Create model with explicit architecture matching pretrained encoder (768/12/12)\nmodel = MIDIOnlyModule(\n    # Encoder params - MUST match pretrained encoder (768/12/12)\n    midi_hidden_dim=CONFIG['midi_hidden_dim'],\n    midi_num_layers=CONFIG['midi_num_layers'],\n    midi_num_heads=CONFIG['midi_num_heads'],\n    max_seq_length=CONFIG['max_seq_length'],\n    # PercePiano self-attention aggregation\n    attention_da=CONFIG['attention_da'],\n    attention_r=CONFIG['attention_r'],\n    # PercePiano classifier head\n    head_hidden_dim=CONFIG['head_hidden_dim'],\n    # Training params\n    learning_rate=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay'],\n    dropout=CONFIG['dropout'],\n    # Architecture selection\n    use_percepiano_architecture=CONFIG['use_percepiano_architecture'],\n    # Legacy params (only used if use_percepiano_architecture=False)\n    lstm_hidden=CONFIG['lstm_hidden'],\n    lstm_layers=CONFIG['lstm_layers'],\n    attention_heads=CONFIG['attention_heads'],\n    shared_hidden=CONFIG['shared_hidden'],\n    task_hidden=CONFIG['task_hidden'],\n)\n\n# Verify architecture matches config\nprint(\"=\" * 70)\nprint(\"MODEL ARCHITECTURE VERIFICATION\")\nprint(\"=\" * 70)\nprint(f\"Expected: {CONFIG['midi_hidden_dim']}d / {CONFIG['midi_num_layers']}L / {CONFIG['midi_num_heads']}H\")\nprint(f\"Actual:   {model.midi_encoder.hidden_size}d / {model.midi_encoder.num_layers}L / {model.midi_encoder.transformer.layers[0].self_attn.num_heads}H\")\nprint(f\"PercePiano architecture: {CONFIG['use_percepiano_architecture']}\")\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(\"=\" * 70)\n\n# Verify config values are correct (not defaults)\nassert CONFIG['midi_hidden_dim'] == 768, f\"ERROR: midi_hidden_dim should be 768, got {CONFIG['midi_hidden_dim']}\"\nassert CONFIG['midi_num_layers'] == 12, f\"ERROR: midi_num_layers should be 12, got {CONFIG['midi_num_layers']}\"\nassert CONFIG['use_percepiano_architecture'] == True, \"ERROR: use_percepiano_architecture should be True\"\n\n# Load pretrained encoder weights if available\npretrained_path = PRETRAIN_CHECKPOINT_DIR / 'encoder_pretrained.pt'\nif pretrained_path.exists():\n    print(f\"\\nLoading pretrained encoder from {pretrained_path}...\")\n    model.midi_encoder.load_pretrained(pretrained_path)\n    print(\"Pretrained weights loaded successfully!\")\nelse:\n    print(\"\\n\" + \"!\" * 70)\n    print(\"WARNING: No pretrained encoder found - training from scratch!\")\n    print(\"This will likely result in lower performance.\")\n    print(\"!\" * 70)\n\nprint(f\"\\nDimensions: {model.dimensions}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CONFIG['checkpoint_dir'],\n",
    "    filename='midi_only-{epoch:02d}-{val_mean_r:.3f}',\n",
    "    monitor='val/mean_r',\n",
    "    mode='max',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val/mean_r',\n",
    "    patience=CONFIG['early_stopping_patience'],\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# LR monitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir='/tmp/logs',\n",
    "    name='midi_only_percepiano',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['max_epochs'],\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    precision=CONFIG['precision'],\n",
    "    gradient_clip_val=CONFIG['gradient_clip_val'],\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured!\")\n",
    "print(f\"  Precision: {CONFIG['precision']}\")\n",
    "print(f\"  Max epochs: {CONFIG['max_epochs']}\")\n",
    "print(f\"  Early stopping patience: {CONFIG['early_stopping_patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints to Google Drive\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"Syncing checkpoints to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 10: Apply Post-Hoc Calibration\n\nIf Pearson r is positive but R^2 is negative, the model has learned relative ranking but predictions are systematically biased. Calibration fixes this."
  },
  {
   "cell_type": "code",
   "source": "# Apply calibration to fix systematic bias\nprint(\"Applying post-hoc calibration...\")\n\n# Load best model\nfrom src.models.midi_only_module import MIDIOnlyModule\nbest_model = MIDIOnlyModule.load_from_checkpoint(checkpoint_callback.best_model_path)\nbest_model.eval()\nbest_model.cuda()\n\n# Calibrate on validation set\ncalibration_results = best_model.calibrate(val_loader, method='both')\n\nprint(\"\\nCalibration Results:\")\nprint(\"=\"*60)\nprint(f\"Baseline R^2:     {calibration_results['baseline_r2']:.4f}\")\nprint(f\"Baseline MSE:     {calibration_results['baseline_mse']:.6f}\")\nprint(f\"---\")\nprint(f\"Isotonic R^2:     {calibration_results['isotonic_r2']:.4f}\")\nprint(f\"Isotonic MSE:     {calibration_results['isotonic_mse']:.6f}\")\nprint(f\"---\")\nprint(f\"Temperature R^2:  {calibration_results['temperature_r2']:.4f}\")\nprint(f\"Temperature MSE:  {calibration_results['temperature_mse']:.6f}\")\nprint(f\"Temperature value: {calibration_results['temperature_value']:.4f}\")\nprint(f\"---\")\nprint(f\"Selected method:  {calibration_results['selected_method']}\")\nprint(\"=\"*60)\n\n# Check improvement\nr2_before = calibration_results['baseline_r2']\nr2_after = calibration_results.get(f\"{calibration_results['selected_method']}_r2\", r2_before)\nif r2_after > r2_before:\n    print(f\"\\nCalibration improved R^2 by {r2_after - r2_before:.4f}\")\nelse:\n    print(f\"\\nCalibration did not improve R^2 (model may already be well-calibrated)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with best checkpoint\n",
    "print(\"\\nRunning test with best checkpoint...\")\n",
    "best_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best checkpoint: {best_path}\")\n",
    "\n",
    "if best_path:\n",
    "    test_results = trainer.test(model, test_loader, ckpt_path=best_path)\n",
    "    print(\"\\nTest Results:\")\n",
    "    for k, v in test_results[0].items():\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load best model\n",
    "best_model = MIDIOnlyModule.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "best_model.eval()\n",
    "best_model.cuda()\n",
    "\n",
    "# Collect predictions\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        outputs = best_model(batch['midi_tokens'], batch['attention_mask'])\n",
    "        all_preds.append(outputs['predictions'].cpu())\n",
    "        all_targets.append(batch['scores'].cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "print(\"Per-Dimension Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Dimension':<20} {'Pearson r':<12} {'R^2':<12} {'MAE':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "dimensions = best_model.dimensions\n",
    "results = {}\n",
    "\n",
    "for i, dim in enumerate(dimensions):\n",
    "    preds = all_preds[:, i]\n",
    "    targets = all_targets[:, i]\n",
    "    \n",
    "    # Pearson correlation\n",
    "    r, p = stats.pearsonr(preds, targets)\n",
    "    \n",
    "    # R-squared\n",
    "    ss_res = np.sum((targets - preds) ** 2)\n",
    "    ss_tot = np.sum((targets - np.mean(targets)) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0\n",
    "    \n",
    "    # MAE\n",
    "    mae = np.mean(np.abs(preds - targets))\n",
    "    \n",
    "    results[dim] = {'r': r, 'r2': r2, 'mae': mae}\n",
    "    print(f\"{dim:<20} {r:<12.4f} {r2:<12.4f} {mae:<12.4f}\")\n",
    "\n",
    "# Overall metrics\n",
    "mean_r = np.mean([v['r'] for v in results.values()])\n",
    "mean_r2 = np.mean([v['r2'] for v in results.values()])\n",
    "mean_mae = np.mean([v['mae'] for v in results.values()])\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"{'MEAN':<20} {mean_r:<12.4f} {mean_r2:<12.4f} {mean_mae:<12.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare to baselines\n",
    "print(\"\\nComparison to PercePiano Baselines:\")\n",
    "print(f\"  Bi-LSTM baseline: R^2 = 0.185\")\n",
    "print(f\"  MidiBERT:         R^2 = 0.313\")\n",
    "print(f\"  Best (HAN):       R^2 = 0.397\")\n",
    "print(f\"  Our model:        R^2 = {mean_r2:.3f}\")\n",
    "\n",
    "if mean_r2 >= 0.185:\n",
    "    print(\"\\n  SUCCESS: Meets baseline target!\")\n",
    "if mean_r2 >= 0.30:\n",
    "    print(\"  SUCCESS: Meets stretch goal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Save Final Model and Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Save final model for inference\n",
    "final_path = Path(CONFIG['checkpoint_dir']) / 'midi_scorer_final.pt'\n",
    "torch.save({\n",
    "    'state_dict': best_model.state_dict(),\n",
    "    'hparams': dict(best_model.hparams),\n",
    "    'dimensions': best_model.dimensions,\n",
    "    'results': results,\n",
    "    'mean_r2': mean_r2,\n",
    "    'mean_r': mean_r,\n",
    "}, final_path)\n",
    "print(f\"Saved final model to {final_path}\")\n",
    "\n",
    "# Final sync to Google Drive\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"\\nFinal sync to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")\n",
    "    print(f\"Checkpoints available at: {CONFIG['gdrive_checkpoint']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}