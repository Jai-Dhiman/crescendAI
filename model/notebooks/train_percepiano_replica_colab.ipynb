{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PercePiano Replica Training (Google Colab)\n",
    "\n",
    "Train the PercePiano replica model using preprocessed VirtuosoNet features.\n",
    "\n",
    "## Attribution\n",
    "\n",
    "> **PercePiano: A Benchmark for Perceptual Evaluation of Piano Performance**  \n",
    "> Park, Jongho and Kim, Dasaem et al.  \n",
    "> ISMIR 2024 / Nature Scientific Reports 2024  \n",
    "> GitHub: https://github.com/JonghoKimSNU/PercePiano\n",
    "\n",
    "## Data\n",
    "\n",
    "Uses preprocessed VirtuosoNet features (79-dim normalized + 5-dim unnorm for augmentation):\n",
    "- Train: 945 samples\n",
    "- Val: 34 samples  \n",
    "- Test: 115 samples\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- Target R-squared: 0.35-0.40 (piece-split)\n",
    "- Training time: ~1-2 hours on T4/A100\n",
    "\n",
    "## Colab-Specific Features\n",
    "\n",
    "- Native Google Drive mounting (no rclone config needed)\n",
    "- Colab Secrets API for sensitive configuration\n",
    "- Runtime detection and GPU info\n",
    "- Session keep-alive utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect runtime environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def is_colab():\n",
    "    \"\"\"Detect if running in Google Colab.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "RUNNING_IN_COLAB = is_colab()\n",
    "print(f\"Running in Colab: {RUNNING_IN_COLAB}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Memory: {gpu_mem:.1f} GB\")\n",
    "    \n",
    "    # GPU type recommendations\n",
    "    if gpu_mem >= 14:\n",
    "        print(\"\\n[INFO] High-memory GPU detected (A100/V100) - can use larger batch sizes\")\n",
    "    elif gpu_mem >= 8:\n",
    "        print(\"\\n[INFO] T4/P100 detected - standard batch size recommended\")\n",
    "    else:\n",
    "        print(\"\\n[WARN] Low-memory GPU - may need to reduce batch size\")\n",
    "else:\n",
    "    print(\"\\n[WARN] No GPU detected! Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\nif RUNNING_IN_COLAB:\n    # Install uv for fast package management\n    !curl -LsSf https://astral.sh/uv/install.sh | sh 2>/dev/null\n    os.environ['PATH'] = f\"{os.environ['HOME']}/.local/bin:{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n\n# Clone repository\nREPO_PATH = '/content/crescendai' if RUNNING_IN_COLAB else '/tmp/crescendai'\n\nif not os.path.exists(REPO_PATH):\n    !git clone https://github.com/Jai-Dhiman/crescendai.git {REPO_PATH}\n\n%cd {REPO_PATH}/model\n!git pull\n!git log -1 --oneline\n\n# Install package and dependencies\n!uv pip install --system -e . 2>/dev/null || pip install -e .\n!pip install tensorboard rich -q\n\nimport torch\nimport pytorch_lightning as pl\nprint(f\"\\nPyTorch: {torch.__version__}\")\nprint(f\"Lightning: {pl.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Google Drive Integration\n",
    "\n",
    "Two options for accessing data from Google Drive:\n",
    "1. **Native Drive Mount** (recommended for Colab) - Simple, fast for small-medium datasets\n",
    "2. **rclone** - Better for very large datasets or external cloud storage\n",
    "\n",
    "The native mount is preferred since it requires no additional configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Configuration\n",
    "USE_NATIVE_DRIVE_MOUNT = True  # Set to False to use rclone instead\n",
    "\n",
    "# Path configuration\n",
    "if RUNNING_IN_COLAB:\n",
    "    DRIVE_MOUNT_POINT = '/content/drive'\n",
    "    DATA_ROOT = Path('/content/percepiano_vnet_split')\n",
    "    CHECKPOINT_ROOT = '/content/checkpoints/percepiano_replica'\n",
    "else:\n",
    "    DRIVE_MOUNT_POINT = '/tmp/drive'\n",
    "    DATA_ROOT = Path('/tmp/percepiano_vnet_split')\n",
    "    CHECKPOINT_ROOT = '/tmp/checkpoints/percepiano_replica'\n",
    "\n",
    "# Google Drive paths (relative to mount point)\n",
    "GDRIVE_DATA_REL_PATH = 'MyDrive/crescendai_data/percepiano_vnet_split'\n",
    "GDRIVE_CHECKPOINT_REL_PATH = 'MyDrive/crescendai_checkpoints/percepiano_replica'\n",
    "\n",
    "# For rclone (if used)\n",
    "GDRIVE_DATA_PATH = 'gdrive:crescendai_data/percepiano_vnet_split'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/percepiano_replica'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PERCEPIANO REPLICA TRAINING (COLAB)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nData directory: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_ROOT}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "DRIVE_AVAILABLE = False\n",
    "RCLONE_AVAILABLE = False\n",
    "\n",
    "if USE_NATIVE_DRIVE_MOUNT and RUNNING_IN_COLAB:\n",
    "    print(\"Using native Google Drive mount...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount(DRIVE_MOUNT_POINT)\n",
    "    \n",
    "    # Check if data exists\n",
    "    drive_data_path = Path(DRIVE_MOUNT_POINT) / GDRIVE_DATA_REL_PATH\n",
    "    if drive_data_path.exists():\n",
    "        DRIVE_AVAILABLE = True\n",
    "        print(f\"\\nData found at: {drive_data_path}\")\n",
    "    else:\n",
    "        print(f\"\\n[WARN] Data not found at: {drive_data_path}\")\n",
    "        print(\"Please ensure your data is at: My Drive/crescendai_data/percepiano_vnet_split/\")\n",
    "else:\n",
    "    # Try rclone\n",
    "    print(\"Checking rclone configuration...\")\n",
    "    \n",
    "    # Install rclone if needed\n",
    "    result = subprocess.run(['which', 'rclone'], capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        print(\"Installing rclone...\")\n",
    "        !curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\"\n",
    "    \n",
    "    result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "    if 'gdrive:' in result.stdout:\n",
    "        RCLONE_AVAILABLE = True\n",
    "        print(\"rclone 'gdrive' remote: CONFIGURED\")\n",
    "    else:\n",
    "        print(\"rclone 'gdrive' remote: NOT CONFIGURED\")\n",
    "        print(\"\\nTo configure rclone with Colab Secrets:\")\n",
    "        print(\"  1. Run 'rclone config' locally to create gdrive remote\")\n",
    "        print(\"  2. Copy ~/.config/rclone/rclone.conf contents\")\n",
    "        print(\"  3. Add as Colab Secret named 'RCLONE_CONFIG'\")\n",
    "        print(\"  4. Re-run this cell\")\n",
    "        \n",
    "        # Try to load from Colab Secrets\n",
    "        if RUNNING_IN_COLAB:\n",
    "            try:\n",
    "                from google.colab import userdata\n",
    "                rclone_config = userdata.get('RCLONE_CONFIG')\n",
    "                if rclone_config:\n",
    "                    os.makedirs(os.path.expanduser('~/.config/rclone'), exist_ok=True)\n",
    "                    with open(os.path.expanduser('~/.config/rclone/rclone.conf'), 'w') as f:\n",
    "                        f.write(rclone_config)\n",
    "                    print(\"\\nLoaded rclone config from Colab Secrets!\")\n",
    "                    RCLONE_AVAILABLE = True\n",
    "            except Exception as e:\n",
    "                print(f\"\\nCould not load RCLONE_CONFIG from secrets: {e}\")\n",
    "\n",
    "print(f\"\\nDrive mount available: {DRIVE_AVAILABLE}\")\n",
    "print(f\"rclone available: {RCLONE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download/Copy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_data_from_drive(src_path: Path, dst_path: Path):\n",
    "    \"\"\"Copy data from mounted Drive to local storage for faster I/O.\"\"\"\n",
    "    print(f\"Copying data from {src_path} to {dst_path}...\")\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        src_split = src_path / split\n",
    "        dst_split = dst_path / split\n",
    "        \n",
    "        if src_split.exists():\n",
    "            dst_split.mkdir(parents=True, exist_ok=True)\n",
    "            files = list(src_split.glob('*.pkl'))\n",
    "            print(f\"  {split}: copying {len(files)} files...\")\n",
    "            for f in files:\n",
    "                shutil.copy2(f, dst_split / f.name)\n",
    "        else:\n",
    "            print(f\"  {split}: NOT FOUND at {src_split}\")\n",
    "    \n",
    "    # Copy stat.pkl\n",
    "    stat_file = src_path / 'stat.pkl'\n",
    "    if stat_file.exists():\n",
    "        shutil.copy2(stat_file, dst_path / 'stat.pkl')\n",
    "        print(f\"  stat.pkl: copied\")\n",
    "    else:\n",
    "        print(f\"  stat.pkl: NOT FOUND\")\n",
    "\n",
    "# Copy data based on available method\n",
    "if DRIVE_AVAILABLE:\n",
    "    drive_data_path = Path(DRIVE_MOUNT_POINT) / GDRIVE_DATA_REL_PATH\n",
    "    copy_data_from_drive(drive_data_path, DATA_ROOT)\n",
    "    \n",
    "    # Also restore checkpoints\n",
    "    drive_ckpt_path = Path(DRIVE_MOUNT_POINT) / GDRIVE_CHECKPOINT_REL_PATH\n",
    "    if drive_ckpt_path.exists():\n",
    "        print(f\"\\nRestoring checkpoints from {drive_ckpt_path}...\")\n",
    "        for f in drive_ckpt_path.glob('*'):\n",
    "            shutil.copy2(f, Path(CHECKPOINT_ROOT) / f.name)\n",
    "        print(\"Checkpoints restored!\")\n",
    "\n",
    "elif RCLONE_AVAILABLE:\n",
    "    print(\"Downloading data using rclone...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRestoring checkpoints...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_CHECKPOINT_PATH, CHECKPOINT_ROOT, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"No data source available!\\n\"\n",
    "        \"Either:\\n\"\n",
    "        \"  1. Mount Google Drive and ensure data exists at the correct path\\n\"\n",
    "        \"  2. Configure rclone with 'gdrive' remote\\n\"\n",
    "        \"  3. Upload data manually to Colab\"\n",
    "    )\n",
    "\n",
    "# Verify data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = DATA_ROOT / split\n",
    "    if split_dir.exists():\n",
    "        count = len(list(split_dir.glob('*.pkl')))\n",
    "        print(f\"  {split}: {count} samples\")\n",
    "    else:\n",
    "        print(f\"  {split}: MISSING!\")\n",
    "\n",
    "stat_file = DATA_ROOT / 'stat.pkl'\n",
    "print(f\"  stat.pkl: {'present' if stat_file.exists() else 'MISSING!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Adjust batch size based on GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if gpu_mem >= 14:\n",
    "        batch_size = 64  # A100\n",
    "    elif gpu_mem >= 8:\n",
    "        batch_size = 32  # T4/P100\n",
    "    else:\n",
    "        batch_size = 16  # K80 or lower\n",
    "else:\n",
    "    batch_size = 8  # CPU fallback\n",
    "\n",
    "# PercePiano Configuration (matched to original paper)\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'data_dir': str(DATA_ROOT),\n",
    "    \n",
    "    # Model input (79 normalized features, unnorm used for augmentation only)\n",
    "    'input_size': 79,\n",
    "    \n",
    "    # HAN Architecture (han_bigger256_concat.yml)\n",
    "    'hidden_size': 256,\n",
    "    'note_layers': 2,\n",
    "    'voice_layers': 2,\n",
    "    'beat_layers': 2,\n",
    "    'measure_layers': 1,\n",
    "    'num_attention_heads': 8,\n",
    "    'final_hidden': 128,\n",
    "    \n",
    "    # Training (parser.py defaults)\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': batch_size,\n",
    "    'max_epochs': 100,\n",
    "    'early_stopping_patience': 20,\n",
    "    'gradient_clip_val': 2.0,\n",
    "    'precision': '16-mixed',\n",
    "    \n",
    "    # Dataset\n",
    "    'max_notes': 1024,\n",
    "    \n",
    "    # Checkpoints\n",
    "    'checkpoint_dir': CHECKPOINT_ROOT,\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create DataLoaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.percepiano.data.percepiano_vnet_dataset import create_vnet_dataloaders\n",
    "from src.percepiano.models.percepiano_replica import PercePianoVNetModule\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader, val_loader, test_loader = create_vnet_dataloaders(\n",
    "    data_dir=CONFIG['data_dir'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    max_notes=CONFIG['max_notes'],\n",
    "    num_workers=2 if RUNNING_IN_COLAB else 0,  # Colab supports workers\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_loader.dataset)} samples\")\n",
    "print(f\"Val: {len(val_loader.dataset)} samples\")\n",
    "print(f\"Test: {len(test_loader.dataset)} samples\")\n",
    "\n",
    "# Create model\n",
    "model = PercePianoVNetModule(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    note_layers=CONFIG['note_layers'],\n",
    "    voice_layers=CONFIG['voice_layers'],\n",
    "    beat_layers=CONFIG['beat_layers'],\n",
    "    measure_layers=CONFIG['measure_layers'],\n",
    "    num_attention_heads=CONFIG['num_attention_heads'],\n",
    "    final_hidden=CONFIG['final_hidden'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    dropout=CONFIG['dropout'],\n",
    ")\n",
    "\n",
    "print(f\"\\nModel parameters: {model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CONFIG['checkpoint_dir'],\n",
    "    filename='percepiano-{epoch:02d}-{val_mean_r2:.4f}',\n",
    "    monitor='val/mean_r2',\n",
    "    mode='max',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val/mean_r2',\n",
    "    patience=CONFIG['early_stopping_patience'],\n",
    "    mode='max',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# Logger - use Colab-friendly paths\n",
    "log_dir = '/content/logs' if RUNNING_IN_COLAB else '/tmp/logs'\n",
    "logger = TensorBoardLogger(save_dir=log_dir, name='percepiano_replica')\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['max_epochs'],\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    precision=CONFIG['precision'] if torch.cuda.is_available() else 32,\n",
    "    gradient_clip_val=CONFIG['gradient_clip_val'],\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.5,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")\n",
    "\n",
    "# TensorBoard in Colab\n",
    "if RUNNING_IN_COLAB:\n",
    "    print(\"\\nTo view TensorBoard, run in a new cell:\")\n",
    "    print(\"  %load_ext tensorboard\")\n",
    "    print(f\"  %tensorboard --logdir {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load TensorBoard (run this cell to enable inline TensorBoard)\n",
    "if RUNNING_IN_COLAB:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir /content/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Session Keep-Alive (Optional)\n",
    "\n",
    "Colab may disconnect after 90 minutes of inactivity. This cell sets up a simple keep-alive mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep-alive for long training sessions (optional)\n",
    "if RUNNING_IN_COLAB:\n",
    "    import IPython\n",
    "    from google.colab import output\n",
    "    \n",
    "    # This creates a simple keep-alive by periodically pinging\n",
    "    # Note: This doesn't guarantee session persistence, but helps\n",
    "    display(IPython.display.Javascript('''\n",
    "        function KeepClicking() {\n",
    "            console.log(\"Keeping session alive...\");\n",
    "            // Simulate activity every 60 seconds\n",
    "            setTimeout(KeepClicking, 60000);\n",
    "        }\n",
    "        KeepClicking();\n",
    "    '''))\n",
    "    print(\"Keep-alive enabled (pings every 60s)\")\n",
    "    print(\"Note: Colab Pro+ allows up to 24h sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPercePiano SOTA baselines:\")\n",
    "print(\"  Bi-LSTM: R^2 = 0.185\")\n",
    "print(\"  MidiBERT: R^2 = 0.313\")\n",
    "print(\"  Bi-LSTM + SA + HAN: R^2 = 0.397 (SOTA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints back to Google Drive\n",
    "print(\"Syncing checkpoints to Google Drive...\")\n",
    "\n",
    "if DRIVE_AVAILABLE:\n",
    "    # Use native drive mount\n",
    "    drive_ckpt_path = Path(DRIVE_MOUNT_POINT) / GDRIVE_CHECKPOINT_REL_PATH\n",
    "    drive_ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for f in Path(CHECKPOINT_ROOT).glob('*'):\n",
    "        shutil.copy2(f, drive_ckpt_path / f.name)\n",
    "    \n",
    "    print(f\"Checkpoints synced to: {drive_ckpt_path}\")\n",
    "    \n",
    "elif RCLONE_AVAILABLE:\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], GDRIVE_CHECKPOINT_PATH, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Checkpoints synced via rclone!\")\n",
    "else:\n",
    "    print(\"[WARN] No sync method available - download checkpoints manually!\")\n",
    "    if RUNNING_IN_COLAB:\n",
    "        from google.colab import files\n",
    "        print(\"Checkpoints available at:\", CHECKPOINT_ROOT)\n",
    "        print(\"Use: files.download(path) to download specific files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COMPREHENSIVE EVALUATION\n",
    "========================\n",
    "This cell performs exhaustive analysis of model performance including:\n",
    "1. Overall metrics (R2, R, MAE, RMSE)\n",
    "2. Per-dimension breakdown with all metrics\n",
    "3. Prediction distribution analysis\n",
    "4. Residual analysis\n",
    "5. Best/worst sample analysis\n",
    "6. Comparison to baselines\n",
    "7. Diagnostic checks\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model\n",
    "best_path = checkpoint_callback.best_model_path\n",
    "print(f\"\\nBest checkpoint: {best_path}\")\n",
    "\n",
    "best_model = PercePianoVNetModule.load_from_checkpoint(best_path)\n",
    "best_model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    best_model.cuda()\n",
    "\n",
    "dimensions = list(best_model.dimensions)\n",
    "n_dims = len(dimensions)\n",
    "\n",
    "# Collect predictions on ALL splits\n",
    "results = {}\n",
    "for split_name, loader in [('train', train_loader), ('val', val_loader), ('test', test_loader)]:\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    sample_info = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            input_features = batch['input_features'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            scores = batch['scores'].to(device)\n",
    "            num_notes = batch['num_notes']\n",
    "            \n",
    "            note_locations = {\n",
    "                'beat': batch['note_locations_beat'].to(device),\n",
    "                'measure': batch['note_locations_measure'].to(device),\n",
    "                'voice': batch['note_locations_voice'].to(device),\n",
    "            }\n",
    "            \n",
    "            outputs = best_model(\n",
    "                input_features=input_features,\n",
    "                note_locations=note_locations,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            \n",
    "            all_preds.append(outputs['predictions'].cpu().numpy())\n",
    "            all_targets.append(scores.cpu().numpy())\n",
    "            \n",
    "            # Store sample info for analysis\n",
    "            for i in range(len(num_notes)):\n",
    "                sample_info.append({\n",
    "                    'batch_idx': batch_idx,\n",
    "                    'sample_idx': i,\n",
    "                    'num_notes': num_notes[i].item(),\n",
    "                })\n",
    "    \n",
    "    results[split_name] = {\n",
    "        'preds': np.concatenate(all_preds),\n",
    "        'targets': np.concatenate(all_targets),\n",
    "        'sample_info': sample_info,\n",
    "    }\n",
    "    print(f\"  {split_name}: {len(results[split_name]['preds'])} samples\")\n",
    "\n",
    "# Use test set for detailed analysis\n",
    "preds = results['test']['preds']\n",
    "targets = results['test']['targets']\n",
    "sample_info = results['test']['sample_info']\n",
    "n_samples = len(preds)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"1. OVERALL METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_r2 = r2_score(targets, preds)\n",
    "overall_r2_per_sample = r2_score(targets, preds, multioutput='raw_values')\n",
    "overall_mae = mean_absolute_error(targets, preds)\n",
    "overall_rmse = np.sqrt(mean_squared_error(targets, preds))\n",
    "\n",
    "# Flatten for correlation\n",
    "flat_preds = preds.flatten()\n",
    "flat_targets = targets.flatten()\n",
    "overall_pearson_r, overall_pearson_p = stats.pearsonr(flat_targets, flat_preds)\n",
    "overall_spearman_r, overall_spearman_p = stats.spearmanr(flat_targets, flat_preds)\n",
    "\n",
    "print(f\"\\n  Test Set ({n_samples} samples, {n_dims} dimensions)\")\n",
    "print(f\"  {'-'*50}\")\n",
    "print(f\"  R-squared (R2):        {overall_r2:+.4f}\")\n",
    "print(f\"  Pearson R:             {overall_pearson_r:+.4f}  (p={overall_pearson_p:.2e})\")\n",
    "print(f\"  Spearman R:            {overall_spearman_r:+.4f}  (p={overall_spearman_p:.2e})\")\n",
    "print(f\"  MAE:                   {overall_mae:.4f}\")\n",
    "print(f\"  RMSE:                  {overall_rmse:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\n  Interpretation:\")\n",
    "if overall_r2 >= 0.35:\n",
    "    print(f\"    [EXCELLENT] R2 >= 0.35 matches published SOTA\")\n",
    "elif overall_r2 >= 0.25:\n",
    "    print(f\"    [GOOD] R2 >= 0.25 is usable for pseudo-labeling\")\n",
    "elif overall_r2 >= 0.10:\n",
    "    print(f\"    [FAIR] R2 >= 0.10 shows some learning, needs improvement\")\n",
    "elif overall_r2 >= 0:\n",
    "    print(f\"    [POOR] R2 > 0 but barely better than mean prediction\")\n",
    "else:\n",
    "    print(f\"    [FAILED] R2 < 0 means model is WORSE than predicting the mean!\")\n",
    "    print(f\"    This indicates a fundamental problem with data or architecture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"2. PER-DIMENSION DETAILED METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate per-dimension metrics\n",
    "dim_metrics = []\n",
    "for i, dim in enumerate(dimensions):\n",
    "    y_true = targets[:, i]\n",
    "    y_pred = preds[:, i]\n",
    "    \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    pearson_r, pearson_p = stats.pearsonr(y_true, y_pred)\n",
    "    spearman_r, spearman_p = stats.spearmanr(y_true, y_pred)\n",
    "    \n",
    "    # Prediction statistics\n",
    "    pred_mean = y_pred.mean()\n",
    "    pred_std = y_pred.std()\n",
    "    target_mean = y_true.mean()\n",
    "    target_std = y_true.std()\n",
    "    \n",
    "    # Residual analysis\n",
    "    residuals = y_true - y_pred\n",
    "    residual_mean = residuals.mean()\n",
    "    residual_std = residuals.std()\n",
    "    \n",
    "    dim_metrics.append({\n",
    "        'dim': dim,\n",
    "        'r2': r2,\n",
    "        'r': pearson_r,\n",
    "        'r_p': pearson_p,\n",
    "        'spearman': spearman_r,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'pred_mean': pred_mean,\n",
    "        'pred_std': pred_std,\n",
    "        'target_mean': target_mean,\n",
    "        'target_std': target_std,\n",
    "        'residual_mean': residual_mean,\n",
    "        'residual_std': residual_std,\n",
    "    })\n",
    "\n",
    "# Sort by R2\n",
    "dim_metrics.sort(key=lambda x: x['r2'], reverse=True)\n",
    "\n",
    "# Print detailed table\n",
    "print(f\"\\n  {'Dimension':<22} {'R2':>8} {'R':>8} {'MAE':>8} {'RMSE':>8} {'Status':<12}\")\n",
    "print(f\"  {'-'*22} {'-'*8} {'-'*8} {'-'*8} {'-'*8} {'-'*12}\")\n",
    "\n",
    "for m in dim_metrics:\n",
    "    # Status indicator\n",
    "    if m['r2'] >= 0.3:\n",
    "        status = \"[GOOD]\"\n",
    "    elif m['r2'] >= 0.1:\n",
    "        status = \"[OK]\"\n",
    "    elif m['r2'] >= 0:\n",
    "        status = \"[WEAK]\"\n",
    "    else:\n",
    "        status = \"[FAILED]\"\n",
    "    \n",
    "    print(f\"  {m['dim']:<22} {m['r2']:>+8.4f} {m['r']:>+8.4f} {m['mae']:>8.4f} {m['rmse']:>8.4f} {status:<12}\")\n",
    "\n",
    "# Summary statistics\n",
    "positive_r2 = sum(1 for m in dim_metrics if m['r2'] > 0)\n",
    "strong_r2 = sum(1 for m in dim_metrics if m['r2'] >= 0.2)\n",
    "negative_r2 = sum(1 for m in dim_metrics if m['r2'] < 0)\n",
    "\n",
    "print(f\"\\n  Summary:\")\n",
    "print(f\"    Positive R2 (> 0):    {positive_r2}/{n_dims}\")\n",
    "print(f\"    Strong R2 (>= 0.2):   {strong_r2}/{n_dims}\")\n",
    "print(f\"    Negative R2 (< 0):    {negative_r2}/{n_dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"3. CROSS-SPLIT COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n  Checking for overfitting (train >> test) or data issues...\")\n",
    "print(f\"\\n  {'Split':<10} {'R2':>10} {'MAE':>10} {'RMSE':>10} {'Samples':>10}\")\n",
    "print(f\"  {'-'*10} {'-'*10} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "\n",
    "split_metrics = {}\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    p = results[split_name]['preds']\n",
    "    t = results[split_name]['targets']\n",
    "    \n",
    "    r2 = r2_score(t, p)\n",
    "    mae = mean_absolute_error(t, p)\n",
    "    rmse = np.sqrt(mean_squared_error(t, p))\n",
    "    \n",
    "    split_metrics[split_name] = {'r2': r2, 'mae': mae, 'rmse': rmse, 'n': len(p)}\n",
    "    \n",
    "    print(f\"  {split_name:<10} {r2:>+10.4f} {mae:>10.4f} {rmse:>10.4f} {len(p):>10}\")\n",
    "\n",
    "# Check for overfitting\n",
    "train_r2 = split_metrics['train']['r2']\n",
    "test_r2 = split_metrics['test']['r2']\n",
    "overfit_gap = train_r2 - test_r2\n",
    "\n",
    "print(f\"\\n  Overfitting analysis:\")\n",
    "print(f\"    Train-Test R2 gap: {overfit_gap:+.4f}\")\n",
    "if overfit_gap > 0.2:\n",
    "    print(f\"    [WARNING] Large gap suggests overfitting!\")\n",
    "elif overfit_gap > 0.1:\n",
    "    print(f\"    [CAUTION] Moderate gap, some overfitting present\")\n",
    "else:\n",
    "    print(f\"    [OK] Gap is acceptable\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"4. COMPARISON TO BASELINES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baselines = {\n",
    "    'Mean Prediction': 0.0,\n",
    "    'Random': -0.5,\n",
    "    'Bi-LSTM (published)': 0.185,\n",
    "    'MidiBERT (published)': 0.313,\n",
    "    'HAN SOTA (published)': 0.397,\n",
    "}\n",
    "\n",
    "print(f\"\\n  {'Model':<30} {'R2':>10} {'vs Ours':>12}\")\n",
    "print(f\"  {'-'*30} {'-'*10} {'-'*12}\")\n",
    "\n",
    "for name, r2 in sorted(baselines.items(), key=lambda x: x[1]):\n",
    "    diff = overall_r2 - r2\n",
    "    diff_str = f\"{diff:+.4f}\" if diff != 0 else \"---\"\n",
    "    marker = \" <-- US\" if name == 'Mean Prediction' and overall_r2 < 0.05 else \"\"\n",
    "    print(f\"  {name:<30} {r2:>+10.4f} {diff_str:>12}{marker}\")\n",
    "\n",
    "print(f\"  {'-'*30} {'-'*10} {'-'*12}\")\n",
    "print(f\"  {'Our Model':<30} {overall_r2:>+10.4f} {'---':>12}\")\n",
    "\n",
    "# Determine rank\n",
    "our_rank = sum(1 for _, r2 in baselines.items() if r2 > overall_r2) + 1\n",
    "print(f\"\\n  Our model ranks #{our_rank} out of {len(baselines) + 1} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Teacher Model and Final Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save teacher model and final sync\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "teacher_path = Path(CONFIG['checkpoint_dir']) / 'percepiano_teacher.pt'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVE TEACHER MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if overall_r2 >= 0.25:\n",
    "    torch.save({\n",
    "        'state_dict': best_model.state_dict(),\n",
    "        'config': {\n",
    "            'input_size': CONFIG['input_size'],\n",
    "            'hidden_size': CONFIG['hidden_size'],\n",
    "            'note_layers': CONFIG['note_layers'],\n",
    "            'voice_layers': CONFIG['voice_layers'],\n",
    "            'beat_layers': CONFIG['beat_layers'],\n",
    "            'measure_layers': CONFIG['measure_layers'],\n",
    "            'num_attention_heads': CONFIG['num_attention_heads'],\n",
    "            'final_hidden': CONFIG['final_hidden'],\n",
    "            'dropout': CONFIG['dropout'],\n",
    "        },\n",
    "        'dimensions': dimensions,\n",
    "        'metrics': {\n",
    "            'overall_r2': overall_r2,\n",
    "            'overall_mae': overall_mae,\n",
    "            'overall_rmse': overall_rmse,\n",
    "            'pearson_r': overall_pearson_r,\n",
    "            'per_dimension': {m['dim']: {'r2': m['r2'], 'r': m['r'], 'mae': m['mae']} for m in dim_metrics},\n",
    "            'split_metrics': split_metrics,\n",
    "        },\n",
    "    }, teacher_path)\n",
    "    \n",
    "    print(f\"\\n  Saved teacher model to: {teacher_path}\")\n",
    "    print(f\"  Teacher R2: {overall_r2:.4f}\")\n",
    "    print(f\"  Teacher MAE: {overall_mae:.4f}\")\n",
    "    print(f\"\\n  This model can be used for pseudo-labeling MAESTRO!\")\n",
    "else:\n",
    "    print(f\"\\n  R2 = {overall_r2:.4f} is below threshold (0.25) for teacher model.\")\n",
    "    print(f\"  Not saving teacher model.\")\n",
    "    print(f\"\\n  Consider:\")\n",
    "    print(f\"    1. Training for more epochs\")\n",
    "    print(f\"    2. Checking data quality\")\n",
    "    print(f\"    3. Reviewing the diagnostic summary above\")\n",
    "\n",
    "# Final sync to Google Drive\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL SYNC TO GOOGLE DRIVE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if DRIVE_AVAILABLE:\n",
    "    drive_ckpt_path = Path(DRIVE_MOUNT_POINT) / GDRIVE_CHECKPOINT_REL_PATH\n",
    "    drive_ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for f in Path(CHECKPOINT_ROOT).glob('*'):\n",
    "        shutil.copy2(f, drive_ckpt_path / f.name)\n",
    "    \n",
    "    print(f\"\\n  Synced to: {drive_ckpt_path}\")\n",
    "    print(f\"  Files synced: {len(list(Path(CHECKPOINT_ROOT).glob('*')))}\")\n",
    "    \n",
    "elif RCLONE_AVAILABLE:\n",
    "    print(f\"\\n  Syncing checkpoints to: {GDRIVE_CHECKPOINT_PATH}\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], GDRIVE_CHECKPOINT_PATH, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(f\"\\n  Sync complete!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n  No sync method available.\")\n",
    "    print(f\"  Checkpoints saved locally at: {CHECKPOINT_ROOT}\")\n",
    "    if RUNNING_IN_COLAB:\n",
    "        print(f\"\\n  To download checkpoints:\")\n",
    "        print(f\"    from google.colab import files\")\n",
    "        print(f\"    files.download('{teacher_path}')\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities: Download Checkpoints Manually\n",
    "\n",
    "Run this cell if you need to download checkpoints directly to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download checkpoints (run only if needed)\n",
    "if RUNNING_IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"Available checkpoints:\")\n",
    "    for f in Path(CHECKPOINT_ROOT).glob('*'):\n",
    "        size_mb = f.stat().st_size / 1e6\n",
    "        print(f\"  {f.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Uncomment to download specific file:\n",
    "    # files.download(str(teacher_path))\n",
    "    \n",
    "    # Or download best checkpoint:\n",
    "    # files.download(checkpoint_callback.best_model_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}