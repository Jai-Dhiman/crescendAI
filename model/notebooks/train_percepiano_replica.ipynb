{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# PercePiano Replica Training - Clean Run\n\nTrain the PercePiano HAN model directly without debugging infrastructure.\nGoal: Establish a clean baseline using our implementation.\n\n## Attribution\n\n> **PercePiano: Piano Performance Evaluation Dataset with Multi-level Perceptual Features**  \n> Park, Kim et al.  \n> Nature Scientific Reports 2024  \n> Paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11450231/  \n> GitHub: https://github.com/JonghoKimSNU/PercePiano\n\n## Target: R2 = 0.397 (Paper SOTA)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rclone\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv and clone repository\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Clone original PercePiano for comparison (needed for data diagnostics)\n",
    "PERCEPIANO_PATH = '/tmp/crescendai/model/data/raw/PercePiano'\n",
    "if not os.path.exists(PERCEPIANO_PATH):\n",
    "    print(\"\\nCloning original PercePiano repository...\")\n",
    "    !git clone https://github.com/JonghoKimSNU/PercePiano.git {PERCEPIANO_PATH}\n",
    "else:\n",
    "    print(f\"\\nPercePiano already present at {PERCEPIANO_PATH}\")\n",
    "\n",
    "# Install dependencies\n",
    "!uv pip install --system -e .\n",
    "!pip install tensorboard rich\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths and Check rclone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = Path('/tmp/percepiano_vnet_84dim')\n",
    "CHECKPOINT_ROOT = Path('/tmp/checkpoints/percepiano_kfold')\n",
    "LOG_ROOT = Path('/tmp/logs/percepiano_kfold')\n",
    "GDRIVE_DATA_PATH = 'gdrive:crescendai_data/percepiano_vnet_84dim'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/percepiano_kfold'\n",
    "\n",
    "# Training control\n",
    "RESTART_TRAINING = True  # Set to True to clear checkpoints and start fresh\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PERCEPIANO REPLICA TRAINING (4-FOLD CV)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear checkpoints if restarting\n",
    "if RESTART_TRAINING and CHECKPOINT_ROOT.exists():\n",
    "    print(f\"\\nRESTART_TRAINING=True: Clearing checkpoints at {CHECKPOINT_ROOT}\")\n",
    "    shutil.rmtree(CHECKPOINT_ROOT)\n",
    "    print(\"  Checkpoints cleared!\")\n",
    "\n",
    "if RESTART_TRAINING and LOG_ROOT.exists():\n",
    "    print(f\"RESTART_TRAINING=True: Clearing logs at {LOG_ROOT}\")\n",
    "    shutil.rmtree(LOG_ROOT)\n",
    "    print(\"  Logs cleared!\")\n",
    "\n",
    "# Create directories\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "LOG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check rclone\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "\n",
    "if 'gdrive:' in result.stdout:\n",
    "    print(\"\\nrclone 'gdrive' remote: CONFIGURED\")\n",
    "    RCLONE_AVAILABLE = True\n",
    "else:\n",
    "    print(\"\\nrclone 'gdrive' remote: NOT CONFIGURED\")\n",
    "    print(\"Run 'rclone config' in terminal to set up Google Drive\")\n",
    "    RCLONE_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nData directory: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_ROOT}\")\n",
    "print(f\"Log directory: {LOG_ROOT}\")\n",
    "print(f\"\\nRESTART_TRAINING: {RESTART_TRAINING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Download Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "if not RCLONE_AVAILABLE:\n",
    "    raise RuntimeError(\"rclone not configured. Run 'rclone config' first.\")\n",
    "\n",
    "# Download preprocessed data\n",
    "print(\"Downloading preprocessed VirtuosoNet features from Google Drive...\")\n",
    "subprocess.run(\n",
    "    ['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'],\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "# Verify data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_samples = 0\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = DATA_ROOT / split\n",
    "    if split_dir.exists():\n",
    "        count = len(list(split_dir.glob('*.pkl')))\n",
    "        total_samples += count\n",
    "        print(f\"  {split}: {count} samples\")\n",
    "    else:\n",
    "        print(f\"  {split}: MISSING!\")\n",
    "\n",
    "print(f\"  Total: {total_samples} samples\")\n",
    "\n",
    "stat_file = DATA_ROOT / 'stat.pkl'\n",
    "print(f\"  stat.pkl: {'present' if stat_file.exists() else 'MISSING!'}\")\n",
    "\n",
    "fold_file = DATA_ROOT / 'fold_assignments.json'\n",
    "print(f\"  fold_assignments.json: {'present' if fold_file.exists() else 'will be created'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 4: Create Fold Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.percepiano.data.kfold_split import (\n",
    "    create_piece_based_folds,\n",
    "    save_fold_assignments,\n",
    "    load_fold_assignments,\n",
    "    print_fold_statistics,\n",
    ")\n",
    "\n",
    "FOLD_FILE = DATA_ROOT / 'fold_assignments.json'\n",
    "N_FOLDS = 4\n",
    "TEST_RATIO = 0.15\n",
    "SEED = 42\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FOLD ASSIGNMENT CREATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Force regeneration to use corrected methodology\n",
    "# - Test set: select pieces until ~15% of SAMPLES (PercePiano methodology)\n",
    "# - CV folds: greedy bin-packing for balanced sample counts (improvement over round-robin)\n",
    "FORCE_REGENERATE = True\n",
    "\n",
    "if FOLD_FILE.exists() and not FORCE_REGENERATE:\n",
    "    print(f\"\\nLoading existing fold assignments from {FOLD_FILE}\")\n",
    "    fold_assignments = load_fold_assignments(FOLD_FILE)\n",
    "else:\n",
    "    if FOLD_FILE.exists():\n",
    "        print(f\"\\nRemoving old fold assignments (regenerating with balanced methodology)...\")\n",
    "        FOLD_FILE.unlink()\n",
    "    \n",
    "    print(f\"\\nCreating new {N_FOLDS}-fold piece-based splits...\")\n",
    "    print(\"  Test set: select pieces until ~15% of SAMPLES\")\n",
    "    print(\"  CV folds: greedy bin-packing for balanced sample counts\")\n",
    "    fold_assignments = create_piece_based_folds(\n",
    "        data_dir=DATA_ROOT,\n",
    "        n_folds=N_FOLDS,\n",
    "        test_ratio=TEST_RATIO,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    save_fold_assignments(fold_assignments, FOLD_FILE)\n",
    "\n",
    "# Print statistics\n",
    "print_fold_statistics(fold_assignments, n_folds=N_FOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "import torch\ntorch.set_float32_matmul_precision('medium')\n\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# Import only what we need\nfrom src.percepiano.training.kfold_trainer import KFoldTrainer, MODEL_TYPE_HAN\n\n# Clean configuration - matches paper exactly\nCONFIG = {\n    # K-Fold settings\n    'n_folds': N_FOLDS,\n    'test_ratio': TEST_RATIO,\n    # Data\n    'data_dir': str(DATA_ROOT),\n    'checkpoint_dir': str(CHECKPOINT_ROOT),\n    'log_dir': str(LOG_ROOT),\n    # Model architecture (matches paper)\n    'input_size': 79,\n    'hidden_size': 256,\n    'note_layers': 2,\n    'voice_layers': 2,\n    'beat_layers': 2,\n    'measure_layers': 1,\n    'num_attention_heads': 8,\n    # Training (matches paper)\n    'learning_rate': 2.5e-5,\n    'weight_decay': 1e-5,\n    'dropout': 0.2,\n    'batch_size': 8,\n    'max_epochs': 200,\n    'early_stopping_patience': 40,  # Paper uses more patience\n    'gradient_clip_val': 2.0,\n    'precision': '32',\n    'max_notes': 5000,\n    'slice_len': 5000,\n    'num_workers': 4,\n    'augment_train': False,\n    # Disable diagnostics for cleaner output\n    'enable_diagnostics': False,\n}\n\nprint(\"=\"*60)\nprint(\"CLEAN HAN TRAINING - MATCHING PAPER CONFIG\")\nprint(\"=\"*60)\nprint(f\"\\nTarget: R2 = 0.397 (Paper SOTA)\")\nprint(\"\\nArchitecture:\")\nprint(f\"  hidden_size: {CONFIG['hidden_size']}\")\nprint(f\"  layers: note={CONFIG['note_layers']}, voice={CONFIG['voice_layers']}, beat={CONFIG['beat_layers']}, measure={CONFIG['measure_layers']}\")\nprint(f\"  attention_heads: {CONFIG['num_attention_heads']}\")\nprint(\"\\nTraining:\")\nprint(f\"  lr: {CONFIG['learning_rate']}, batch_size: {CONFIG['batch_size']}\")\nprint(f\"  max_epochs: {CONFIG['max_epochs']}, patience: {CONFIG['early_stopping_patience']}\")\nprint(f\"  diagnostics: {CONFIG['enable_diagnostics']}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "## Step 6: Initialize HAN Trainer\n\nTrain the full Hierarchical Attention Network (HAN) model directly."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "import pytorch_lightning as pl\n\n# Set seed for reproducibility\npl.seed_everything(42, workers=True)\n\n# Train on Fold 2 (longest pieces, best for hierarchy)\nFOLD_ID = 2\n\nprint(\"=\"*60)\nprint(\"HAN TRAINER INITIALIZATION\")\nprint(\"=\"*60)\nprint(f\"\\nTraining Fold: {FOLD_ID} (longest pieces)\")\n\n# Initialize trainer\nhan_trainer = KFoldTrainer(\n    config=CONFIG,\n    fold_assignments=fold_assignments,\n    data_dir=DATA_ROOT,\n    checkpoint_dir=CHECKPOINT_ROOT / \"han\",\n    log_dir=LOG_ROOT / \"han\",\n    n_folds=N_FOLDS,\n    model_type=MODEL_TYPE_HAN,\n)\n\nprint(f\"\\nCheckpoint dir: {han_trainer.checkpoint_dir}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "id": "qdk0xmb8i2q",
   "source": "# Ready to train - no debug mode needed for clean run",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "## Step 7: Train HAN Model\n\nTrain the full HAN model. Target: R2 = 0.397 (Paper SOTA)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nCLEAN HAN TRAINING\n\nTrain the full Hierarchical Attention Network directly.\nTarget: R2 = 0.397 (Paper SOTA)\n\"\"\"\n\nprint(\"=\"*70)\nprint(\"HAN MODEL TRAINING\")\nprint(\"=\"*70)\nprint(f\"\\nFold: {FOLD_ID}\")\nprint(f\"Target: R2 = 0.397 (Paper SOTA)\")\nprint(\"=\"*70)\n\n# Train\nhan_metrics = han_trainer.train_fold(\n    fold_id=FOLD_ID,\n    verbose=True,\n    resume_from_checkpoint=False,\n)\nhan_trainer.save_results()\n\n# Store for analysis\ntrained_model = han_trainer.get_trained_model(FOLD_ID)\ntrained_metrics = han_metrics\n\n# Results\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*70)\nprint(f\"\\n  Val R2: {han_metrics.val_r2:+.4f}\")\nprint(f\"  Val Loss: {han_metrics.val_loss:.6f}\")\nprint(f\"  Best Epoch: {han_metrics.best_epoch}\")\nprint(f\"  Epochs Trained: {han_metrics.epochs_trained}\")\nprint(f\"\\n  Target: R2 = 0.397 (Paper SOTA)\")\n\nif han_metrics.val_r2 >= 0.35:\n    print(f\"  [SUCCESS] Approaching SOTA!\")\nelif han_metrics.val_r2 >= 0.30:\n    print(f\"  [GOOD] Strong performance\")\nelif han_metrics.val_r2 >= 0.20:\n    print(f\"  [PARTIAL] Reasonable but below target\")\nelse:\n    print(f\"  [ISSUE] Below expected - needs investigation\")\n\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "## Step 8: Per-Dimension Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nPer-dimension R2 analysis\n\"\"\"\n\nfrom src.percepiano.models.percepiano_replica import PERCEPIANO_DIMENSIONS\n\nprint(\"=\"*60)\nprint(\"PER-DIMENSION R2 ANALYSIS\")\nprint(\"=\"*60)\n\nif hasattr(trained_metrics, 'per_dim_r2') and trained_metrics.per_dim_r2:\n    print(f\"\\n  {'Dimension':<25} {'R2':>10}\")\n    print(f\"  {'-'*25} {'-'*10}\")\n    \n    # Sort by R2 (best first)\n    dim_data = [(dim, trained_metrics.per_dim_r2.get(dim, 0)) for dim in PERCEPIANO_DIMENSIONS]\n    dim_data.sort(key=lambda x: x[1], reverse=True)\n    \n    for dim, r2 in dim_data:\n        status = \"[OK]\" if r2 >= 0.2 else \"[LOW]\" if r2 >= 0 else \"[NEG]\"\n        print(f\"  {dim:<25} {r2:>+10.4f} {status}\")\n    \n    # Summary\n    positive = sum(1 for _, r2 in dim_data if r2 > 0)\n    strong = sum(1 for _, r2 in dim_data if r2 >= 0.2)\n    print(f\"\\n  Positive R2: {positive}/19\")\n    print(f\"  Strong R2 (>=0.2): {strong}/19\")\nelse:\n    print(\"\\n  [Per-dimension R2 not available]\")\n\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "tsmwiy2nm7p",
   "metadata": {},
   "source": "## Step 9: Sync to Google Drive"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dzp8ujd2miw",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nSync checkpoints to Google Drive\n\"\"\"\n\nimport subprocess\n\nprint(\"=\"*60)\nprint(\"SYNC CHECKPOINTS TO GOOGLE DRIVE\")\nprint(\"=\"*60)\n\nif RCLONE_AVAILABLE:\n    ckpt_dir = han_trainer.checkpoint_dir\n    if ckpt_dir.exists():\n        gdrive_path = f\"{GDRIVE_CHECKPOINT_PATH}/{ckpt_dir.name}\"\n        print(f\"\\nSyncing HAN checkpoints...\")\n        subprocess.run(\n            ['rclone', 'copy', str(ckpt_dir), gdrive_path, '--progress'],\n            capture_output=False\n        )\n    \n    # Sync fold assignments\n    print(f\"\\nSyncing fold assignments...\")\n    subprocess.run(\n        ['rclone', 'copy', str(FOLD_FILE), GDRIVE_DATA_PATH, '--progress'],\n        capture_output=False\n    )\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"SYNC COMPLETE\")\n    print(\"=\"*60)\nelse:\n    print(\"\\nrclone not available - checkpoints saved locally\")\n    print(f\"Location: {han_trainer.checkpoint_dir}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}