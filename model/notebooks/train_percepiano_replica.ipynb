{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PercePiano Replica Training (4-Fold Cross-Validation)\n",
    "\n",
    "Train the PercePiano replica model using 4-fold piece-based cross-validation,\n",
    "matching the methodology and hyperparameters from the PercePiano paper SOTA.\n",
    "\n",
    "## Attribution\n",
    "\n",
    "> **PercePiano: Piano Performance Evaluation Dataset with Multi-level Perceptual Features**  \n",
    "> Park, Kim et al.  \n",
    "> Nature Scientific Reports 2024  \n",
    "> Paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11450231/  \n",
    "> GitHub: https://github.com/JonghoKimSNU/PercePiano\n",
    "\n",
    "## Methodology\n",
    "\n",
    "Following the exact approach from `m2pf_dataset_compositionfold.py`:\n",
    "\n",
    "- **Piece-based splits**: All performances of the same piece stay in the same fold\n",
    "- **Test set**: Select pieces randomly until reaching ~15% of SAMPLES (not pieces)\n",
    "- **4-fold CV**: Remaining pieces distributed round-robin across folds\n",
    "- **Per-fold normalization**: Stats computed from training folds only\n",
    "\n",
    "## Hyperparameters (SOTA Configuration - R2 = 0.397)\n",
    "\n",
    "These parameters match the published SOTA from `2_run_comp_multilevel_total.sh` and `han_bigger256_concat.yml`:\n",
    "\n",
    "| Parameter | SOTA Value | Notes |\n",
    "|-----------|------------|-------|\n",
    "| input_size | 79 | SOTA uses 79 base features (includes section_tempo) |\n",
    "| batch_size | 8 | From SOTA training script |\n",
    "| learning_rate | 2.5e-5 | From SOTA training script |\n",
    "| hidden_size | 256 | HAN encoder dimension |\n",
    "| prediction_head | 512->512->19 | From model_m2pf.py (NOT config's final_fc_size) |\n",
    "| dropout | 0.2 | Regularization |\n",
    "| augment_train | False | SOTA doesn't use key augmentation |\n",
    "| max_epochs | 200 | Extended training window |\n",
    "| early_stopping_patience | 40 | More patience for convergence |\n",
    "| gradient_clip_val | 2.0 | From parser.py |\n",
    "| **precision** | **32** | **FP32 (original uses FP32, not mixed precision)** |\n",
    "| **max_notes/slice_len** | **5000** | **SOTA slice size for overlapping sampling** |\n",
    "\n",
    "## Key Fixes Applied (Round 8 - 2025-12-26)\n",
    "\n",
    "### CRITICAL: Slice Sampling (Round 8)\n",
    "\n",
    "The single most impactful discrepancy identified:\n",
    "\n",
    "| Aspect | Original PercePiano | Previous Implementation | Impact |\n",
    "|--------|---------------------|------------------------|--------|\n",
    "| Samples/performance | 3-5 overlapping slices | 1 sample | 3-5x less training data |\n",
    "| Training samples | ~600-1000 slices | ~200 samples | Critical for learning |\n",
    "| Slice regeneration | Each epoch | None | No variation |\n",
    "\n",
    "**Fix**: Added `make_slicing_indexes_by_measure()` and `SliceRegenerationCallback`.\n",
    "\n",
    "### Round History\n",
    "\n",
    "| Round | Changes | Result |\n",
    "|-------|---------|--------|\n",
    "| 1-5 | Various fixes (precision, attention, init) | R2 stuck around 0 |\n",
    "| 6 | Match original architecture (no LayerNorm, 512->512, LR 2.5e-5) | Zero context_vector gradients |\n",
    "| 7 | Fix data pipeline (79 features, PackedSequence) | R2 = 0.0017 (prediction collapse) |\n",
    "| **8** | **Add SLICE SAMPLING (3-5 slices/sample, epoch regeneration)** | **Pending** |\n",
    "\n",
    "See `docs/EXPERIMENT_LOG.md` for full investigation details.\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- Target R2: 0.35-0.40 (matching published SOTA of 0.397)\n",
    "- Training time: ~8-12 hours on T4, ~3-5 hours on A100 (all 4 folds)\n",
    "- With slice sampling: ~600-1000 training slices (vs ~200 samples before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rclone\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv and clone repository\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Clone original PercePiano for comparison (needed for data diagnostics)\n",
    "PERCEPIANO_PATH = '/tmp/crescendai/model/data/raw/PercePiano'\n",
    "if not os.path.exists(PERCEPIANO_PATH):\n",
    "    print(\"\\nCloning original PercePiano repository...\")\n",
    "    !git clone https://github.com/JonghoKimSNU/PercePiano.git {PERCEPIANO_PATH}\n",
    "else:\n",
    "    print(f\"\\nPercePiano already present at {PERCEPIANO_PATH}\")\n",
    "\n",
    "# Install dependencies\n",
    "!uv pip install --system -e .\n",
    "!pip install tensorboard rich\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths and Check rclone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = Path('/tmp/percepiano_vnet_84dim')\n",
    "CHECKPOINT_ROOT = Path('/tmp/checkpoints/percepiano_kfold')\n",
    "LOG_ROOT = Path('/tmp/logs/percepiano_kfold')\n",
    "GDRIVE_DATA_PATH = 'gdrive:crescendai_data/percepiano_vnet_84dim'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/percepiano_kfold'\n",
    "\n",
    "# Training control\n",
    "RESTART_TRAINING = True  # Set to True to clear checkpoints and start fresh\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PERCEPIANO REPLICA TRAINING (4-FOLD CV)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear checkpoints if restarting\n",
    "if RESTART_TRAINING and CHECKPOINT_ROOT.exists():\n",
    "    print(f\"\\nRESTART_TRAINING=True: Clearing checkpoints at {CHECKPOINT_ROOT}\")\n",
    "    shutil.rmtree(CHECKPOINT_ROOT)\n",
    "    print(\"  Checkpoints cleared!\")\n",
    "\n",
    "if RESTART_TRAINING and LOG_ROOT.exists():\n",
    "    print(f\"RESTART_TRAINING=True: Clearing logs at {LOG_ROOT}\")\n",
    "    shutil.rmtree(LOG_ROOT)\n",
    "    print(\"  Logs cleared!\")\n",
    "\n",
    "# Create directories\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "LOG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check rclone\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "\n",
    "if 'gdrive:' in result.stdout:\n",
    "    print(\"\\nrclone 'gdrive' remote: CONFIGURED\")\n",
    "    RCLONE_AVAILABLE = True\n",
    "else:\n",
    "    print(\"\\nrclone 'gdrive' remote: NOT CONFIGURED\")\n",
    "    print(\"Run 'rclone config' in terminal to set up Google Drive\")\n",
    "    RCLONE_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nData directory: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_ROOT}\")\n",
    "print(f\"Log directory: {LOG_ROOT}\")\n",
    "print(f\"\\nRESTART_TRAINING: {RESTART_TRAINING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Download Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "if not RCLONE_AVAILABLE:\n",
    "    raise RuntimeError(\"rclone not configured. Run 'rclone config' first.\")\n",
    "\n",
    "# Download preprocessed data\n",
    "print(\"Downloading preprocessed VirtuosoNet features from Google Drive...\")\n",
    "subprocess.run(\n",
    "    ['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'],\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "# Verify data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_samples = 0\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = DATA_ROOT / split\n",
    "    if split_dir.exists():\n",
    "        count = len(list(split_dir.glob('*.pkl')))\n",
    "        total_samples += count\n",
    "        print(f\"  {split}: {count} samples\")\n",
    "    else:\n",
    "        print(f\"  {split}: MISSING!\")\n",
    "\n",
    "print(f\"  Total: {total_samples} samples\")\n",
    "\n",
    "stat_file = DATA_ROOT / 'stat.pkl'\n",
    "print(f\"  stat.pkl: {'present' if stat_file.exists() else 'MISSING!'}\")\n",
    "\n",
    "fold_file = DATA_ROOT / 'fold_assignments.json'\n",
    "print(f\"  fold_assignments.json: {'present' if fold_file.exists() else 'will be created'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 4: Create Fold Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.percepiano.data.kfold_split import (\n",
    "    create_piece_based_folds,\n",
    "    save_fold_assignments,\n",
    "    load_fold_assignments,\n",
    "    print_fold_statistics,\n",
    ")\n",
    "\n",
    "FOLD_FILE = DATA_ROOT / 'fold_assignments.json'\n",
    "N_FOLDS = 4\n",
    "TEST_RATIO = 0.15\n",
    "SEED = 42\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FOLD ASSIGNMENT CREATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Force regeneration to use corrected methodology\n",
    "# - Test set: select pieces until ~15% of SAMPLES (PercePiano methodology)\n",
    "# - CV folds: greedy bin-packing for balanced sample counts (improvement over round-robin)\n",
    "FORCE_REGENERATE = True\n",
    "\n",
    "if FOLD_FILE.exists() and not FORCE_REGENERATE:\n",
    "    print(f\"\\nLoading existing fold assignments from {FOLD_FILE}\")\n",
    "    fold_assignments = load_fold_assignments(FOLD_FILE)\n",
    "else:\n",
    "    if FOLD_FILE.exists():\n",
    "        print(f\"\\nRemoving old fold assignments (regenerating with balanced methodology)...\")\n",
    "        FOLD_FILE.unlink()\n",
    "    \n",
    "    print(f\"\\nCreating new {N_FOLDS}-fold piece-based splits...\")\n",
    "    print(\"  Test set: select pieces until ~15% of SAMPLES\")\n",
    "    print(\"  CV folds: greedy bin-packing for balanced sample counts\")\n",
    "    fold_assignments = create_piece_based_folds(\n",
    "        data_dir=DATA_ROOT,\n",
    "        n_folds=N_FOLDS,\n",
    "        test_ratio=TEST_RATIO,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    save_fold_assignments(fold_assignments, FOLD_FILE)\n",
    "\n",
    "# Print statistics\n",
    "print_fold_statistics(fold_assignments, n_folds=N_FOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Enable better CUDA error reporting\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "CONFIG = {\n",
    "    # K-Fold settings\n",
    "    'n_folds': N_FOLDS,\n",
    "    'test_ratio': TEST_RATIO,\n",
    "    # Data\n",
    "    'data_dir': str(DATA_ROOT),\n",
    "    'checkpoint_dir': str(CHECKPOINT_ROOT),\n",
    "    'log_dir': str(LOG_ROOT),\n",
    "    'input_size': 79,\n",
    "    'hidden_size': 256,\n",
    "    'note_layers': 2,\n",
    "    'voice_layers': 2,\n",
    "    'beat_layers': 2,\n",
    "    'measure_layers': 1,\n",
    "    'num_attention_heads': 8,\n",
    "    'learning_rate': 2.5e-5,\n",
    "    'weight_decay': 1e-5,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 8,\n",
    "    'max_epochs': 200,\n",
    "    'early_stopping_patience': 20,\n",
    "    'gradient_clip_val': 2.0,\n",
    "    'precision': '32',\n",
    "    'max_notes': 5000,\n",
    "    'slice_len': 5000,\n",
    "    'num_workers': 4,\n",
    "    'augment_train': False,\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION (SOTA - ROUND 8: SLICE SAMPLING)\")\n",
    "print(\"=\"*60)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Print training dynamics info\n",
    "print(f\"  LR after decay: {CONFIG['learning_rate'] * 0.98:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u15ydvumlrm",
   "source": "## Step 5b: Pre-Training Data Diagnostics (CRITICAL)\n\nRun this BEFORE training to validate data and detect index issues that can break hierarchy.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vuwtbajzrag",
   "source": "\"\"\"\nPRE-TRAINING DATA DIAGNOSTICS\n\nThis cell validates the data pipeline before training to catch issues that\nwould cause the hierarchical components (beat/measure attention) to fail.\n\nKey checks:\n1. Index format (should start from 1 after densification)\n2. Zero-shifted values (should have no negatives)\n3. Boundary detection (== 1 vs > 0 equivalence)\n4. Slice statistics\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom src.percepiano.data.percepiano_vnet_dataset import (\n    PercePianoKFoldDataset,\n    percepiano_pack_collate,\n)\nfrom src.percepiano.training.diagnostics import analyze_indices\n\nprint(\"=\" * 70)\nprint(\"PRE-TRAINING DATA DIAGNOSTICS\")\nprint(\"=\" * 70)\n\n# Create a test dataset for fold 0\ntest_ds = PercePianoKFoldDataset(\n    data_dir=DATA_ROOT,\n    fold_assignments=fold_assignments,\n    fold_id=0,\n    mode=\"train\",\n    max_notes=CONFIG['max_notes'],\n    slice_len=CONFIG.get('slice_len', CONFIG['max_notes']),\n)\n\n# Create a simple dataloader (no packing for easier analysis)\nsimple_loader = DataLoader(test_ds, batch_size=4, shuffle=False, num_workers=0)\n\n# Get one batch\nbatch = next(iter(simple_loader))\n\nprint(\"\\n[1] BATCH SHAPE ANALYSIS:\")\nprint(f\"  input_features: {batch['input_features'].shape}\")\nprint(f\"  note_locations_beat: {batch['note_locations_beat'].shape}\")\nprint(f\"  note_locations_measure: {batch['note_locations_measure'].shape}\")\nprint(f\"  note_locations_voice: {batch['note_locations_voice'].shape}\")\nprint(f\"  scores: {batch['scores'].shape}\")\nprint(f\"  num_notes: {[batch['num_notes'][i].item() if hasattr(batch['num_notes'][i], 'item') else batch['num_notes'][i] for i in range(4)]}\")\n\nprint(\"\\n[2] INDEX ANALYSIS:\")\nidx_stats = analyze_indices(\n    batch['note_locations_beat'],\n    batch['note_locations_measure'],\n)\n\nprint(f\"  Beat indices: min={idx_stats['beat_min']}, max={idx_stats['beat_max']}\")\nprint(f\"  Measure indices: min={idx_stats['measure_min']}, max={idx_stats['measure_max']}\")\n\n# Check if indices start from 1 (required for hierarchy_utils)\nif idx_stats['beat_min'] == 1:\n    print(f\"  [OK] Beat indices start from 1 (required)\")\nelif idx_stats['beat_min'] == 0:\n    print(f\"  [WARNING] Beat indices start from 0 (should be 1)\")\nelse:\n    print(f\"  [ERROR] Beat indices start from {idx_stats['beat_min']} (unexpected)\")\n\nprint(\"\\n[3] ZERO-SHIFTED INDEX CHECK:\")\nprint(f\"  Beat zero-shifted range: [{idx_stats['beat_zero_shifted_min']}, {idx_stats['beat_zero_shifted_max']}]\")\nprint(f\"  Measure zero-shifted range: [{idx_stats['measure_zero_shifted_min']}, {idx_stats['measure_zero_shifted_max']}]\")\n\nif idx_stats['negative_beat_count'] > 0:\n    print(f\"  [CRITICAL ERROR] {idx_stats['negative_beat_count']} negative zero-shifted beat values!\")\n    print(f\"    This WILL break span_beat_to_note_num!\")\nelse:\n    print(f\"  [OK] No negative zero-shifted values\")\n\nif idx_stats['negative_measure_count'] > 0:\n    print(f\"  [WARNING] {idx_stats['negative_measure_count']} negative zero-shifted measure values\")\n\nprint(\"\\n[4] BOUNDARY DETECTION CHECK:\")\nprint(f\"  diff == 1 count: {idx_stats['diff_equals_1_count']}\")\nprint(f\"  diff > 0 count: {idx_stats['diff_greater_0_count']}\")\nprint(f\"  diff < 0 count: {idx_stats['diff_less_0_count']} (should only be at padding boundary)\")\n\nif idx_stats['diff_equals_1_count'] == idx_stats['diff_greater_0_count']:\n    print(f\"  [OK] == 1 and > 0 are equivalent (indices are sequential)\")\nelse:\n    print(f\"  [WARNING] == 1 ({idx_stats['diff_equals_1_count']}) != > 0 ({idx_stats['diff_greater_0_count']})\")\n    print(f\"    Indices may not be properly densified!\")\n\nif idx_stats['non_sequential_samples'] > 0:\n    print(f\"  [WARNING] {idx_stats['non_sequential_samples']}/4 samples have non-sequential indices\")\n\nprint(\"\\n[5] SLICE STATISTICS:\")\nprint(f\"  Total slices in dataset: {len(test_ds)}\")\nprint(f\"  Underlying samples: {len(test_ds.sample_files)}\")\nprint(f\"  Avg slices per sample: {len(test_ds) / len(test_ds.sample_files):.1f}\")\n\nprint(\"\\n[6] FIRST SAMPLE BEAT INDICES (first 50 values):\")\nfirst_beat = batch['note_locations_beat'][0].numpy()\nfirst_num_notes = batch['num_notes'][0] if isinstance(batch['num_notes'][0], int) else batch['num_notes'][0].item()\nprint(f\"  {first_beat[:min(50, first_num_notes)].tolist()}\")\n\n# Check for unique beats\nunique_beats = np.unique(first_beat[:first_num_notes])\nprint(f\"  Unique beat values: {len(unique_beats)}\")\nprint(f\"  First 10 unique beats: {unique_beats[:10].tolist()}\")\n\nprint(\"\\n[7] INPUT FEATURE STATISTICS:\")\nfeatures = batch['input_features']\nprint(f\"  Overall: mean={features.mean():.4f}, std={features.std():.4f}\")\nprint(f\"  Range: [{features.min():.4f}, {features.max():.4f}]\")\n\n# Check for NaN/Inf\nnan_count = torch.isnan(features).sum().item()\ninf_count = torch.isinf(features).sum().item()\nif nan_count > 0:\n    print(f\"  [ERROR] {nan_count} NaN values detected!\")\nif inf_count > 0:\n    print(f\"  [ERROR] {inf_count} Inf values detected!\")\nif nan_count == 0 and inf_count == 0:\n    print(f\"  [OK] No NaN or Inf values\")\n\nprint(\"\\n[8] LABEL STATISTICS:\")\nscores = batch['scores']\nprint(f\"  Mean per dimension: {scores.mean(dim=0).tolist()[:5]} ... (first 5)\")\nprint(f\"  Std per dimension: {scores.std(dim=0).tolist()[:5]} ... (first 5)\")\nprint(f\"  Range: [{scores.min():.4f}, {scores.max():.4f}]\")\n\nif scores.min() < 0 or scores.max() > 1:\n    print(f\"  [WARNING] Labels outside [0, 1] range!\")\nelse:\n    print(f\"  [OK] Labels in [0, 1] range\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DIAGNOSTIC SUMMARY\")\nprint(\"=\" * 70)\n\nissues = []\nif idx_stats['beat_min'] != 1:\n    issues.append(\"Beat indices don't start from 1\")\nif idx_stats['negative_beat_count'] > 0:\n    issues.append(f\"Negative zero-shifted values ({idx_stats['negative_beat_count']})\")\nif idx_stats['diff_equals_1_count'] != idx_stats['diff_greater_0_count']:\n    issues.append(\"Non-sequential indices detected\")\nif nan_count > 0 or inf_count > 0:\n    issues.append(\"NaN/Inf in features\")\n\nif issues:\n    print(f\"\\n[ISSUES FOUND] {len(issues)} issues that may affect training:\")\n    for i, issue in enumerate(issues, 1):\n        print(f\"  {i}. {issue}\")\n    print(\"\\nConsider fixing these before training!\")\nelse:\n    print(\"\\n[ALL CHECKS PASSED] Data pipeline looks correct!\")\n    print(\"Proceed to training.\")\n\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 6: Initialize K-Fold Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.percepiano.training.kfold_trainer import KFoldTrainer\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Create K-Fold trainer\n",
    "kfold_trainer = KFoldTrainer(\n",
    "    config=CONFIG,\n",
    "    fold_assignments=fold_assignments,\n",
    "    data_dir=DATA_ROOT,\n",
    "    checkpoint_dir=CHECKPOINT_ROOT,\n",
    "    log_dir=LOG_ROOT,\n",
    "    n_folds=N_FOLDS,\n",
    ")\n",
    "\n",
    "print(\"K-Fold Trainer initialized\")\n",
    "print(f\"  Folds: {N_FOLDS}\")\n",
    "print(f\"  Checkpoints: {CHECKPOINT_ROOT}\")\n",
    "print(f\"  Logs: {LOG_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 7: Train All Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"TRAINING SINGLE FOLD (DIAGNOSTIC RUN)\")\nprint(\"=\"*60)\nprint(\"\\nPercePiano SOTA baselines:\")\nprint(\"  Bi-LSTM: R2 = 0.185\")\nprint(\"  MidiBERT: R2 = 0.313\")\nprint(\"  Bi-LSTM + SA + HAN: R2 = 0.397 (SOTA)\")\nprint(\"=\"*60)\n\n# Train single fold for diagnostic purposes\nFOLD_TO_TRAIN = 1  # Fold 1 is typically well-balanced\n\nfold_metrics = kfold_trainer.train_fold(\n    fold_id=FOLD_TO_TRAIN,\n    verbose=True,\n    resume_from_checkpoint=False,\n)\n\n# Save results\nkfold_trainer.save_results()\n\n# IMPORTANT: Store trained model for diagnostics (Round 12 fix)\n# This avoids needing to load from checkpoint in subsequent cells\ntrained_model = kfold_trainer.get_trained_model(FOLD_TO_TRAIN)\nif trained_model is not None:\n    print(f\"\\n  Trained model stored in memory for diagnostics\")\nelse:\n    print(f\"\\n  [WARNING] Could not retrieve trained model - will need checkpoint\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"FOLD {FOLD_TO_TRAIN} TRAINING COMPLETE\")\nprint(\"=\"*60)\nprint(f\"  Val R2: {fold_metrics.val_r2:+.4f}\")\nprint(f\"  Val Pearson: {fold_metrics.val_pearson:+.4f}\")\nprint(f\"  Epochs: {fold_metrics.epochs_trained}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints to Google Drive after training\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"Syncing checkpoints to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', str(CHECKPOINT_ROOT), GDRIVE_CHECKPOINT_PATH, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 8: Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all fold models on held-out test set\n",
    "test_results = kfold_trainer.evaluate_on_test(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tsmwiy2nm7p",
   "source": "## Step 8b: Post-Training Hierarchy Diagnostics\n\nAnalyze why the hierarchical components (beat/measure attention) may not be contributing.\nThis cell loads the best model from fold 1 and runs comprehensive diagnostics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dzp8ujd2miw",
   "source": "\"\"\"\nPOST-TRAINING HIERARCHY DIAGNOSTICS\n\nThis cell runs comprehensive diagnostics to understand why hierarchical \ncomponents (beat/measure attention) may not be contributing to model performance.\n\nKey metrics:\n1. Activation variances at each hierarchy level\n2. Attention entropy (collapsed vs distributed)\n3. Contribution analysis (% variance from each component)\n4. Ablation comparison (full model vs Bi-LSTM only)\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader\nfrom src.percepiano.models.percepiano_replica import PercePianoVNetModule\nfrom src.percepiano.data.percepiano_vnet_dataset import (\n    PercePianoKFoldDataset,\n    percepiano_pack_collate,\n)\nfrom src.percepiano.training.diagnostics import (\n    DiagnosticCallback,\n    run_full_diagnostics,\n    compute_attention_entropy,\n)\nfrom sklearn.metrics import r2_score\n\nprint(\"=\" * 70)\nprint(\"POST-TRAINING HIERARCHY DIAGNOSTICS\")\nprint(\"=\" * 70)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = None\n\n# PRIORITY 1: Use in-memory model from training (Round 12 fix)\nif 'trained_model' in dir() and trained_model is not None:\n    model = trained_model.to(device)\n    model.eval()\n    print(f\"\\nUsing in-memory trained model (no checkpoint loading needed)\")\n    print(f\"Device: {device}\")\nelse:\n    # FALLBACK: Try loading from checkpoint\n    print(f\"\\nNo in-memory model found - attempting checkpoint loading...\")\n    fold_dir = CHECKPOINT_ROOT / \"fold_1\"\n    best_ckpts = list(fold_dir.glob(\"best-*.ckpt\"))\n    if not best_ckpts:\n        print(f\"No checkpoints found in {fold_dir}\")\n        print(\"Run Step 7 (training) first to create model.\")\n    else:\n        best_ckpt = max(best_ckpts, key=lambda p: p.stat().st_mtime)\n        print(f\"Loading checkpoint: {best_ckpt}\")\n        \n        model = PercePianoVNetModule.load_from_checkpoint(\n            str(best_ckpt),\n            input_size=CONFIG.get(\"input_size\", 79),\n            hidden_size=CONFIG.get(\"hidden_size\", 256),\n            note_layers=CONFIG.get(\"note_layers\", 2),\n            voice_layers=CONFIG.get(\"voice_layers\", 2),\n            beat_layers=CONFIG.get(\"beat_layers\", 2),\n            measure_layers=CONFIG.get(\"measure_layers\", 1),\n            num_attention_heads=CONFIG.get(\"num_attention_heads\", 8),\n            dropout=CONFIG.get(\"dropout\", 0.2),\n        )\n        model = model.to(device)\n        model.eval()\n        print(f\"Device: {device}\")\n\nif model is not None:\n    # Create validation dataset\n    val_ds = PercePianoKFoldDataset(\n        data_dir=DATA_ROOT,\n        fold_assignments=fold_assignments,\n        fold_id=1,\n        mode=\"val\",\n        max_notes=CONFIG['max_notes'],\n        slice_len=CONFIG.get('slice_len', CONFIG['max_notes']),\n    )\n    val_loader = DataLoader(\n        val_ds, \n        batch_size=4, \n        shuffle=False, \n        num_workers=0,\n        collate_fn=percepiano_pack_collate,\n    )\n    print(f\"Validation samples: {len(val_ds)}\")\n    \n    # Run full diagnostics\n    print(\"\\nRunning full diagnostic analysis...\")\n    diag_results = run_full_diagnostics(model, val_loader, device, num_batches=5)\n    \n    # Print activation statistics\n    print(\"\\n\" + \"=\" * 70)\n    print(\"ACTIVATION VARIANCE ANALYSIS\")\n    print(\"=\" * 70)\n    \n    print(f\"\\n{'Component':<25} {'Mean Std':>12} {'Min Std':>12} {'Max Std':>12} {'Status':<15}\")\n    print(f\"{'-'*25} {'-'*12} {'-'*12} {'-'*12} {'-'*15}\")\n    \n    key_activations = [\n        ('input_std', 0.3, 0.6, 'Input features'),\n        ('x_embedded_std', 0.15, 0.4, 'After embedding'),\n        ('note_out_std', 0.1, 0.4, 'Note LSTM'),\n        ('voice_out_std', 0.1, 0.4, 'Voice LSTM'),\n        ('hidden_out_std', 0.1, 0.4, 'Hidden (note+voice)'),\n        ('beat_nodes_std', 0.05, 0.3, 'Beat nodes'),\n        ('beat_out_std', 0.05, 0.3, 'Beat LSTM'),\n        ('beat_spanned_std', 0.05, 0.3, 'Beat spanned'),\n        ('measure_nodes_std', 0.05, 0.3, 'Measure nodes'),\n        ('measure_out_std', 0.05, 0.3, 'Measure LSTM'),\n        ('measure_spanned_std', 0.05, 0.3, 'Measure spanned'),\n        ('contracted_std', 0.05, 0.3, 'Contracted'),\n        ('aggregated_std', 0.1, 0.5, 'Aggregated'),\n        ('predictions_std', 0.1, 0.25, 'Predictions'),\n    ]\n    \n    issues = []\n    for key, low_thresh, high_thresh, name in key_activations:\n        if key in diag_results['activation_stats']:\n            stats = diag_results['activation_stats'][key]\n            mean_std = stats['mean']\n            min_std = stats['min']\n            max_std = stats['max']\n            \n            if mean_std < low_thresh:\n                status = \"[LOW - ISSUE]\"\n                issues.append(f\"{name}: std={mean_std:.4f} < {low_thresh}\")\n            elif mean_std > high_thresh:\n                status = \"[HIGH]\"\n            else:\n                status = \"[OK]\"\n            \n            print(f\"{name:<25} {mean_std:>12.4f} {min_std:>12.4f} {max_std:>12.4f} {status:<15}\")\n    \n    # Contribution analysis\n    print(\"\\n\" + \"=\" * 70)\n    print(\"HIERARCHY CONTRIBUTION ANALYSIS\")\n    print(\"=\" * 70)\n    \n    contribution_keys = [\n        ('hidden_out_contribution', 'Bi-LSTM (note+voice)'),\n        ('beat_spanned_contribution', 'Beat hierarchy'),\n        ('measure_spanned_contribution', 'Measure hierarchy'),\n    ]\n    \n    print(f\"\\nFraction of total variance from each component:\")\n    for key, name in contribution_keys:\n        if key in diag_results['activation_stats']:\n            contrib = diag_results['activation_stats'][key]['mean']\n            bar = '#' * int(contrib * 50)\n            print(f\"  {name:<25} {contrib:>6.1%} |{bar}\")\n            \n            if key == 'beat_spanned_contribution' and contrib < 0.10:\n                issues.append(f\"Beat hierarchy contributing only {contrib:.1%} (expected ~30%)\")\n            if key == 'measure_spanned_contribution' and contrib < 0.05:\n                issues.append(f\"Measure hierarchy contributing only {contrib:.1%} (expected ~10%)\")\n    \n    # Index statistics\n    print(\"\\n\" + \"=\" * 70)\n    print(\"INDEX STATISTICS (HIERARCHY HEALTH)\")\n    print(\"=\" * 70)\n    \n    idx_stats = diag_results['index_stats']\n    print(f\"\\n  Negative zero-shifted beats: {idx_stats.get('negative_beat_count', {}).get('total', 'N/A')}\")\n    print(f\"  Negative zero-shifted measures: {idx_stats.get('negative_measure_count', {}).get('total', 'N/A')}\")\n    \n    if idx_stats.get('negative_beat_count', {}).get('total', 0) > 0:\n        issues.append(\"Negative zero-shifted beat indices detected!\")\n    \n    # Summary\n    print(\"\\n\" + \"=\" * 70)\n    print(\"DIAGNOSTIC SUMMARY\")\n    print(\"=\" * 70)\n    \n    if issues:\n        print(f\"\\n[{len(issues)} ISSUES DETECTED]\")\n        for i, issue in enumerate(issues, 1):\n            print(f\"  {i}. {issue}\")\n        \n        print(\"\\nLikely root causes:\")\n        if any('beat_spanned' in issue or 'Beat hierarchy' in issue for issue in issues):\n            print(\"  - Beat attention may be collapsed (uniform weights)\")\n            print(\"  - span_beat_to_note_num may have index mapping issues\")\n        if any('measure_spanned' in issue or 'Measure hierarchy' in issue for issue in issues):\n            print(\"  - Measure hierarchy not aggregating properly\")\n        if any('note_out' in issue.lower() or 'lstm' in issue.lower() for issue in issues):\n            print(\"  - LSTM outputs have very low variance\")\n            print(\"  - Check PackedSequence handling\")\n    else:\n        print(\"\\n[ALL CHECKS PASSED]\")\n        print(\"Hierarchy components appear to be working correctly.\")\n        print(\"If R2 is still low, investigate:\")\n        print(\"  - Learning rate scheduling\")\n        print(\"  - Number of training epochs\")\n        print(\"  - Label alignment issues\")\n    \n    print(\"=\" * 70)\nelse:\n    print(\"\\n[SKIPPED] No model available for diagnostics.\")\n    print(\"Run Step 7 (training) first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vhlsirr5pb",
   "source": "## Step 8c: Manual Ablation Test\n\nDirectly compare full model R2 vs Bi-LSTM only R2 to measure hierarchy contribution.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vaomaz2syki",
   "source": "\"\"\"\nMANUAL ABLATION TEST\n\nCompare full model performance vs Bi-LSTM only (zeroing out hierarchy).\nThis directly measures how much the beat/measure components contribute.\n\nExpected results (from PercePiano paper):\n- Bi-LSTM alone: R2 = 0.185\n- + Score Alignment: R2 = 0.304 (+0.119)\n- + HAN: R2 = 0.397 (+0.093)\n\nIf our ablation shows hierarchy_gain < 0.05, the hierarchy is broken.\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom sklearn.metrics import r2_score\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, PackedSequence\n\nprint(\"=\" * 70)\nprint(\"MANUAL ABLATION TEST: Full Model vs Bi-LSTM Only\")\nprint(\"=\" * 70)\n\n# Check if model is available (from previous cell or needs to be loaded)\nif 'model' not in dir() or model is None:\n    # Try to use in-memory trained model (Round 12 fix)\n    if 'trained_model' in dir() and trained_model is not None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        model = trained_model.to(device)\n        model.eval()\n        print(f\"\\nUsing in-memory trained model\")\n        print(f\"Device: {device}\")\n    else:\n        print(\"Model not loaded. Run the previous diagnostic cell first,\")\n        print(\"or run Step 7 (training) to create a trained model.\")\n        model = None\n\nif model is not None:\n    model.eval()\n    device = next(model.parameters()).device\n    \n    # Create val_loader if not already created\n    if 'val_loader' not in dir():\n        from torch.utils.data import DataLoader\n        from src.percepiano.data.percepiano_vnet_dataset import (\n            PercePianoKFoldDataset,\n            percepiano_pack_collate,\n        )\n        val_ds = PercePianoKFoldDataset(\n            data_dir=DATA_ROOT,\n            fold_assignments=fold_assignments,\n            fold_id=1,\n            mode=\"val\",\n            max_notes=CONFIG['max_notes'],\n            slice_len=CONFIG.get('slice_len', CONFIG['max_notes']),\n        )\n        val_loader = DataLoader(\n            val_ds, \n            batch_size=4, \n            shuffle=False, \n            num_workers=0,\n            collate_fn=percepiano_pack_collate,\n        )\n    \n    # Collect predictions\n    full_preds = []\n    ablated_preds = []\n    targets = []\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(val_loader):\n            if batch_idx >= 10:  # Limit for speed\n                break\n                \n            # Move batch to device\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            \n            input_features = batch['input_features']\n            note_locations = {\n                'beat': batch['note_locations_beat'],\n                'measure': batch['note_locations_measure'],\n                'voice': batch['note_locations_voice'],\n            }\n            \n            # Full model prediction\n            outputs = model(input_features, note_locations)\n            full_preds.append(outputs['predictions'].cpu())\n            targets.append(batch['scores'].cpu())\n            \n            # Ablated prediction (Bi-LSTM only)\n            # We need to manually forward through the model zeroing out hierarchy\n            \n            # Handle PackedSequence\n            if isinstance(input_features, PackedSequence):\n                x_padded, lengths = pad_packed_sequence(input_features, batch_first=True)\n            else:\n                x_padded = input_features\n                lengths = batch.get('lengths', torch.tensor([x_padded.shape[1]] * x_padded.shape[0]))\n            \n            han = model.han_encoder\n            \n            # Project input\n            x_embedded = han.note_fc(x_padded)\n            \n            # Compute actual lengths\n            from src.percepiano.models.hierarchy_utils import compute_actual_lengths\n            actual_lengths = compute_actual_lengths(note_locations['beat'])\n            \n            # Note LSTM\n            x_packed = pack_padded_sequence(\n                x_embedded,\n                actual_lengths.cpu().clamp(min=1),\n                batch_first=True,\n                enforce_sorted=False,\n            )\n            note_out, _ = han.note_lstm(x_packed)\n            note_out, _ = pad_packed_sequence(note_out, batch_first=True, total_length=x_embedded.shape[1])\n            \n            # Voice processing\n            voice_out = han._run_voice_processing(x_embedded, note_locations['voice'], actual_lengths)\n            \n            # Combined hidden (Bi-LSTM output)\n            hidden_out = torch.cat([note_out, voice_out], dim=-1)\n            \n            # Zero out hierarchy (ablation)\n            batch_size, seq_len = hidden_out.shape[:2]\n            beat_spanned = torch.zeros(batch_size, seq_len, han.beat_size * 2, device=device)\n            measure_spanned = torch.zeros(batch_size, seq_len, han.measure_size * 2, device=device)\n            \n            # Concatenate with zeroed hierarchy\n            total_note_cat = torch.cat([hidden_out, beat_spanned, measure_spanned], dim=-1)\n            \n            # Performance contractor\n            contracted = model.performance_contractor(total_note_cat)\n            \n            # Final attention\n            attention_mask = note_locations['beat'] > 0\n            aggregated = model.final_attention(contracted, mask=attention_mask)\n            \n            # Prediction head\n            logits = model.prediction_head(aggregated)\n            ablated_pred = torch.sigmoid(logits)\n            ablated_preds.append(ablated_pred.cpu())\n    \n    # Compute R2 scores\n    full_preds = torch.cat(full_preds).numpy()\n    ablated_preds = torch.cat(ablated_preds).numpy()\n    targets = torch.cat(targets).numpy()\n    \n    full_r2 = r2_score(targets, full_preds)\n    ablated_r2 = r2_score(targets, ablated_preds)\n    hierarchy_gain = full_r2 - ablated_r2\n    \n    print(f\"\\nResults on {len(targets)} validation samples:\")\n    print(f\"\\n  {'Model':<25} {'R2':>10}\")\n    print(f\"  {'-'*25} {'-'*10}\")\n    print(f\"  {'Full Model':<25} {full_r2:>+10.4f}\")\n    print(f\"  {'Bi-LSTM Only (ablated)':<25} {ablated_r2:>+10.4f}\")\n    print(f\"  {'-'*25} {'-'*10}\")\n    print(f\"  {'Hierarchy Gain':<25} {hierarchy_gain:>+10.4f}\")\n    \n    print(f\"\\n\" + \"=\" * 70)\n    print(\"INTERPRETATION\")\n    print(\"=\" * 70)\n    \n    print(f\"\\nExpected hierarchy gains (from PercePiano paper):\")\n    print(f\"  Score Alignment: +0.119 R2\")\n    print(f\"  HAN:             +0.093 R2\")\n    print(f\"  Total:           +0.212 R2\")\n    \n    print(f\"\\nOur hierarchy gain: {hierarchy_gain:+.4f} R2\")\n    \n    if hierarchy_gain < 0.01:\n        print(f\"\\n[CRITICAL] Hierarchy is NOT contributing!\")\n        print(f\"  The beat/measure components add < 0.01 R2\")\n        print(f\"  This confirms the hierarchical processing is broken.\")\n        print(f\"\\n  Possible causes:\")\n        print(f\"  1. span_beat_to_note_num mapping is incorrect\")\n        print(f\"  2. Beat/measure attention weights collapsed to uniform\")\n        print(f\"  3. Zero-shifted indices have negative values\")\n        print(f\"  4. LSTM outputs have near-zero variance\")\n    elif hierarchy_gain < 0.05:\n        print(f\"\\n[WARNING] Hierarchy contributing only {hierarchy_gain:.3f} R2\")\n        print(f\"  Expected ~0.21, getting < 0.05\")\n        print(f\"  Hierarchy is partially working but underperforming.\")\n    elif hierarchy_gain < 0.15:\n        print(f\"\\n[PARTIAL] Hierarchy contributing {hierarchy_gain:.3f} R2\")\n        print(f\"  Better than nothing, but expected ~0.21\")\n        print(f\"  Some component may still have issues.\")\n    else:\n        print(f\"\\n[GOOD] Hierarchy contributing {hierarchy_gain:.3f} R2\")\n        print(f\"  This is in the expected range (~0.21).\")\n        print(f\"  If overall R2 is still low, investigate other factors.\")\n    \n    print(\"=\" * 70)\nelse:\n    print(\"\\n[SKIPPED] No model available for ablation test.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 9: Per-Fold Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nfrom src.percepiano.models.percepiano_replica import PERCEPIANO_DIMENSIONS\n\n# Compute aggregate metrics from trainer (in case cell was run separately)\naggregate_metrics = kfold_trainer._compute_aggregate_metrics()\n\nprint(\"=\"*80)\nprint(\"PER-FOLD VALIDATION RESULTS\")\nprint(\"=\"*80)\n\nprint(f\"\\n{'Fold':<6} {'Val R2':>10} {'Val Pearson':>12} {'Val MAE':>10} {'Val RMSE':>10} {'Epochs':>8} {'Time (s)':>10}\")\nprint(f\"{'-'*6} {'-'*10} {'-'*12} {'-'*10} {'-'*10} {'-'*8} {'-'*10}\")\n\nfor m in kfold_trainer.fold_metrics:\n    print(f\"{m.fold_id:<6} {m.val_r2:>+10.4f} {m.val_pearson:>+12.4f} {m.val_mae:>10.4f} {m.val_rmse:>10.4f} {m.epochs_trained:>8} {m.training_time_seconds:>10.1f}\")\n\nprint(f\"{'-'*6} {'-'*10} {'-'*12} {'-'*10} {'-'*10} {'-'*8} {'-'*10}\")\nprint(f\"{'Mean':<6} {aggregate_metrics.mean_r2:>+10.4f} {aggregate_metrics.mean_pearson:>+12.4f} {aggregate_metrics.mean_mae:>10.4f} {aggregate_metrics.mean_rmse:>10.4f}\")\nprint(f\"{'Std':<6} {aggregate_metrics.std_r2:>+10.4f} {aggregate_metrics.std_pearson:>+12.4f} {aggregate_metrics.std_mae:>10.4f} {aggregate_metrics.std_rmse:>10.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Step 10: Per-Dimension Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# Ensure aggregate_metrics is available (in case cell was run separately)\nif 'aggregate_metrics' not in dir():\n    aggregate_metrics = kfold_trainer._compute_aggregate_metrics()\n\nprint(\"=\"*80)\nprint(\"PER-DIMENSION R2 (Mean +/- Std across folds)\")\nprint(\"=\"*80)\n\n# Sort dimensions by mean R2\nsorted_dims = sorted(\n    aggregate_metrics.per_dim_mean_r2.items(),\n    key=lambda x: x[1],\n    reverse=True\n)\n\nprint(f\"\\n{'Dimension':<25} {'Mean R2':>10} {'Std R2':>10} {'Status':<12}\")\nprint(f\"{'-'*25} {'-'*10} {'-'*10} {'-'*12}\")\n\nfor dim, mean_r2 in sorted_dims:\n    std_r2 = aggregate_metrics.per_dim_std_r2[dim]\n    \n    if mean_r2 >= 0.3:\n        status = \"[GOOD]\"\n    elif mean_r2 >= 0.1:\n        status = \"[OK]\"\n    elif mean_r2 >= 0:\n        status = \"[WEAK]\"\n    else:\n        status = \"[FAILED]\"\n    \n    print(f\"{dim:<25} {mean_r2:>+10.4f} {std_r2:>10.4f} {status:<12}\")\n\n# Summary\npositive = sum(1 for d, r in sorted_dims if r > 0)\nstrong = sum(1 for d, r in sorted_dims if r >= 0.2)\nn_dims = len(sorted_dims)\n\nprint(f\"\\nSummary: {positive}/{n_dims} positive R2, {strong}/{n_dims} strong (>= 0.2)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Step 11: Final Summary and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport torch\nfrom pathlib import Path\n\n# Ensure aggregate_metrics is available (in case cell was run separately)\nif 'aggregate_metrics' not in dir():\n    aggregate_metrics = kfold_trainer._compute_aggregate_metrics()\n\nprint(\"=\"*80)\nprint(\"FINAL SUMMARY\")\nprint(\"=\"*80)\n\n# Cross-validation results\nprint(f\"\\n4-Fold Cross-Validation Results:\")\nprint(f\"  Mean R2:       {aggregate_metrics.mean_r2:.4f} +/- {aggregate_metrics.std_r2:.4f}\")\nprint(f\"  Mean Pearson:  {aggregate_metrics.mean_pearson:.4f} +/- {aggregate_metrics.std_pearson:.4f}\")\nprint(f\"  Mean Spearman: {aggregate_metrics.mean_spearman:.4f} +/- {aggregate_metrics.std_spearman:.4f}\")\nprint(f\"  Mean MAE:      {aggregate_metrics.mean_mae:.4f} +/- {aggregate_metrics.std_mae:.4f}\")\nprint(f\"  Mean RMSE:     {aggregate_metrics.mean_rmse:.4f} +/- {aggregate_metrics.std_rmse:.4f}\")\nprint(f\"  Training time: {aggregate_metrics.total_training_time/60:.1f} minutes\")\n\n# Test set results\nprint(f\"\\nTest Set (Ensemble of 4 models):\")\nprint(f\"  R2:       {test_results['ensemble']['r2']:.4f}\")\nprint(f\"  Pearson:  {test_results['ensemble']['pearson']:.4f}\")\nprint(f\"  Spearman: {test_results['ensemble']['spearman']:.4f}\")\nprint(f\"  MAE:      {test_results['ensemble']['mae']:.4f}\")\nprint(f\"  RMSE:     {test_results['ensemble']['rmse']:.4f}\")\n\n# Comparison to baselines\nprint(f\"\\nComparison to PercePiano baselines:\")\nprint(f\"  Bi-LSTM:      R2 = 0.185\")\nprint(f\"  MidiBERT:     R2 = 0.313\")\nprint(f\"  HAN SOTA:     R2 = 0.397\")\nprint(f\"  Ours (CV):    R2 = {aggregate_metrics.mean_r2:.3f} +/- {aggregate_metrics.std_r2:.3f}\")\nprint(f\"  Ours (Test):  R2 = {test_results['ensemble']['r2']:.3f}\")\n\n# Interpretation\ncv_r2 = aggregate_metrics.mean_r2\ntest_r2 = test_results['ensemble']['r2']\n\nprint(f\"\\nInterpretation:\")\nif cv_r2 >= 0.35:\n    print(f\"  [EXCELLENT] CV R2 >= 0.35 matches published SOTA!\")\nelif cv_r2 >= 0.25:\n    print(f\"  [GOOD] CV R2 >= 0.25 is usable for pseudo-labeling\")\nelif cv_r2 >= 0.10:\n    print(f\"  [FAIR] CV R2 >= 0.10 shows learning, needs improvement\")\nelse:\n    print(f\"  [NEEDS WORK] CV R2 < 0.10, significant improvement needed\")\n\n# Save ensemble model if good enough\nif cv_r2 >= 0.25:\n    print(f\"\\nModel qualifies for pseudo-labeling MAESTRO!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final sync to Google Drive\n",
    "print(\"=\"*60)\n",
    "print(\"SYNC TO GOOGLE DRIVE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(f\"\\nSyncing all checkpoints and results...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', str(CHECKPOINT_ROOT), GDRIVE_CHECKPOINT_PATH, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    \n",
    "    # Also sync fold assignments back to data directory\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', str(FOLD_FILE), GDRIVE_DATA_PATH, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSync complete!\")\n",
    "    print(f\"  Checkpoints: {GDRIVE_CHECKPOINT_PATH}\")\n",
    "    print(f\"  Fold assignments: {GDRIVE_DATA_PATH}\")\n",
    "else:\n",
    "    print(f\"\\nrclone not available - skipping sync\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}