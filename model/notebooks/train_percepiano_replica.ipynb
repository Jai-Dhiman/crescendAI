{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# PercePiano SOTA Replica Training - VirtuosoNet Features\n\n**Goal**: Replicate PercePiano's SOTA results (R-squared = 0.35-0.40) using the exact VirtuosoNet 78-dim features.\n\n## Attribution\n\nThis notebook implements the architecture from:\n\n> **PercePiano: A Benchmark for Perceptual Evaluation of Piano Performance**  \n> Park, Jongho and Kim, Dasaem et al.  \n> ISMIR 2024 / Nature Scientific Reports 2024  \n> GitHub: https://github.com/JonghoKimSNU/PercePiano\n\n## Key Changes from Previous Version\n\n| Aspect | Previous (Broken) | This Version (Fixed) |\n|--------|-------------------|---------------------|\n| Input Features | 20 custom + 256 global concat | **78-dim VirtuosoNet features** |\n| HAN Input | 276-dim (wrong) | **78-dim (correct)** |\n| Global Context | Concatenated to every note | **Not used (matches PercePiano)** |\n| Feature Source | Custom score alignment | **VirtuosoNet preprocessing** |\n\n## What This Notebook Does\n\n1. Downloads PercePiano data and scores from Google Drive\n2. Runs VirtuosoNet preprocessing to extract 78-dim features\n3. Trains the faithful PercePiano replica with correct features\n4. Evaluates against SOTA baselines\n5. Saves the trained model as a **Teacher Model** for pseudo-labeling MAESTRO\n\n## Expected Results\n\n- **Target R-squared**: 0.35-0.40 (piece-split)\n- **Training time**: ~1-2 hours on T4/A100\n- **Model size**: ~8-10M parameters"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rclone for Google Drive sync\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv and clone repository\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Install dependencies\n",
    "!uv pip install --system -e .\n",
    "!pip install tensorboard rich\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_ROOT = '/tmp/checkpoints/percepiano_replica'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/percepiano_replica'\n",
    "GDRIVE_DATA_PATH = 'gdrive:percepiano_data'\n",
    "DATA_ROOT = Path('/tmp/percepiano_data')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PERCEPIANO REPLICA - SOTA REPRODUCTION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Reference: Park et al., 'PercePiano', ISMIR/Nature 2024\")\n",
    "print(\"GitHub: https://github.com/JonghoKimSNU/PercePiano\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check rclone\n",
    "print(\"\\nChecking rclone configuration...\")\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "\n",
    "if 'gdrive:' in result.stdout:\n",
    "    print(\"  rclone 'gdrive' remote: CONFIGURED\")\n",
    "    RCLONE_AVAILABLE = True\n",
    "    \n",
    "    # Restore existing checkpoints\n",
    "    print(\"\\nRestoring checkpoints from Google Drive (if any)...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_CHECKPOINT_PATH, CHECKPOINT_ROOT, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "else:\n",
    "    print(\"  rclone 'gdrive' remote: NOT CONFIGURED\")\n",
    "    print(\"  Run 'rclone config' in terminal to set up Google Drive\")\n",
    "    RCLONE_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nCheckpoint directory: {CHECKPOINT_ROOT}\")\n",
    "print(f\"rclone available: {RCLONE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Download PercePiano Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "DATA_ROOT = Path('/tmp/percepiano_data')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download PercePiano data\n",
    "train_file = DATA_ROOT / 'percepiano_train.json'\n",
    "if train_file.exists():\n",
    "    print(f\"Data already exists at {DATA_ROOT}\")\n",
    "else:\n",
    "    print(\"Downloading PercePiano data from Google Drive...\")\n",
    "    result = subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "\n",
    "# Verify data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    path = DATA_ROOT / f'percepiano_{split}.json'\n",
    "    if path.exists():\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        has_scores = sum(1 for s in data if s.get('score_path'))\n",
    "        print(f\"{split}: {len(data)} samples ({has_scores} with score paths)\")\n",
    "    else:\n",
    "        print(f\"ERROR: {path} not found!\")\n",
    "\n",
    "# Check MIDI and score files\n",
    "midi_dir = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'all_2rounds'\n",
    "score_dir = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'score_xml'\n",
    "\n",
    "if midi_dir.exists():\n",
    "    midi_files = list(midi_dir.glob('*.mid'))\n",
    "    print(f\"\\nMIDI files: {len(midi_files)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"MIDI directory not found at {midi_dir}\")\n",
    "\n",
    "if score_dir.exists():\n",
    "    score_files = list(score_dir.glob('*.musicxml'))\n",
    "    print(f\"Score files: {len(score_files)}\")\n",
    "    if len(score_files) == 0:\n",
    "        raise FileNotFoundError(f\"No MusicXML files found in {score_dir}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Score directory not found at {score_dir}\\n\"\n",
    "        \"Run: rclone copy gdrive:percepiano_data/PercePiano/virtuoso/data/score_xml/ {score_dir}/\"\n",
    "    )\n",
    "\n",
    "# Store paths for later\n",
    "MIDI_DIR = midi_dir\n",
    "SCORE_DIR = score_dir\n",
    "\n",
    "print(\"\\n[OK] All required files present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Update Paths for Thunder Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# All 19 PercePiano dimensions\n",
    "PERCEPIANO_DIMENSIONS = [\n",
    "    \"timing\", \"articulation_length\", \"articulation_touch\",\n",
    "    \"pedal_amount\", \"pedal_clarity\", \"timbre_variety\",\n",
    "    \"timbre_depth\", \"timbre_brightness\", \"timbre_loudness\",\n",
    "    \"dynamic_range\", \"tempo\", \"space\", \"balance\", \"drama\",\n",
    "    \"mood_valence\", \"mood_energy\", \"mood_imagination\",\n",
    "    \"sophistication\", \"interpretation\",\n",
    "]\n",
    "\n",
    "def update_paths_for_thunder(data_root: Path):\n",
    "    \"\"\"Update paths in JSON files for Thunder Compute environment.\"\"\"\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        path = data_root / f'percepiano_{split}.json'\n",
    "        \n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for sample in data:\n",
    "            # Update MIDI path\n",
    "            filename = Path(sample['midi_path']).name\n",
    "            sample['midi_path'] = str(MIDI_DIR / filename)\n",
    "            \n",
    "            # Make sure scores dict uses all 19 dimensions\n",
    "            if 'percepiano_scores' in sample:\n",
    "                pp_scores = sample['percepiano_scores'][:19]\n",
    "                sample['scores'] = {\n",
    "                    dim: pp_scores[i]\n",
    "                    for i, dim in enumerate(PERCEPIANO_DIMENSIONS)\n",
    "                }\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"Updated {split}: {len(data)} samples\")\n",
    "\n",
    "update_paths_for_thunder(DATA_ROOT)\n",
    "\n",
    "# Verify\n",
    "with open(DATA_ROOT / 'percepiano_train.json') as f:\n",
    "    sample = json.load(f)[0]\n",
    "\n",
    "print(f\"\\nSample MIDI path: {sample['midi_path']}\")\n",
    "print(f\"Sample score path: {sample.get('score_path', 'N/A')}\")\n",
    "print(f\"Dimensions: {len(sample['scores'])}\")\n",
    "print(f\"MIDI exists: {Path(sample['midi_path']).exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Pre-Flight Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-flight validation - FAIL FAST if requirements not met\n",
    "from src.utils.preflight_validation import run_preflight_validation, PreflightValidationError\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRE-FLIGHT VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    run_preflight_validation(\n",
    "        data_dir=DATA_ROOT,\n",
    "        score_dir=SCORE_DIR,\n",
    "        pretrained_checkpoint=None,  # PercePiano replica doesn't use pre-trained encoder\n",
    "        require_pretrained=False,    # Training from scratch with Bi-LSTM\n",
    "        min_score_coverage=0.95,\n",
    "    )\n",
    "    print(\"\\n[OK] Pre-flight validation PASSED - ready to train\")\n",
    "except PreflightValidationError as e:\n",
    "    print(f\"\\n[VALIDATION FAILED]\\n{e}\")\n",
    "    raise RuntimeError(\"Fix the issues above before training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration\n",
    "\n",
    "Configuration matched exactly to PercePiano paper's `han_bigger256_concat.yml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "import torch\ntorch.set_float32_matmul_precision('medium')\n\n# PercePiano SOTA Configuration\n# From: https://github.com/JonghoKimSNU/PercePiano\n# File: virtuoso/ymls/shared/label19/han_bigger256_concat.yml\nCONFIG = {\n    # Data\n    'data_dir': str(DATA_ROOT),\n    'score_dir': str(SCORE_DIR),\n    'vnet_data_dir': str(DATA_ROOT / 'percepiano_vnet'),  # VirtuosoNet preprocessed features\n    \n    # VirtuosoNet Feature Dimension (CRITICAL FIX)\n    'input_size': 78,  # VirtuosoNet 78-dim features (NOT 20 + 256 = 276!)\n    \n    # HAN Architecture (matched to PercePiano)\n    'hidden_size': 256,        # PercePiano: 256 for all levels\n    'note_layers': 2,          # PercePiano: 2\n    'voice_layers': 2,         # PercePiano: 2\n    'beat_layers': 2,          # PercePiano: 2\n    'measure_layers': 1,       # PercePiano: 1\n    'num_attention_heads': 8,  # PercePiano: 8\n    'final_hidden': 128,       # PercePiano: 128\n    \n    # Training (matched to PercePiano)\n    'learning_rate': 2.5e-5,   # PercePiano: 2.5e-5\n    'weight_decay': 0.01,\n    'dropout': 0.2,            # PercePiano: 0.2\n    'batch_size': 8,           # PercePiano: 8\n    'max_epochs': 100,\n    'early_stopping_patience': 20,\n    'gradient_clip_val': 1.0,\n    'precision': '16-mixed',\n    \n    # Dataset\n    'max_notes': 1024,\n    \n    # Checkpoints\n    'checkpoint_dir': CHECKPOINT_ROOT,\n    'gdrive_checkpoint': GDRIVE_CHECKPOINT_PATH,\n}\n\nprint(\"=\"*70)\nprint(\"PERCEPIANO REPLICA CONFIGURATION (FIXED)\")\nprint(\"=\"*70)\nprint(\"Reference: han_bigger256_concat.yml from PercePiano repo\")\nprint(\"=\"*70)\nprint(\"\\nCRITICAL FIX: Using VirtuosoNet 78-dim features instead of 20+256 concat\")\nprint(\"=\"*70)\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")\nprint(\"=\"*70)\nprint(\"\\nKey fixes from broken version:\")\nprint(\"  - Input features: 78-dim VirtuosoNet (was 20 + 256 = 276)\")\nprint(\"  - No global context concatenation (matches PercePiano exactly)\")\nprint(\"  - Using VirtuosoNet preprocessing for features\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "## Step 6: VirtuosoNet Preprocessing (CRITICAL)\n\nThis step extracts the exact 78-dim features used in the original PercePiano paper. If preprocessing has already been done, this cell will skip."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport subprocess\nimport shutil\nimport os\n\n# First, ensure VirtuosoNet modules are available by cloning PercePiano repo\nvirtuoso_module_path = Path('/tmp/crescendai/model/data/raw/PercePiano/virtuoso/virtuoso/pyScoreParser')\npercepiano_path = Path('/tmp/crescendai/model/data/raw/PercePiano')\n\nif not (virtuoso_module_path / 'feature_extraction.py').exists():\n    print(\"VirtuosoNet modules not found. Cloning PercePiano repository...\")\n    \n    # Remove existing directory if it exists (might be empty or corrupted)\n    if percepiano_path.exists():\n        shutil.rmtree(percepiano_path)\n    \n    percepiano_path.parent.mkdir(parents=True, exist_ok=True)\n    \n    # Clone the repo\n    result = subprocess.run(\n        ['git', 'clone', '--depth', '1',\n         'https://github.com/JonghoKimSNU/PercePiano.git',\n         str(percepiano_path)],\n        capture_output=True,\n        text=True\n    )\n    \n    if result.returncode != 0:\n        print(f\"Failed to clone PercePiano: {result.stderr}\")\n        raise RuntimeError(\"Could not clone PercePiano repository\")\n    \n    print(\"PercePiano cloned successfully!\")\n    print(f\"VirtuosoNet modules at: {virtuoso_module_path}\")\nelse:\n    print(f\"VirtuosoNet modules already available at {virtuoso_module_path}\")\n\n# Check if VirtuosoNet features exist\nvnet_dir = Path(CONFIG['vnet_data_dir'])\nvnet_train_dir = vnet_dir / 'train'\n\nif vnet_train_dir.exists() and list(vnet_train_dir.glob('*.pkl')):\n    print(f\"\\nVirtuosoNet features already exist at {vnet_dir}\")\n    print(f\"  train: {len(list((vnet_dir / 'train').glob('*.pkl')))} samples\")\n    print(f\"  val: {len(list((vnet_dir / 'val').glob('*.pkl')))} samples\")\n    print(f\"  test: {len(list((vnet_dir / 'test').glob('*.pkl')))} samples\")\nelse:\n    print(\"\\nVirtuosoNet features not found. Running preprocessing...\")\n    print(\"This extracts the exact 78-dim features used in the original PercePiano paper.\")\n    print(\"\")\n    \n    # Run preprocessing script\n    # Pass DATA_ROOT directly - the script will find JSON files and score_xml there\n    result = subprocess.run(\n        ['python', 'scripts/preprocess_percepiano_vnet.py',\n         '--data_root', str(DATA_ROOT),\n         '--output_dir', str(vnet_dir)],\n        cwd='/tmp/crescendai/model',\n        capture_output=True,\n        text=True\n    )\n    \n    print(result.stdout)\n    if result.returncode != 0:\n        print(f\"Preprocessing failed: {result.stderr}\")\n        print(\"\\nNOTE: VirtuosoNet preprocessing requires MusicXML parsing.\")\n        print(\"If this fails, you may need to manually align MIDI files to scores.\")\n        print(\"\")\n        print(\"Alternative: Download pre-processed features from Google Drive:\")\n        print(f\"  rclone copy gdrive:percepiano_data/percepiano_vnet {vnet_dir}\")\n    else:\n        print(\"Preprocessing complete!\")\n\n# Verify features exist\nif vnet_train_dir.exists():\n    num_train = len(list((vnet_dir / 'train').glob('*.pkl')))\n    print(f\"\\nVirtuosoNet features ready: {num_train} training samples\")\nelse:\n    raise RuntimeError(\n        f\"VirtuosoNet features not available at {vnet_dir}.\\n\"\n        \"Run preprocessing or download from Google Drive.\"\n    )"
  },
  {
   "cell_type": "code",
   "id": "6mxyh63i7qm",
   "source": "# Patch PercePiano data_class.py to fix silent failures\n# The original code has bare `except:` clauses that swallow errors silently\n# This patch replaces them with explicit exception handling\n\nimport re\nfrom pathlib import Path\n\ndata_class_path = Path('/tmp/crescendai/model/data/raw/PercePiano/virtuoso/virtuoso/pyScoreParser/data_class.py')\n\nif data_class_path.exists():\n    print(\"Patching PercePiano data_class.py to fix silent failures...\")\n    \n    content = data_class_path.read_text()\n    original_content = content\n    \n    # Patch 1: Fix bare except in load_all_piece (line ~95-98)\n    # Change from printing error to re-raising with context\n    old_pattern1 = r\"except Exception as ex:\\s+# TODO: TGK: this is ambiguous.*?\\s+print\\(f'Error while processing \\{scores\\[n\\]\\}\\. Error type :\\{ex\\}'\\)\"\n    new_code1 = \"\"\"except Exception as ex:\n                # Re-raise with full context instead of silently continuing\n                raise RuntimeError(f'Error loading piece {scores[n]}: {type(ex).__name__}: {ex}') from ex\"\"\"\n    content = re.sub(old_pattern1, new_code1, content, flags=re.DOTALL)\n    \n    # Patch 2: Fix bare except in performance alignment (line ~332-335)\n    old_pattern2 = r\"except:\\s+perform_data = None\\s+print\\(f'Cannot align \\{perform\\}'\\)\\s+self\\.performances\\.append\\(None\\)\"\n    new_code2 = \"\"\"except (ValueError, IndexError, FileNotFoundError, OSError) as e:\n                        # Explicit exception handling - re-raise with context\n                        raise RuntimeError(f'Alignment failed for {perform}: {type(e).__name__}: {e}') from e\"\"\"\n    content = re.sub(old_pattern2, new_code2, content)\n    \n    # Patch 3: Fix bare except in Nakamura alignment (line ~425)\n    old_pattern3 = r\"except:\\s+print\\('Error to process \\{\\}'\\.format\\(midi_file_path\\)\\)\"\n    new_code3 = \"\"\"except subprocess.CalledProcessError as e:\n            print(f'Alignment tool failed for {midi_file_path}: {e}')\"\"\"\n    content = re.sub(old_pattern3, new_code3, content)\n    \n    # Patch 4: Fix second bare except in Nakamura alignment retry (line ~435)\n    old_pattern4 = r\"except:\\s+align_success = False\\s+print\\('Fail to process \\{\\}'\\.format\\(midi_file_path\\)\\)\\s+os\\.chdir\\(current_dir\\)\"\n    new_code4 = \"\"\"except subprocess.CalledProcessError as e2:\n                align_success = False\n                raise RuntimeError(f'Alignment tool failed after retry for {midi_file_path}: {e2}') from e2\"\"\"\n    content = re.sub(old_pattern4, new_code4, content)\n    \n    if content != original_content:\n        data_class_path.write_text(content)\n        print(\"[OK] Patched data_class.py - silent failures now raise explicit exceptions\")\n    else:\n        print(\"[INFO] data_class.py already patched or patterns not found\")\nelse:\n    print(\"[SKIP] data_class.py not found yet - will be available after cloning\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "## Step 7: Create DataLoaders and Model\n\nUsing the VirtuosoNet preprocessed features (78-dim) with the faithful PercePiano replica architecture."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "from src.data.percepiano_vnet_dataset import create_vnet_dataloaders\nfrom src.models.percepiano_replica import PercePianoVNetModule\n\n# Create DataLoaders with VirtuosoNet features\ntrain_loader, val_loader, test_loader = create_vnet_dataloaders(\n    data_dir=CONFIG['vnet_data_dir'],\n    batch_size=CONFIG['batch_size'],\n    max_notes=CONFIG['max_notes'],\n    num_workers=4,\n)\n\nprint(f\"Train: {len(train_loader.dataset)} samples\")\nprint(f\"Val: {len(val_loader.dataset)} samples\")\nprint(f\"Test: {len(test_loader.dataset)} samples\")\n\n# Create model with VirtuosoNet features (78-dim input)\nmodel = PercePianoVNetModule(\n    # VirtuosoNet input dimension\n    input_size=CONFIG['input_size'],  # 78-dim features\n    # HAN dimensions (matched to PercePiano)\n    hidden_size=CONFIG['hidden_size'],\n    note_layers=CONFIG['note_layers'],\n    voice_layers=CONFIG['voice_layers'],\n    beat_layers=CONFIG['beat_layers'],\n    measure_layers=CONFIG['measure_layers'],\n    num_attention_heads=CONFIG['num_attention_heads'],\n    final_hidden=CONFIG['final_hidden'],\n    # Training\n    learning_rate=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay'],\n    dropout=CONFIG['dropout'],\n)\n\n# Count parameters\ntotal_params = model.count_parameters()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PERCEPIANO REPLICA MODEL (VirtuosoNet Features)\")\nprint(\"=\"*70)\nprint(f\"Architecture: Bi-LSTM + HAN (Note -> Voice -> Beat -> Measure)\")\nprint(f\"Input features: {CONFIG['input_size']} (VirtuosoNet)\")\nprint(f\"Hidden size: {CONFIG['hidden_size']}\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"\")\nprint(f\"Key fix: Input is 78-dim VirtuosoNet features directly\")\nprint(f\"         (NOT 20 + 256 global context = 276 like before)\")\nprint(f\"\")\nprint(f\"Target R-squared: 0.35-0.40 (piece-split)\")\nprint(f\"Dimensions: {len(model.dimensions)}\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 8: Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Checkpoint callback - monitor mean R-squared\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CONFIG['checkpoint_dir'],\n",
    "    filename='percepiano_replica-{epoch:02d}-{val_mean_r2:.4f}',\n",
    "    monitor='val/mean_r2',\n",
    "    mode='max',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val/mean_r2',\n",
    "    patience=CONFIG['early_stopping_patience'],\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# LR monitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir='/tmp/logs',\n",
    "    name='percepiano_replica',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['max_epochs'],\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    precision=CONFIG['precision'],\n",
    "    gradient_clip_val=CONFIG['gradient_clip_val'],\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.5,  # Validate twice per epoch\n",
    ")\n",
    "\n",
    "print(\"Trainer configured!\")\n",
    "print(f\"  Precision: {CONFIG['precision']}\")\n",
    "print(f\"  Max epochs: {CONFIG['max_epochs']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Early stopping patience: {CONFIG['early_stopping_patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 9: Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Train\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING TRAINING - PercePiano SOTA Replica\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey metrics to watch:\")\n",
    "print(\"  - val/mean_r2: Overall R-squared (target: 0.35-0.40)\")\n",
    "print(\"  - val/timing_r2: Timing dimension (should be highest)\")\n",
    "print(\"  - val/tempo_r2: Tempo dimension\")\n",
    "print(\"\")\n",
    "print(\"PercePiano SOTA baselines:\")\n",
    "print(\"  - Bi-LSTM: R^2 = 0.185\")\n",
    "print(\"  - MidiBERT: R^2 = 0.313\")\n",
    "print(\"  - Bi-LSTM + SA + HAN: R^2 = 0.397 (SOTA)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints to Google Drive\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"Syncing checkpoints to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Step 10: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with best checkpoint\n",
    "print(\"Running test with best checkpoint...\")\n",
    "best_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best checkpoint: {best_path}\")\n",
    "\n",
    "if best_path:\n",
    "    test_results = trainer.test(model, test_loader, ckpt_path=best_path)\n",
    "    print(\"\\nTest Results:\")\n",
    "    for k, v in test_results[0].items():\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\n\n# Load best model\nfrom src.models.percepiano_replica import PercePianoVNetModule\nbest_model = PercePianoVNetModule.load_from_checkpoint(checkpoint_callback.best_model_path)\nbest_model.eval()\nbest_model.cuda()\n\n# Collect predictions on test set\nall_preds = []\nall_targets = []\n\nprint(\"Collecting predictions on test set...\")\nwith torch.no_grad():\n    for batch in test_loader:\n        # Move batch to GPU\n        input_features = batch['input_features'].cuda()\n        attention_mask = batch['attention_mask'].cuda()\n        scores = batch['scores'].cuda()\n        \n        note_locations = {\n            'beat': batch['note_locations_beat'].cuda(),\n            'measure': batch['note_locations_measure'].cuda(),\n            'voice': batch['note_locations_voice'].cuda(),\n        }\n        \n        # Forward pass with VirtuosoNet features\n        outputs = best_model(\n            input_features=input_features,\n            note_locations=note_locations,\n            attention_mask=attention_mask,\n        )\n        \n        all_preds.append(outputs['predictions'].cpu())\n        all_targets.append(scores.cpu())\n\nall_preds = torch.cat(all_preds).numpy()\nall_targets = torch.cat(all_targets).numpy()\ndimensions = best_model.dimensions\n\nprint(f\"Collected {len(all_preds)} test samples across {len(dimensions)} dimensions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import (\n",
    "    compute_all_metrics,\n",
    "    PerDimensionAnalysis,\n",
    "    compare_to_sota,\n",
    "    format_comparison_table,\n",
    "    create_results_table,\n",
    "    PERCEPIANO_BASELINES,\n",
    ")\n",
    "\n",
    "# Compute all metrics\n",
    "metrics = compute_all_metrics(\n",
    "    predictions=all_preds,\n",
    "    targets=all_targets,\n",
    "    dimension_names=list(dimensions),\n",
    ")\n",
    "\n",
    "# Print results table\n",
    "print(create_results_table(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to SOTA baselines\n",
    "our_r2 = metrics['r2'].value\n",
    "per_dim_r2 = metrics['r2'].per_dimension\n",
    "\n",
    "comparison = compare_to_sota(\n",
    "    model_r2=our_r2,\n",
    "    model_name=\"PercePiano Replica (CrescendAI)\",\n",
    "    split_type=\"piece\",\n",
    "    per_dimension_r2=per_dim_r2,\n",
    ")\n",
    "\n",
    "print(format_comparison_table(comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*70)\n",
    "print(\"PERCEPIANO REPLICA - RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. OVERALL PERFORMANCE\")\n",
    "print(f\"   Mean R^2: {our_r2:.4f}\")\n",
    "print(f\"   Target (0.35-0.40): {'ACHIEVED' if our_r2 >= 0.35 else 'CLOSE' if our_r2 >= 0.30 else 'NOT YET'}\")\n",
    "\n",
    "print(f\"\\n2. COMPARISON TO PUBLISHED BASELINES\")\n",
    "print(f\"   Bi-LSTM baseline: 0.185\")\n",
    "print(f\"   MidiBERT: 0.313\")\n",
    "print(f\"   Bi-LSTM + SA + HAN (SOTA): 0.397\")\n",
    "print(f\"   Our replica: {our_r2:.4f}\")\n",
    "\n",
    "print(f\"\\n3. MODEL SIZE\")\n",
    "print(f\"   Parameters: {best_model.count_parameters():,}\")\n",
    "print(f\"   vs Previous (51.5M): {51_500_000 / best_model.count_parameters():.1f}x smaller\")\n",
    "\n",
    "print(f\"\\n4. TOP 5 DIMENSIONS\")\n",
    "sorted_dims = sorted(per_dim_r2.items(), key=lambda x: x[1], reverse=True)\n",
    "for dim, r2 in sorted_dims[:5]:\n",
    "    print(f\"   {dim}: {r2:.4f}\")\n",
    "\n",
    "print(f\"\\n5. BOTTOM 5 DIMENSIONS (need improvement)\")\n",
    "for dim, r2 in sorted_dims[-5:]:\n",
    "    print(f\"   {dim}: {r2:.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Step 11: Save as Teacher Model\n",
    "\n",
    "If the model achieves R-squared >= 0.30, save it as a **Teacher Model** for pseudo-labeling MAESTRO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom pathlib import Path\n\n# Save as teacher model\nteacher_path = Path(CONFIG['checkpoint_dir']) / 'percepiano_teacher.pt'\n\nif our_r2 >= 0.25:  # Minimum threshold for useful teacher\n    torch.save({\n        'state_dict': best_model.state_dict(),\n        'config': {\n            'input_size': CONFIG['input_size'],\n            'hidden_size': CONFIG['hidden_size'],\n            'note_layers': CONFIG['note_layers'],\n            'voice_layers': CONFIG['voice_layers'],\n            'beat_layers': CONFIG['beat_layers'],\n            'measure_layers': CONFIG['measure_layers'],\n            'num_attention_heads': CONFIG['num_attention_heads'],\n            'final_hidden': CONFIG['final_hidden'],\n            'dropout': CONFIG['dropout'],\n        },\n        'dimensions': list(dimensions),\n        'metrics': {\n            'r2': our_r2,\n            'per_dimension_r2': per_dim_r2,\n        },\n        'sota_comparison': {\n            'rank': comparison['rank'],\n            'total_baselines': comparison['total_baselines'],\n            'vs_best_baseline': comparison['improvement_vs_best'],\n        },\n        'architecture': 'PercePiano Replica (Bi-LSTM + HAN) with VirtuosoNet 78-dim features',\n        'reference': 'https://github.com/JonghoKimSNU/PercePiano',\n    }, teacher_path)\n    \n    print(f\"Saved teacher model to {teacher_path}\")\n    print(f\"Teacher R^2: {our_r2:.4f}\")\n    print(f\"\\nThis model can be used for pseudo-labeling MAESTRO!\")\n    print(f\"Run: python scripts/pseudo_label_maestro.py --teacher {teacher_path}\")\nelse:\n    print(f\"R^2 = {our_r2:.4f} is below threshold (0.25) for teacher model.\")\n    print(\"Consider:\")\n    print(\"  1. Training for more epochs\")\n    print(\"  2. Adjusting hyperparameters\")\n    print(\"  3. Checking data quality\")\n\n# Final sync to Google Drive\nif RCLONE_AVAILABLE:\n    print(\"\\nFinal sync to Google Drive...\")\n    subprocess.run(\n        ['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'],\n        capture_output=False\n    )\n    print(\"Sync complete!\")\n    print(f\"Checkpoints available at: {CONFIG['gdrive_checkpoint']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If R-squared >= 0.30:\n",
    "\n",
    "1. **Pseudo-label MAESTRO**: Use this teacher model to generate labels for MAESTRO dataset\n",
    "2. **Train larger model**: With expanded dataset (~6000 samples), train a larger model\n",
    "3. **Noisy Student**: Apply noisy student training for potential improvement over teacher\n",
    "\n",
    "If R-squared < 0.30:\n",
    "\n",
    "1. Check if validation set is too small (only 27 samples)\n",
    "2. Consider k-fold cross-validation for more robust estimates\n",
    "3. Verify data preprocessing matches PercePiano exactly\n",
    "\n",
    "---\n",
    "\n",
    "**Attribution**: This model replicates the architecture from PercePiano (Park et al., ISMIR/Nature 2024).  \n",
    "GitHub: https://github.com/JonghoKimSNU/PercePiano"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}