{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PercePiano SOTA Replica Training - VirtuosoNet Features\n",
    "\n",
    "**Goal**: Replicate PercePiano's SOTA results (R-squared = 0.35-0.40) using the exact VirtuosoNet features.\n",
    "\n",
    "## Attribution\n",
    "\n",
    "This notebook implements the architecture from:\n",
    "\n",
    "> **PercePiano: A Benchmark for Perceptual Evaluation of Piano Performance**  \n",
    "> Park, Jongho and Kim, Dasaem et al.  \n",
    "> ISMIR 2024 / Nature Scientific Reports 2024  \n",
    "> GitHub: https://github.com/JonghoKimSNU/PercePiano\n",
    "\n",
    "## Critical Fixes Applied (Dec 2024)\n",
    "\n",
    "| Parameter | Previous (Broken) | Fixed Value | Source |\n",
    "|-----------|-------------------|-------------|--------|\n",
    "| input_size | 79 | **84** | 79 base + 5 unnorm features |\n",
    "| learning_rate | 2.5e-5 | **1e-4** | parser.py:119 |\n",
    "| weight_decay | 0.01 | **1e-5** | parser.py:135 |\n",
    "| batch_size | 8 | **32** | parser.py:107 |\n",
    "| gradient_clip_val | 1.0 | **2.0** | parser.py:159 |\n",
    "| Voice LSTM input | 512-dim (note output) | **256-dim (embeddings)** | encoder_score.py:496-516 |\n",
    "| Key augmentation | estimated pitch range | **midi_pitch_unnorm** | data_for_training.py:412-419 |\n",
    "\n",
    "**Feature Layout (84-dim)**:\n",
    "- Indices 0-78: Base VirtuosoNet features (z-score normalized where applicable)\n",
    "- Index 79: midi_pitch_unnorm (raw MIDI pitch 21-108, for key augmentation)\n",
    "- Index 80-83: duration_unnorm, beat_importance_unnorm, measure_length_unnorm, following_rest_unnorm\n",
    "\n",
    "**Most Critical Fixes**: \n",
    "1. Voice LSTM now receives 256-dim projected embeddings in parallel with Note LSTM, NOT the 512-dim Note LSTM output sequentially.\n",
    "2. Key augmentation now uses midi_pitch_unnorm (raw MIDI pitch) to calculate valid shift range, matching original PercePiano.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. Downloads PercePiano data and scores from Google Drive\n",
    "2. Runs VirtuosoNet preprocessing to extract 84-dim features (79 base + 5 unnorm)\n",
    "3. Trains the faithful PercePiano replica with correct features and hyperparameters\n",
    "4. Evaluates against SOTA baselines\n",
    "5. Saves the trained model as a **Teacher Model** for pseudo-labeling MAESTRO\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- **Target R-squared**: 0.35-0.40 (piece-split)\n",
    "- **Training time**: ~1-2 hours on T4/A100\n",
    "- **Model size**: ~8-10M parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rclone for Google Drive sync\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv and clone repository\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Install dependencies\n",
    "!uv pip install --system -e .\n",
    "!pip install tensorboard rich\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_ROOT = '/tmp/checkpoints/percepiano_replica'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/percepiano_replica'\n",
    "GDRIVE_DATA_PATH = 'gdrive:percepiano_data'\n",
    "DATA_ROOT = Path('/tmp/percepiano_data')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PERCEPIANO REPLICA - SOTA REPRODUCTION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Reference: Park et al., 'PercePiano', ISMIR/Nature 2024\")\n",
    "print(\"GitHub: https://github.com/JonghoKimSNU/PercePiano\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check rclone\n",
    "print(\"\\nChecking rclone configuration...\")\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "\n",
    "if 'gdrive:' in result.stdout:\n",
    "    print(\"  rclone 'gdrive' remote: CONFIGURED\")\n",
    "    RCLONE_AVAILABLE = True\n",
    "    \n",
    "    # Restore existing checkpoints\n",
    "    print(\"\\nRestoring checkpoints from Google Drive (if any)...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_CHECKPOINT_PATH, CHECKPOINT_ROOT, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "else:\n",
    "    print(\"  rclone 'gdrive' remote: NOT CONFIGURED\")\n",
    "    print(\"  Run 'rclone config' in terminal to set up Google Drive\")\n",
    "    RCLONE_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nCheckpoint directory: {CHECKPOINT_ROOT}\")\n",
    "print(f\"rclone available: {RCLONE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Download PercePiano Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "DATA_ROOT = Path('/tmp/percepiano_data')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download PercePiano data\n",
    "train_file = DATA_ROOT / 'percepiano_train.json'\n",
    "if train_file.exists():\n",
    "    print(f\"Data already exists at {DATA_ROOT}\")\n",
    "else:\n",
    "    print(\"Downloading PercePiano data from Google Drive...\")\n",
    "    result = subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "\n",
    "# Verify data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    path = DATA_ROOT / f'percepiano_{split}.json'\n",
    "    if path.exists():\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        has_scores = sum(1 for s in data if s.get('score_path'))\n",
    "        print(f\"{split}: {len(data)} samples ({has_scores} with score paths)\")\n",
    "    else:\n",
    "        print(f\"ERROR: {path} not found!\")\n",
    "\n",
    "# Check MIDI and score files\n",
    "midi_dir = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'all_2rounds'\n",
    "score_dir = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'score_xml'\n",
    "\n",
    "if midi_dir.exists():\n",
    "    midi_files = list(midi_dir.glob('*.mid'))\n",
    "    print(f\"\\nMIDI files: {len(midi_files)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"MIDI directory not found at {midi_dir}\")\n",
    "\n",
    "if score_dir.exists():\n",
    "    score_files = list(score_dir.glob('*.musicxml'))\n",
    "    print(f\"Score files: {len(score_files)}\")\n",
    "    if len(score_files) == 0:\n",
    "        raise FileNotFoundError(f\"No MusicXML files found in {score_dir}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Score directory not found at {score_dir}\\n\"\n",
    "        \"Run: rclone copy gdrive:percepiano_data/PercePiano/virtuoso/data/score_xml/ {score_dir}/\"\n",
    "    )\n",
    "\n",
    "# Store paths for later\n",
    "MIDI_DIR = midi_dir\n",
    "SCORE_DIR = score_dir\n",
    "\n",
    "print(\"\\n[OK] All required files present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Update Paths for Thunder Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# All 19 PercePiano dimensions\n",
    "PERCEPIANO_DIMENSIONS = [\n",
    "    \"timing\", \"articulation_length\", \"articulation_touch\",\n",
    "    \"pedal_amount\", \"pedal_clarity\", \"timbre_variety\",\n",
    "    \"timbre_depth\", \"timbre_brightness\", \"timbre_loudness\",\n",
    "    \"dynamic_range\", \"tempo\", \"space\", \"balance\", \"drama\",\n",
    "    \"mood_valence\", \"mood_energy\", \"mood_imagination\",\n",
    "    \"sophistication\", \"interpretation\",\n",
    "]\n",
    "\n",
    "def update_paths_for_thunder(data_root: Path):\n",
    "    \"\"\"Update paths in JSON files for Thunder Compute environment.\"\"\"\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        path = data_root / f'percepiano_{split}.json'\n",
    "        \n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for sample in data:\n",
    "            # Update MIDI path\n",
    "            filename = Path(sample['midi_path']).name\n",
    "            sample['midi_path'] = str(MIDI_DIR / filename)\n",
    "            \n",
    "            # Make sure scores dict uses all 19 dimensions\n",
    "            if 'percepiano_scores' in sample:\n",
    "                pp_scores = sample['percepiano_scores'][:19]\n",
    "                sample['scores'] = {\n",
    "                    dim: pp_scores[i]\n",
    "                    for i, dim in enumerate(PERCEPIANO_DIMENSIONS)\n",
    "                }\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"Updated {split}: {len(data)} samples\")\n",
    "\n",
    "update_paths_for_thunder(DATA_ROOT)\n",
    "\n",
    "# Verify\n",
    "with open(DATA_ROOT / 'percepiano_train.json') as f:\n",
    "    sample = json.load(f)[0]\n",
    "\n",
    "print(f\"\\nSample MIDI path: {sample['midi_path']}\")\n",
    "print(f\"Sample score path: {sample.get('score_path', 'N/A')}\")\n",
    "print(f\"Dimensions: {len(sample['scores'])}\")\n",
    "print(f\"MIDI exists: {Path(sample['midi_path']).exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Pre-Flight Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-flight validation - FAIL FAST if requirements not met\n",
    "from src.utils.preflight_validation import run_preflight_validation, PreflightValidationError\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRE-FLIGHT VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    run_preflight_validation(\n",
    "        data_dir=DATA_ROOT,\n",
    "        score_dir=SCORE_DIR,\n",
    "        pretrained_checkpoint=None,  # PercePiano replica doesn't use pre-trained encoder\n",
    "        require_pretrained=False,    # Training from scratch with Bi-LSTM\n",
    "        min_score_coverage=0.95,\n",
    "    )\n",
    "    print(\"\\n[OK] Pre-flight validation PASSED - ready to train\")\n",
    "except PreflightValidationError as e:\n",
    "    print(f\"\\n[VALIDATION FAILED]\\n{e}\")\n",
    "    raise RuntimeError(\"Fix the issues above before training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration\n",
    "\n",
    "Configuration matched exactly to PercePiano paper's `han_bigger256_concat.yml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# PercePiano SOTA Configuration\n",
    "# From: https://github.com/JonghoKimSNU/PercePiano\n",
    "# File: virtuoso/ymls/shared/label19/han_bigger256_concat.yml\n",
    "# Training params: virtuoso/virtuoso/parser.py\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'data_dir': str(DATA_ROOT),\n",
    "    'score_dir': str(SCORE_DIR),\n",
    "    'vnet_data_dir': str(DATA_ROOT / 'percepiano_vnet'),  # VirtuosoNet preprocessed features\n",
    "    \n",
    "    # VirtuosoNet Feature Dimension\n",
    "    # 79 base features (14 scalar + 13 pitch + 5 tempo + 4 dynamic + 9 time_sig + 6 slur_beam + 17 composer + 9 notation + 2 tempo_primo)\n",
    "    # + 5 unnorm features (midi_pitch_unnorm, duration_unnorm, beat_importance_unnorm, measure_length_unnorm, following_rest_unnorm)\n",
    "    # = 84 total features\n",
    "    # The unnorm features preserve raw values BEFORE normalization, critical for key augmentation\n",
    "    'input_size': 84,  # VirtuosoNet features (79 base + 5 unnorm)\n",
    "    \n",
    "    # HAN Architecture (matched to PercePiano han_bigger256_concat.yml)\n",
    "    'hidden_size': 256,        # PercePiano: 256 for all levels\n",
    "    'note_layers': 2,          # PercePiano: 2\n",
    "    'voice_layers': 2,         # PercePiano: 2\n",
    "    'beat_layers': 2,          # PercePiano: 2\n",
    "    'measure_layers': 1,       # PercePiano: 1\n",
    "    'num_attention_heads': 8,  # PercePiano: 8\n",
    "    'final_hidden': 128,       # PercePiano: 128\n",
    "    \n",
    "    # Training (matched to original PercePiano parser.py)\n",
    "    'learning_rate': 1e-4,     # Original: parser.py:119 (was 2.5e-5 - WRONG!)\n",
    "    'weight_decay': 1e-5,      # Original: parser.py:135 (was 0.01 - WRONG!)\n",
    "    'dropout': 0.2,            # PercePiano: 0.2\n",
    "    'batch_size': 32,          # Original: parser.py:107 (was 8 - WRONG!)\n",
    "    'max_epochs': 100,\n",
    "    'early_stopping_patience': 20,\n",
    "    'gradient_clip_val': 2.0,  # Original: parser.py:159 (was 1.0 - WRONG!)\n",
    "    'precision': '16-mixed',\n",
    "    \n",
    "    # Dataset\n",
    "    'max_notes': 1024,\n",
    "    \n",
    "    # Checkpoints\n",
    "    'checkpoint_dir': CHECKPOINT_ROOT,\n",
    "    'gdrive_checkpoint': GDRIVE_CHECKPOINT_PATH,\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PERCEPIANO REPLICA CONFIGURATION (FIXED)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Reference: han_bigger256_concat.yml + parser.py from PercePiano repo\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nCRITICAL FIXES APPLIED:\")\n",
    "print(\"  - input_size: 84 (79 base + 5 unnorm, was 79)\")\n",
    "print(\"  - learning_rate: 1e-4 (was 2.5e-5, parser.py:119)\")\n",
    "print(\"  - weight_decay: 1e-5 (was 0.01, parser.py:135)\")\n",
    "print(\"  - batch_size: 32 (was 8, parser.py:107)\")\n",
    "print(\"  - gradient_clip_val: 2.0 (was 1.0, parser.py:159)\")\n",
    "print(\"  - Voice LSTM: Now receives 256-dim embeddings (was 512-dim note output)\")\n",
    "print(\"  - Key augmentation: Now uses midi_pitch_unnorm for pitch range\")\n",
    "print(\"=\"*70)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 6: VirtuosoNet Preprocessing (CRITICAL)\n",
    "\n",
    "This step extracts the 84-dim VirtuosoNet features used in the original PercePiano paper:\n",
    "- **79 base features**: Normalized where applicable (z-score normalization)\n",
    "- **5 unnorm features**: Raw values preserved BEFORE normalization (critical for key augmentation)\n",
    "\n",
    "The unnorm features are:\n",
    "- `midi_pitch_unnorm`: Raw MIDI pitch (21-108) for accurate key augmentation\n",
    "- `duration_unnorm`, `beat_importance_unnorm`, `measure_length_unnorm`, `following_rest_unnorm`\n",
    "\n",
    "If preprocessing has already been done, this cell will skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import shutil\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Expected feature dimension (79 base + 5 unnorm = 84)\n",
    "EXPECTED_FEATURE_DIM = 84\n",
    "\n",
    "def check_feature_dimension(vnet_dir: Path) -> int:\n",
    "    \"\"\"Check the feature dimension of existing preprocessed data.\"\"\"\n",
    "    train_dir = vnet_dir / 'train'\n",
    "    if not train_dir.exists():\n",
    "        return 0\n",
    "    \n",
    "    pkl_files = list(train_dir.glob('*.pkl'))\n",
    "    if not pkl_files:\n",
    "        return 0\n",
    "    \n",
    "    # Load first file and check dimension\n",
    "    with open(pkl_files[0], 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    if 'input' in data:\n",
    "        return data['input'].shape[1]\n",
    "    return 0\n",
    "\n",
    "# First, ensure VirtuosoNet modules are available by cloning PercePiano repo\n",
    "virtuoso_module_path = Path('/tmp/crescendai/model/data/raw/PercePiano/virtuoso/virtuoso/pyScoreParser')\n",
    "percepiano_path = Path('/tmp/crescendai/model/data/raw/PercePiano')\n",
    "\n",
    "if not (virtuoso_module_path / 'feature_extraction.py').exists():\n",
    "    print(\"VirtuosoNet modules not found. Cloning PercePiano repository...\")\n",
    "    \n",
    "    # Remove existing directory if it exists (might be empty or corrupted)\n",
    "    if percepiano_path.exists():\n",
    "        shutil.rmtree(percepiano_path)\n",
    "    \n",
    "    percepiano_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Clone the repo\n",
    "    result = subprocess.run(\n",
    "        ['git', 'clone', '--depth', '1',\n",
    "         'https://github.com/JonghoKimSNU/PercePiano.git',\n",
    "         str(percepiano_path)],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"Failed to clone PercePiano: {result.stderr}\")\n",
    "        raise RuntimeError(\"Could not clone PercePiano repository\")\n",
    "    \n",
    "    print(\"PercePiano cloned successfully!\")\n",
    "    print(f\"VirtuosoNet modules at: {virtuoso_module_path}\")\n",
    "else:\n",
    "    print(f\"VirtuosoNet modules already available at {virtuoso_module_path}\")\n",
    "\n",
    "# Check if VirtuosoNet features exist AND have correct dimension\n",
    "vnet_dir = Path(CONFIG['vnet_data_dir'])\n",
    "vnet_train_dir = vnet_dir / 'train'\n",
    "\n",
    "needs_preprocessing = False\n",
    "if vnet_train_dir.exists() and list(vnet_train_dir.glob('*.pkl')):\n",
    "    # Check feature dimension\n",
    "    current_dim = check_feature_dimension(vnet_dir)\n",
    "    \n",
    "    if current_dim == EXPECTED_FEATURE_DIM:\n",
    "        print(f\"\\nVirtuosoNet features already exist with correct dimension ({current_dim}-dim)\")\n",
    "        print(f\"  train: {len(list((vnet_dir / 'train').glob('*.pkl')))} samples\")\n",
    "        print(f\"  val: {len(list((vnet_dir / 'val').glob('*.pkl')))} samples\")\n",
    "        print(f\"  test: {len(list((vnet_dir / 'test').glob('*.pkl')))} samples\")\n",
    "    else:\n",
    "        print(f\"\\n[WARNING] Existing features have wrong dimension: {current_dim} (expected {EXPECTED_FEATURE_DIM})\")\n",
    "        print(f\"  This is likely old 79-dim data without unnorm features.\")\n",
    "        print(f\"  Deleting old data and re-preprocessing...\")\n",
    "        \n",
    "        # Delete old preprocessed data\n",
    "        shutil.rmtree(vnet_dir)\n",
    "        needs_preprocessing = True\n",
    "else:\n",
    "    needs_preprocessing = True\n",
    "\n",
    "if needs_preprocessing:\n",
    "    print(\"\\nVirtuosoNet features not found or outdated. Running preprocessing...\")\n",
    "    print(f\"This extracts {EXPECTED_FEATURE_DIM}-dim features (79 base + 5 unnorm) for key augmentation.\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Run preprocessing script\n",
    "    # Pass DATA_ROOT directly - the script will find JSON files and score_xml there\n",
    "    result = subprocess.run(\n",
    "        ['python', 'scripts/preprocess_percepiano_vnet.py',\n",
    "         '--data_root', str(DATA_ROOT),\n",
    "         '--output_dir', str(vnet_dir)],\n",
    "        cwd='/tmp/crescendai/model',\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Preprocessing failed: {result.stderr}\")\n",
    "        print(\"\\nNOTE: VirtuosoNet preprocessing requires MusicXML parsing.\")\n",
    "        print(\"If this fails, you may need to manually align MIDI files to scores.\")\n",
    "        print(\"\")\n",
    "        print(\"Alternative: Download pre-processed features from Google Drive:\")\n",
    "        print(f\"  rclone copy gdrive:percepiano_data/percepiano_vnet {vnet_dir}\")\n",
    "    else:\n",
    "        print(\"Preprocessing complete!\")\n",
    "        \n",
    "        # Verify new dimension\n",
    "        new_dim = check_feature_dimension(vnet_dir)\n",
    "        if new_dim == EXPECTED_FEATURE_DIM:\n",
    "            print(f\"[OK] Features have correct dimension: {new_dim}\")\n",
    "        else:\n",
    "            print(f\"[ERROR] Features still have wrong dimension: {new_dim} (expected {EXPECTED_FEATURE_DIM})\")\n",
    "\n",
    "# Verify features exist with correct dimension\n",
    "if vnet_train_dir.exists():\n",
    "    num_train = len(list((vnet_dir / 'train').glob('*.pkl')))\n",
    "    final_dim = check_feature_dimension(vnet_dir)\n",
    "    print(f\"\\nVirtuosoNet features ready: {num_train} training samples, {final_dim}-dim\")\n",
    "    \n",
    "    if final_dim != EXPECTED_FEATURE_DIM:\n",
    "        raise RuntimeError(\n",
    "            f\"Feature dimension mismatch: got {final_dim}, expected {EXPECTED_FEATURE_DIM}.\\n\"\n",
    "            f\"Delete {vnet_dir} and re-run this cell.\"\n",
    "        )\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"VirtuosoNet features not available at {vnet_dir}.\\n\"\n",
    "        \"Run preprocessing or download from Google Drive.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9msdij9m94m",
   "metadata": {},
   "source": [
    "## Step 6.1: Data Quality Diagnostics (CRITICAL)\n",
    "\n",
    "This cell performs comprehensive data quality checks on the preprocessed VirtuosoNet features.\n",
    "If you see RED warnings, they indicate potential causes for poor training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yd7d8v6nnti",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATA QUALITY DIAGNOSTICS\n",
    "========================\n",
    "This cell checks for common issues that cause training to fail:\n",
    "1. Insufficient preprocessed samples\n",
    "2. NaN/Inf values in features\n",
    "3. Label range issues (should be 0-1)\n",
    "4. Note location format issues (critical for hierarchical attention)\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import random\n",
    "\n",
    "console = Console()\n",
    "vnet_dir = Path(CONFIG['vnet_data_dir'])\n",
    "\n",
    "def diagnose_sample(pkl_path: Path) -> dict:\n",
    "    \"\"\"Analyze a single preprocessed sample for issues.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    result = {\n",
    "        'name': pkl_path.stem,\n",
    "        'issues': issues,\n",
    "    }\n",
    "    \n",
    "    # Check input features\n",
    "    input_feat = data.get('input')\n",
    "    if input_feat is None:\n",
    "        issues.append(\"CRITICAL: Missing 'input' key\")\n",
    "        return result\n",
    "    \n",
    "    result['shape'] = input_feat.shape\n",
    "    result['num_notes'] = input_feat.shape[0]\n",
    "    result['feature_dim'] = input_feat.shape[1]\n",
    "    \n",
    "    # Check for NaN/Inf\n",
    "    nan_count = np.isnan(input_feat).sum()\n",
    "    inf_count = np.isinf(input_feat).sum()\n",
    "    if nan_count > 0:\n",
    "        issues.append(f\"CRITICAL: {nan_count} NaN values in features\")\n",
    "    if inf_count > 0:\n",
    "        issues.append(f\"CRITICAL: {inf_count} Inf values in features\")\n",
    "    \n",
    "    # Check feature dimension\n",
    "    if input_feat.shape[1] != 84:\n",
    "        issues.append(f\"WARNING: Feature dim is {input_feat.shape[1]}, expected 84\")\n",
    "    \n",
    "    # Check labels\n",
    "    labels = data.get('labels')\n",
    "    if labels is None:\n",
    "        issues.append(\"CRITICAL: Missing 'labels' key\")\n",
    "    else:\n",
    "        result['labels'] = labels\n",
    "        result['label_min'] = float(labels.min())\n",
    "        result['label_max'] = float(labels.max())\n",
    "        \n",
    "        if labels.min() < 0 or labels.max() > 1:\n",
    "            issues.append(f\"CRITICAL: Labels out of [0,1] range: [{labels.min():.3f}, {labels.max():.3f}]\")\n",
    "        \n",
    "        # Check for constant labels (zero variance)\n",
    "        for i, val in enumerate(labels):\n",
    "            if np.std([val]) == 0:  # Single value, will be compared across samples later\n",
    "                pass  # Individual sample check not meaningful\n",
    "    \n",
    "    # Check note locations (CRITICAL for hierarchical attention)\n",
    "    note_loc = data.get('note_location')\n",
    "    if note_loc is None:\n",
    "        issues.append(\"CRITICAL: Missing 'note_location' key\")\n",
    "    else:\n",
    "        for key in ['beat', 'measure', 'voice']:\n",
    "            if key not in note_loc:\n",
    "                issues.append(f\"CRITICAL: Missing '{key}' in note_location\")\n",
    "            else:\n",
    "                indices = np.array(note_loc[key])\n",
    "                result[f'{key}_min'] = int(indices.min())\n",
    "                result[f'{key}_max'] = int(indices.max())\n",
    "                \n",
    "                # Check for gaps in beat indices (critical for boundary detection)\n",
    "                if key == 'beat':\n",
    "                    unique = np.unique(indices)\n",
    "                    expected = np.arange(unique.min(), unique.max() + 1)\n",
    "                    missing = set(expected) - set(unique)\n",
    "                    if missing and len(missing) <= 10:\n",
    "                        issues.append(f\"WARNING: Beat indices have gaps: missing {sorted(missing)[:5]}...\")\n",
    "                    elif missing:\n",
    "                        issues.append(f\"WARNING: Beat indices have {len(missing)} gaps\")\n",
    "                    \n",
    "                    # Check if starts from 1 (expected) or 0\n",
    "                    if indices.min() == 0:\n",
    "                        issues.append(\"INFO: Beat indices start from 0 (hierarchy_utils expects 1)\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Count samples per split\n",
    "print(\"=\"*70)\n",
    "print(\"DATA QUALITY DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_counts = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = vnet_dir / split\n",
    "    if split_dir.exists():\n",
    "        pkl_files = list(split_dir.glob('*.pkl'))\n",
    "        sample_counts[split] = len(pkl_files)\n",
    "    else:\n",
    "        sample_counts[split] = 0\n",
    "        console.print(f\"[red]CRITICAL: {split} directory not found![/red]\")\n",
    "\n",
    "print(\"\\n1. SAMPLE COUNTS\")\n",
    "print(\"-\" * 40)\n",
    "for split, count in sample_counts.items():\n",
    "    status = \"[green]OK[/green]\" if count > 0 else \"[red]MISSING[/red]\"\n",
    "    console.print(f\"  {split:5s}: {count:4d} samples  {status}\")\n",
    "\n",
    "total_samples = sum(sample_counts.values())\n",
    "if total_samples < 100:\n",
    "    console.print(f\"\\n[red]CRITICAL: Only {total_samples} total samples - likely preprocessing failures![/red]\")\n",
    "    console.print(\"[yellow]Check preprocessing logs for errors.[/yellow]\")\n",
    "\n",
    "# Sample 5 files from each split for detailed analysis\n",
    "print(\"\\n2. SAMPLE QUALITY CHECK\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "all_issues = []\n",
    "all_results = []\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = vnet_dir / split\n",
    "    if not split_dir.exists():\n",
    "        continue\n",
    "    \n",
    "    pkl_files = list(split_dir.glob('*.pkl'))\n",
    "    if not pkl_files:\n",
    "        continue\n",
    "    \n",
    "    # Sample up to 5 files\n",
    "    sample_files = random.sample(pkl_files, min(5, len(pkl_files)))\n",
    "    \n",
    "    console.print(f\"\\n[cyan]{split.upper()} split ({len(sample_files)} samples checked):[/cyan]\")\n",
    "    \n",
    "    for pkl_path in sample_files:\n",
    "        result = diagnose_sample(pkl_path)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        if result['issues']:\n",
    "            for issue in result['issues']:\n",
    "                console.print(f\"  {pkl_path.stem[:30]:30s} - [red]{issue}[/red]\")\n",
    "                all_issues.append((split, pkl_path.stem, issue))\n",
    "        else:\n",
    "            console.print(f\"  {pkl_path.stem[:30]:30s} - [green]OK[/green] (shape: {result.get('shape')})\")\n",
    "\n",
    "# Aggregate statistics\n",
    "print(\"\\n3. AGGREGATE STATISTICS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if all_results:\n",
    "    # Feature dimensions\n",
    "    dims = [r['feature_dim'] for r in all_results if 'feature_dim' in r]\n",
    "    if dims:\n",
    "        console.print(f\"  Feature dimensions: {set(dims)}\")\n",
    "        if len(set(dims)) > 1:\n",
    "            console.print(\"[red]  CRITICAL: Inconsistent feature dimensions![/red]\")\n",
    "    \n",
    "    # Note counts\n",
    "    notes = [r['num_notes'] for r in all_results if 'num_notes' in r]\n",
    "    if notes:\n",
    "        console.print(f\"  Notes per sample: min={min(notes)}, max={max(notes)}, mean={np.mean(notes):.0f}\")\n",
    "    \n",
    "    # Label statistics\n",
    "    label_mins = [r['label_min'] for r in all_results if 'label_min' in r]\n",
    "    label_maxs = [r['label_max'] for r in all_results if 'label_max' in r]\n",
    "    if label_mins:\n",
    "        console.print(f\"  Label range: [{min(label_mins):.3f}, {max(label_maxs):.3f}]\")\n",
    "        if min(label_mins) < 0 or max(label_maxs) > 1:\n",
    "            console.print(\"[red]  CRITICAL: Labels outside [0,1] - sigmoid output mismatch![/red]\")\n",
    "    \n",
    "    # Beat index statistics\n",
    "    beat_mins = [r['beat_min'] for r in all_results if 'beat_min' in r]\n",
    "    beat_maxs = [r['beat_max'] for r in all_results if 'beat_max' in r]\n",
    "    if beat_mins:\n",
    "        console.print(f\"  Beat indices: min={min(beat_mins)}, max={max(beat_maxs)}\")\n",
    "        if min(beat_mins) == 0:\n",
    "            console.print(\"[yellow]  INFO: Beat indices start at 0 - check hierarchy_utils compatibility[/yellow]\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n4. DIAGNOSTIC SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "critical_count = sum(1 for _, _, issue in all_issues if 'CRITICAL' in issue)\n",
    "warning_count = sum(1 for _, _, issue in all_issues if 'WARNING' in issue)\n",
    "\n",
    "if critical_count > 0:\n",
    "    console.print(f\"[red]CRITICAL ISSUES: {critical_count}[/red]\")\n",
    "    console.print(\"[red]Training will likely fail or produce garbage results.[/red]\")\n",
    "elif warning_count > 0:\n",
    "    console.print(f\"[yellow]WARNINGS: {warning_count}[/yellow]\")\n",
    "    console.print(\"[yellow]Training may work but with degraded performance.[/yellow]\")\n",
    "else:\n",
    "    console.print(\"[green]No issues detected - data looks healthy![/green]\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6mxyh63i7qm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch PercePiano data_class.py to fix silent failures\n",
    "# The original code has bare `except:` clauses that swallow errors silently\n",
    "# This patch replaces them with explicit exception handling\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "data_class_path = Path('/tmp/crescendai/model/data/raw/PercePiano/virtuoso/virtuoso/pyScoreParser/data_class.py')\n",
    "\n",
    "if data_class_path.exists():\n",
    "    print(\"Patching PercePiano data_class.py to fix silent failures...\")\n",
    "    \n",
    "    content = data_class_path.read_text()\n",
    "    original_content = content\n",
    "    \n",
    "    # Patch 1: Fix bare except in load_all_piece (line ~95-98)\n",
    "    # Change from printing error to re-raising with context\n",
    "    old_pattern1 = r\"except Exception as ex:\\s+# TODO: TGK: this is ambiguous.*?\\s+print\\(f'Error while processing \\{scores\\[n\\]\\}\\. Error type :\\{ex\\}'\\)\"\n",
    "    new_code1 = \"\"\"except Exception as ex:\n",
    "                # Re-raise with full context instead of silently continuing\n",
    "                raise RuntimeError(f'Error loading piece {scores[n]}: {type(ex).__name__}: {ex}') from ex\"\"\"\n",
    "    content = re.sub(old_pattern1, new_code1, content, flags=re.DOTALL)\n",
    "    \n",
    "    # Patch 2: Fix bare except in performance alignment (line ~332-335)\n",
    "    old_pattern2 = r\"except:\\s+perform_data = None\\s+print\\(f'Cannot align \\{perform\\}'\\)\\s+self\\.performances\\.append\\(None\\)\"\n",
    "    new_code2 = \"\"\"except (ValueError, IndexError, FileNotFoundError, OSError) as e:\n",
    "                        # Explicit exception handling - re-raise with context\n",
    "                        raise RuntimeError(f'Alignment failed for {perform}: {type(e).__name__}: {e}') from e\"\"\"\n",
    "    content = re.sub(old_pattern2, new_code2, content)\n",
    "    \n",
    "    # Patch 3: Fix bare except in Nakamura alignment (line ~425)\n",
    "    old_pattern3 = r\"except:\\s+print\\('Error to process \\{\\}'\\.format\\(midi_file_path\\)\\)\"\n",
    "    new_code3 = \"\"\"except subprocess.CalledProcessError as e:\n",
    "            print(f'Alignment tool failed for {midi_file_path}: {e}')\"\"\"\n",
    "    content = re.sub(old_pattern3, new_code3, content)\n",
    "    \n",
    "    # Patch 4: Fix second bare except in Nakamura alignment retry (line ~435)\n",
    "    old_pattern4 = r\"except:\\s+align_success = False\\s+print\\('Fail to process \\{\\}'\\.format\\(midi_file_path\\)\\)\\s+os\\.chdir\\(current_dir\\)\"\n",
    "    new_code4 = \"\"\"except subprocess.CalledProcessError as e2:\n",
    "                align_success = False\n",
    "                raise RuntimeError(f'Alignment tool failed after retry for {midi_file_path}: {e2}') from e2\"\"\"\n",
    "    content = re.sub(old_pattern4, new_code4, content)\n",
    "    \n",
    "    if content != original_content:\n",
    "        data_class_path.write_text(content)\n",
    "        print(\"[OK] Patched data_class.py - silent failures now raise explicit exceptions\")\n",
    "    else:\n",
    "        print(\"[INFO] data_class.py already patched or patterns not found\")\n",
    "else:\n",
    "    print(\"[SKIP] data_class.py not found yet - will be available after cloning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 7: Create DataLoaders and Model\n",
    "\n",
    "Using the VirtuosoNet preprocessed features (84-dim: 79 base + 5 unnorm) with the faithful PercePiano replica architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "from src.data.percepiano_vnet_dataset import create_vnet_dataloaders\nfrom src.models.percepiano_replica import PercePianoVNetModule\n\n# Create DataLoaders with VirtuosoNet features (84-dim: 79 base + 5 unnorm)\n# Note: batch_size defaults to 32 in create_vnet_dataloaders (matched to original PercePiano)\n# Key augmentation uses midi_pitch_unnorm (index 79) to calculate valid pitch shift range\n# NOTE: Using num_workers=0 to avoid shared memory issues on Thunder Compute\ntrain_loader, val_loader, test_loader = create_vnet_dataloaders(\n    data_dir=CONFIG['vnet_data_dir'],\n    batch_size=CONFIG['batch_size'],  # 32 (original PercePiano parser.py:107)\n    max_notes=CONFIG['max_notes'],\n    num_workers=0,  # Avoid shared memory issues on Thunder Compute\n)\n\nprint(f\"Train: {len(train_loader.dataset)} samples\")\nprint(f\"Val: {len(val_loader.dataset)} samples\")\nprint(f\"Test: {len(test_loader.dataset)} samples\")\nprint(f\"Batch size: {CONFIG['batch_size']}\")\n\n# Create model with VirtuosoNet features (84-dim input: 79 base + 5 unnorm)\n# Note: PercePianoVNetModule defaults are now matched to original PercePiano:\n#   - input_size: 84 (79 base + 5 unnorm for key augmentation)\n#   - learning_rate: 1e-4 (parser.py:119)\n#   - weight_decay: 1e-5 (parser.py:135)\n#   - Voice LSTM input: 256-dim embeddings (NOT 512-dim note output)\nmodel = PercePianoVNetModule(\n    # VirtuosoNet input dimension (79 base + 5 unnorm)\n    input_size=CONFIG['input_size'],  # 84-dim features\n    # HAN dimensions (matched to PercePiano han_bigger256_concat.yml)\n    hidden_size=CONFIG['hidden_size'],\n    note_layers=CONFIG['note_layers'],\n    voice_layers=CONFIG['voice_layers'],\n    beat_layers=CONFIG['beat_layers'],\n    measure_layers=CONFIG['measure_layers'],\n    num_attention_heads=CONFIG['num_attention_heads'],\n    final_hidden=CONFIG['final_hidden'],\n    # Training (matched to original PercePiano parser.py)\n    learning_rate=CONFIG['learning_rate'],   # 1e-4 (parser.py:119)\n    weight_decay=CONFIG['weight_decay'],     # 1e-5 (parser.py:135)\n    dropout=CONFIG['dropout'],\n)\n\n# Count parameters\ntotal_params = model.count_parameters()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"PERCEPIANO REPLICA MODEL (VirtuosoNet Features)\")\nprint(\"=\"*70)\nprint(f\"Architecture: Bi-LSTM + HAN (Note -> Voice -> Beat -> Measure)\")\nprint(f\"Input features: {CONFIG['input_size']} (79 base + 5 unnorm)\")\nprint(f\"Hidden size: {CONFIG['hidden_size']}\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"\")\nprint(f\"CRITICAL FIXES APPLIED:\")\nprint(f\"  1. input_size: 84 (79 base + 5 unnorm, matches original _unnorm features)\")\nprint(f\"  2. Voice LSTM receives 256-dim embeddings (was 512-dim note output)\")\nprint(f\"  3. learning_rate: {CONFIG['learning_rate']} (was 2.5e-5)\")\nprint(f\"  4. weight_decay: {CONFIG['weight_decay']} (was 0.01)\")\nprint(f\"  5. batch_size: {CONFIG['batch_size']} (was 8)\")\nprint(f\"  6. gradient_clip_val: {CONFIG['gradient_clip_val']} (set in Trainer)\")\nprint(f\"  7. Key augmentation uses midi_pitch_unnorm for pitch range calculation\")\nprint(f\"  8. Beat/measure indices shifted to start from 1 (hierarchy_utils fix)\")\nprint(f\"\")\nprint(f\"Target R-squared: 0.35-0.40 (piece-split)\")\nprint(f\"Dimensions: {len(model.dimensions)}\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "dfqdgymzb7s",
   "source": "## Step 7.1: Simple Baseline Test (Optional)\n\nThis cell tests if a simple linear model can learn anything from the data.\nIf even a simple model fails (R^2 < 0), the issue is definitely in the data pipeline, not the architecture.\n\nSkip this cell for normal training - only run if diagnosing training failures.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "j23oe88s2r",
   "source": "# OPTIONAL: Simple baseline test - skip this cell for normal training\n# Only run if you need to verify the data pipeline works\n\nRUN_BASELINE_TEST = False  # Set to True to run this test\n\nif RUN_BASELINE_TEST:\n    import torch\n    import torch.nn as nn\n    from sklearn.metrics import r2_score\n    from tqdm import tqdm\n    \n    print(\"=\"*60)\n    print(\"SIMPLE BASELINE TEST\")\n    print(\"=\"*60)\n    print(\"Testing if a simple linear model can learn from the data...\")\n    print(\"If this fails, the issue is in the data pipeline, not the architecture.\\n\")\n    \n    # Simple linear model: just mean-pool features and predict scores\n    class SimpleBaseline(nn.Module):\n        def __init__(self, input_dim=84, hidden_dim=64, output_dim=19):\n            super().__init__()\n            self.net = nn.Sequential(\n                nn.Linear(input_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(0.2),\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(0.2),\n                nn.Linear(hidden_dim, output_dim),\n                nn.Sigmoid()  # Match PercePiano replica\n            )\n        \n        def forward(self, x, mask):\n            # x: (batch, seq, 84), mask: (batch, seq)\n            # Mean pool over valid notes\n            mask_expanded = mask.unsqueeze(-1).float()  # (batch, seq, 1)\n            masked_x = x * mask_expanded\n            summed = masked_x.sum(dim=1)  # (batch, 84)\n            counts = mask_expanded.sum(dim=1).clamp(min=1)  # (batch, 1)\n            pooled = summed / counts  # (batch, 84)\n            return self.net(pooled)\n    \n    # Create model\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    simple_model = SimpleBaseline().to(device)\n    optimizer = torch.optim.Adam(simple_model.parameters(), lr=1e-3)\n    criterion = nn.MSELoss()\n    \n    print(f\"Simple model parameters: {sum(p.numel() for p in simple_model.parameters()):,}\")\n    print(f\"Device: {device}\\n\")\n    \n    # Train for 10 epochs\n    num_epochs = 10\n    for epoch in range(num_epochs):\n        simple_model.train()\n        train_losses = []\n        \n        for batch in train_loader:\n            x = batch['input_features'].to(device)\n            mask = batch['attention_mask'].to(device)\n            targets = batch['scores'].to(device)\n            \n            preds = simple_model(x, mask)\n            loss = criterion(preds, targets)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            train_losses.append(loss.item())\n        \n        # Validate\n        simple_model.eval()\n        all_preds, all_targets = [], []\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                x = batch['input_features'].to(device)\n                mask = batch['attention_mask'].to(device)\n                targets = batch['scores'].to(device)\n                \n                preds = simple_model(x, mask)\n                all_preds.append(preds.cpu())\n                all_targets.append(targets.cpu())\n        \n        all_preds = torch.cat(all_preds).numpy()\n        all_targets = torch.cat(all_targets).numpy()\n        \n        r2 = r2_score(all_targets, all_preds)\n        train_loss = sum(train_losses) / len(train_losses)\n        \n        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val R2: {r2:.4f}\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"BASELINE TEST RESULTS\")\n    print(\"=\"*60)\n    \n    if r2 > 0:\n        print(f\"[OK] Simple model achieved R2 = {r2:.4f}\")\n        print(\"The data pipeline appears to work. Issue may be in the HAN architecture.\")\n    else:\n        print(f\"[FAIL] Simple model has R2 = {r2:.4f} (negative!)\")\n        print(\"Even a simple model cannot learn from this data.\")\n        print(\"\\nPossible causes:\")\n        print(\"  1. Labels are in wrong scale (should be 0-1)\")\n        print(\"  2. Features contain NaN/Inf values\")\n        print(\"  3. Features are all zeros or constant\")\n        print(\"  4. Label-feature alignment is wrong\")\n        print(\"\\nRun the Data Quality Diagnostics cell for more details.\")\n    print(\"=\"*60)\n    \n    # Clean up\n    del simple_model, optimizer\n    torch.cuda.empty_cache()\nelse:\n    print(\"[SKIP] Simple baseline test - set RUN_BASELINE_TEST = True to run\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 8: Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor, Callback\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom rich.console import Console\nimport time\nimport numpy as np\n\n# Custom callback for detailed training progress with prediction diagnostics\nclass TrainingProgressCallback(Callback):\n    \"\"\"Custom callback to show detailed training progress and diagnose training issues.\"\"\"\n    \n    def __init__(self, log_every_n_batches: int = 10):\n        super().__init__()\n        self.log_every_n_batches = log_every_n_batches\n        self.epoch_start_time = None\n        self.train_losses = []\n        self.console = Console()\n        self.first_batch_logged = False\n        self.prediction_variances = []\n        \n    def on_train_epoch_start(self, trainer, pl_module):\n        self.epoch_start_time = time.time()\n        self.train_losses = []\n        self.first_batch_logged = False\n        self.console.print(f\"\\n[bold cyan]{'='*60}[/bold cyan]\")\n        self.console.print(f\"[bold cyan]Epoch {trainer.current_epoch + 1}/{trainer.max_epochs}[/bold cyan]\")\n        self.console.print(f\"[bold cyan]{'='*60}[/bold cyan]\")\n        \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        # Handle both tensor and dict outputs\n        if outputs is not None:\n            if isinstance(outputs, dict) and 'loss' in outputs:\n                loss_val = outputs['loss'].item() if hasattr(outputs['loss'], 'item') else outputs['loss']\n            elif hasattr(outputs, 'item'):\n                loss_val = outputs.item()\n            else:\n                loss_val = float(outputs) if outputs is not None else 0.0\n            self.train_losses.append(loss_val)\n        \n        # DIAGNOSTIC: Log first batch predictions on epoch 0\n        if trainer.current_epoch == 0 and batch_idx == 0 and not self.first_batch_logged:\n            self.first_batch_logged = True\n            self._diagnose_first_batch(trainer, pl_module, batch)\n        \n        # Log progress\n        total_batches = len(trainer.train_dataloader)\n        if (batch_idx + 1) % self.log_every_n_batches == 0 or batch_idx == total_batches - 1:\n            recent_losses = self.train_losses[-self.log_every_n_batches:]\n            avg_loss = sum(recent_losses) / len(recent_losses) if recent_losses else 0\n            progress_pct = (batch_idx + 1) / total_batches * 100\n            elapsed = time.time() - self.epoch_start_time\n            eta = elapsed / (batch_idx + 1) * (total_batches - batch_idx - 1)\n            \n            # Get current learning rate\n            lr = trainer.optimizers[0].param_groups[0]['lr']\n            \n            self.console.print(\n                f\"  Step [{batch_idx + 1:4d}/{total_batches}] \"\n                f\"({progress_pct:5.1f}%) | \"\n                f\"Loss: {avg_loss:.4f} | \"\n                f\"LR: {lr:.2e} | \"\n                f\"Elapsed: {elapsed:5.0f}s | \"\n                f\"ETA: {eta:5.0f}s\"\n            )\n    \n    def _diagnose_first_batch(self, trainer, pl_module, batch):\n        \"\"\"Diagnose model predictions on first batch to detect issues early.\"\"\"\n        import torch\n        \n        self.console.print(f\"\\n[yellow]PREDICTION DIAGNOSTICS (First Batch)[/yellow]\")\n        \n        try:\n            pl_module.eval()\n            with torch.no_grad():\n                # Move batch to device\n                device = pl_module.device\n                input_features = batch['input_features'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                scores = batch['scores'].to(device)\n                \n                note_locations = {\n                    'beat': batch['note_locations_beat'].to(device),\n                    'measure': batch['note_locations_measure'].to(device),\n                    'voice': batch['note_locations_voice'].to(device),\n                }\n                \n                # Forward pass\n                outputs = pl_module(\n                    input_features=input_features,\n                    note_locations=note_locations,\n                    attention_mask=attention_mask,\n                )\n                \n                preds = outputs['predictions'].cpu().numpy()\n                targets = scores.cpu().numpy()\n                \n                # Prediction statistics\n                pred_mean = preds.mean()\n                pred_std = preds.std()\n                pred_min = preds.min()\n                pred_max = preds.max()\n                \n                target_mean = targets.mean()\n                target_std = targets.std()\n                \n                self.console.print(f\"  Predictions: mean={pred_mean:.4f}, std={pred_std:.4f}, range=[{pred_min:.4f}, {pred_max:.4f}]\")\n                self.console.print(f\"  Targets:     mean={target_mean:.4f}, std={target_std:.4f}\")\n                \n                # Check for issues\n                if pred_std < 0.01:\n                    self.console.print(f\"  [red]WARNING: Prediction std < 0.01 - model may be collapsing to constant![/red]\")\n                \n                if abs(pred_mean - 0.5) > 0.3:\n                    self.console.print(f\"  [yellow]INFO: Predictions shifted from 0.5 - check initialization[/yellow]\")\n                \n                # Per-dimension variance check\n                per_dim_std = preds.std(axis=0)\n                collapsed_dims = np.sum(per_dim_std < 0.01)\n                if collapsed_dims > 0:\n                    self.console.print(f\"  [red]WARNING: {collapsed_dims}/19 dimensions have std < 0.01 (collapsed)[/red]\")\n                \n                self.prediction_variances.append(per_dim_std)\n                \n        except Exception as e:\n            self.console.print(f\"  [red]Error in diagnostics: {e}[/red]\")\n        finally:\n            pl_module.train()\n    \n    def on_train_epoch_end(self, trainer, pl_module):\n        epoch_time = time.time() - self.epoch_start_time\n        avg_loss = sum(self.train_losses) / len(self.train_losses) if self.train_losses else 0\n        \n        self.console.print(f\"\\n[green]Train epoch complete[/green] | \"\n                          f\"Avg Loss: {avg_loss:.4f} | \"\n                          f\"Time: {epoch_time:.1f}s\")\n    \n    def on_validation_epoch_start(self, trainer, pl_module):\n        self.console.print(f\"\\n[yellow]Running validation...[/yellow]\")\n        self.val_start_time = time.time()\n    \n    def on_validation_epoch_end(self, trainer, pl_module):\n        val_time = time.time() - self.val_start_time\n        \n        # Get validation metrics\n        metrics = trainer.callback_metrics\n        mean_r2 = metrics.get('val/mean_r2', None)\n        val_loss = metrics.get('val/loss', None)\n        \n        # Print validation summary\n        self.console.print(f\"[green]Validation complete[/green] ({val_time:.1f}s)\")\n        if val_loss is not None:\n            self.console.print(f\"  Val Loss: {float(val_loss):.4f}\")\n        if mean_r2 is not None:\n            r2_val = float(mean_r2)\n            self.console.print(f\"  [bold]Mean R2: {r2_val:.4f}[/bold]\")\n            \n            # DIAGNOSTIC: Flag if R2 is negative\n            if r2_val < 0:\n                self.console.print(f\"  [red]WARNING: R2 < 0 means model is worse than predicting mean![/red]\")\n                self.console.print(f\"  [yellow]Check: 1) Data quality 2) Label scale 3) Note location format[/yellow]\")\n        \n        # Collect per-dimension R2 values\n        dim_r2s = {}\n        for key, value in metrics.items():\n            if key.startswith('val/') and key.endswith('_r2') and key != 'val/mean_r2':\n                dim_name = key.replace('val/', '').replace('_r2', '')\n                dim_r2s[dim_name] = float(value)\n        \n        if dim_r2s:\n            sorted_dims = sorted(dim_r2s.items(), key=lambda x: x[1], reverse=True)\n            \n            # Count negative R2 dimensions\n            negative_dims = sum(1 for _, r2 in sorted_dims if r2 < 0)\n            if negative_dims > 10:\n                self.console.print(f\"\\n  [red]WARNING: {negative_dims}/19 dimensions have R2 < 0![/red]\")\n            \n            # Show top 5\n            self.console.print(f\"\\n  [cyan]Top 5 dimensions:[/cyan]\")\n            for dim, r2 in sorted_dims[:5]:\n                bar = '#' * int(max(0, r2) * 20)  # Visual bar\n                color = \"green\" if r2 > 0 else \"red\"\n                self.console.print(f\"    {dim:20s}: [{color}]{r2:+.4f}[/{color}] {bar}\")\n            \n            # Show bottom 3 if we have enough dimensions\n            if len(sorted_dims) > 8:\n                self.console.print(f\"\\n  [cyan]Bottom 3 dimensions:[/cyan]\")\n                for dim, r2 in sorted_dims[-3:]:\n                    bar = '#' * int(max(0, r2) * 20)\n                    color = \"green\" if r2 > 0 else \"red\"\n                    self.console.print(f\"    {dim:20s}: [{color}]{r2:+.4f}[/{color}] {bar}\")\n        \n        # Best model tracking\n        if hasattr(trainer, 'checkpoint_callback') and trainer.checkpoint_callback is not None:\n            best_r2 = trainer.checkpoint_callback.best_model_score\n            if best_r2 is not None:\n                self.console.print(f\"\\n  [bold magenta]Best R2 so far: {float(best_r2):.4f}[/bold magenta]\")\n\n# Checkpoint callback - monitor mean R-squared\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=CONFIG['checkpoint_dir'],\n    filename='percepiano_replica-{epoch:02d}-{val_mean_r2:.4f}',\n    monitor='val/mean_r2',\n    mode='max',\n    save_top_k=3,\n    save_last=True,\n)\n\n# Early stopping\nearly_stopping = EarlyStopping(\n    monitor='val/mean_r2',\n    patience=CONFIG['early_stopping_patience'],\n    mode='max',\n    verbose=True,\n)\n\n# LR monitor\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n# Custom progress callback\ntraining_progress = TrainingProgressCallback(log_every_n_batches=10)\n\n# Logger\nlogger = TensorBoardLogger(\n    save_dir='/tmp/logs',\n    name='percepiano_replica',\n)\n\n# Trainer with gradient clipping matched to original PercePiano (parser.py:159)\ntrainer = pl.Trainer(\n    max_epochs=CONFIG['max_epochs'],\n    accelerator='gpu',\n    devices=1,\n    precision=CONFIG['precision'],\n    gradient_clip_val=CONFIG['gradient_clip_val'],  # 2.0 (original PercePiano parser.py:159)\n    callbacks=[checkpoint_callback, early_stopping, lr_monitor, training_progress],\n    logger=logger,\n    log_every_n_steps=10,\n    val_check_interval=0.5,  # Validate twice per epoch\n    enable_progress_bar=True,\n)\n\nprint(\"=\"*60)\nprint(\"TRAINER CONFIGURATION\")\nprint(\"=\"*60)\nprint(f\"  Precision: {CONFIG['precision']}\")\nprint(f\"  Max epochs: {CONFIG['max_epochs']}\")\nprint(f\"  Batch size: {CONFIG['batch_size']} (original: 32)\")\nprint(f\"  Learning rate: {CONFIG['learning_rate']} (original: 1e-4)\")\nprint(f\"  Weight decay: {CONFIG['weight_decay']} (original: 1e-5)\")\nprint(f\"  Gradient clip: {CONFIG['gradient_clip_val']} (original: 2.0)\")\nprint(f\"  Early stopping patience: {CONFIG['early_stopping_patience']}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 9: Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Train\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING TRAINING - PercePiano SOTA Replica\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey metrics to watch:\")\n",
    "print(\"  - val/mean_r2: Overall R-squared (target: 0.35-0.40)\")\n",
    "print(\"  - val/timing_r2: Timing dimension (should be highest)\")\n",
    "print(\"  - val/tempo_r2: Tempo dimension\")\n",
    "print(\"\")\n",
    "print(\"PercePiano SOTA baselines:\")\n",
    "print(\"  - Bi-LSTM: R^2 = 0.185\")\n",
    "print(\"  - MidiBERT: R^2 = 0.313\")\n",
    "print(\"  - Bi-LSTM + SA + HAN: R^2 = 0.397 (SOTA)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints to Google Drive\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"Syncing checkpoints to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Step 10: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with best checkpoint\n",
    "print(\"Running test with best checkpoint...\")\n",
    "best_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best checkpoint: {best_path}\")\n",
    "\n",
    "if best_path:\n",
    "    test_results = trainer.test(model, test_loader, ckpt_path=best_path)\n",
    "    print(\"\\nTest Results:\")\n",
    "    for k, v in test_results[0].items():\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load best model\n",
    "from src.models.percepiano_replica import PercePianoVNetModule\n",
    "best_model = PercePianoVNetModule.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "best_model.eval()\n",
    "best_model.cuda()\n",
    "\n",
    "# Collect predictions on test set\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "print(\"Collecting predictions on test set...\")\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move batch to GPU\n",
    "        input_features = batch['input_features'].cuda()\n",
    "        attention_mask = batch['attention_mask'].cuda()\n",
    "        scores = batch['scores'].cuda()\n",
    "        \n",
    "        note_locations = {\n",
    "            'beat': batch['note_locations_beat'].cuda(),\n",
    "            'measure': batch['note_locations_measure'].cuda(),\n",
    "            'voice': batch['note_locations_voice'].cuda(),\n",
    "        }\n",
    "        \n",
    "        # Forward pass with VirtuosoNet features\n",
    "        outputs = best_model(\n",
    "            input_features=input_features,\n",
    "            note_locations=note_locations,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        all_preds.append(outputs['predictions'].cpu())\n",
    "        all_targets.append(scores.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "dimensions = best_model.dimensions\n",
    "\n",
    "print(f\"Collected {len(all_preds)} test samples across {len(dimensions)} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import (\n",
    "    compute_all_metrics,\n",
    "    PerDimensionAnalysis,\n",
    "    compare_to_sota,\n",
    "    format_comparison_table,\n",
    "    create_results_table,\n",
    "    PERCEPIANO_BASELINES,\n",
    ")\n",
    "\n",
    "# Compute all metrics\n",
    "metrics = compute_all_metrics(\n",
    "    predictions=all_preds,\n",
    "    targets=all_targets,\n",
    "    dimension_names=list(dimensions),\n",
    ")\n",
    "\n",
    "# Print results table\n",
    "print(create_results_table(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to SOTA baselines\n",
    "our_r2 = metrics['r2'].value\n",
    "per_dim_r2 = metrics['r2'].per_dimension\n",
    "\n",
    "comparison = compare_to_sota(\n",
    "    model_r2=our_r2,\n",
    "    model_name=\"PercePiano Replica (CrescendAI)\",\n",
    "    split_type=\"piece\",\n",
    "    per_dimension_r2=per_dim_r2,\n",
    ")\n",
    "\n",
    "print(format_comparison_table(comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*70)\n",
    "print(\"PERCEPIANO REPLICA - RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. OVERALL PERFORMANCE\")\n",
    "print(f\"   Mean R^2: {our_r2:.4f}\")\n",
    "print(f\"   Target (0.35-0.40): {'ACHIEVED' if our_r2 >= 0.35 else 'CLOSE' if our_r2 >= 0.30 else 'NOT YET'}\")\n",
    "\n",
    "print(f\"\\n2. COMPARISON TO PUBLISHED BASELINES\")\n",
    "print(f\"   Bi-LSTM baseline: 0.185\")\n",
    "print(f\"   MidiBERT: 0.313\")\n",
    "print(f\"   Bi-LSTM + SA + HAN (SOTA): 0.397\")\n",
    "print(f\"   Our replica: {our_r2:.4f}\")\n",
    "\n",
    "print(f\"\\n3. MODEL SIZE\")\n",
    "print(f\"   Parameters: {best_model.count_parameters():,}\")\n",
    "print(f\"   vs Previous (51.5M): {51_500_000 / best_model.count_parameters():.1f}x smaller\")\n",
    "\n",
    "print(f\"\\n4. TOP 5 DIMENSIONS\")\n",
    "sorted_dims = sorted(per_dim_r2.items(), key=lambda x: x[1], reverse=True)\n",
    "for dim, r2 in sorted_dims[:5]:\n",
    "    print(f\"   {dim}: {r2:.4f}\")\n",
    "\n",
    "print(f\"\\n5. BOTTOM 5 DIMENSIONS (need improvement)\")\n",
    "for dim, r2 in sorted_dims[-5:]:\n",
    "    print(f\"   {dim}: {r2:.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Step 11: Save as Teacher Model\n",
    "\n",
    "If the model achieves R-squared >= 0.30, save it as a **Teacher Model** for pseudo-labeling MAESTRO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Save as teacher model\n",
    "teacher_path = Path(CONFIG['checkpoint_dir']) / 'percepiano_teacher.pt'\n",
    "\n",
    "if our_r2 >= 0.25:  # Minimum threshold for useful teacher\n",
    "    torch.save({\n",
    "        'state_dict': best_model.state_dict(),\n",
    "        'config': {\n",
    "            'input_size': CONFIG['input_size'],  # 84 (79 base + 5 unnorm)\n",
    "            'hidden_size': CONFIG['hidden_size'],\n",
    "            'note_layers': CONFIG['note_layers'],\n",
    "            'voice_layers': CONFIG['voice_layers'],\n",
    "            'beat_layers': CONFIG['beat_layers'],\n",
    "            'measure_layers': CONFIG['measure_layers'],\n",
    "            'num_attention_heads': CONFIG['num_attention_heads'],\n",
    "            'final_hidden': CONFIG['final_hidden'],\n",
    "            'dropout': CONFIG['dropout'],\n",
    "        },\n",
    "        'dimensions': list(dimensions),\n",
    "        'metrics': {\n",
    "            'r2': our_r2,\n",
    "            'per_dimension_r2': per_dim_r2,\n",
    "        },\n",
    "        'sota_comparison': {\n",
    "            'rank': comparison['rank'],\n",
    "            'total_baselines': comparison['total_baselines'],\n",
    "            'vs_best_baseline': comparison['improvement_vs_best'],\n",
    "        },\n",
    "        'architecture': 'PercePiano Replica (Bi-LSTM + HAN) with VirtuosoNet 84-dim features (79 base + 5 unnorm)',\n",
    "        'reference': 'https://github.com/JonghoKimSNU/PercePiano',\n",
    "    }, teacher_path)\n",
    "    \n",
    "    print(f\"Saved teacher model to {teacher_path}\")\n",
    "    print(f\"Teacher R^2: {our_r2:.4f}\")\n",
    "    print(f\"\\nThis model can be used for pseudo-labeling MAESTRO!\")\n",
    "    print(f\"Run: python scripts/pseudo_label_maestro.py --teacher {teacher_path}\")\n",
    "else:\n",
    "    print(f\"R^2 = {our_r2:.4f} is below threshold (0.25) for teacher model.\")\n",
    "    print(\"Consider:\")\n",
    "    print(\"  1. Training for more epochs\")\n",
    "    print(\"  2. Adjusting hyperparameters\")\n",
    "    print(\"  3. Checking data quality\")\n",
    "\n",
    "# Final sync to Google Drive\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"\\nFinal sync to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")\n",
    "    print(f\"Checkpoints available at: {CONFIG['gdrive_checkpoint']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If R-squared >= 0.30:\n",
    "\n",
    "1. **Pseudo-label MAESTRO**: Use this teacher model to generate labels for MAESTRO dataset\n",
    "2. **Train larger model**: With expanded dataset (~6000 samples), train a larger model\n",
    "3. **Noisy Student**: Apply noisy student training for potential improvement over teacher\n",
    "\n",
    "If R-squared < 0.30:\n",
    "\n",
    "1. Check if validation set is too small (only 27 samples)\n",
    "2. Consider k-fold cross-validation for more robust estimates\n",
    "3. Verify data preprocessing matches PercePiano exactly\n",
    "\n",
    "---\n",
    "\n",
    "**Attribution**: This model replicates the architecture from PercePiano (Park et al., ISMIR/Nature 2024).  \n",
    "GitHub: https://github.com/JonghoKimSNU/PercePiano"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}