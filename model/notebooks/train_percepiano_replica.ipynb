{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PercePiano Replica Training\n",
    "\n",
    "Train the PercePiano replica model using preprocessed VirtuosoNet features.\n",
    "\n",
    "## Attribution\n",
    "\n",
    "> **PercePiano: A Benchmark for Perceptual Evaluation of Piano Performance**  \n",
    "> Park, Jongho and Kim, Dasaem et al.  \n",
    "> ISMIR 2024 / Nature Scientific Reports 2024  \n",
    "> GitHub: https://github.com/JonghoKimSNU/PercePiano\n",
    "\n",
    "## Data\n",
    "\n",
    "Uses preprocessed VirtuosoNet features (79-dim normalized + 5-dim unnorm for augmentation):\n",
    "- Train: 945 samples\n",
    "- Val: 34 samples  \n",
    "- Test: 115 samples\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- Target R-squared: 0.35-0.40 (piece-split)\n",
    "- Training time: ~1-2 hours on T4/A100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rclone\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv and clone repository\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Install dependencies\n",
    "!uv pip install --system -e .\n",
    "!pip install tensorboard rich\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths and Check rclone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = Path('/tmp/percepiano_vnet_split')\n",
    "CHECKPOINT_ROOT = '/tmp/checkpoints/percepiano_replica'\n",
    "GDRIVE_DATA_PATH = 'gdrive:crescendai_data/percepiano_vnet_split'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/percepiano_replica'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PERCEPIANO REPLICA TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check rclone\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "\n",
    "if 'gdrive:' in result.stdout:\n",
    "    print(\"rclone 'gdrive' remote: CONFIGURED\")\n",
    "    RCLONE_AVAILABLE = True\n",
    "else:\n",
    "    print(\"rclone 'gdrive' remote: NOT CONFIGURED\")\n",
    "    print(\"Run 'rclone config' in terminal to set up Google Drive\")\n",
    "    RCLONE_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nData directory: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Download Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "if not RCLONE_AVAILABLE:\n",
    "    raise RuntimeError(\"rclone not configured. Run 'rclone config' first.\")\n",
    "\n",
    "# Download preprocessed data\n",
    "print(\"Downloading preprocessed VirtuosoNet features from Google Drive...\")\n",
    "subprocess.run(\n",
    "    ['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'],\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "# Verify data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = DATA_ROOT / split\n",
    "    if split_dir.exists():\n",
    "        count = len(list(split_dir.glob('*.pkl')))\n",
    "        print(f\"  {split}: {count} samples\")\n",
    "    else:\n",
    "        print(f\"  {split}: MISSING!\")\n",
    "\n",
    "stat_file = DATA_ROOT / 'stat.pkl'\n",
    "print(f\"  stat.pkl: {'present' if stat_file.exists() else 'MISSING!'}\")\n",
    "\n",
    "# Restore existing checkpoints\n",
    "print(\"\\nRestoring checkpoints from Google Drive (if any)...\")\n",
    "subprocess.run(\n",
    "    ['rclone', 'copy', GDRIVE_CHECKPOINT_PATH, CHECKPOINT_ROOT, '--progress'],\n",
    "    capture_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 4: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# PercePiano Configuration (matched to original paper)\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'data_dir': str(DATA_ROOT),\n",
    "    \n",
    "    # Model input (79 normalized features, unnorm used for augmentation only)\n",
    "    'input_size': 79,\n",
    "    \n",
    "    # HAN Architecture (han_bigger256_concat.yml)\n",
    "    'hidden_size': 256,\n",
    "    'note_layers': 2,\n",
    "    'voice_layers': 2,\n",
    "    'beat_layers': 2,\n",
    "    'measure_layers': 1,\n",
    "    'num_attention_heads': 8,\n",
    "    'final_hidden': 128,\n",
    "    \n",
    "    # Training (parser.py defaults)\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 32,\n",
    "    'max_epochs': 100,\n",
    "    'early_stopping_patience': 20,\n",
    "    'gradient_clip_val': 2.0,\n",
    "    'precision': '16-mixed',\n",
    "    \n",
    "    # Dataset\n",
    "    'max_notes': 1024,\n",
    "    \n",
    "    # Checkpoints\n",
    "    'checkpoint_dir': CHECKPOINT_ROOT,\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 5: Create DataLoaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.percepiano.data.percepiano_vnet_dataset import create_vnet_dataloaders\n",
    "from src.percepiano.models.percepiano_replica import PercePianoVNetModule\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader, val_loader, test_loader = create_vnet_dataloaders(\n",
    "    data_dir=CONFIG['data_dir'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    max_notes=CONFIG['max_notes'],\n",
    "    num_workers=0,  # Avoid shared memory issues\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_loader.dataset)} samples\")\n",
    "print(f\"Val: {len(val_loader.dataset)} samples\")\n",
    "print(f\"Test: {len(test_loader.dataset)} samples\")\n",
    "\n",
    "# Create model\n",
    "model = PercePianoVNetModule(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    note_layers=CONFIG['note_layers'],\n",
    "    voice_layers=CONFIG['voice_layers'],\n",
    "    beat_layers=CONFIG['beat_layers'],\n",
    "    measure_layers=CONFIG['measure_layers'],\n",
    "    num_attention_heads=CONFIG['num_attention_heads'],\n",
    "    final_hidden=CONFIG['final_hidden'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    dropout=CONFIG['dropout'],\n",
    ")\n",
    "\n",
    "print(f\"\\nModel parameters: {model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 6: Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CONFIG['checkpoint_dir'],\n",
    "    filename='percepiano-{epoch:02d}-{val_mean_r2:.4f}',\n",
    "    monitor='val/mean_r2',\n",
    "    mode='max',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val/mean_r2',\n",
    "    patience=CONFIG['early_stopping_patience'],\n",
    "    mode='max',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(save_dir='/tmp/logs', name='percepiano_replica')\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['max_epochs'],\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    precision=CONFIG['precision'],\n",
    "    gradient_clip_val=CONFIG['gradient_clip_val'],\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.5,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 7: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPercePiano SOTA baselines:\")\n",
    "print(\"  Bi-LSTM: R^2 = 0.185\")\n",
    "print(\"  MidiBERT: R^2 = 0.313\")\n",
    "print(\"  Bi-LSTM + SA + HAN: R^2 = 0.397 (SOTA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints to Google Drive\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"Syncing checkpoints to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], GDRIVE_CHECKPOINT_PATH, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 8: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nCOMPREHENSIVE EVALUATION\n========================\nThis cell performs exhaustive analysis of model performance including:\n1. Overall metrics (R2, R, MAE, RMSE)\n2. Per-dimension breakdown with all metrics\n3. Prediction distribution analysis\n4. Residual analysis\n5. Best/worst sample analysis\n6. Comparison to baselines\n7. Diagnostic checks\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom collections import defaultdict\n\nprint(\"=\"*80)\nprint(\"COMPREHENSIVE MODEL EVALUATION\")\nprint(\"=\"*80)\n\n# Load best model\nbest_path = checkpoint_callback.best_model_path\nprint(f\"\\nBest checkpoint: {best_path}\")\n\nbest_model = PercePianoVNetModule.load_from_checkpoint(best_path)\nbest_model.eval()\nbest_model.cuda()\n\ndimensions = list(best_model.dimensions)\nn_dims = len(dimensions)\n\n# Collect predictions on ALL splits\nresults = {}\nfor split_name, loader in [('train', train_loader), ('val', val_loader), ('test', test_loader)]:\n    all_preds = []\n    all_targets = []\n    sample_info = []\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(loader):\n            input_features = batch['input_features'].cuda()\n            attention_mask = batch['attention_mask'].cuda()\n            scores = batch['scores'].cuda()\n            num_notes = batch['num_notes']\n            \n            note_locations = {\n                'beat': batch['note_locations_beat'].cuda(),\n                'measure': batch['note_locations_measure'].cuda(),\n                'voice': batch['note_locations_voice'].cuda(),\n            }\n            \n            outputs = best_model(\n                input_features=input_features,\n                note_locations=note_locations,\n                attention_mask=attention_mask,\n            )\n            \n            all_preds.append(outputs['predictions'].cpu().numpy())\n            all_targets.append(scores.cpu().numpy())\n            \n            # Store sample info for analysis\n            for i in range(len(num_notes)):\n                sample_info.append({\n                    'batch_idx': batch_idx,\n                    'sample_idx': i,\n                    'num_notes': num_notes[i].item(),\n                })\n    \n    results[split_name] = {\n        'preds': np.concatenate(all_preds),\n        'targets': np.concatenate(all_targets),\n        'sample_info': sample_info,\n    }\n    print(f\"  {split_name}: {len(results[split_name]['preds'])} samples\")\n\n# Use test set for detailed analysis\npreds = results['test']['preds']\ntargets = results['test']['targets']\nsample_info = results['test']['sample_info']\nn_samples = len(preds)\n\nprint(f\"\\n{'='*80}\")\nprint(\"1. OVERALL METRICS\")\nprint(\"=\"*80)\n\n# Calculate overall metrics\noverall_r2 = r2_score(targets, preds)\noverall_r2_per_sample = r2_score(targets, preds, multioutput='raw_values')\noverall_mae = mean_absolute_error(targets, preds)\noverall_rmse = np.sqrt(mean_squared_error(targets, preds))\n\n# Flatten for correlation\nflat_preds = preds.flatten()\nflat_targets = targets.flatten()\noverall_pearson_r, overall_pearson_p = stats.pearsonr(flat_targets, flat_preds)\noverall_spearman_r, overall_spearman_p = stats.spearmanr(flat_targets, flat_preds)\n\nprint(f\"\\n  Test Set ({n_samples} samples, {n_dims} dimensions)\")\nprint(f\"  {'-'*50}\")\nprint(f\"  R-squared (R2):        {overall_r2:+.4f}\")\nprint(f\"  Pearson R:             {overall_pearson_r:+.4f}  (p={overall_pearson_p:.2e})\")\nprint(f\"  Spearman R:            {overall_spearman_r:+.4f}  (p={overall_spearman_p:.2e})\")\nprint(f\"  MAE:                   {overall_mae:.4f}\")\nprint(f\"  RMSE:                  {overall_rmse:.4f}\")\n\n# Interpretation\nprint(f\"\\n  Interpretation:\")\nif overall_r2 >= 0.35:\n    print(f\"    [EXCELLENT] R2 >= 0.35 matches published SOTA\")\nelif overall_r2 >= 0.25:\n    print(f\"    [GOOD] R2 >= 0.25 is usable for pseudo-labeling\")\nelif overall_r2 >= 0.10:\n    print(f\"    [FAIR] R2 >= 0.10 shows some learning, needs improvement\")\nelif overall_r2 >= 0:\n    print(f\"    [POOR] R2 > 0 but barely better than mean prediction\")\nelse:\n    print(f\"    [FAILED] R2 < 0 means model is WORSE than predicting the mean!\")\n    print(f\"    This indicates a fundamental problem with data or architecture.\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"2. PER-DIMENSION DETAILED METRICS\")\nprint(\"=\"*80)\n\n# Calculate per-dimension metrics\ndim_metrics = []\nfor i, dim in enumerate(dimensions):\n    y_true = targets[:, i]\n    y_pred = preds[:, i]\n    \n    r2 = r2_score(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    pearson_r, pearson_p = stats.pearsonr(y_true, y_pred)\n    spearman_r, spearman_p = stats.spearmanr(y_true, y_pred)\n    \n    # Prediction statistics\n    pred_mean = y_pred.mean()\n    pred_std = y_pred.std()\n    target_mean = y_true.mean()\n    target_std = y_true.std()\n    \n    # Residual analysis\n    residuals = y_true - y_pred\n    residual_mean = residuals.mean()\n    residual_std = residuals.std()\n    \n    dim_metrics.append({\n        'dim': dim,\n        'r2': r2,\n        'r': pearson_r,\n        'r_p': pearson_p,\n        'spearman': spearman_r,\n        'mae': mae,\n        'rmse': rmse,\n        'pred_mean': pred_mean,\n        'pred_std': pred_std,\n        'target_mean': target_mean,\n        'target_std': target_std,\n        'residual_mean': residual_mean,\n        'residual_std': residual_std,\n    })\n\n# Sort by R2\ndim_metrics.sort(key=lambda x: x['r2'], reverse=True)\n\n# Print detailed table\nprint(f\"\\n  {'Dimension':<22} {'R2':>8} {'R':>8} {'MAE':>8} {'RMSE':>8} {'Status':<12}\")\nprint(f\"  {'-'*22} {'-'*8} {'-'*8} {'-'*8} {'-'*8} {'-'*12}\")\n\nfor m in dim_metrics:\n    # Status indicator\n    if m['r2'] >= 0.3:\n        status = \"[GOOD]\"\n    elif m['r2'] >= 0.1:\n        status = \"[OK]\"\n    elif m['r2'] >= 0:\n        status = \"[WEAK]\"\n    else:\n        status = \"[FAILED]\"\n    \n    print(f\"  {m['dim']:<22} {m['r2']:>+8.4f} {m['r']:>+8.4f} {m['mae']:>8.4f} {m['rmse']:>8.4f} {status:<12}\")\n\n# Summary statistics\npositive_r2 = sum(1 for m in dim_metrics if m['r2'] > 0)\nstrong_r2 = sum(1 for m in dim_metrics if m['r2'] >= 0.2)\nnegative_r2 = sum(1 for m in dim_metrics if m['r2'] < 0)\n\nprint(f\"\\n  Summary:\")\nprint(f\"    Positive R2 (> 0):    {positive_r2}/{n_dims}\")\nprint(f\"    Strong R2 (>= 0.2):   {strong_r2}/{n_dims}\")\nprint(f\"    Negative R2 (< 0):    {negative_r2}/{n_dims}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"3. PREDICTION DISTRIBUTION ANALYSIS\")\nprint(\"=\"*80)\n\nprint(f\"\\n  Checking for prediction collapse or bias...\")\nprint(f\"\\n  {'Dimension':<22} {'Pred Mean':>10} {'Pred Std':>10} {'Tgt Mean':>10} {'Tgt Std':>10} {'Issue':<15}\")\nprint(f\"  {'-'*22} {'-'*10} {'-'*10} {'-'*10} {'-'*10} {'-'*15}\")\n\ncollapsed_dims = []\nbiased_dims = []\n\nfor m in dim_metrics:\n    issues = []\n    \n    # Check for collapsed predictions (very low std)\n    if m['pred_std'] < 0.05:\n        issues.append(\"COLLAPSED\")\n        collapsed_dims.append(m['dim'])\n    \n    # Check for systematic bias (mean shift > 0.1)\n    bias = abs(m['pred_mean'] - m['target_mean'])\n    if bias > 0.1:\n        issues.append(f\"BIAS={bias:.2f}\")\n        biased_dims.append(m['dim'])\n    \n    # Check for under-variation (pred_std << target_std)\n    if m['pred_std'] < m['target_std'] * 0.5:\n        issues.append(\"LOW_VAR\")\n    \n    issue_str = \", \".join(issues) if issues else \"OK\"\n    \n    print(f\"  {m['dim']:<22} {m['pred_mean']:>10.4f} {m['pred_std']:>10.4f} {m['target_mean']:>10.4f} {m['target_std']:>10.4f} {issue_str:<15}\")\n\nif collapsed_dims:\n    print(f\"\\n  WARNING: {len(collapsed_dims)} dimensions have collapsed predictions!\")\n    print(f\"    Collapsed: {collapsed_dims}\")\nif biased_dims:\n    print(f\"\\n  WARNING: {len(biased_dims)} dimensions show systematic bias!\")\n    print(f\"    Biased: {biased_dims}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"4. RESIDUAL ANALYSIS\")\nprint(\"=\"*80)\n\nprint(f\"\\n  {'Dimension':<22} {'Residual Mean':>14} {'Residual Std':>14} {'Skewness':>10} {'Kurtosis':>10}\")\nprint(f\"  {'-'*22} {'-'*14} {'-'*14} {'-'*10} {'-'*10}\")\n\nfor m in dim_metrics:\n    y_true = targets[:, dimensions.index(m['dim'])]\n    y_pred = preds[:, dimensions.index(m['dim'])]\n    residuals = y_true - y_pred\n    \n    skewness = stats.skew(residuals)\n    kurtosis = stats.kurtosis(residuals)\n    \n    print(f\"  {m['dim']:<22} {m['residual_mean']:>+14.4f} {m['residual_std']:>14.4f} {skewness:>+10.2f} {kurtosis:>+10.2f}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"5. CROSS-SPLIT COMPARISON\")\nprint(\"=\"*80)\n\nprint(f\"\\n  Checking for overfitting (train >> test) or data issues...\")\nprint(f\"\\n  {'Split':<10} {'R2':>10} {'MAE':>10} {'RMSE':>10} {'Samples':>10}\")\nprint(f\"  {'-'*10} {'-'*10} {'-'*10} {'-'*10} {'-'*10}\")\n\nsplit_metrics = {}\nfor split_name in ['train', 'val', 'test']:\n    p = results[split_name]['preds']\n    t = results[split_name]['targets']\n    \n    r2 = r2_score(t, p)\n    mae = mean_absolute_error(t, p)\n    rmse = np.sqrt(mean_squared_error(t, p))\n    \n    split_metrics[split_name] = {'r2': r2, 'mae': mae, 'rmse': rmse, 'n': len(p)}\n    \n    print(f\"  {split_name:<10} {r2:>+10.4f} {mae:>10.4f} {rmse:>10.4f} {len(p):>10}\")\n\n# Check for overfitting\ntrain_r2 = split_metrics['train']['r2']\ntest_r2 = split_metrics['test']['r2']\noverfit_gap = train_r2 - test_r2\n\nprint(f\"\\n  Overfitting analysis:\")\nprint(f\"    Train-Test R2 gap: {overfit_gap:+.4f}\")\nif overfit_gap > 0.2:\n    print(f\"    [WARNING] Large gap suggests overfitting!\")\nelif overfit_gap > 0.1:\n    print(f\"    [CAUTION] Moderate gap, some overfitting present\")\nelse:\n    print(f\"    [OK] Gap is acceptable\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"6. COMPARISON TO BASELINES\")\nprint(\"=\"*80)\n\nbaselines = {\n    'Mean Prediction': 0.0,\n    'Random': -0.5,\n    'Bi-LSTM (published)': 0.185,\n    'MidiBERT (published)': 0.313,\n    'HAN SOTA (published)': 0.397,\n}\n\nprint(f\"\\n  {'Model':<30} {'R2':>10} {'vs Ours':>12}\")\nprint(f\"  {'-'*30} {'-'*10} {'-'*12}\")\n\nfor name, r2 in sorted(baselines.items(), key=lambda x: x[1]):\n    diff = overall_r2 - r2\n    diff_str = f\"{diff:+.4f}\" if diff != 0 else \"---\"\n    marker = \" <-- US\" if name == 'Mean Prediction' and overall_r2 < 0.05 else \"\"\n    print(f\"  {name:<30} {r2:>+10.4f} {diff_str:>12}{marker}\")\n\nprint(f\"  {'-'*30} {'-'*10} {'-'*12}\")\nprint(f\"  {'Our Model':<30} {overall_r2:>+10.4f} {'---':>12}\")\n\n# Determine rank\nour_rank = sum(1 for _, r2 in baselines.items() if r2 > overall_r2) + 1\nprint(f\"\\n  Our model ranks #{our_rank} out of {len(baselines) + 1} models\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"7. WORST AND BEST SAMPLES\")\nprint(\"=\"*80)\n\n# Calculate per-sample error\nsample_errors = np.mean(np.abs(targets - preds), axis=1)\nsample_r2s = [r2_score(targets[i], preds[i]) for i in range(n_samples)]\n\n# Worst samples\nworst_indices = np.argsort(sample_errors)[-5:][::-1]\nprint(f\"\\n  WORST 5 SAMPLES (highest MAE):\")\nprint(f\"  {'Index':<8} {'MAE':>10} {'Num Notes':>12}\")\nprint(f\"  {'-'*8} {'-'*10} {'-'*12}\")\nfor idx in worst_indices:\n    info = sample_info[idx]\n    print(f\"  {idx:<8} {sample_errors[idx]:>10.4f} {info['num_notes']:>12}\")\n\n# Best samples\nbest_indices = np.argsort(sample_errors)[:5]\nprint(f\"\\n  BEST 5 SAMPLES (lowest MAE):\")\nprint(f\"  {'Index':<8} {'MAE':>10} {'Num Notes':>12}\")\nprint(f\"  {'-'*8} {'-'*10} {'-'*12}\")\nfor idx in best_indices:\n    info = sample_info[idx]\n    print(f\"  {idx:<8} {sample_errors[idx]:>10.4f} {info['num_notes']:>12}\")\n\n# Correlation between num_notes and error\nnum_notes_arr = np.array([info['num_notes'] for info in sample_info])\ncorr, p_val = stats.pearsonr(num_notes_arr, sample_errors)\nprint(f\"\\n  Correlation (num_notes vs error): r={corr:+.3f} (p={p_val:.3f})\")\nif abs(corr) > 0.3:\n    if corr > 0:\n        print(f\"    [WARNING] Longer pieces have higher errors - model struggles with length\")\n    else:\n        print(f\"    [INFO] Shorter pieces have higher errors - may need more context\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"8. DIAGNOSTIC SUMMARY\")\nprint(\"=\"*80)\n\nissues = []\nwarnings = []\ngood = []\n\n# Check overall performance\nif overall_r2 < 0:\n    issues.append(f\"R2 is negative ({overall_r2:.4f}) - model worse than mean prediction\")\nelif overall_r2 < 0.1:\n    warnings.append(f\"R2 is low ({overall_r2:.4f}) - barely learning\")\nelse:\n    good.append(f\"R2 is acceptable ({overall_r2:.4f})\")\n\n# Check for collapsed dimensions\nif len(collapsed_dims) > 0:\n    issues.append(f\"{len(collapsed_dims)} dimensions have collapsed predictions\")\n\n# Check for negative R2 dimensions\nif negative_r2 > n_dims // 2:\n    issues.append(f\"Majority of dimensions ({negative_r2}/{n_dims}) have negative R2\")\nelif negative_r2 > 0:\n    warnings.append(f\"{negative_r2} dimensions have negative R2\")\n\n# Check overfitting\nif overfit_gap > 0.2:\n    warnings.append(f\"Significant overfitting (train-test gap: {overfit_gap:.3f})\")\n\n# Check validation set size\nval_size = len(results['val']['preds'])\nif val_size < 50:\n    warnings.append(f\"Validation set very small ({val_size} samples) - metrics may be unstable\")\n\nprint(f\"\\n  ISSUES ({len(issues)}):\")\nif issues:\n    for issue in issues:\n        print(f\"    [X] {issue}\")\nelse:\n    print(f\"    None!\")\n\nprint(f\"\\n  WARNINGS ({len(warnings)}):\")\nif warnings:\n    for warning in warnings:\n        print(f\"    [!] {warning}\")\nelse:\n    print(f\"    None!\")\n\nprint(f\"\\n  GOOD ({len(good)}):\")\nif good:\n    for g in good:\n        print(f\"    [+] {g}\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"EVALUATION COMPLETE\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Save teacher model and final sync\nimport torch\nfrom pathlib import Path\n\nteacher_path = Path(CONFIG['checkpoint_dir']) / 'percepiano_teacher.pt'\n\nprint(\"=\"*60)\nprint(\"SAVE TEACHER MODEL\")\nprint(\"=\"*60)\n\nif overall_r2 >= 0.25:\n    torch.save({\n        'state_dict': best_model.state_dict(),\n        'config': {\n            'input_size': CONFIG['input_size'],\n            'hidden_size': CONFIG['hidden_size'],\n            'note_layers': CONFIG['note_layers'],\n            'voice_layers': CONFIG['voice_layers'],\n            'beat_layers': CONFIG['beat_layers'],\n            'measure_layers': CONFIG['measure_layers'],\n            'num_attention_heads': CONFIG['num_attention_heads'],\n            'final_hidden': CONFIG['final_hidden'],\n            'dropout': CONFIG['dropout'],\n        },\n        'dimensions': dimensions,\n        'metrics': {\n            'overall_r2': overall_r2,\n            'overall_mae': overall_mae,\n            'overall_rmse': overall_rmse,\n            'pearson_r': overall_pearson_r,\n            'per_dimension': {m['dim']: {'r2': m['r2'], 'r': m['r'], 'mae': m['mae']} for m in dim_metrics},\n            'split_metrics': split_metrics,\n        },\n    }, teacher_path)\n    \n    print(f\"\\n  Saved teacher model to: {teacher_path}\")\n    print(f\"  Teacher R2: {overall_r2:.4f}\")\n    print(f\"  Teacher MAE: {overall_mae:.4f}\")\n    print(f\"\\n  This model can be used for pseudo-labeling MAESTRO!\")\nelse:\n    print(f\"\\n  R2 = {overall_r2:.4f} is below threshold (0.25) for teacher model.\")\n    print(f\"  Not saving teacher model.\")\n    print(f\"\\n  Consider:\")\n    print(f\"    1. Training for more epochs\")\n    print(f\"    2. Checking data quality\")\n    print(f\"    3. Reviewing the diagnostic summary above\")\n\n# Final sync to Google Drive\nprint(f\"\\n{'='*60}\")\nprint(\"SYNC TO GOOGLE DRIVE\")\nprint(\"=\"*60)\n\nif RCLONE_AVAILABLE:\n    print(f\"\\n  Syncing checkpoints to: {GDRIVE_CHECKPOINT_PATH}\")\n    subprocess.run(\n        ['rclone', 'copy', CONFIG['checkpoint_dir'], GDRIVE_CHECKPOINT_PATH, '--progress'],\n        capture_output=False\n    )\n    print(f\"\\n  Sync complete!\")\n    print(f\"  Remote location: {GDRIVE_CHECKPOINT_PATH}\")\nelse:\n    print(f\"\\n  rclone not available - skipping sync\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}