{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# PercePiano Replica Training (4-Fold Cross-Validation)\n\nTrain the PercePiano replica model using 4-fold piece-based cross-validation,\nmatching the methodology and hyperparameters from the PercePiano paper SOTA.\n\n## Attribution\n\n> **PercePiano: Piano Performance Evaluation Dataset with Multi-level Perceptual Features**  \n> Park, Kim et al.  \n> Nature Scientific Reports 2024  \n> Paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11450231/  \n> GitHub: https://github.com/JonghoKimSNU/PercePiano\n\n## Methodology\n\nFollowing the exact approach from `m2pf_dataset_compositionfold.py`:\n\n- **Piece-based splits**: All performances of the same piece stay in the same fold\n- **Test set**: Select pieces randomly until reaching ~15% of SAMPLES (not pieces)\n- **4-fold CV**: Remaining pieces distributed round-robin across folds\n- **Per-fold normalization**: Stats computed from training folds only\n\n## Hyperparameters (SOTA Configuration - R2 = 0.397)\n\nThese parameters match the published SOTA from `2_run_comp_multilevel_total.sh` and `han_bigger256_concat.yml`:\n\n| Parameter | SOTA Value | Notes |\n|-----------|------------|-------|\n| input_size | 79 | SOTA uses 79 base features (includes section_tempo) |\n| batch_size | 8 | From SOTA training script |\n| learning_rate | 2.5e-5 | From SOTA training script |\n| hidden_size | 256 | HAN encoder dimension |\n| prediction_head | 512->512->19 | From model_m2pf.py (NOT config's final_fc_size) |\n| dropout | 0.2 | Regularization |\n| augment_train | False | SOTA doesn't use key augmentation |\n| max_epochs | 200 | Extended training window |\n| early_stopping_patience | 40 | More patience for convergence |\n| gradient_clip_val | 2.0 | From parser.py |\n| **precision** | **32** | **FP32 (original uses FP32, not mixed precision)** |\n| **max_notes/slice_len** | **5000** | **SOTA slice size for overlapping sampling** |\n\n## Key Fixes Applied (Round 8 - 2025-12-26)\n\n### CRITICAL: Slice Sampling (Round 8)\n\nThe single most impactful discrepancy identified:\n\n| Aspect | Original PercePiano | Previous Implementation | Impact |\n|--------|---------------------|------------------------|--------|\n| Samples/performance | 3-5 overlapping slices | 1 sample | 3-5x less training data |\n| Training samples | ~600-1000 slices | ~200 samples | Critical for learning |\n| Slice regeneration | Each epoch | None | No variation |\n\n**Fix**: Added `make_slicing_indexes_by_measure()` and `SliceRegenerationCallback`.\n\n### Round History\n\n| Round | Changes | Result |\n|-------|---------|--------|\n| 1-5 | Various fixes (precision, attention, init) | R2 stuck around 0 |\n| 6 | Match original architecture (no LayerNorm, 512->512, LR 2.5e-5) | Zero context_vector gradients |\n| 7 | Fix data pipeline (79 features, PackedSequence) | R2 = 0.0017 (prediction collapse) |\n| **8** | **Add SLICE SAMPLING (3-5 slices/sample, epoch regeneration)** | **Pending** |\n\nSee `docs/EXPERIMENT_LOG.md` for full investigation details.\n\n## Expected Results\n\n- Target R2: 0.35-0.40 (matching published SOTA of 0.397)\n- Training time: ~8-12 hours on T4, ~3-5 hours on A100 (all 4 folds)\n- With slice sampling: ~600-1000 training slices (vs ~200 samples before)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rclone\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv and clone repository\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Clone original PercePiano for comparison (needed for data diagnostics)\n",
    "PERCEPIANO_PATH = '/tmp/crescendai/model/data/raw/PercePiano'\n",
    "if not os.path.exists(PERCEPIANO_PATH):\n",
    "    print(\"\\nCloning original PercePiano repository...\")\n",
    "    !git clone https://github.com/JonghoKimSNU/PercePiano.git {PERCEPIANO_PATH}\n",
    "else:\n",
    "    print(f\"\\nPercePiano already present at {PERCEPIANO_PATH}\")\n",
    "\n",
    "# Install dependencies\n",
    "!uv pip install --system -e .\n",
    "!pip install tensorboard rich\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths and Check rclone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport subprocess\nimport shutil\nfrom pathlib import Path\n\n# Paths\nDATA_ROOT = Path('/tmp/percepiano_vnet_84dim')\nCHECKPOINT_ROOT = Path('/tmp/checkpoints/percepiano_kfold')\nLOG_ROOT = Path('/tmp/logs/percepiano_kfold')\nGDRIVE_DATA_PATH = 'gdrive:crescendai_data/percepiano_vnet_84dim'\nGDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/percepiano_kfold'\n\n# Training control\nRESTART_TRAINING = True  # Set to True to clear checkpoints and start fresh\n\nprint(\"=\"*60)\nprint(\"PERCEPIANO REPLICA TRAINING (4-FOLD CV)\")\nprint(\"=\"*60)\n\n# Clear checkpoints if restarting\nif RESTART_TRAINING and CHECKPOINT_ROOT.exists():\n    print(f\"\\nRESTART_TRAINING=True: Clearing checkpoints at {CHECKPOINT_ROOT}\")\n    shutil.rmtree(CHECKPOINT_ROOT)\n    print(\"  Checkpoints cleared!\")\n\nif RESTART_TRAINING and LOG_ROOT.exists():\n    print(f\"RESTART_TRAINING=True: Clearing logs at {LOG_ROOT}\")\n    shutil.rmtree(LOG_ROOT)\n    print(\"  Logs cleared!\")\n\n# Create directories\nCHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\nLOG_ROOT.mkdir(parents=True, exist_ok=True)\nDATA_ROOT.mkdir(parents=True, exist_ok=True)\n\n# Check rclone\nresult = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n\nif 'gdrive:' in result.stdout:\n    print(\"\\nrclone 'gdrive' remote: CONFIGURED\")\n    RCLONE_AVAILABLE = True\nelse:\n    print(\"\\nrclone 'gdrive' remote: NOT CONFIGURED\")\n    print(\"Run 'rclone config' in terminal to set up Google Drive\")\n    RCLONE_AVAILABLE = False\n\nprint(f\"\\nData directory: {DATA_ROOT}\")\nprint(f\"Checkpoint directory: {CHECKPOINT_ROOT}\")\nprint(f\"Log directory: {LOG_ROOT}\")\nprint(f\"\\nRESTART_TRAINING: {RESTART_TRAINING}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Download Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "if not RCLONE_AVAILABLE:\n",
    "    raise RuntimeError(\"rclone not configured. Run 'rclone config' first.\")\n",
    "\n",
    "# Download preprocessed data\n",
    "print(\"Downloading preprocessed VirtuosoNet features from Google Drive...\")\n",
    "subprocess.run(\n",
    "    ['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'],\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "# Verify data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_samples = 0\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = DATA_ROOT / split\n",
    "    if split_dir.exists():\n",
    "        count = len(list(split_dir.glob('*.pkl')))\n",
    "        total_samples += count\n",
    "        print(f\"  {split}: {count} samples\")\n",
    "    else:\n",
    "        print(f\"  {split}: MISSING!\")\n",
    "\n",
    "print(f\"  Total: {total_samples} samples\")\n",
    "\n",
    "stat_file = DATA_ROOT / 'stat.pkl'\n",
    "print(f\"  stat.pkl: {'present' if stat_file.exists() else 'MISSING!'}\")\n",
    "\n",
    "fold_file = DATA_ROOT / 'fold_assignments.json'\n",
    "print(f\"  fold_assignments.json: {'present' if fold_file.exists() else 'will be created'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 4: Create Fold Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.percepiano.data.kfold_split import (\n",
    "    create_piece_based_folds,\n",
    "    save_fold_assignments,\n",
    "    load_fold_assignments,\n",
    "    print_fold_statistics,\n",
    ")\n",
    "\n",
    "FOLD_FILE = DATA_ROOT / 'fold_assignments.json'\n",
    "N_FOLDS = 4\n",
    "TEST_RATIO = 0.15\n",
    "SEED = 42\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FOLD ASSIGNMENT CREATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Force regeneration to use corrected methodology\n",
    "# - Test set: select pieces until ~15% of SAMPLES (PercePiano methodology)\n",
    "# - CV folds: greedy bin-packing for balanced sample counts (improvement over round-robin)\n",
    "FORCE_REGENERATE = True\n",
    "\n",
    "if FOLD_FILE.exists() and not FORCE_REGENERATE:\n",
    "    print(f\"\\nLoading existing fold assignments from {FOLD_FILE}\")\n",
    "    fold_assignments = load_fold_assignments(FOLD_FILE)\n",
    "else:\n",
    "    if FOLD_FILE.exists():\n",
    "        print(f\"\\nRemoving old fold assignments (regenerating with balanced methodology)...\")\n",
    "        FOLD_FILE.unlink()\n",
    "    \n",
    "    print(f\"\\nCreating new {N_FOLDS}-fold piece-based splits...\")\n",
    "    print(\"  Test set: select pieces until ~15% of SAMPLES\")\n",
    "    print(\"  CV folds: greedy bin-packing for balanced sample counts\")\n",
    "    fold_assignments = create_piece_based_folds(\n",
    "        data_dir=DATA_ROOT,\n",
    "        n_folds=N_FOLDS,\n",
    "        test_ratio=TEST_RATIO,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    save_fold_assignments(fold_assignments, FOLD_FILE)\n",
    "\n",
    "# Print statistics\n",
    "print_fold_statistics(fold_assignments, n_folds=N_FOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## Step 5: Training Configuration (SOTA - Round 8)\n\nConfiguration matched to published SOTA (R2 = 0.397) with all architecture fixes:\n\n**SOTA Parameters:**\n- `input_size = 79` (includes section_tempo, matches original)\n- `learning_rate = 2.5e-5` (from training script)\n- `batch_size = 8`\n- `prediction_head = 512->512->19` (from model_m2pf.py, NOT config)\n- `augment_train = False`\n\n**Round 8 CRITICAL Fix (2025-12-26): SLICE SAMPLING**\n\n| Fix | Description |\n|-----|-------------|\n| max_notes | **5000** (matches SOTA slice size) |\n| slice_len | **5000** (overlapping slices per performance) |\n| SliceRegenerationCallback | Regenerates slices each epoch |\n| Training samples | ~600-1000 slices (was ~200 samples) |\n\nThe original PercePiano creates 3-5 overlapping slices per performance and regenerates\nthem each epoch. This was the critical missing piece.\n\nSee `docs/EXPERIMENT_LOG.md` and `docs/PERCEPIANO_SOTA_REFERENCE.md` for full details."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "import torch\ntorch.set_float32_matmul_precision('medium')\n\n# Enable better CUDA error reporting\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# PercePiano SOTA Configuration (Round 8 - Slice Sampling)\n# Hyperparameters matched EXACTLY to published SOTA (R2 = 0.397)\n#\n# Sources:\n#   - Training script: 2_run_comp_multilevel_total.sh\n#   - Config: han_bigger256_concat.yml\n#   - Model code: model_m2pf.py (for prediction head size)\n#   - Paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11450231/\n#\n# ROUND 8 FIXES (2025-12-26):\n#   - Added SLICE SAMPLING (critical for SOTA)\n#   - max_notes: 5000 (matches original slice_len)\n#   - slice_len: 5000 (3-5 overlapping slices per performance)\n#   - SliceRegenerationCallback (regenerates slices each epoch)\n#   - Increases effective training samples from ~200 to ~600-1000\n#\n# See docs/EXPERIMENT_LOG.md for full history\n\nCONFIG = {\n    # K-Fold settings\n    'n_folds': N_FOLDS,\n    'test_ratio': TEST_RATIO,\n    \n    # Data\n    'data_dir': str(DATA_ROOT),\n    'checkpoint_dir': str(CHECKPOINT_ROOT),\n    'log_dir': str(LOG_ROOT),\n    \n    # Model input (79 base features, matches original PercePiano)\n    'input_size': 79,\n    \n    # HAN Architecture (han_bigger256_concat.yml)\n    'hidden_size': 256,\n    'note_layers': 2,\n    'voice_layers': 2,\n    'beat_layers': 2,\n    'measure_layers': 1,\n    'num_attention_heads': 8,\n    # NOTE: Prediction head is 512->512->19, controlled by model code\n    # The config's final_fc_size=128 is for DECODER (not used)\n    \n    # Training (SOTA values from 2_run_comp_multilevel_total.sh)\n    'learning_rate': 2.5e-5,  # SOTA: 2.5e-5 (NOT 5e-5!)\n    'weight_decay': 1e-5,\n    'dropout': 0.2,\n    'batch_size': 8,\n    'max_epochs': 200,\n    'early_stopping_patience': 40,\n    'gradient_clip_val': 2.0,\n    \n    # CRITICAL: Use FP32 precision (original PercePiano uses FP32)\n    'precision': '32',\n    \n    # SOTA Slice Sampling (CRITICAL - Round 8)\n    # Original PercePiano creates 3-5 overlapping slices per performance\n    # and regenerates them each epoch for data augmentation\n    'max_notes': 5000,    # SOTA: 5000 notes per slice\n    'slice_len': 5000,    # SOTA: 5000 (same as max_notes)\n    'num_workers': 4,\n    'augment_train': False,\n}\n\nprint(\"=\"*60)\nprint(\"TRAINING CONFIGURATION (SOTA - ROUND 8: SLICE SAMPLING)\")\nprint(\"=\"*60)\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")\n\n# Print training dynamics info\n# With slice sampling, we get ~3-5x more samples\nestimated_slices = 744 * 3.5  # ~2600 slices from ~744 samples\nsteps_per_epoch = int(estimated_slices // CONFIG['batch_size'])\nlr_decay_epoch = 3000 // steps_per_epoch if steps_per_epoch > 0 else 999\nprint(f\"\\nTraining dynamics (with slice sampling):\")\nprint(f\"  Estimated slices: ~{int(estimated_slices)} (3-5x original samples)\")\nprint(f\"  Steps per epoch: ~{steps_per_epoch}\")\nprint(f\"  LR decay (StepLR) at epoch: ~{lr_decay_epoch}\")\nprint(f\"  LR after decay: {CONFIG['learning_rate'] * 0.98:.2e}\")\n\nprint(f\"\\nRound 8 fixes applied:\")\nprint(f\"  - SLICE SAMPLING (critical for SOTA)\")\nprint(f\"  - max_notes/slice_len: 5000 (matches original)\")\nprint(f\"  - SliceRegenerationCallback (slices regenerated each epoch)\")\nprint(f\"  - Expected training samples: ~600-1000 (was ~200)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 6: Initialize K-Fold Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.percepiano.training.kfold_trainer import KFoldTrainer\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Create K-Fold trainer\n",
    "kfold_trainer = KFoldTrainer(\n",
    "    config=CONFIG,\n",
    "    fold_assignments=fold_assignments,\n",
    "    data_dir=DATA_ROOT,\n",
    "    checkpoint_dir=CHECKPOINT_ROOT,\n",
    "    log_dir=LOG_ROOT,\n",
    "    n_folds=N_FOLDS,\n",
    ")\n",
    "\n",
    "print(\"K-Fold Trainer initialized\")\n",
    "print(f\"  Folds: {N_FOLDS}\")\n",
    "print(f\"  Checkpoints: {CHECKPOINT_ROOT}\")\n",
    "print(f\"  Logs: {LOG_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 7: Train All Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STARTING 4-FOLD CROSS-VALIDATION TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPercePiano SOTA baselines:\")\n",
    "print(\"  Bi-LSTM: R2 = 0.185\")\n",
    "print(\"  MidiBERT: R2 = 0.313\")\n",
    "print(\"  Bi-LSTM + SA + HAN: R2 = 0.397 (SOTA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train folds in order: 1, 2, 3, 0\n",
    "# This trains the balanced folds first (train > val), leaving the\n",
    "# imbalanced fold 0 (train=240, val=807) for last\n",
    "FOLD_ORDER = [1, 2, 3, 0]\n",
    "\n",
    "# Train all folds\n",
    "aggregate_metrics = kfold_trainer.train_all_folds(verbose=True, fold_order=FOLD_ORDER)\n",
    "\n",
    "# Save results\n",
    "kfold_trainer.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints to Google Drive after training\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"Syncing checkpoints to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', str(CHECKPOINT_ROOT), GDRIVE_CHECKPOINT_PATH, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 8: Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all fold models on held-out test set\n",
    "test_results = kfold_trainer.evaluate_on_test(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 9: Per-Fold Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.percepiano.models.percepiano_replica import PERCEPIANO_DIMENSIONS\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PER-FOLD VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Fold':<6} {'Val R2':>10} {'Val Pearson':>12} {'Val MAE':>10} {'Val RMSE':>10} {'Epochs':>8} {'Time (s)':>10}\")\n",
    "print(f\"{'-'*6} {'-'*10} {'-'*12} {'-'*10} {'-'*10} {'-'*8} {'-'*10}\")\n",
    "\n",
    "for m in kfold_trainer.fold_metrics:\n",
    "    print(f\"{m.fold_id:<6} {m.val_r2:>+10.4f} {m.val_pearson:>+12.4f} {m.val_mae:>10.4f} {m.val_rmse:>10.4f} {m.epochs_trained:>8} {m.training_time_seconds:>10.1f}\")\n",
    "\n",
    "print(f\"{'-'*6} {'-'*10} {'-'*12} {'-'*10} {'-'*10} {'-'*8} {'-'*10}\")\n",
    "print(f\"{'Mean':<6} {aggregate_metrics.mean_r2:>+10.4f} {aggregate_metrics.mean_pearson:>+12.4f} {aggregate_metrics.mean_mae:>10.4f} {aggregate_metrics.mean_rmse:>10.4f}\")\n",
    "print(f\"{'Std':<6} {aggregate_metrics.std_r2:>+10.4f} {aggregate_metrics.std_pearson:>+12.4f} {aggregate_metrics.std_mae:>10.4f} {aggregate_metrics.std_rmse:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Step 10: Per-Dimension Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PER-DIMENSION R2 (Mean +/- Std across folds)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort dimensions by mean R2\n",
    "sorted_dims = sorted(\n",
    "    aggregate_metrics.per_dim_mean_r2.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'Dimension':<25} {'Mean R2':>10} {'Std R2':>10} {'Status':<12}\")\n",
    "print(f\"{'-'*25} {'-'*10} {'-'*10} {'-'*12}\")\n",
    "\n",
    "for dim, mean_r2 in sorted_dims:\n",
    "    std_r2 = aggregate_metrics.per_dim_std_r2[dim]\n",
    "    \n",
    "    if mean_r2 >= 0.3:\n",
    "        status = \"[GOOD]\"\n",
    "    elif mean_r2 >= 0.1:\n",
    "        status = \"[OK]\"\n",
    "    elif mean_r2 >= 0:\n",
    "        status = \"[WEAK]\"\n",
    "    else:\n",
    "        status = \"[FAILED]\"\n",
    "    \n",
    "    print(f\"{dim:<25} {mean_r2:>+10.4f} {std_r2:>10.4f} {status:<12}\")\n",
    "\n",
    "# Summary\n",
    "positive = sum(1 for d, r in sorted_dims if r > 0)\n",
    "strong = sum(1 for d, r in sorted_dims if r >= 0.2)\n",
    "n_dims = len(sorted_dims)\n",
    "\n",
    "print(f\"\\nSummary: {positive}/{n_dims} positive R2, {strong}/{n_dims} strong (>= 0.2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Step 11: Final Summary and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cross-validation results\n",
    "print(f\"\\n4-Fold Cross-Validation Results:\")\n",
    "print(f\"  Mean R2:       {aggregate_metrics.mean_r2:.4f} +/- {aggregate_metrics.std_r2:.4f}\")\n",
    "print(f\"  Mean Pearson:  {aggregate_metrics.mean_pearson:.4f} +/- {aggregate_metrics.std_pearson:.4f}\")\n",
    "print(f\"  Mean Spearman: {aggregate_metrics.mean_spearman:.4f} +/- {aggregate_metrics.std_spearman:.4f}\")\n",
    "print(f\"  Mean MAE:      {aggregate_metrics.mean_mae:.4f} +/- {aggregate_metrics.std_mae:.4f}\")\n",
    "print(f\"  Mean RMSE:     {aggregate_metrics.mean_rmse:.4f} +/- {aggregate_metrics.std_rmse:.4f}\")\n",
    "print(f\"  Training time: {aggregate_metrics.total_training_time/60:.1f} minutes\")\n",
    "\n",
    "# Test set results\n",
    "print(f\"\\nTest Set (Ensemble of 4 models):\")\n",
    "print(f\"  R2:       {test_results['ensemble']['r2']:.4f}\")\n",
    "print(f\"  Pearson:  {test_results['ensemble']['pearson']:.4f}\")\n",
    "print(f\"  Spearman: {test_results['ensemble']['spearman']:.4f}\")\n",
    "print(f\"  MAE:      {test_results['ensemble']['mae']:.4f}\")\n",
    "print(f\"  RMSE:     {test_results['ensemble']['rmse']:.4f}\")\n",
    "\n",
    "# Comparison to baselines\n",
    "print(f\"\\nComparison to PercePiano baselines:\")\n",
    "print(f\"  Bi-LSTM:      R2 = 0.185\")\n",
    "print(f\"  MidiBERT:     R2 = 0.313\")\n",
    "print(f\"  HAN SOTA:     R2 = 0.397\")\n",
    "print(f\"  Ours (CV):    R2 = {aggregate_metrics.mean_r2:.3f} +/- {aggregate_metrics.std_r2:.3f}\")\n",
    "print(f\"  Ours (Test):  R2 = {test_results['ensemble']['r2']:.3f}\")\n",
    "\n",
    "# Interpretation\n",
    "cv_r2 = aggregate_metrics.mean_r2\n",
    "test_r2 = test_results['ensemble']['r2']\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "if cv_r2 >= 0.35:\n",
    "    print(f\"  [EXCELLENT] CV R2 >= 0.35 matches published SOTA!\")\n",
    "elif cv_r2 >= 0.25:\n",
    "    print(f\"  [GOOD] CV R2 >= 0.25 is usable for pseudo-labeling\")\n",
    "elif cv_r2 >= 0.10:\n",
    "    print(f\"  [FAIR] CV R2 >= 0.10 shows learning, needs improvement\")\n",
    "else:\n",
    "    print(f\"  [NEEDS WORK] CV R2 < 0.10, significant improvement needed\")\n",
    "\n",
    "# Save ensemble model if good enough\n",
    "if cv_r2 >= 0.25:\n",
    "    print(f\"\\nModel qualifies for pseudo-labeling MAESTRO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final sync to Google Drive\n",
    "print(\"=\"*60)\n",
    "print(\"SYNC TO GOOGLE DRIVE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(f\"\\nSyncing all checkpoints and results...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', str(CHECKPOINT_ROOT), GDRIVE_CHECKPOINT_PATH, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    \n",
    "    # Also sync fold assignments back to data directory\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', str(FOLD_FILE), GDRIVE_DATA_PATH, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSync complete!\")\n",
    "    print(f\"  Checkpoints: {GDRIVE_CHECKPOINT_PATH}\")\n",
    "    print(f\"  Fold assignments: {GDRIVE_DATA_PATH}\")\n",
    "else:\n",
    "    print(f\"\\nrclone not available - skipping sync\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}