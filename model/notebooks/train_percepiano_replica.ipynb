{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PercePiano Replica Training (4-Fold Cross-Validation)\n",
    "\n",
    "Train the PercePiano replica model using 4-fold piece-based cross-validation,\n",
    "matching the methodology and hyperparameters from the PercePiano paper SOTA.\n",
    "\n",
    "## Attribution\n",
    "\n",
    "> **PercePiano: Piano Performance Evaluation Dataset with Multi-level Perceptual Features**  \n",
    "> Park, Kim et al.  \n",
    "> Nature Scientific Reports 2024  \n",
    "> Paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11450231/  \n",
    "> GitHub: https://github.com/JonghoKimSNU/PercePiano\n",
    "\n",
    "## Methodology\n",
    "\n",
    "Following the exact approach from `m2pf_dataset_compositionfold.py`:\n",
    "\n",
    "- **Piece-based splits**: All performances of the same piece stay in the same fold\n",
    "- **Test set**: Select pieces randomly until reaching ~15% of SAMPLES (not pieces)\n",
    "- **4-fold CV**: Remaining pieces distributed round-robin across folds\n",
    "- **Per-fold normalization**: Stats computed from training folds only\n",
    "\n",
    "## Hyperparameters (SOTA Configuration - R2 = 0.397)\n",
    "\n",
    "These parameters match the published SOTA from `2_run_comp_multilevel_total.sh` and `han_bigger256_concat.yml`:\n",
    "\n",
    "| Parameter | SOTA Value | Notes |\n",
    "|-----------|------------|-------|\n",
    "| input_size | 79 | SOTA uses 79 base features (includes section_tempo) |\n",
    "| batch_size | 8 | From SOTA training script |\n",
    "| learning_rate | 2.5e-5 | From SOTA training script |\n",
    "| hidden_size | 256 | HAN encoder dimension |\n",
    "| prediction_head | 512->512->19 | From model_m2pf.py (NOT config's final_fc_size) |\n",
    "| dropout | 0.2 | Regularization |\n",
    "| augment_train | False | SOTA doesn't use key augmentation |\n",
    "| max_epochs | 200 | Extended training window |\n",
    "| early_stopping_patience | 40 | More patience for convergence |\n",
    "| gradient_clip_val | 2.0 | From parser.py |\n",
    "| **precision** | **32** | **FP32 (original uses FP32, not mixed precision)** |\n",
    "| **max_notes/slice_len** | **5000** | **SOTA slice size for overlapping sampling** |\n",
    "\n",
    "## Key Fixes Applied (Round 8 - 2025-12-26)\n",
    "\n",
    "### CRITICAL: Slice Sampling (Round 8)\n",
    "\n",
    "The single most impactful discrepancy identified:\n",
    "\n",
    "| Aspect | Original PercePiano | Previous Implementation | Impact |\n",
    "|--------|---------------------|------------------------|--------|\n",
    "| Samples/performance | 3-5 overlapping slices | 1 sample | 3-5x less training data |\n",
    "| Training samples | ~600-1000 slices | ~200 samples | Critical for learning |\n",
    "| Slice regeneration | Each epoch | None | No variation |\n",
    "\n",
    "**Fix**: Added `make_slicing_indexes_by_measure()` and `SliceRegenerationCallback`.\n",
    "\n",
    "### Round History\n",
    "\n",
    "| Round | Changes | Result |\n",
    "|-------|---------|--------|\n",
    "| 1-5 | Various fixes (precision, attention, init) | R2 stuck around 0 |\n",
    "| 6 | Match original architecture (no LayerNorm, 512->512, LR 2.5e-5) | Zero context_vector gradients |\n",
    "| 7 | Fix data pipeline (79 features, PackedSequence) | R2 = 0.0017 (prediction collapse) |\n",
    "| **8** | **Add SLICE SAMPLING (3-5 slices/sample, epoch regeneration)** | **Pending** |\n",
    "\n",
    "See `docs/EXPERIMENT_LOG.md` for full investigation details.\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- Target R2: 0.35-0.40 (matching published SOTA of 0.397)\n",
    "- Training time: ~8-12 hours on T4, ~3-5 hours on A100 (all 4 folds)\n",
    "- With slice sampling: ~600-1000 training slices (vs ~200 samples before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rclone\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv and clone repository\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Clone original PercePiano for comparison (needed for data diagnostics)\n",
    "PERCEPIANO_PATH = '/tmp/crescendai/model/data/raw/PercePiano'\n",
    "if not os.path.exists(PERCEPIANO_PATH):\n",
    "    print(\"\\nCloning original PercePiano repository...\")\n",
    "    !git clone https://github.com/JonghoKimSNU/PercePiano.git {PERCEPIANO_PATH}\n",
    "else:\n",
    "    print(f\"\\nPercePiano already present at {PERCEPIANO_PATH}\")\n",
    "\n",
    "# Install dependencies\n",
    "!uv pip install --system -e .\n",
    "!pip install tensorboard rich\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths and Check rclone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = Path('/tmp/percepiano_vnet_84dim')\n",
    "CHECKPOINT_ROOT = Path('/tmp/checkpoints/percepiano_kfold')\n",
    "LOG_ROOT = Path('/tmp/logs/percepiano_kfold')\n",
    "GDRIVE_DATA_PATH = 'gdrive:crescendai_data/percepiano_vnet_84dim'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/percepiano_kfold'\n",
    "\n",
    "# Training control\n",
    "RESTART_TRAINING = True  # Set to True to clear checkpoints and start fresh\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PERCEPIANO REPLICA TRAINING (4-FOLD CV)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear checkpoints if restarting\n",
    "if RESTART_TRAINING and CHECKPOINT_ROOT.exists():\n",
    "    print(f\"\\nRESTART_TRAINING=True: Clearing checkpoints at {CHECKPOINT_ROOT}\")\n",
    "    shutil.rmtree(CHECKPOINT_ROOT)\n",
    "    print(\"  Checkpoints cleared!\")\n",
    "\n",
    "if RESTART_TRAINING and LOG_ROOT.exists():\n",
    "    print(f\"RESTART_TRAINING=True: Clearing logs at {LOG_ROOT}\")\n",
    "    shutil.rmtree(LOG_ROOT)\n",
    "    print(\"  Logs cleared!\")\n",
    "\n",
    "# Create directories\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "LOG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check rclone\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "\n",
    "if 'gdrive:' in result.stdout:\n",
    "    print(\"\\nrclone 'gdrive' remote: CONFIGURED\")\n",
    "    RCLONE_AVAILABLE = True\n",
    "else:\n",
    "    print(\"\\nrclone 'gdrive' remote: NOT CONFIGURED\")\n",
    "    print(\"Run 'rclone config' in terminal to set up Google Drive\")\n",
    "    RCLONE_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nData directory: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_ROOT}\")\n",
    "print(f\"Log directory: {LOG_ROOT}\")\n",
    "print(f\"\\nRESTART_TRAINING: {RESTART_TRAINING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Download Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "if not RCLONE_AVAILABLE:\n",
    "    raise RuntimeError(\"rclone not configured. Run 'rclone config' first.\")\n",
    "\n",
    "# Download preprocessed data\n",
    "print(\"Downloading preprocessed VirtuosoNet features from Google Drive...\")\n",
    "subprocess.run(\n",
    "    ['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'],\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "# Verify data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_samples = 0\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = DATA_ROOT / split\n",
    "    if split_dir.exists():\n",
    "        count = len(list(split_dir.glob('*.pkl')))\n",
    "        total_samples += count\n",
    "        print(f\"  {split}: {count} samples\")\n",
    "    else:\n",
    "        print(f\"  {split}: MISSING!\")\n",
    "\n",
    "print(f\"  Total: {total_samples} samples\")\n",
    "\n",
    "stat_file = DATA_ROOT / 'stat.pkl'\n",
    "print(f\"  stat.pkl: {'present' if stat_file.exists() else 'MISSING!'}\")\n",
    "\n",
    "fold_file = DATA_ROOT / 'fold_assignments.json'\n",
    "print(f\"  fold_assignments.json: {'present' if fold_file.exists() else 'will be created'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 4: Create Fold Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.percepiano.data.kfold_split import (\n",
    "    create_piece_based_folds,\n",
    "    save_fold_assignments,\n",
    "    load_fold_assignments,\n",
    "    print_fold_statistics,\n",
    ")\n",
    "\n",
    "FOLD_FILE = DATA_ROOT / 'fold_assignments.json'\n",
    "N_FOLDS = 4\n",
    "TEST_RATIO = 0.15\n",
    "SEED = 42\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FOLD ASSIGNMENT CREATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Force regeneration to use corrected methodology\n",
    "# - Test set: select pieces until ~15% of SAMPLES (PercePiano methodology)\n",
    "# - CV folds: greedy bin-packing for balanced sample counts (improvement over round-robin)\n",
    "FORCE_REGENERATE = True\n",
    "\n",
    "if FOLD_FILE.exists() and not FORCE_REGENERATE:\n",
    "    print(f\"\\nLoading existing fold assignments from {FOLD_FILE}\")\n",
    "    fold_assignments = load_fold_assignments(FOLD_FILE)\n",
    "else:\n",
    "    if FOLD_FILE.exists():\n",
    "        print(f\"\\nRemoving old fold assignments (regenerating with balanced methodology)...\")\n",
    "        FOLD_FILE.unlink()\n",
    "    \n",
    "    print(f\"\\nCreating new {N_FOLDS}-fold piece-based splits...\")\n",
    "    print(\"  Test set: select pieces until ~15% of SAMPLES\")\n",
    "    print(\"  CV folds: greedy bin-packing for balanced sample counts\")\n",
    "    fold_assignments = create_piece_based_folds(\n",
    "        data_dir=DATA_ROOT,\n",
    "        n_folds=N_FOLDS,\n",
    "        test_ratio=TEST_RATIO,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    save_fold_assignments(fold_assignments, FOLD_FILE)\n",
    "\n",
    "# Print statistics\n",
    "print_fold_statistics(fold_assignments, n_folds=N_FOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "import torch\ntorch.set_float32_matmul_precision('medium')\n\n# Enable better CUDA error reporting\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# Import model type constants\nfrom src.percepiano.training.kfold_trainer import MODEL_TYPE_HAN, MODEL_TYPE_BASELINE\n\nCONFIG = {\n    # K-Fold settings\n    'n_folds': N_FOLDS,\n    'test_ratio': TEST_RATIO,\n    # Data\n    'data_dir': str(DATA_ROOT),\n    'checkpoint_dir': str(CHECKPOINT_ROOT),\n    'log_dir': str(LOG_ROOT),\n    'input_size': 79,\n    'hidden_size': 256,\n    'note_layers': 2,\n    'voice_layers': 2,\n    'beat_layers': 2,\n    'measure_layers': 1,\n    'num_attention_heads': 8,\n    'learning_rate': 2.5e-5,\n    'weight_decay': 1e-5,\n    'dropout': 0.2,\n    'batch_size': 8,\n    'max_epochs': 200,\n    'early_stopping_patience': 20,\n    'gradient_clip_val': 2.0,\n    'precision': '32',\n    'max_notes': 5000,\n    'slice_len': 5000,\n    'num_workers': 4,\n    'augment_train': False,\n}\n\nprint(\"=\"*60)\nprint(\"TRAINING CONFIGURATION (SOTA - ROUND 13)\")\nprint(\"=\"*60)\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")\n\nprint(f\"\\nModel types available:\")\nprint(f\"  MODEL_TYPE_BASELINE = '{MODEL_TYPE_BASELINE}' (7-layer Bi-LSTM, expected R2 ~0.19)\")\nprint(f\"  MODEL_TYPE_HAN = '{MODEL_TYPE_HAN}' (Hierarchical, expected R2 ~0.40)\")"
  },
  {
   "cell_type": "markdown",
   "id": "u15ydvumlrm",
   "source": "## Step 5b: Pre-Training Data Diagnostics (CRITICAL)\n\nRun this BEFORE training to validate data and detect index issues that can break hierarchy.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vuwtbajzrag",
   "source": "\"\"\"\nPRE-TRAINING DATA DIAGNOSTICS\n\nThis cell validates the data pipeline before training to catch issues that\nwould cause the hierarchical components (beat/measure attention) to fail.\n\nKey checks:\n1. Index format (should start from 1 after densification)\n2. Zero-shifted values (should have no negatives)\n3. Boundary detection (== 1 vs > 0 equivalence)\n4. Slice statistics\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom src.percepiano.data.percepiano_vnet_dataset import (\n    PercePianoKFoldDataset,\n    percepiano_pack_collate,\n)\nfrom src.percepiano.training.diagnostics import analyze_indices\n\nprint(\"=\" * 70)\nprint(\"PRE-TRAINING DATA DIAGNOSTICS\")\nprint(\"=\" * 70)\n\n# Create a test dataset for fold 0\ntest_ds = PercePianoKFoldDataset(\n    data_dir=DATA_ROOT,\n    fold_assignments=fold_assignments,\n    fold_id=0,\n    mode=\"train\",\n    max_notes=CONFIG['max_notes'],\n    slice_len=CONFIG.get('slice_len', CONFIG['max_notes']),\n)\n\n# Create a simple dataloader (no packing for easier analysis)\nsimple_loader = DataLoader(test_ds, batch_size=4, shuffle=False, num_workers=0)\n\n# Get one batch\nbatch = next(iter(simple_loader))\n\nprint(\"\\n[1] BATCH SHAPE ANALYSIS:\")\nprint(f\"  input_features: {batch['input_features'].shape}\")\nprint(f\"  note_locations_beat: {batch['note_locations_beat'].shape}\")\nprint(f\"  note_locations_measure: {batch['note_locations_measure'].shape}\")\nprint(f\"  note_locations_voice: {batch['note_locations_voice'].shape}\")\nprint(f\"  scores: {batch['scores'].shape}\")\nprint(f\"  num_notes: {[batch['num_notes'][i].item() if hasattr(batch['num_notes'][i], 'item') else batch['num_notes'][i] for i in range(4)]}\")\n\nprint(\"\\n[2] INDEX ANALYSIS:\")\nidx_stats = analyze_indices(\n    batch['note_locations_beat'],\n    batch['note_locations_measure'],\n)\n\nprint(f\"  Beat indices: min={idx_stats['beat_min']}, max={idx_stats['beat_max']}\")\nprint(f\"  Measure indices: min={idx_stats['measure_min']}, max={idx_stats['measure_max']}\")\n\n# Check if indices start from 1 (required for hierarchy_utils)\nif idx_stats['beat_min'] == 1:\n    print(f\"  [OK] Beat indices start from 1 (required)\")\nelif idx_stats['beat_min'] == 0:\n    print(f\"  [WARNING] Beat indices start from 0 (should be 1)\")\nelse:\n    print(f\"  [ERROR] Beat indices start from {idx_stats['beat_min']} (unexpected)\")\n\nprint(\"\\n[3] ZERO-SHIFTED INDEX CHECK:\")\nprint(f\"  Beat zero-shifted range: [{idx_stats['beat_zero_shifted_min']}, {idx_stats['beat_zero_shifted_max']}]\")\nprint(f\"  Measure zero-shifted range: [{idx_stats['measure_zero_shifted_min']}, {idx_stats['measure_zero_shifted_max']}]\")\n\nif idx_stats['negative_beat_count'] > 0:\n    print(f\"  [CRITICAL ERROR] {idx_stats['negative_beat_count']} negative zero-shifted beat values!\")\n    print(f\"    This WILL break span_beat_to_note_num!\")\nelse:\n    print(f\"  [OK] No negative zero-shifted values\")\n\nif idx_stats['negative_measure_count'] > 0:\n    print(f\"  [WARNING] {idx_stats['negative_measure_count']} negative zero-shifted measure values\")\n\nprint(\"\\n[4] BOUNDARY DETECTION CHECK:\")\nprint(f\"  diff == 1 count: {idx_stats['diff_equals_1_count']}\")\nprint(f\"  diff > 0 count: {idx_stats['diff_greater_0_count']}\")\nprint(f\"  diff < 0 count: {idx_stats['diff_less_0_count']} (should only be at padding boundary)\")\n\nif idx_stats['diff_equals_1_count'] == idx_stats['diff_greater_0_count']:\n    print(f\"  [OK] == 1 and > 0 are equivalent (indices are sequential)\")\nelse:\n    print(f\"  [WARNING] == 1 ({idx_stats['diff_equals_1_count']}) != > 0 ({idx_stats['diff_greater_0_count']})\")\n    print(f\"    Indices may not be properly densified!\")\n\nif idx_stats['non_sequential_samples'] > 0:\n    print(f\"  [WARNING] {idx_stats['non_sequential_samples']}/4 samples have non-sequential indices\")\n\nprint(\"\\n[5] SLICE STATISTICS:\")\nprint(f\"  Total slices in dataset: {len(test_ds)}\")\nprint(f\"  Underlying samples: {len(test_ds.sample_files)}\")\nprint(f\"  Avg slices per sample: {len(test_ds) / len(test_ds.sample_files):.1f}\")\n\nprint(\"\\n[6] FIRST SAMPLE BEAT INDICES (first 50 values):\")\nfirst_beat = batch['note_locations_beat'][0].numpy()\nfirst_num_notes = batch['num_notes'][0] if isinstance(batch['num_notes'][0], int) else batch['num_notes'][0].item()\nprint(f\"  {first_beat[:min(50, first_num_notes)].tolist()}\")\n\n# Check for unique beats\nunique_beats = np.unique(first_beat[:first_num_notes])\nprint(f\"  Unique beat values: {len(unique_beats)}\")\nprint(f\"  First 10 unique beats: {unique_beats[:10].tolist()}\")\n\nprint(\"\\n[7] INPUT FEATURE STATISTICS:\")\nfeatures = batch['input_features']\nprint(f\"  Overall: mean={features.mean():.4f}, std={features.std():.4f}\")\nprint(f\"  Range: [{features.min():.4f}, {features.max():.4f}]\")\n\n# Check for NaN/Inf\nnan_count = torch.isnan(features).sum().item()\ninf_count = torch.isinf(features).sum().item()\nif nan_count > 0:\n    print(f\"  [ERROR] {nan_count} NaN values detected!\")\nif inf_count > 0:\n    print(f\"  [ERROR] {inf_count} Inf values detected!\")\nif nan_count == 0 and inf_count == 0:\n    print(f\"  [OK] No NaN or Inf values\")\n\nprint(\"\\n[8] LABEL STATISTICS:\")\nscores = batch['scores']\nprint(f\"  Mean per dimension: {scores.mean(dim=0).tolist()[:5]} ... (first 5)\")\nprint(f\"  Std per dimension: {scores.std(dim=0).tolist()[:5]} ... (first 5)\")\nprint(f\"  Range: [{scores.min():.4f}, {scores.max():.4f}]\")\n\nif scores.min() < 0 or scores.max() > 1:\n    print(f\"  [WARNING] Labels outside [0, 1] range!\")\nelse:\n    print(f\"  [OK] Labels in [0, 1] range\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DIAGNOSTIC SUMMARY\")\nprint(\"=\" * 70)\n\nissues = []\nif idx_stats['beat_min'] != 1:\n    issues.append(\"Beat indices don't start from 1\")\nif idx_stats['negative_beat_count'] > 0:\n    issues.append(f\"Negative zero-shifted values ({idx_stats['negative_beat_count']})\")\nif idx_stats['diff_equals_1_count'] != idx_stats['diff_greater_0_count']:\n    issues.append(\"Non-sequential indices detected\")\nif nan_count > 0 or inf_count > 0:\n    issues.append(\"NaN/Inf in features\")\n\nif issues:\n    print(f\"\\n[ISSUES FOUND] {len(issues)} issues that may affect training:\")\n    for i, issue in enumerate(issues, 1):\n        print(f\"  {i}. {issue}\")\n    print(\"\\nConsider fixing these before training!\")\nelse:\n    print(\"\\n[ALL CHECKS PASSED] Data pipeline looks correct!\")\n    print(\"Proceed to training.\")\n\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 6: Initialize K-Fold Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "from src.percepiano.training.kfold_trainer import KFoldTrainer, MODEL_TYPE_HAN, MODEL_TYPE_BASELINE\nimport pytorch_lightning as pl\n\n# Set seed for reproducibility\npl.seed_everything(42, workers=True)\n\n# Create K-Fold trainers for BOTH models\n# This allows us to compare Baseline vs HAN directly\n\nprint(\"=\"*60)\nprint(\"INITIALIZING TRAINERS\")\nprint(\"=\"*60)\n\n# 1. Baseline trainer (7-layer Bi-LSTM)\nprint(\"\\n[1] Bi-LSTM Baseline Trainer:\")\nbaseline_trainer = KFoldTrainer(\n    config=CONFIG,\n    fold_assignments=fold_assignments,\n    data_dir=DATA_ROOT,\n    checkpoint_dir=CHECKPOINT_ROOT,\n    log_dir=LOG_ROOT,\n    n_folds=N_FOLDS,\n    model_type=MODEL_TYPE_BASELINE,\n)\n\n# 2. HAN trainer (Hierarchical)\nprint(\"\\n[2] HAN (Hierarchical) Trainer:\")\nhan_trainer = KFoldTrainer(\n    config=CONFIG,\n    fold_assignments=fold_assignments,\n    data_dir=DATA_ROOT,\n    checkpoint_dir=CHECKPOINT_ROOT,\n    log_dir=LOG_ROOT,\n    n_folds=N_FOLDS,\n    model_type=MODEL_TYPE_HAN,\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Both trainers initialized!\")\nprint(f\"  Baseline checkpoints: {baseline_trainer.checkpoint_dir}\")\nprint(f\"  HAN checkpoints: {han_trainer.checkpoint_dir}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 7: Train All Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nTRAIN BOTH MODELS FOR COMPARISON - ALL 4 FOLDS\n\nThis cell trains:\n1. Bi-LSTM Baseline (7-layer) - Expected R2 ~0.19\n2. HAN (Hierarchical) - Expected R2 ~0.40\n\nThe difference (HAN R2 - Baseline R2) is the TRUE hierarchy gain.\nExpected hierarchy gain: ~+0.21 R2\n\"\"\"\n\nprint(\"=\"*60)\nprint(\"TRAINING BOTH MODELS (ALL 4 FOLDS)\")\nprint(\"=\"*60)\nprint(\"\\nPercePiano SOTA baselines:\")\nprint(\"  Bi-LSTM (VirtuosoNetSingle): R2 = 0.185\")\nprint(\"  HAN (VirtuosoNetMultiLevel): R2 = 0.397\")\nprint(\"  Hierarchy gain: +0.212\")\nprint(\"=\"*60)\n\n# Train all 4 folds for both models\nFOLDS_TO_TRAIN = [1, 2, 3, 4]\n\n# ========================================\n# STEP 1: Train Bi-LSTM Baseline (All 4 Folds)\n# ========================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 1: TRAINING BI-LSTM BASELINE (ALL 4 FOLDS)\")\nprint(\"=\"*60)\nprint(\"Expected R2: ~0.19 (matching original VirtuosoNetSingle)\")\n\nfor fold_id in FOLDS_TO_TRAIN:\n    print(f\"\\n{'='*60}\")\n    print(f\"BASELINE - FOLD {fold_id}/{len(FOLDS_TO_TRAIN)}\")\n    print(f\"{'='*60}\")\n    \n    baseline_metrics = baseline_trainer.train_fold(\n        fold_id=fold_id,\n        verbose=True,\n        resume_from_checkpoint=False,\n    )\n    \n    print(f\"\\n  Fold {fold_id} Val R2: {baseline_metrics.val_r2:+.4f}\")\n\nbaseline_trainer.save_results()\n\n# Compute aggregate baseline metrics\nbaseline_agg = baseline_trainer._compute_aggregate_metrics()\nprint(f\"\\n{'='*60}\")\nprint(f\"BASELINE COMPLETE - Mean R2: {baseline_agg.mean_r2:+.4f} (+/- {baseline_agg.std_r2:.4f})\")\nprint(f\"{'='*60}\")\n\n# ========================================\n# STEP 2: Train HAN (All 4 Folds)\n# ========================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 2: TRAINING HAN (ALL 4 FOLDS)\")\nprint(\"=\"*60)\nprint(\"Expected R2: ~0.40 (matching original VirtuosoNetMultiLevel)\")\n\nfor fold_id in FOLDS_TO_TRAIN:\n    print(f\"\\n{'='*60}\")\n    print(f\"HAN - FOLD {fold_id}/{len(FOLDS_TO_TRAIN)}\")\n    print(f\"{'='*60}\")\n    \n    han_metrics = han_trainer.train_fold(\n        fold_id=fold_id,\n        verbose=True,\n        resume_from_checkpoint=False,\n    )\n    \n    print(f\"\\n  Fold {fold_id} Val R2: {han_metrics.val_r2:+.4f}\")\n\nhan_trainer.save_results()\n\n# Compute aggregate HAN metrics\nhan_agg = han_trainer._compute_aggregate_metrics()\nprint(f\"\\n{'='*60}\")\nprint(f\"HAN COMPLETE - Mean R2: {han_agg.mean_r2:+.4f} (+/- {han_agg.std_r2:.4f})\")\nprint(f\"{'='*60}\")\n\n# ========================================\n# COMPARISON SUMMARY\n# ========================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE - FULL COMPARISON (4-FOLD CV)\")\nprint(\"=\"*60)\n\nhierarchy_gain = han_agg.mean_r2 - baseline_agg.mean_r2\n\nprint(f\"\\n  {'Model':<25} {'Mean R2':>10} {'Std R2':>10} {'Expected':>10}\")\nprint(f\"  {'-'*25} {'-'*10} {'-'*10} {'-'*10}\")\nprint(f\"  {'Bi-LSTM Baseline':<25} {baseline_agg.mean_r2:>+10.4f} {baseline_agg.std_r2:>10.4f} {'~0.19':>10}\")\nprint(f\"  {'HAN (Hierarchical)':<25} {han_agg.mean_r2:>+10.4f} {han_agg.std_r2:>10.4f} {'~0.40':>10}\")\nprint(f\"  {'-'*25} {'-'*10} {'-'*10} {'-'*10}\")\nprint(f\"  {'Hierarchy Gain':<25} {hierarchy_gain:>+10.4f} {'':>10} {'~+0.21':>10}\")\n\nprint(f\"\\n  Interpretation:\")\nif baseline_agg.mean_r2 < 0.10:\n    print(f\"  [CRITICAL] Baseline R2 ({baseline_agg.mean_r2:.3f}) is very low!\")\n    print(f\"  This suggests a fundamental issue with data or training pipeline.\")\nelif hierarchy_gain < 0.05:\n    print(f\"  [WARNING] Hierarchy gain ({hierarchy_gain:.3f}) is too low!\")\n    print(f\"  Baseline works but HAN is not adding value.\")\nelif hierarchy_gain < 0.15:\n    print(f\"  [PARTIAL] Hierarchy gain ({hierarchy_gain:.3f}) is below expected (~0.21).\")\n    print(f\"  Some hierarchy contribution, but room for improvement.\")\nelse:\n    print(f\"  [GOOD] Hierarchy gain ({hierarchy_gain:.3f}) is close to expected (~0.21)!\")\n\nprint(\"=\"*60)\n\n# Store for later use\ntrained_models = {\n    'baseline': baseline_trainer.get_trained_model(FOLDS_TO_TRAIN[-1]),\n    'han': han_trainer.get_trained_model(FOLDS_TO_TRAIN[-1]),\n}\ntrained_metrics = {\n    'baseline': baseline_agg,\n    'han': han_agg,\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints to Google Drive after training\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"Syncing checkpoints to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', str(CHECKPOINT_ROOT), GDRIVE_CHECKPOINT_PATH, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 8: Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate both models on held-out test set\nprint(\"=\"*60)\nprint(\"TEST SET EVALUATION (BOTH MODELS)\")\nprint(\"=\"*60)\n\nprint(\"\\n[1] Bi-LSTM Baseline on Test Set:\")\nbaseline_test_results = baseline_trainer.evaluate_on_test(verbose=True)\n\nprint(\"\\n[2] HAN on Test Set:\")\nhan_test_results = han_trainer.evaluate_on_test(verbose=True)\n\n# Store for comparison\ntest_results = {\n    'baseline': baseline_test_results,\n    'han': han_test_results,\n}"
  },
  {
   "cell_type": "markdown",
   "id": "tsmwiy2nm7p",
   "source": "## Step 8b: Post-Training Hierarchy Diagnostics\n\nAnalyze why the hierarchical components (beat/measure attention) may not be contributing.\nThis cell loads the best model from fold 1 and runs comprehensive diagnostics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dzp8ujd2miw",
   "source": "\"\"\"\nPOST-TRAINING MODEL COMPARISON\n\nCompare Bi-LSTM Baseline vs HAN on the same validation set.\nThis gives the TRUE hierarchy contribution measurement.\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import r2_score\n\nprint(\"=\" * 70)\nprint(\"POST-TRAINING MODEL COMPARISON\")\nprint(\"=\" * 70)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Get models from training\nif 'trained_models' in dir() and trained_models.get('baseline') and trained_models.get('han'):\n    baseline_model = trained_models['baseline'].to(device).eval()\n    han_model = trained_models['han'].to(device).eval()\n    print(f\"\\nUsing in-memory trained models\")\nelse:\n    print(f\"\\nNo trained models found - run training cell first!\")\n    baseline_model = None\n    han_model = None\n\nif baseline_model is not None and han_model is not None:\n    # Create validation dataloader\n    from src.percepiano.data.percepiano_vnet_dataset import (\n        PercePianoKFoldDataset,\n        percepiano_pack_collate,\n    )\n    \n    val_ds = PercePianoKFoldDataset(\n        data_dir=DATA_ROOT,\n        fold_assignments=fold_assignments,\n        fold_id=FOLD_TO_TRAIN,\n        mode=\"val\",\n        max_notes=CONFIG['max_notes'],\n        slice_len=CONFIG.get('slice_len', CONFIG['max_notes']),\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=4,\n        shuffle=False,\n        num_workers=0,\n        collate_fn=percepiano_pack_collate,\n    )\n    \n    print(f\"Validation samples: {len(val_ds)}\")\n    print(f\"Device: {device}\")\n    \n    # Collect predictions from both models\n    baseline_preds = []\n    han_preds = []\n    targets = []\n    \n    from torch.nn.utils.rnn import PackedSequence\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            # Move batch to device\n            batch_on_device = {}\n            for k, v in batch.items():\n                if isinstance(v, torch.Tensor):\n                    batch_on_device[k] = v.to(device)\n                elif isinstance(v, PackedSequence):\n                    batch_on_device[k] = PackedSequence(\n                        v.data.to(device),\n                        v.batch_sizes,\n                        v.sorted_indices.to(device) if v.sorted_indices is not None else None,\n                        v.unsorted_indices.to(device) if v.unsorted_indices is not None else None,\n                    )\n                else:\n                    batch_on_device[k] = v\n            \n            note_locations = {\n                'beat': batch_on_device['note_locations_beat'],\n                'measure': batch_on_device['note_locations_measure'],\n                'voice': batch_on_device['note_locations_voice'],\n            }\n            \n            # Baseline prediction\n            baseline_out = baseline_model(\n                batch_on_device['input_features'],\n                note_locations,\n                batch_on_device.get('attention_mask'),\n                batch_on_device.get('lengths'),\n            )\n            baseline_preds.append(baseline_out['predictions'].cpu())\n            \n            # HAN prediction\n            han_out = han_model(\n                batch_on_device['input_features'],\n                note_locations,\n                batch_on_device.get('attention_mask'),\n                batch_on_device.get('lengths'),\n            )\n            han_preds.append(han_out['predictions'].cpu())\n            \n            targets.append(batch_on_device['scores'].cpu())\n    \n    # Compute R2 scores\n    baseline_preds = torch.cat(baseline_preds).numpy()\n    han_preds = torch.cat(han_preds).numpy()\n    targets = torch.cat(targets).numpy()\n    \n    baseline_r2_fresh = r2_score(targets, baseline_preds)\n    han_r2_fresh = r2_score(targets, han_preds)\n    hierarchy_gain_fresh = han_r2_fresh - baseline_r2_fresh\n    \n    # Per-dimension analysis\n    from src.percepiano.models.percepiano_replica import PERCEPIANO_DIMENSIONS\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"OVERALL COMPARISON\")\n    print(\"=\" * 70)\n    \n    print(f\"\\n  {'Model':<25} {'R2':>10} {'Expected':>10} {'Status':<15}\")\n    print(f\"  {'-'*25} {'-'*10} {'-'*10} {'-'*15}\")\n    print(f\"  {'Bi-LSTM Baseline':<25} {baseline_r2_fresh:>+10.4f} {'~0.19':>10} \", end=\"\")\n    if baseline_r2_fresh >= 0.15:\n        print(\"[OK]\")\n    elif baseline_r2_fresh >= 0.10:\n        print(\"[LOW]\")\n    else:\n        print(\"[CRITICAL]\")\n    \n    print(f\"  {'HAN (Hierarchical)':<25} {han_r2_fresh:>+10.4f} {'~0.40':>10} \", end=\"\")\n    if han_r2_fresh >= 0.35:\n        print(\"[OK]\")\n    elif han_r2_fresh >= 0.20:\n        print(\"[LOW]\")\n    else:\n        print(\"[CRITICAL]\")\n    \n    print(f\"  {'-'*25} {'-'*10} {'-'*10} {'-'*15}\")\n    print(f\"  {'Hierarchy Gain':<25} {hierarchy_gain_fresh:>+10.4f} {'~+0.21':>10} \", end=\"\")\n    if hierarchy_gain_fresh >= 0.15:\n        print(\"[GOOD]\")\n    elif hierarchy_gain_fresh >= 0.05:\n        print(\"[PARTIAL]\")\n    else:\n        print(\"[NONE]\")\n    \n    # Per-dimension comparison\n    print(\"\\n\" + \"=\" * 70)\n    print(\"PER-DIMENSION R2 COMPARISON\")\n    print(\"=\" * 70)\n    \n    print(f\"\\n  {'Dimension':<30} {'Baseline':>10} {'HAN':>10} {'Gain':>10}\")\n    print(f\"  {'-'*30} {'-'*10} {'-'*10} {'-'*10}\")\n    \n    dim_gains = []\n    for i, dim in enumerate(PERCEPIANO_DIMENSIONS):\n        baseline_dim_r2 = r2_score(targets[:, i], baseline_preds[:, i])\n        han_dim_r2 = r2_score(targets[:, i], han_preds[:, i])\n        dim_gain = han_dim_r2 - baseline_dim_r2\n        dim_gains.append((dim, baseline_dim_r2, han_dim_r2, dim_gain))\n    \n    # Sort by gain\n    dim_gains.sort(key=lambda x: x[3], reverse=True)\n    \n    for dim, baseline_dim, han_dim, gain in dim_gains:\n        indicator = \"+\" if gain > 0 else \"\"\n        print(f\"  {dim:<30} {baseline_dim:>+10.4f} {han_dim:>+10.4f} {indicator}{gain:>9.4f}\")\n    \n    positive_gain_dims = sum(1 for _, _, _, g in dim_gains if g > 0)\n    print(f\"\\n  Dimensions with positive hierarchy gain: {positive_gain_dims}/{len(PERCEPIANO_DIMENSIONS)}\")\n    \n    # Diagnosis\n    print(\"\\n\" + \"=\" * 70)\n    print(\"DIAGNOSIS\")\n    print(\"=\" * 70)\n    \n    if baseline_r2_fresh < 0.10:\n        print(f\"\\n  [CRITICAL] Baseline R2 = {baseline_r2_fresh:.4f} is very low!\")\n        print(f\"  This indicates a problem in the data pipeline or training loop.\")\n        print(f\"  The issue is NOT specific to HAN - fix the baseline first.\")\n        print(f\"\\n  Likely causes:\")\n        print(f\"    1. Data preprocessing issue (feature scaling, normalization)\")\n        print(f\"    2. Label alignment problem\")\n        print(f\"    3. Training instability (learning rate, gradient clipping)\")\n    elif hierarchy_gain_fresh < 0.05:\n        print(f\"\\n  [WARNING] Baseline works (R2={baseline_r2_fresh:.4f}) but hierarchy gain is low ({hierarchy_gain_fresh:.4f})\")\n        print(f\"  The Bi-LSTM is learning, but HAN hierarchy is not helping.\")\n        print(f\"\\n  Likely causes:\")\n        print(f\"    1. Beat/measure attention collapsed to uniform\")\n        print(f\"    2. span_beat_to_note_num index mapping issue\")\n        print(f\"    3. HAN architecture mismatch with original\")\n    elif han_r2_fresh < 0.30:\n        print(f\"\\n  [PARTIAL] Both models learning, but below SOTA.\")\n        print(f\"  Baseline: {baseline_r2_fresh:.4f} (expected ~0.19)\")\n        print(f\"  HAN: {han_r2_fresh:.4f} (expected ~0.40)\")\n        print(f\"  Hierarchy gain: {hierarchy_gain_fresh:.4f} (expected ~0.21)\")\n        print(f\"\\n  Suggestions:\")\n        print(f\"    1. Train for more epochs\")\n        print(f\"    2. Try all 4 folds and average\")\n        print(f\"    3. Check if slice sampling is working correctly\")\n    else:\n        print(f\"\\n  [GOOD] Results approaching SOTA!\")\n        print(f\"  Baseline: {baseline_r2_fresh:.4f}\")\n        print(f\"  HAN: {han_r2_fresh:.4f}\")\n        print(f\"  Hierarchy gain: {hierarchy_gain_fresh:.4f}\")\n    \n    print(\"=\" * 70)\nelse:\n    print(\"\\n[SKIPPED] No models available for comparison.\")\n    print(\"Run the training cell first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vhlsirr5pb",
   "source": "## Step 8c: Manual Ablation Test\n\nDirectly compare full model R2 vs Bi-LSTM only R2 to measure hierarchy contribution.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vaomaz2syki",
   "source": "\"\"\"\nNOTE: This cell is DEPRECATED.\n\nThe proper Baseline vs HAN comparison is now done in the training cell (Step 7)\nand the comparison cell (Step 8b) using the actual PercePianoBiLSTMBaseline model.\n\nThe old \"zeroed hierarchy\" ablation approach was incorrect because it:\n1. Used 4-layer split LSTMs instead of 7-layer single LSTM\n2. Still included voice processing (which baseline doesn't have)\n3. Fed 2048-dim (half zeros) to contractor instead of 512-dim dense\n\nSee the comparison cell above for the correct Baseline vs HAN comparison.\n\"\"\"\n\nprint(\"=\"*70)\nprint(\"NOTE: Manual ablation cell deprecated\")\nprint(\"=\"*70)\nprint(\"\\nThe proper comparison is now done using:\")\nprint(\"  1. PercePianoBiLSTMBaseline (7-layer LSTM matching VirtuosoNetSingle)\")\nprint(\"  2. PercePianoVNetModule (HAN matching VirtuosoNetMultiLevel)\")\nprint(\"\\nSee the comparison results in the cells above.\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 9: Per-Fold Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nfrom src.percepiano.models.percepiano_replica import PERCEPIANO_DIMENSIONS\n\n# Per-fold results for BOTH models\nprint(\"=\"*80)\nprint(\"PER-FOLD VALIDATION RESULTS\")\nprint(\"=\"*80)\n\nfor model_name, trainer in [(\"Bi-LSTM Baseline\", baseline_trainer), (\"HAN\", han_trainer)]:\n    aggregate_metrics = trainer._compute_aggregate_metrics()\n    \n    print(f\"\\n{model_name}:\")\n    print(f\"{'Fold':<6} {'Val R2':>10} {'Val Pearson':>12} {'Val MAE':>10} {'Val RMSE':>10} {'Epochs':>8} {'Time (s)':>10}\")\n    print(f\"{'-'*6} {'-'*10} {'-'*12} {'-'*10} {'-'*10} {'-'*8} {'-'*10}\")\n\n    for m in trainer.fold_metrics:\n        print(f\"{m.fold_id:<6} {m.val_r2:>+10.4f} {m.val_pearson:>+12.4f} {m.val_mae:>10.4f} {m.val_rmse:>10.4f} {m.epochs_trained:>8} {m.training_time_seconds:>10.1f}\")\n\n    print(f\"{'-'*6} {'-'*10} {'-'*12} {'-'*10} {'-'*10} {'-'*8} {'-'*10}\")\n    print(f\"{'Mean':<6} {aggregate_metrics.mean_r2:>+10.4f} {aggregate_metrics.mean_pearson:>+12.4f} {aggregate_metrics.mean_mae:>10.4f} {aggregate_metrics.mean_rmse:>10.4f}\")\n    print(f\"{'Std':<6} {aggregate_metrics.std_r2:>+10.4f} {aggregate_metrics.std_pearson:>+12.4f} {aggregate_metrics.std_mae:>10.4f} {aggregate_metrics.std_rmse:>10.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Step 10: Per-Dimension Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# Per-dimension analysis for HAN model (the one we care about for SOTA comparison)\nhan_agg = han_trainer._compute_aggregate_metrics()\n\nprint(\"=\"*80)\nprint(\"PER-DIMENSION R2 FOR HAN MODEL (Mean +/- Std across folds)\")\nprint(\"=\"*80)\n\n# Sort dimensions by mean R2\nsorted_dims = sorted(\n    han_agg.per_dim_mean_r2.items(),\n    key=lambda x: x[1],\n    reverse=True\n)\n\nprint(f\"\\n{'Dimension':<25} {'Mean R2':>10} {'Std R2':>10} {'Status':<12}\")\nprint(f\"{'-'*25} {'-'*10} {'-'*10} {'-'*12}\")\n\nfor dim, mean_r2 in sorted_dims:\n    std_r2 = han_agg.per_dim_std_r2[dim]\n    \n    if mean_r2 >= 0.3:\n        status = \"[GOOD]\"\n    elif mean_r2 >= 0.1:\n        status = \"[OK]\"\n    elif mean_r2 >= 0:\n        status = \"[WEAK]\"\n    else:\n        status = \"[FAILED]\"\n    \n    print(f\"{dim:<25} {mean_r2:>+10.4f} {std_r2:>10.4f} {status:<12}\")\n\n# Summary\npositive = sum(1 for d, r in sorted_dims if r > 0)\nstrong = sum(1 for d, r in sorted_dims if r >= 0.2)\nn_dims = len(sorted_dims)\n\nprint(f\"\\nSummary: {positive}/{n_dims} positive R2, {strong}/{n_dims} strong (>= 0.2)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Step 11: Final Summary and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport torch\nfrom pathlib import Path\n\nprint(\"=\"*80)\nprint(\"FINAL SUMMARY - BASELINE VS HAN COMPARISON\")\nprint(\"=\"*80)\n\n# Get metrics from trainers\nbaseline_agg = baseline_trainer._compute_aggregate_metrics()\nhan_agg = han_trainer._compute_aggregate_metrics()\n\n# Cross-validation results comparison\nprint(f\"\\n{'='*80}\")\nprint(\"CROSS-VALIDATION RESULTS (FOLD 1)\")\nprint(f\"{'='*80}\")\n\nprint(f\"\\n  {'Metric':<20} {'Baseline':>12} {'HAN':>12} {'Gain':>12} {'Expected':>12}\")\nprint(f\"  {'-'*20} {'-'*12} {'-'*12} {'-'*12} {'-'*12}\")\n\nbaseline_cv_r2 = baseline_agg.mean_r2\nhan_cv_r2 = han_agg.mean_r2\ncv_gain = han_cv_r2 - baseline_cv_r2\n\nprint(f\"  {'R2':<20} {baseline_cv_r2:>+12.4f} {han_cv_r2:>+12.4f} {cv_gain:>+12.4f} {'+0.21':>12}\")\nprint(f\"  {'Pearson':<20} {baseline_agg.mean_pearson:>+12.4f} {han_agg.mean_pearson:>+12.4f}\")\nprint(f\"  {'MAE':<20} {baseline_agg.mean_mae:>12.4f} {han_agg.mean_mae:>12.4f}\")\nprint(f\"  {'RMSE':<20} {baseline_agg.mean_rmse:>12.4f} {han_agg.mean_rmse:>12.4f}\")\n\n# Test set results\nprint(f\"\\n{'='*80}\")\nprint(\"TEST SET RESULTS (ENSEMBLE)\")\nprint(f\"{'='*80}\")\n\nif 'test_results' in dir() and test_results.get('baseline') and test_results.get('han'):\n    baseline_test = test_results['baseline']['ensemble']\n    han_test = test_results['han']['ensemble']\n    test_gain = han_test['r2'] - baseline_test['r2']\n    \n    print(f\"\\n  {'Metric':<20} {'Baseline':>12} {'HAN':>12} {'Gain':>12}\")\n    print(f\"  {'-'*20} {'-'*12} {'-'*12} {'-'*12}\")\n    print(f\"  {'R2':<20} {baseline_test['r2']:>+12.4f} {han_test['r2']:>+12.4f} {test_gain:>+12.4f}\")\n    print(f\"  {'Pearson':<20} {baseline_test['pearson']:>+12.4f} {han_test['pearson']:>+12.4f}\")\n    print(f\"  {'MAE':<20} {baseline_test['mae']:>12.4f} {han_test['mae']:>12.4f}\")\nelse:\n    print(\"\\n  [Test results not available - run test evaluation cell]\")\n\n# Comparison to PercePiano baselines\nprint(f\"\\n{'='*80}\")\nprint(\"COMPARISON TO PERCEPIANO PAPER\")\nprint(f\"{'='*80}\")\n\nprint(f\"\\n  {'Model':<30} {'Paper R2':>12} {'Our R2':>12} {'Match':>10}\")\nprint(f\"  {'-'*30} {'-'*12} {'-'*12} {'-'*10}\")\nprint(f\"  {'Bi-LSTM (VirtuosoNetSingle)':<30} {'0.185':>12} {baseline_cv_r2:>+12.4f}\", end=\"\")\nif abs(baseline_cv_r2 - 0.185) < 0.05:\n    print(f\"{'[OK]':>10}\")\nelif baseline_cv_r2 > 0.135:\n    print(f\"{'[CLOSE]':>10}\")\nelse:\n    print(f\"{'[LOW]':>10}\")\n\nprint(f\"  {'HAN (VirtuosoNetMultiLevel)':<30} {'0.397':>12} {han_cv_r2:>+12.4f}\", end=\"\")\nif abs(han_cv_r2 - 0.397) < 0.05:\n    print(f\"{'[OK]':>10}\")\nelif han_cv_r2 > 0.30:\n    print(f\"{'[CLOSE]':>10}\")\nelse:\n    print(f\"{'[LOW]':>10}\")\n\nprint(f\"  {'Hierarchy Gain':<30} {'+0.212':>12} {cv_gain:>+12.4f}\", end=\"\")\nif cv_gain > 0.15:\n    print(f\"{'[GOOD]':>10}\")\nelif cv_gain > 0.05:\n    print(f\"{'[PARTIAL]':>10}\")\nelse:\n    print(f\"{'[NONE]':>10}\")\n\n# Final interpretation\nprint(f\"\\n{'='*80}\")\nprint(\"INTERPRETATION\")\nprint(f\"{'='*80}\")\n\nif baseline_cv_r2 < 0.10:\n    print(f\"\\n  [CRITICAL] Baseline R2 ({baseline_cv_r2:.3f}) is very low!\")\n    print(f\"  There is a fundamental issue with the data or training pipeline.\")\n    print(f\"  Fix the baseline first before debugging HAN.\")\nelif cv_gain < 0.05 and baseline_cv_r2 >= 0.10:\n    print(f\"\\n  [WARNING] Baseline works but hierarchy gain is low ({cv_gain:.3f})!\")\n    print(f\"  The Bi-LSTM learns but HAN hierarchy is not helping.\")\n    print(f\"  Debug: beat/measure attention, span_beat_to_note_num, index mapping.\")\nelif han_cv_r2 >= 0.35:\n    print(f\"\\n  [SUCCESS] HAN R2 ({han_cv_r2:.3f}) approaching SOTA (0.397)!\")\n    print(f\"  Model is ready for use or further tuning.\")\nelif han_cv_r2 >= 0.25:\n    print(f\"\\n  [GOOD] HAN R2 ({han_cv_r2:.3f}) is usable for pseudo-labeling.\")\n    print(f\"  Consider training all 4 folds and more epochs.\")\nelse:\n    print(f\"\\n  [NEEDS WORK] HAN R2 ({han_cv_r2:.3f}) needs improvement.\")\n    print(f\"  Check hierarchy diagnostics for specific issues.\")\n\nprint(f\"\\n{'='*80}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": "# Final sync to Google Drive\nprint(\"=\"*60)\nprint(\"SYNC TO GOOGLE DRIVE\")\nprint(\"=\"*60)\n\nif RCLONE_AVAILABLE:\n    print(f\"\\nSyncing all checkpoints and results...\")\n    \n    # Sync baseline checkpoints\n    print(f\"\\n[1] Syncing Baseline checkpoints...\")\n    subprocess.run(\n        ['rclone', 'copy', str(baseline_trainer.checkpoint_dir), \n         f\"{GDRIVE_CHECKPOINT_PATH}/percepiano_baseline\", '--progress'],\n        capture_output=False\n    )\n    \n    # Sync HAN checkpoints\n    print(f\"\\n[2] Syncing HAN checkpoints...\")\n    subprocess.run(\n        ['rclone', 'copy', str(han_trainer.checkpoint_dir), \n         f\"{GDRIVE_CHECKPOINT_PATH}/percepiano\", '--progress'],\n        capture_output=False\n    )\n    \n    # Also sync fold assignments back to data directory\n    subprocess.run(\n        ['rclone', 'copy', str(FOLD_FILE), GDRIVE_DATA_PATH, '--progress'],\n        capture_output=False\n    )\n    \n    print(f\"\\nSync complete!\")\n    print(f\"  Baseline checkpoints: {GDRIVE_CHECKPOINT_PATH}/percepiano_baseline\")\n    print(f\"  HAN checkpoints: {GDRIVE_CHECKPOINT_PATH}/percepiano\")\n    print(f\"  Fold assignments: {GDRIVE_DATA_PATH}\")\nelse:\n    print(f\"\\nrclone not available - skipping sync\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"TRAINING COMPLETE\")\nprint(f\"{'='*60}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}