{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PercePiano SOTA Replica Training\n",
    "\n",
    "**Goal**: Replicate PercePiano's SOTA results (R-squared = 0.35-0.40) using their exact architecture.\n",
    "\n",
    "## Attribution\n",
    "\n",
    "This notebook implements the architecture from:\n",
    "\n",
    "> **PercePiano: A Benchmark for Perceptual Evaluation of Piano Performance**  \n",
    "> Park, Jongho and Kim, Dasaem et al.  \n",
    "> ISMIR 2024 / Nature Scientific Reports 2024  \n",
    "> GitHub: https://github.com/JonghoKimSNU/PercePiano\n",
    "\n",
    "## Key Differences from Our Previous Approach\n",
    "\n",
    "| Aspect | Our Previous Model | PercePiano SOTA |\n",
    "|--------|-------------------|----------------|\n",
    "| MIDI Encoder | 12-layer Transformer (768-dim) | **Bi-LSTM (256-dim)** |\n",
    "| Total Parameters | 51.5M | **~8-10M** |\n",
    "| Learning Rate | 1e-4 | **2.5e-5** |\n",
    "| Dropout | 0.1 | **0.2** |\n",
    "| Attention Heads | 4-12 | **8** |\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. Downloads PercePiano data and scores from Google Drive\n",
    "2. Runs pre-flight validation to ensure all data is present\n",
    "3. Trains a lightweight Bi-LSTM + HAN model matching PercePiano's architecture\n",
    "4. Evaluates against SOTA baselines\n",
    "5. Saves the trained model as a **Teacher Model** for pseudo-labeling MAESTRO\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- **Target R-squared**: 0.35-0.40 (piece-split)\n",
    "- **Training time**: ~1-2 hours on T4/A100\n",
    "- **Model size**: ~8-10M parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rclone for Google Drive sync\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv and clone repository\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "# Install dependencies\n",
    "!uv pip install --system -e .\n",
    "!pip install tensorboard rich\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_ROOT = '/tmp/checkpoints/percepiano_replica'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/percepiano_replica'\n",
    "GDRIVE_DATA_PATH = 'gdrive:percepiano_data'\n",
    "DATA_ROOT = Path('/tmp/percepiano_data')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PERCEPIANO REPLICA - SOTA REPRODUCTION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Reference: Park et al., 'PercePiano', ISMIR/Nature 2024\")\n",
    "print(\"GitHub: https://github.com/JonghoKimSNU/PercePiano\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check rclone\n",
    "print(\"\\nChecking rclone configuration...\")\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "\n",
    "if 'gdrive:' in result.stdout:\n",
    "    print(\"  rclone 'gdrive' remote: CONFIGURED\")\n",
    "    RCLONE_AVAILABLE = True\n",
    "    \n",
    "    # Restore existing checkpoints\n",
    "    print(\"\\nRestoring checkpoints from Google Drive (if any)...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_CHECKPOINT_PATH, CHECKPOINT_ROOT, '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "else:\n",
    "    print(\"  rclone 'gdrive' remote: NOT CONFIGURED\")\n",
    "    print(\"  Run 'rclone config' in terminal to set up Google Drive\")\n",
    "    RCLONE_AVAILABLE = False\n",
    "\n",
    "print(f\"\\nCheckpoint directory: {CHECKPOINT_ROOT}\")\n",
    "print(f\"rclone available: {RCLONE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Download PercePiano Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "DATA_ROOT = Path('/tmp/percepiano_data')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download PercePiano data\n",
    "train_file = DATA_ROOT / 'percepiano_train.json'\n",
    "if train_file.exists():\n",
    "    print(f\"Data already exists at {DATA_ROOT}\")\n",
    "else:\n",
    "    print(\"Downloading PercePiano data from Google Drive...\")\n",
    "    result = subprocess.run(\n",
    "        ['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "\n",
    "# Verify data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    path = DATA_ROOT / f'percepiano_{split}.json'\n",
    "    if path.exists():\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        has_scores = sum(1 for s in data if s.get('score_path'))\n",
    "        print(f\"{split}: {len(data)} samples ({has_scores} with score paths)\")\n",
    "    else:\n",
    "        print(f\"ERROR: {path} not found!\")\n",
    "\n",
    "# Check MIDI and score files\n",
    "midi_dir = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'all_2rounds'\n",
    "score_dir = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'score_xml'\n",
    "\n",
    "if midi_dir.exists():\n",
    "    midi_files = list(midi_dir.glob('*.mid'))\n",
    "    print(f\"\\nMIDI files: {len(midi_files)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"MIDI directory not found at {midi_dir}\")\n",
    "\n",
    "if score_dir.exists():\n",
    "    score_files = list(score_dir.glob('*.musicxml'))\n",
    "    print(f\"Score files: {len(score_files)}\")\n",
    "    if len(score_files) == 0:\n",
    "        raise FileNotFoundError(f\"No MusicXML files found in {score_dir}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Score directory not found at {score_dir}\\n\"\n",
    "        \"Run: rclone copy gdrive:percepiano_data/PercePiano/virtuoso/data/score_xml/ {score_dir}/\"\n",
    "    )\n",
    "\n",
    "# Store paths for later\n",
    "MIDI_DIR = midi_dir\n",
    "SCORE_DIR = score_dir\n",
    "\n",
    "print(\"\\n[OK] All required files present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Update Paths for Thunder Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# All 19 PercePiano dimensions\n",
    "PERCEPIANO_DIMENSIONS = [\n",
    "    \"timing\", \"articulation_length\", \"articulation_touch\",\n",
    "    \"pedal_amount\", \"pedal_clarity\", \"timbre_variety\",\n",
    "    \"timbre_depth\", \"timbre_brightness\", \"timbre_loudness\",\n",
    "    \"dynamic_range\", \"tempo\", \"space\", \"balance\", \"drama\",\n",
    "    \"mood_valence\", \"mood_energy\", \"mood_imagination\",\n",
    "    \"sophistication\", \"interpretation\",\n",
    "]\n",
    "\n",
    "def update_paths_for_thunder(data_root: Path):\n",
    "    \"\"\"Update paths in JSON files for Thunder Compute environment.\"\"\"\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        path = data_root / f'percepiano_{split}.json'\n",
    "        \n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for sample in data:\n",
    "            # Update MIDI path\n",
    "            filename = Path(sample['midi_path']).name\n",
    "            sample['midi_path'] = str(MIDI_DIR / filename)\n",
    "            \n",
    "            # Make sure scores dict uses all 19 dimensions\n",
    "            if 'percepiano_scores' in sample:\n",
    "                pp_scores = sample['percepiano_scores'][:19]\n",
    "                sample['scores'] = {\n",
    "                    dim: pp_scores[i]\n",
    "                    for i, dim in enumerate(PERCEPIANO_DIMENSIONS)\n",
    "                }\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"Updated {split}: {len(data)} samples\")\n",
    "\n",
    "update_paths_for_thunder(DATA_ROOT)\n",
    "\n",
    "# Verify\n",
    "with open(DATA_ROOT / 'percepiano_train.json') as f:\n",
    "    sample = json.load(f)[0]\n",
    "\n",
    "print(f\"\\nSample MIDI path: {sample['midi_path']}\")\n",
    "print(f\"Sample score path: {sample.get('score_path', 'N/A')}\")\n",
    "print(f\"Dimensions: {len(sample['scores'])}\")\n",
    "print(f\"MIDI exists: {Path(sample['midi_path']).exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Pre-Flight Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-flight validation - FAIL FAST if requirements not met\n",
    "from src.utils.preflight_validation import run_preflight_validation, PreflightValidationError\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRE-FLIGHT VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    run_preflight_validation(\n",
    "        data_dir=DATA_ROOT,\n",
    "        score_dir=SCORE_DIR,\n",
    "        pretrained_checkpoint=None,  # PercePiano replica doesn't use pre-trained encoder\n",
    "        require_pretrained=False,    # Training from scratch with Bi-LSTM\n",
    "        min_score_coverage=0.95,\n",
    "    )\n",
    "    print(\"\\n[OK] Pre-flight validation PASSED - ready to train\")\n",
    "except PreflightValidationError as e:\n",
    "    print(f\"\\n[VALIDATION FAILED]\\n{e}\")\n",
    "    raise RuntimeError(\"Fix the issues above before training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration\n",
    "\n",
    "Configuration matched exactly to PercePiano paper's `han_bigger256_concat.yml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# PercePiano SOTA Configuration\n",
    "# From: https://github.com/JonghoKimSNU/PercePiano\n",
    "# File: virtuoso/ymls/shared/label19/han_bigger256_concat.yml\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'data_dir': str(DATA_ROOT),\n",
    "    'score_dir': str(SCORE_DIR),\n",
    "    \n",
    "    # HAN Architecture (matched to PercePiano)\n",
    "    'hidden_size': 256,        # PercePiano: 256 for all levels\n",
    "    'note_layers': 2,          # PercePiano: 2\n",
    "    'voice_layers': 2,         # PercePiano: 2\n",
    "    'beat_layers': 2,          # PercePiano: 2\n",
    "    'measure_layers': 1,       # PercePiano: 1\n",
    "    'num_attention_heads': 8,  # PercePiano: 8\n",
    "    'final_hidden': 128,       # PercePiano: 128\n",
    "    \n",
    "    # Training (matched to PercePiano)\n",
    "    'learning_rate': 2.5e-5,   # PercePiano: 2.5e-5\n",
    "    'weight_decay': 0.01,\n",
    "    'dropout': 0.2,            # PercePiano: 0.2\n",
    "    'batch_size': 8,           # PercePiano: 8\n",
    "    'max_epochs': 100,\n",
    "    'early_stopping_patience': 20,\n",
    "    'gradient_clip_val': 1.0,\n",
    "    'precision': '16-mixed',\n",
    "    \n",
    "    # Score features\n",
    "    'score_note_features': 20,\n",
    "    'score_global_features': 12,\n",
    "    'max_score_notes': 1024,\n",
    "    'max_tempo_segments': 256,\n",
    "    \n",
    "    # Checkpoints\n",
    "    'checkpoint_dir': CHECKPOINT_ROOT,\n",
    "    'gdrive_checkpoint': GDRIVE_CHECKPOINT_PATH,\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PERCEPIANO REPLICA CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Reference: han_bigger256_concat.yml from PercePiano repo\")\n",
    "print(\"=\"*70)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey differences from our previous approach:\")\n",
    "print(\"  - Bi-LSTM encoder instead of 12-layer Transformer\")\n",
    "print(\"  - 256 hidden dim instead of 768 (~10x fewer params)\")\n",
    "print(\"  - Learning rate 2.5e-5 instead of 1e-4 (4x lower)\")\n",
    "print(\"  - Dropout 0.2 instead of 0.1 (2x more regularization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 6: Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.data.percepiano_score_dataset import create_score_dataloaders\n",
    "\n",
    "train_loader, val_loader, test_loader = create_score_dataloaders(\n",
    "    data_dir=Path(CONFIG['data_dir']),\n",
    "    score_dir=Path(CONFIG['score_dir']),\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    max_midi_seq_length=512,  # Not used for PercePiano replica (no MIDI encoder)\n",
    "    max_score_notes=CONFIG['max_score_notes'],\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)} ({len(train_loader) * CONFIG['batch_size']} samples)\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  score_note_features: {batch['score_note_features'].shape}\")\n",
    "print(f\"  score_global_features: {batch['score_global_features'].shape}\")\n",
    "print(f\"  score_tempo_curve: {batch['score_tempo_curve'].shape}\")\n",
    "print(f\"  scores: {batch['scores'].shape}\")\n",
    "\n",
    "# Check for note_locations (required for HAN)\n",
    "if 'note_locations_beat' in batch:\n",
    "    print(f\"\\nHierarchical features (note_locations):\")\n",
    "    print(f\"  note_locations_beat: {batch['note_locations_beat'].shape}\")\n",
    "    print(f\"  note_locations_measure: {batch['note_locations_measure'].shape}\")\n",
    "    print(f\"  note_locations_voice: {batch['note_locations_voice'].shape}\")\n",
    "else:\n",
    "    raise ValueError(\"note_locations not found in batch - required for HAN encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 7: Create PercePiano Replica Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.percepiano_replica import PercePianoReplicaModule\n",
    "\n",
    "# Create model with PercePiano configuration\n",
    "model = PercePianoReplicaModule(\n",
    "    # Input dimensions\n",
    "    score_note_features=CONFIG['score_note_features'],\n",
    "    score_global_features=CONFIG['score_global_features'],\n",
    "    # HAN dimensions (matched to PercePiano)\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    note_layers=CONFIG['note_layers'],\n",
    "    voice_layers=CONFIG['voice_layers'],\n",
    "    beat_layers=CONFIG['beat_layers'],\n",
    "    measure_layers=CONFIG['measure_layers'],\n",
    "    num_attention_heads=CONFIG['num_attention_heads'],\n",
    "    final_hidden=CONFIG['final_hidden'],\n",
    "    # Training\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    dropout=CONFIG['dropout'],\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_parameters()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PERCEPIANO REPLICA MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Architecture: Bi-LSTM + HAN (Note -> Beat -> Measure)\")\n",
    "print(f\"Hidden size: {CONFIG['hidden_size']}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"\")\n",
    "print(f\"Comparison:\")\n",
    "print(f\"  Our previous model: 51,500,000 params\")\n",
    "print(f\"  PercePiano replica: {total_params:,} params\")\n",
    "print(f\"  Reduction: {51_500_000 / total_params:.1f}x smaller\")\n",
    "print(f\"\")\n",
    "print(f\"Target R-squared: 0.35-0.40 (piece-split)\")\n",
    "print(f\"Dimensions: {len(model.dimensions)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 8: Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Checkpoint callback - monitor mean R-squared\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CONFIG['checkpoint_dir'],\n",
    "    filename='percepiano_replica-{epoch:02d}-{val_mean_r2:.4f}',\n",
    "    monitor='val/mean_r2',\n",
    "    mode='max',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val/mean_r2',\n",
    "    patience=CONFIG['early_stopping_patience'],\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "# LR monitor\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir='/tmp/logs',\n",
    "    name='percepiano_replica',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['max_epochs'],\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    precision=CONFIG['precision'],\n",
    "    gradient_clip_val=CONFIG['gradient_clip_val'],\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.5,  # Validate twice per epoch\n",
    ")\n",
    "\n",
    "print(\"Trainer configured!\")\n",
    "print(f\"  Precision: {CONFIG['precision']}\")\n",
    "print(f\"  Max epochs: {CONFIG['max_epochs']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Early stopping patience: {CONFIG['early_stopping_patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 9: Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Train\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING TRAINING - PercePiano SOTA Replica\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey metrics to watch:\")\n",
    "print(\"  - val/mean_r2: Overall R-squared (target: 0.35-0.40)\")\n",
    "print(\"  - val/timing_r2: Timing dimension (should be highest)\")\n",
    "print(\"  - val/tempo_r2: Tempo dimension\")\n",
    "print(\"\")\n",
    "print(\"PercePiano SOTA baselines:\")\n",
    "print(\"  - Bi-LSTM: R^2 = 0.185\")\n",
    "print(\"  - MidiBERT: R^2 = 0.313\")\n",
    "print(\"  - Bi-LSTM + SA + HAN: R^2 = 0.397 (SOTA)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints to Google Drive\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"Syncing checkpoints to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Step 10: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with best checkpoint\n",
    "print(\"Running test with best checkpoint...\")\n",
    "best_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best checkpoint: {best_path}\")\n",
    "\n",
    "if best_path:\n",
    "    test_results = trainer.test(model, test_loader, ckpt_path=best_path)\n",
    "    print(\"\\nTest Results:\")\n",
    "    for k, v in test_results[0].items():\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load best model\n",
    "from src.models.percepiano_replica import PercePianoReplicaModule\n",
    "best_model = PercePianoReplicaModule.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "best_model.eval()\n",
    "best_model.cuda()\n",
    "\n",
    "# Collect predictions on test set\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "print(\"Collecting predictions on test set...\")\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        note_locations = {\n",
    "            'beat': batch['note_locations_beat'],\n",
    "            'measure': batch['note_locations_measure'],\n",
    "            'voice': batch['note_locations_voice'],\n",
    "        }\n",
    "        outputs = best_model(\n",
    "            batch['score_note_features'],\n",
    "            batch['score_global_features'],\n",
    "            batch['score_tempo_curve'],\n",
    "            note_locations,\n",
    "            batch.get('score_attention_mask'),\n",
    "        )\n",
    "        all_preds.append(outputs['predictions'].cpu())\n",
    "        all_targets.append(batch['scores'].cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "dimensions = best_model.dimensions\n",
    "\n",
    "print(f\"Collected {len(all_preds)} test samples across {len(dimensions)} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import (\n",
    "    compute_all_metrics,\n",
    "    PerDimensionAnalysis,\n",
    "    compare_to_sota,\n",
    "    format_comparison_table,\n",
    "    create_results_table,\n",
    "    PERCEPIANO_BASELINES,\n",
    ")\n",
    "\n",
    "# Compute all metrics\n",
    "metrics = compute_all_metrics(\n",
    "    predictions=all_preds,\n",
    "    targets=all_targets,\n",
    "    dimension_names=list(dimensions),\n",
    ")\n",
    "\n",
    "# Print results table\n",
    "print(create_results_table(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to SOTA baselines\n",
    "our_r2 = metrics['r2'].value\n",
    "per_dim_r2 = metrics['r2'].per_dimension\n",
    "\n",
    "comparison = compare_to_sota(\n",
    "    model_r2=our_r2,\n",
    "    model_name=\"PercePiano Replica (CrescendAI)\",\n",
    "    split_type=\"piece\",\n",
    "    per_dimension_r2=per_dim_r2,\n",
    ")\n",
    "\n",
    "print(format_comparison_table(comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*70)\n",
    "print(\"PERCEPIANO REPLICA - RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. OVERALL PERFORMANCE\")\n",
    "print(f\"   Mean R^2: {our_r2:.4f}\")\n",
    "print(f\"   Target (0.35-0.40): {'ACHIEVED' if our_r2 >= 0.35 else 'CLOSE' if our_r2 >= 0.30 else 'NOT YET'}\")\n",
    "\n",
    "print(f\"\\n2. COMPARISON TO PUBLISHED BASELINES\")\n",
    "print(f\"   Bi-LSTM baseline: 0.185\")\n",
    "print(f\"   MidiBERT: 0.313\")\n",
    "print(f\"   Bi-LSTM + SA + HAN (SOTA): 0.397\")\n",
    "print(f\"   Our replica: {our_r2:.4f}\")\n",
    "\n",
    "print(f\"\\n3. MODEL SIZE\")\n",
    "print(f\"   Parameters: {best_model.count_parameters():,}\")\n",
    "print(f\"   vs Previous (51.5M): {51_500_000 / best_model.count_parameters():.1f}x smaller\")\n",
    "\n",
    "print(f\"\\n4. TOP 5 DIMENSIONS\")\n",
    "sorted_dims = sorted(per_dim_r2.items(), key=lambda x: x[1], reverse=True)\n",
    "for dim, r2 in sorted_dims[:5]:\n",
    "    print(f\"   {dim}: {r2:.4f}\")\n",
    "\n",
    "print(f\"\\n5. BOTTOM 5 DIMENSIONS (need improvement)\")\n",
    "for dim, r2 in sorted_dims[-5:]:\n",
    "    print(f\"   {dim}: {r2:.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Step 11: Save as Teacher Model\n",
    "\n",
    "If the model achieves R-squared >= 0.30, save it as a **Teacher Model** for pseudo-labeling MAESTRO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Save as teacher model\n",
    "teacher_path = Path(CONFIG['checkpoint_dir']) / 'percepiano_teacher.pt'\n",
    "\n",
    "if our_r2 >= 0.25:  # Minimum threshold for useful teacher\n",
    "    torch.save({\n",
    "        'state_dict': best_model.state_dict(),\n",
    "        'hparams': dict(best_model.hparams),\n",
    "        'dimensions': list(dimensions),\n",
    "        'metrics': {\n",
    "            'r2': our_r2,\n",
    "            'per_dimension_r2': per_dim_r2,\n",
    "        },\n",
    "        'sota_comparison': {\n",
    "            'rank': comparison['rank'],\n",
    "            'total_baselines': comparison['total_baselines'],\n",
    "            'vs_best_baseline': comparison['improvement_vs_best'],\n",
    "        },\n",
    "        'architecture': 'PercePiano Replica (Bi-LSTM + HAN)',\n",
    "        'reference': 'https://github.com/JonghoKimSNU/PercePiano',\n",
    "    }, teacher_path)\n",
    "    \n",
    "    print(f\"Saved teacher model to {teacher_path}\")\n",
    "    print(f\"Teacher R^2: {our_r2:.4f}\")\n",
    "    print(f\"\\nThis model can be used for pseudo-labeling MAESTRO!\")\n",
    "    print(f\"Run: python scripts/pseudo_label_maestro.py --teacher {teacher_path}\")\n",
    "else:\n",
    "    print(f\"R^2 = {our_r2:.4f} is below threshold (0.25) for teacher model.\")\n",
    "    print(\"Consider:\")\n",
    "    print(\"  1. Training for more epochs\")\n",
    "    print(\"  2. Adjusting hyperparameters\")\n",
    "    print(\"  3. Checking data quality\")\n",
    "\n",
    "# Final sync to Google Drive\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"\\nFinal sync to Google Drive...\")\n",
    "    subprocess.run(\n",
    "        ['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'],\n",
    "        capture_output=False\n",
    "    )\n",
    "    print(\"Sync complete!\")\n",
    "    print(f\"Checkpoints available at: {CONFIG['gdrive_checkpoint']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If R-squared >= 0.30:\n",
    "\n",
    "1. **Pseudo-label MAESTRO**: Use this teacher model to generate labels for MAESTRO dataset\n",
    "2. **Train larger model**: With expanded dataset (~6000 samples), train a larger model\n",
    "3. **Noisy Student**: Apply noisy student training for potential improvement over teacher\n",
    "\n",
    "If R-squared < 0.30:\n",
    "\n",
    "1. Check if validation set is too small (only 27 samples)\n",
    "2. Consider k-fold cross-validation for more robust estimates\n",
    "3. Verify data preprocessing matches PercePiano exactly\n",
    "\n",
    "---\n",
    "\n",
    "**Attribution**: This model replicates the architecture from PercePiano (Park et al., ISMIR/Nature 2024).  \n",
    "GitHub: https://github.com/JonghoKimSNU/PercePiano"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
