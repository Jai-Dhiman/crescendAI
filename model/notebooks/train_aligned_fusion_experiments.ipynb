{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Aligned Audio + Fusion Experiments\n",
    "\n",
    "Uses PercePiano fold assignments for proper apples-to-apples comparison.\n",
    "\n",
    "## Why This Notebook Exists\n",
    "Previous experiments used different fold assignments for audio vs symbolic models,\n",
    "causing 60-80% data leakage in symbolic predictions. This notebook fixes that by:\n",
    "- Training audio model on PercePiano's original fold splits\n",
    "- Using symbolic predictions from PercePiano models on their correct validation sets\n",
    "- Running fusion on properly aligned predictions\n",
    "\n",
    "## Experiments\n",
    "- **Audio**: MERT layers 7-12 + MLP (best config from Phase 2)\n",
    "- **Symbolic**: PercePiano HAN predictions (from existing checkpoints)\n",
    "- **Fusion**: Simple average, weighted, ridge stacking, confidence-weighted\n",
    "\n",
    "## Requirements\n",
    "- A100 GPU (80GB VRAM)\n",
    "- rclone configured with `gdrive:` remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA deterministic mode\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(success|already)\" || echo \"rclone ok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers librosa soundfile pytorch_lightning scipy scikit-learn --quiet\n",
    "\n",
    "REPO_DIR = '/tmp/crescendai'\n",
    "if os.path.exists(REPO_DIR):\n",
    "    !cd {REPO_DIR} && git pull origin main\n",
    "else:\n",
    "    !git clone https://github.com/jai-dhiman/crescendai.git {REPO_DIR}\n",
    "print(f\"Repo: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, f'{REPO_DIR}/model/src')\n",
    "\n",
    "import json\n",
    "import subprocess\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from audio_experiments import PERCEPIANO_DIMENSIONS, BASE_CONFIG, SEED\n",
    "from audio_experiments.models import BaseMERTModel\n",
    "from audio_experiments.training import (\n",
    "    run_4fold_mert_experiment,\n",
    "    should_run_experiment, sync_experiment_to_gdrive,\n",
    "    get_completed_experiments, print_experiment_status,\n",
    "    run_bootstrap_experiment, run_paired_tests_experiment,\n",
    "    run_multiple_correction_experiment, run_simple_fusion_experiment,\n",
    "    run_weighted_fusion_experiment, run_ridge_fusion_experiment,\n",
    "    run_confidence_fusion_experiment, run_weight_stability_experiment,\n",
    "    run_category_fusion_experiment, run_error_correlation_experiment,\n",
    "    save_fusion_experiment,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "print(\"Imports: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_ROOT = Path('/tmp/aligned_fusion')\n",
    "AUDIO_DIR = DATA_ROOT / 'audio'\n",
    "LABEL_DIR = DATA_ROOT / 'labels'\n",
    "MERT_CACHE = DATA_ROOT / 'mert_cache'\n",
    "CHECKPOINT_ROOT = DATA_ROOT / 'checkpoints'\n",
    "RESULTS_DIR = DATA_ROOT / 'results'\n",
    "LOG_DIR = DATA_ROOT / 'logs'\n",
    "\n",
    "# GDrive paths\n",
    "GDRIVE_AUDIO = 'gdrive:crescendai_data/audio_baseline/percepiano_rendered'\n",
    "GDRIVE_LABELS = 'gdrive:crescendai_data/percepiano_labels'\n",
    "GDRIVE_FOLDS = 'gdrive:crescendai_data/percepiano_fold_assignments.json'  # PercePiano splits!\n",
    "GDRIVE_MERT = 'gdrive:crescendai_data/audio_baseline/mert_embeddings'\n",
    "GDRIVE_SYMBOLIC_CKPTS = 'gdrive:crescendai_data/checkpoints/percepiano_original'\n",
    "GDRIVE_RESULTS = 'gdrive:crescendai_data/checkpoints/aligned_fusion'\n",
    "\n",
    "for d in [DATA_ROOT, AUDIO_DIR, LABEL_DIR, MERT_CACHE, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def rclone(cmd, desc):\n",
    "    print(f\"{desc}...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Warning: {result.stderr[:200]}\")\n",
    "\n",
    "# Check rclone\n",
    "r = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "if 'gdrive:' not in r.stdout:\n",
    "    raise RuntimeError(\"rclone gdrive not configured\")\n",
    "\n",
    "# Download data\n",
    "rclone(['rclone', 'copy', GDRIVE_LABELS, str(LABEL_DIR)], \"Labels\")\n",
    "rclone(['rclone', 'copyto', GDRIVE_FOLDS, str(DATA_ROOT / 'folds.json')], \"PercePiano folds\")\n",
    "\n",
    "# Load labels and folds\n",
    "with open(LABEL_DIR / 'label_2round_mean_reg_19_with0_rm_highstd0.json') as f:\n",
    "    LABELS = json.load(f)\n",
    "with open(DATA_ROOT / 'folds.json') as f:\n",
    "    FOLD_ASSIGNMENTS_RAW = json.load(f)\n",
    "\n",
    "print(f\"Labels: {len(LABELS)}\")\n",
    "print(\"Folds:\", [f\"fold_{i}: {len(FOLD_ASSIGNMENTS_RAW.get(f'fold_{i}', []))}\" for i in range(4)])\n",
    "print(f\"Test: {len(FOLD_ASSIGNMENTS_RAW.get('test', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Audio Model Training\n",
    "\n",
    "Train MERT layers 7-12 + MLP on PercePiano fold assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERT Embeddings Setup\n",
    "from audio_experiments.extractors import extract_mert_for_layer_range\n",
    "\n",
    "# Download audio files for MERT extraction\n",
    "print(\"Downloading audio files...\")\n",
    "rclone(['rclone', 'copy', GDRIVE_AUDIO, str(AUDIO_DIR), '--progress'], \"Audio files\")\n",
    "print(f\"Audio files: {len(list(AUDIO_DIR.glob('*.wav')))}\")\n",
    "\n",
    "# Get all keys we need\n",
    "all_fold_keys = set()\n",
    "for fold_id in range(4):\n",
    "    all_fold_keys.update(FOLD_ASSIGNMENTS_RAW.get(f\"fold_{fold_id}\", []))\n",
    "ALL_KEYS = sorted(set(LABELS.keys()) & all_fold_keys)\n",
    "print(f\"Total samples needed: {len(ALL_KEYS)}\")\n",
    "\n",
    "# Extract L7-12 embeddings\n",
    "extract_mert_for_layer_range(7, 13, AUDIO_DIR, MERT_CACHE, ALL_KEYS)\n",
    "print(f\"MERT L7-12 embeddings: {len(list(MERT_CACHE.glob('*.pt')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing audio experiments\n",
    "ALL_RESULTS = {}\n",
    "AUDIO_EXP_ID = 'audio_mert_L7-12'\n",
    "\n",
    "COMPLETED_CACHE = get_completed_experiments(GDRIVE_RESULTS)\n",
    "print_experiment_status([AUDIO_EXP_ID], COMPLETED_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train audio model on PercePiano folds\n",
    "if should_run_experiment(AUDIO_EXP_ID, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    print(\"Training audio model on PercePiano fold assignments...\")\n",
    "    \n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling='mean',\n",
    "            loss_type='mse', max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "    \n",
    "    ALL_RESULTS[AUDIO_EXP_ID] = run_4fold_mert_experiment(\n",
    "        AUDIO_EXP_ID, 'MERT L7-12 + MLP (PercePiano folds)',\n",
    "        make_mert_model, MERT_CACHE, LABELS, FOLD_ASSIGNMENTS_RAW,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        AUDIO_EXP_ID, ALL_RESULTS[AUDIO_EXP_ID],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )\n",
    "else:\n",
    "    # Load existing results\n",
    "    result_file = RESULTS_DIR / f'{AUDIO_EXP_ID}.json'\n",
    "    if result_file.exists():\n",
    "        with open(result_file) as f:\n",
    "            ALL_RESULTS[AUDIO_EXP_ID] = json.load(f)\n",
    "    print(f\"Audio experiment already complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Generate Predictions\n",
    "\n",
    "Generate audio and symbolic predictions on validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate audio predictions from trained checkpoints\n",
    "def generate_audio_predictions(fold_assignments, checkpoint_dir, mert_cache, device):\n",
    "    \"\"\"Generate CV predictions using held-out fold for each sample.\"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    # Load all fold models\n",
    "    models = {}\n",
    "    for fold in range(4):\n",
    "        ckpt_path = checkpoint_dir / AUDIO_EXP_ID / f\"fold{fold}_best.ckpt\"\n",
    "        if ckpt_path.exists():\n",
    "            model = BaseMERTModel.load_from_checkpoint(ckpt_path)\n",
    "            model = model.to(device).eval()\n",
    "            models[fold] = model\n",
    "            print(f\"Loaded fold {fold} model\")\n",
    "    \n",
    "    # Generate predictions for each fold's validation set\n",
    "    for fold_id in range(4):\n",
    "        if fold_id not in models:\n",
    "            continue\n",
    "        model = models[fold_id]\n",
    "        fold_keys = fold_assignments.get(f\"fold_{fold_id}\", [])\n",
    "        \n",
    "        for key in fold_keys:\n",
    "            embed_path = mert_cache / f\"{key}.pt\"\n",
    "            if not embed_path.exists():\n",
    "                continue\n",
    "            \n",
    "            embeddings = torch.load(embed_path, weights_only=True)\n",
    "            if embeddings.shape[0] > 1000:\n",
    "                embeddings = embeddings[:1000]\n",
    "            \n",
    "            embeddings = embeddings.unsqueeze(0).to(device)\n",
    "            attention_mask = torch.ones(1, embeddings.shape[1], dtype=torch.bool, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred = model(embeddings, attention_mask)\n",
    "            \n",
    "            predictions[key] = pred.squeeze(0).cpu().numpy().tolist()\n",
    "        \n",
    "        print(f\"Fold {fold_id}: {len([k for k in fold_keys if k in predictions])} predictions\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Generate audio predictions\n",
    "audio_preds_file = DATA_ROOT / 'audio_predictions.json'\n",
    "if not audio_preds_file.exists():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    audio_predictions = generate_audio_predictions(\n",
    "        FOLD_ASSIGNMENTS_RAW, CHECKPOINT_ROOT, MERT_CACHE, device\n",
    "    )\n",
    "    with open(audio_preds_file, 'w') as f:\n",
    "        json.dump(audio_predictions, f)\n",
    "    print(f\"Saved {len(audio_predictions)} audio predictions\")\n",
    "else:\n",
    "    with open(audio_preds_file) as f:\n",
    "        audio_predictions = json.load(f)\n",
    "    print(f\"Loaded {len(audio_predictions)} audio predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for PercePiano symbolic model\n",
    "from types import ModuleType\n",
    "\n",
    "PERCEPIANO_ROOT = Path('/tmp/PercePiano')\n",
    "if not PERCEPIANO_ROOT.exists():\n",
    "    !git clone https://github.com/JonghoKimSNU/PercePiano.git /tmp/PercePiano\n",
    "\n",
    "PERCEPIANO_PATH = PERCEPIANO_ROOT / 'virtuoso' / 'virtuoso'\n",
    "!pip install omegaconf --quiet\n",
    "\n",
    "# Patch numpy 2.0 compatibility\n",
    "if not hasattr(np.lib, 'arraysetops'):\n",
    "    arraysetops = ModuleType('numpy.lib.arraysetops')\n",
    "    arraysetops.isin = np.isin\n",
    "    sys.modules['numpy.lib.arraysetops'] = arraysetops\n",
    "    np.lib.arraysetops = arraysetops\n",
    "\n",
    "sys.path.insert(0, str(PERCEPIANO_PATH / 'pyScoreParser'))\n",
    "sys.path.insert(0, str(PERCEPIANO_PATH))\n",
    "print(f\"PercePiano path: {PERCEPIANO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PercePiano data and checkpoints\n",
    "PP_DATA_ROOT = DATA_ROOT / 'percepiano_data'\n",
    "PP_CKPT_ROOT = DATA_ROOT / 'percepiano_ckpts'\n",
    "PP_DATA_ROOT.mkdir(exist_ok=True)\n",
    "PP_CKPT_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "rclone(['rclone', 'copy', 'gdrive:crescendai_data/percepiano_original', str(PP_DATA_ROOT)], \"PercePiano data\")\n",
    "rclone(['rclone', 'copy', GDRIVE_SYMBOLIC_CKPTS, str(PP_CKPT_ROOT)], \"PercePiano checkpoints\")\n",
    "\n",
    "print(f\"PercePiano folds: {len(list(PP_DATA_ROOT.glob('fold*')))}\")\n",
    "print(f\"PercePiano checkpoints: {len(list(PP_CKPT_ROOT.glob('*.pt')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate symbolic predictions (correctly aligned with PercePiano folds)\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from model_m2pf import VirtuosoNetMultiLevel\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "\n",
    "def extract_label_key(filename):\n",
    "    name = filename.replace('.pkl', '').replace('.mid', '')\n",
    "    if name.startswith('all_2rounds_'):\n",
    "        name = name[len('all_2rounds_'):]\n",
    "    return name\n",
    "\n",
    "def load_sample(pkl_path, max_notes=5000):\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    x = torch.tensor(data['input'], dtype=torch.float32)\n",
    "    if len(x) > max_notes:\n",
    "        x = x[:max_notes]\n",
    "    note_locations = {\n",
    "        'beat': torch.tensor(data['note_location']['beat'][:len(x)], dtype=torch.long),\n",
    "        'measure': torch.tensor(data['note_location']['measure'][:len(x)], dtype=torch.long),\n",
    "        'voice': torch.tensor(data['note_location']['voice'][:len(x)], dtype=torch.long),\n",
    "        'section': torch.tensor(data['note_location']['section'][:len(x)], dtype=torch.long),\n",
    "    }\n",
    "    return x, note_locations\n",
    "\n",
    "def predict_single(model, x, note_locations, device, sigmoid):\n",
    "    batch_x = pack_sequence([x], enforce_sorted=True).to(device)\n",
    "    note_locs = {k: v.unsqueeze(0).to(device) for k, v in note_locations.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_x, None, None, note_locs)\n",
    "        pred = sigmoid(outputs[-1]).squeeze(0).cpu().numpy()\n",
    "    return pred\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate symbolic predictions\n",
    "symbolic_preds_file = DATA_ROOT / 'symbolic_predictions.json'\n",
    "\n",
    "if not symbolic_preds_file.exists():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    # Load config\n",
    "    CONFIG_PATH = PERCEPIANO_PATH.parent / 'ymls' / 'shared' / 'label19' / 'han_measnote_nomask_bigger256.yml'\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    net_param = OmegaConf.create(config['nn_params'])\n",
    "    net_param.graph_keys = []\n",
    "    \n",
    "    symbolic_predictions = {}\n",
    "    \n",
    "    for fold_id in range(4):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FOLD {fold_id}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        # Load fold checkpoint\n",
    "        checkpoint_path = PP_CKPT_ROOT / f'fold{fold_id}_best.pt'\n",
    "        if not checkpoint_path.exists():\n",
    "            print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Load fold stats\n",
    "        fold_path = PP_DATA_ROOT / f'fold{fold_id}'\n",
    "        with open(fold_path / 'train' / 'stat.pkl', 'rb') as f:\n",
    "            fold_stats = pickle.load(f)\n",
    "        \n",
    "        # Update input size\n",
    "        net_param.input_size = max(v[1] for v in fold_stats['key_to_dim']['input'].values())\n",
    "        \n",
    "        # Load model\n",
    "        model = VirtuosoNetMultiLevel(net_param, fold_stats, multi_level=\"total_note_cat\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Loaded model (R2={checkpoint['r2']:.4f}, epoch {checkpoint['epoch']})\")\n",
    "        \n",
    "        # KEY FIX: Use PercePiano fold's VALID directory, not audio fold assignments!\n",
    "        valid_dir = fold_path / 'valid'\n",
    "        valid_files = [f for f in valid_dir.glob('*.pkl') if f.name != 'stat.pkl']\n",
    "        \n",
    "        print(f\"Generating predictions for {len(valid_files)} validation samples...\")\n",
    "        \n",
    "        count = 0\n",
    "        for pkl_file in valid_files:\n",
    "            key = extract_label_key(pkl_file.name)\n",
    "            if key not in LABELS:\n",
    "                continue\n",
    "            \n",
    "            x, note_locations = load_sample(pkl_file)\n",
    "            pred = predict_single(model, x, note_locations, device, sigmoid)\n",
    "            symbolic_predictions[key] = pred.tolist()\n",
    "            count += 1\n",
    "        \n",
    "        print(f\"Generated {count} predictions for fold {fold_id}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    with open(symbolic_preds_file, 'w') as f:\n",
    "        json.dump(symbolic_predictions, f)\n",
    "    print(f\"\\nSaved {len(symbolic_predictions)} symbolic predictions\")\n",
    "else:\n",
    "    with open(symbolic_preds_file) as f:\n",
    "        symbolic_predictions = json.load(f)\n",
    "    print(f\"Loaded {len(symbolic_predictions)} symbolic predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align predictions and labels\n",
    "# Convert fold assignments to sample_key -> fold_id mapping\n",
    "FOLD_ASSIGNMENTS = {}\n",
    "for fold_id in range(4):\n",
    "    for key in FOLD_ASSIGNMENTS_RAW.get(f\"fold_{fold_id}\", []):\n",
    "        FOLD_ASSIGNMENTS[key] = fold_id\n",
    "\n",
    "# Get aligned sample keys (have audio, symbolic, and labels)\n",
    "SAMPLE_KEYS = sorted(\n",
    "    set(audio_predictions.keys()) & \n",
    "    set(symbolic_predictions.keys()) & \n",
    "    set(LABELS.keys()) &\n",
    "    set(FOLD_ASSIGNMENTS.keys())\n",
    ")\n",
    "\n",
    "print(f\"Audio predictions: {len(audio_predictions)}\")\n",
    "print(f\"Symbolic predictions: {len(symbolic_predictions)}\")\n",
    "print(f\"Aligned samples: {len(SAMPLE_KEYS)}\")\n",
    "\n",
    "# Create arrays\n",
    "LABELS_ARR = np.array([LABELS[k][:19] for k in SAMPLE_KEYS])\n",
    "AUDIO_ARR = np.array([audio_predictions[k] for k in SAMPLE_KEYS])\n",
    "SYMBOLIC_ARR = np.array([symbolic_predictions[k] for k in SAMPLE_KEYS])\n",
    "\n",
    "print(f\"Shapes: labels={LABELS_ARR.shape}, audio={AUDIO_ARR.shape}, symbolic={SYMBOLIC_ARR.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUSION_EXPERIMENT_IDS = [\n",
    "    'S0_bootstrap', 'S1_paired_tests', 'S2_multiple_correction',\n",
    "    'F0_simple', 'F1_weighted', 'F2_ridge', 'F3_confidence',\n",
    "    'A0_stability', 'A1_category', 'A2_error_corr',\n",
    "]\n",
    "\n",
    "FUSION_COMPLETED = get_completed_experiments(GDRIVE_RESULTS)\n",
    "print_experiment_status(FUSION_EXPERIMENT_IDS, FUSION_COMPLETED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S0: Bootstrap CIs\n",
    "if should_run_experiment('S0_bootstrap', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['S0_bootstrap'] = run_bootstrap_experiment(\n",
    "        'S0_bootstrap', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR, n_bootstrap=10000\n",
    "    )\n",
    "    save_fusion_experiment('S0_bootstrap', ALL_RESULTS['S0_bootstrap'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('S0_bootstrap', ALL_RESULTS['S0_bootstrap'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S1: Paired Tests\n",
    "if should_run_experiment('S1_paired_tests', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['S1_paired_tests'] = run_paired_tests_experiment(\n",
    "        'S1_paired_tests', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR\n",
    "    )\n",
    "    save_fusion_experiment('S1_paired_tests', ALL_RESULTS['S1_paired_tests'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('S1_paired_tests', ALL_RESULTS['S1_paired_tests'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S2: Multiple Correction\n",
    "if should_run_experiment('S2_multiple_correction', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    if 'S1_paired_tests' not in ALL_RESULTS:\n",
    "        with open(RESULTS_DIR / 'S1_paired_tests.json') as f:\n",
    "            ALL_RESULTS['S1_paired_tests'] = json.load(f)\n",
    "    \n",
    "    ALL_RESULTS['S2_multiple_correction'] = run_multiple_correction_experiment(\n",
    "        'S2_multiple_correction', ALL_RESULTS['S1_paired_tests']\n",
    "    )\n",
    "    save_fusion_experiment('S2_multiple_correction', ALL_RESULTS['S2_multiple_correction'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('S2_multiple_correction', ALL_RESULTS['S2_multiple_correction'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Fusion Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F0: Simple Average\n",
    "if should_run_experiment('F0_simple', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['F0_simple'] = run_simple_fusion_experiment(\n",
    "        'F0_simple', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR\n",
    "    )\n",
    "    save_fusion_experiment('F0_simple', ALL_RESULTS['F0_simple'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F0_simple', ALL_RESULTS['F0_simple'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1: Weighted Fusion (CV)\n",
    "if should_run_experiment('F1_weighted', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['F1_weighted'] = run_weighted_fusion_experiment(\n",
    "        'F1_weighted', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR,\n",
    "        FOLD_ASSIGNMENTS, SAMPLE_KEYS\n",
    "    )\n",
    "    save_fusion_experiment('F1_weighted', ALL_RESULTS['F1_weighted'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F1_weighted', ALL_RESULTS['F1_weighted'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F2: Ridge Stacking\n",
    "if should_run_experiment('F2_ridge', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['F2_ridge'] = run_ridge_fusion_experiment(\n",
    "        'F2_ridge', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR,\n",
    "        FOLD_ASSIGNMENTS, SAMPLE_KEYS\n",
    "    )\n",
    "    save_fusion_experiment('F2_ridge', ALL_RESULTS['F2_ridge'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F2_ridge', ALL_RESULTS['F2_ridge'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F3: Confidence Weighted\n",
    "if should_run_experiment('F3_confidence', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['F3_confidence'] = run_confidence_fusion_experiment(\n",
    "        'F3_confidence', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR\n",
    "    )\n",
    "    save_fusion_experiment('F3_confidence', ALL_RESULTS['F3_confidence'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F3_confidence', ALL_RESULTS['F3_confidence'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0: Weight Stability\n",
    "if should_run_experiment('A0_stability', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    if 'F1_weighted' not in ALL_RESULTS:\n",
    "        with open(RESULTS_DIR / 'F1_weighted.json') as f:\n",
    "            ALL_RESULTS['F1_weighted'] = json.load(f)\n",
    "    \n",
    "    ALL_RESULTS['A0_stability'] = run_weight_stability_experiment(\n",
    "        'A0_stability', ALL_RESULTS['F1_weighted']['fold_weights']\n",
    "    )\n",
    "    save_fusion_experiment('A0_stability', ALL_RESULTS['A0_stability'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('A0_stability', ALL_RESULTS['A0_stability'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1: Category Fusion\n",
    "if should_run_experiment('A1_category', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['A1_category'] = run_category_fusion_experiment(\n",
    "        'A1_category', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR,\n",
    "        FOLD_ASSIGNMENTS, SAMPLE_KEYS\n",
    "    )\n",
    "    save_fusion_experiment('A1_category', ALL_RESULTS['A1_category'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('A1_category', ALL_RESULTS['A1_category'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2: Error Correlation\n",
    "if should_run_experiment('A2_error_corr', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['A2_error_corr'] = run_error_correlation_experiment(\n",
    "        'A2_error_corr', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR\n",
    "    )\n",
    "    save_fusion_experiment('A2_error_corr', ALL_RESULTS['A2_error_corr'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('A2_error_corr', ALL_RESULTS['A2_error_corr'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results from disk\n",
    "for exp_id in FUSION_EXPERIMENT_IDS:\n",
    "    if exp_id not in ALL_RESULTS:\n",
    "        result_file = RESULTS_DIR / f\"{exp_id}.json\"\n",
    "        if result_file.exists():\n",
    "            with open(result_file) as f:\n",
    "                ALL_RESULTS[exp_id] = json.load(f)\n",
    "\n",
    "# Print summary\n",
    "print(\"=\"*80)\n",
    "print(\"ALIGNED FUSION RESULTS (PercePiano Folds)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Baselines\n",
    "audio_r2 = ALL_RESULTS.get('S0_bootstrap', {}).get('audio', {}).get('overall', {}).get('r2', 0)\n",
    "symbolic_r2 = ALL_RESULTS.get('S0_bootstrap', {}).get('symbolic', {}).get('overall', {}).get('r2', 0)\n",
    "best_single = max(audio_r2, symbolic_r2)\n",
    "\n",
    "print(f\"\\n{'Model':<25} {'R2':>10} {'95% CI':>25} {'vs Best':>12}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "if 'S0_bootstrap' in ALL_RESULTS:\n",
    "    s0 = ALL_RESULTS['S0_bootstrap']\n",
    "    a = s0['audio']['overall']\n",
    "    s = s0['symbolic']['overall']\n",
    "    print(f\"{'Audio (MERT L7-12)':<25} {a['r2']:>10.4f} [{a['ci_lower']:.3f}, {a['ci_upper']:.3f}] {'---':>12}\")\n",
    "    print(f\"{'Symbolic (PercePiano)':<25} {s['r2']:>10.4f} [{s['ci_lower']:.3f}, {s['ci_upper']:.3f}] {'---':>12}\")\n",
    "\n",
    "print(\"-\"*75)\n",
    "\n",
    "fusion_exps = [('F0_simple', 'Simple Average'), ('F1_weighted', 'Weighted CV'),\n",
    "               ('F2_ridge', 'Ridge Stacking'), ('F3_confidence', 'Confidence')]\n",
    "for exp_id, name in fusion_exps:\n",
    "    if exp_id in ALL_RESULTS:\n",
    "        r = ALL_RESULTS[exp_id]\n",
    "        r2 = r['overall_r2']\n",
    "        b = r['bootstrap']['overall']\n",
    "        diff = r2 - best_single\n",
    "        print(f\"{name:<25} {r2:>10.4f} [{b['ci_lower']:.3f}, {b['ci_upper']:.3f}] {diff:>+12.4f}\")\n",
    "\n",
    "print(\"=\"*75)\n",
    "\n",
    "# Key comparison\n",
    "print(f\"\\nKEY FINDING:\")\n",
    "if audio_r2 > symbolic_r2:\n",
    "    print(f\"  Audio beats Symbolic by {audio_r2 - symbolic_r2:.4f} R2\")\n",
    "else:\n",
    "    print(f\"  Symbolic beats Audio by {symbolic_r2 - audio_r2:.4f} R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final sync\n",
    "with open(RESULTS_DIR / 'aligned_fusion_all_results.json', 'w') as f:\n",
    "    json.dump(ALL_RESULTS, f, indent=2, default=str)\n",
    "\n",
    "subprocess.run(['rclone', 'copy', str(RESULTS_DIR), GDRIVE_RESULTS], capture_output=True)\n",
    "print(f\"Done! Results at: {GDRIVE_RESULTS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
