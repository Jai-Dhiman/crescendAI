{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Aligned Audio + Fusion Experiments\n",
    "\n",
    "Uses PercePiano fold assignments for proper apples-to-apples comparison.\n",
    "\n",
    "## Why This Notebook Exists\n",
    "Previous experiments used different fold assignments for audio vs symbolic models,\n",
    "causing 60-80% data leakage in symbolic predictions. This notebook fixes that by:\n",
    "- Training audio model on PercePiano's original fold splits\n",
    "- Using symbolic predictions from PercePiano models on their correct validation sets\n",
    "- Running fusion on properly aligned predictions\n",
    "\n",
    "## Experiments\n",
    "- **Audio**: MERT layers 7-12 + MLP (best config from Phase 2)\n",
    "- **Symbolic**: PercePiano HAN predictions (from existing checkpoints)\n",
    "- **Fusion**: Simple average, weighted, ridge stacking, confidence-weighted\n",
    "\n",
    "## Requirements\n",
    "- A100 GPU (80GB VRAM)\n",
    "- rclone configured with `gdrive:` remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA deterministic mode\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(success|already)\" || echo \"rclone ok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers librosa soundfile pytorch_lightning scipy scikit-learn nnAudio pretty_midi --quiet\n",
    "\n",
    "REPO_DIR = '/tmp/crescendai'\n",
    "if os.path.exists(REPO_DIR):\n",
    "    !cd {REPO_DIR} && git pull origin main\n",
    "else:\n",
    "    !git clone https://github.com/jai-dhiman/crescendai.git {REPO_DIR}\n",
    "print(f\"Repo: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, f'{REPO_DIR}/model/src')\n",
    "\n",
    "import json\n",
    "import subprocess\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from audio_experiments import PERCEPIANO_DIMENSIONS, BASE_CONFIG, SEED\n",
    "from audio_experiments.models import BaseMERTModel\n",
    "from audio_experiments.training import (\n",
    "    run_4fold_mert_experiment,\n",
    "    should_run_experiment, sync_experiment_to_gdrive,\n",
    "    get_completed_experiments, print_experiment_status,\n",
    "    run_bootstrap_experiment, run_paired_tests_experiment,\n",
    "    run_multiple_correction_experiment, run_simple_fusion_experiment,\n",
    "    run_weighted_fusion_experiment, run_ridge_fusion_experiment,\n",
    "    run_confidence_fusion_experiment, run_weight_stability_experiment,\n",
    "    run_category_fusion_experiment, run_error_correlation_experiment,\n",
    "    save_fusion_experiment,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "print(\"Imports: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_ROOT = Path('/tmp/aligned_fusion')\n",
    "AUDIO_DIR = DATA_ROOT / 'audio'\n",
    "LABEL_DIR = DATA_ROOT / 'labels'\n",
    "MERT_CACHE = DATA_ROOT / 'mert_cache'\n",
    "CHECKPOINT_ROOT = DATA_ROOT / 'checkpoints'\n",
    "RESULTS_DIR = DATA_ROOT / 'results'\n",
    "LOG_DIR = DATA_ROOT / 'logs'\n",
    "\n",
    "# GDrive paths\n",
    "GDRIVE_AUDIO = 'gdrive:crescendai_data/audio_baseline/percepiano_rendered'\n",
    "GDRIVE_LABELS = 'gdrive:crescendai_data/percepiano_labels'\n",
    "GDRIVE_FOLDS = 'gdrive:crescendai_data/percepiano_fold_assignments.json'  # PercePiano splits!\n",
    "GDRIVE_MERT = 'gdrive:crescendai_data/audio_baseline/mert_embeddings'\n",
    "GDRIVE_SYMBOLIC_CKPTS = 'gdrive:crescendai_data/checkpoints/percepiano_original'\n",
    "GDRIVE_RESULTS = 'gdrive:crescendai_data/checkpoints/aligned_fusion'\n",
    "\n",
    "for d in [DATA_ROOT, AUDIO_DIR, LABEL_DIR, MERT_CACHE, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def rclone(cmd, desc):\n",
    "    print(f\"{desc}...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Warning: {result.stderr[:200]}\")\n",
    "\n",
    "# Check rclone\n",
    "r = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "if 'gdrive:' not in r.stdout:\n",
    "    raise RuntimeError(\"rclone gdrive not configured\")\n",
    "\n",
    "# Download data\n",
    "rclone(['rclone', 'copy', GDRIVE_LABELS, str(LABEL_DIR)], \"Labels\")\n",
    "rclone(['rclone', 'copyto', GDRIVE_FOLDS, str(DATA_ROOT / 'folds.json')], \"PercePiano folds\")\n",
    "\n",
    "# Load labels and folds\n",
    "with open(LABEL_DIR / 'label_2round_mean_reg_19_with0_rm_highstd0.json') as f:\n",
    "    LABELS = json.load(f)\n",
    "with open(DATA_ROOT / 'folds.json') as f:\n",
    "    FOLD_ASSIGNMENTS_RAW = json.load(f)\n",
    "\n",
    "print(f\"Labels: {len(LABELS)}\")\n",
    "print(\"Folds:\", [f\"fold_{i}: {len(FOLD_ASSIGNMENTS_RAW.get(f'fold_{i}', []))}\" for i in range(4)])\n",
    "print(f\"Test: {len(FOLD_ASSIGNMENTS_RAW.get('test', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Audio Model Training\n",
    "\n",
    "Train MERT layers 7-12 + MLP on PercePiano fold assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERT Embeddings Setup\n",
    "from audio_experiments.extractors import extract_mert_for_layer_range\n",
    "\n",
    "# Download audio files for MERT extraction\n",
    "print(\"Downloading audio files...\")\n",
    "rclone(['rclone', 'copy', GDRIVE_AUDIO, str(AUDIO_DIR), '--progress'], \"Audio files\")\n",
    "print(f\"Audio files: {len(list(AUDIO_DIR.glob('*.wav')))}\")\n",
    "\n",
    "# Get all keys we need\n",
    "all_fold_keys = set()\n",
    "for fold_id in range(4):\n",
    "    all_fold_keys.update(FOLD_ASSIGNMENTS_RAW.get(f\"fold_{fold_id}\", []))\n",
    "ALL_KEYS = sorted(set(LABELS.keys()) & all_fold_keys)\n",
    "print(f\"Total samples needed: {len(ALL_KEYS)}\")\n",
    "\n",
    "# Extract L7-12 embeddings\n",
    "extract_mert_for_layer_range(7, 13, AUDIO_DIR, MERT_CACHE, ALL_KEYS)\n",
    "print(f\"MERT L7-12 embeddings: {len(list(MERT_CACHE.glob('*.pt')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing audio experiments\n",
    "ALL_RESULTS = {}\n",
    "AUDIO_EXP_ID = 'audio_mert_L7-12'\n",
    "\n",
    "COMPLETED_CACHE = get_completed_experiments(GDRIVE_RESULTS)\n",
    "print_experiment_status([AUDIO_EXP_ID], COMPLETED_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train audio model on PercePiano folds\n",
    "if should_run_experiment(AUDIO_EXP_ID, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    print(\"Training audio model on PercePiano fold assignments...\")\n",
    "    \n",
    "    def make_mert_model(cfg):\n",
    "        return BaseMERTModel(\n",
    "            input_dim=cfg['input_dim'], hidden_dim=cfg['hidden_dim'],\n",
    "            dropout=cfg['dropout'], learning_rate=cfg['learning_rate'],\n",
    "            weight_decay=cfg['weight_decay'], pooling='mean',\n",
    "            loss_type='mse', max_epochs=cfg['max_epochs'],\n",
    "        )\n",
    "    \n",
    "    ALL_RESULTS[AUDIO_EXP_ID] = run_4fold_mert_experiment(\n",
    "        AUDIO_EXP_ID, 'MERT L7-12 + MLP (PercePiano folds)',\n",
    "        make_mert_model, MERT_CACHE, LABELS, FOLD_ASSIGNMENTS_RAW,\n",
    "        BASE_CONFIG, CHECKPOINT_ROOT, RESULTS_DIR, LOG_DIR\n",
    "    )\n",
    "    sync_experiment_to_gdrive(\n",
    "        AUDIO_EXP_ID, ALL_RESULTS[AUDIO_EXP_ID],\n",
    "        RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS\n",
    "    )\n",
    "else:\n",
    "    # Load existing results\n",
    "    result_file = RESULTS_DIR / f'{AUDIO_EXP_ID}.json'\n",
    "    if result_file.exists():\n",
    "        with open(result_file) as f:\n",
    "            ALL_RESULTS[AUDIO_EXP_ID] = json.load(f)\n",
    "    print(f\"Audio experiment already complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Generate Predictions\n",
    "\n",
    "Generate audio and symbolic predictions on validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate audio predictions from trained checkpoints\n",
    "def generate_audio_predictions(fold_assignments, checkpoint_dir, mert_cache, device):\n",
    "    \"\"\"Generate CV predictions using held-out fold for each sample.\"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    # Load all fold models\n",
    "    models = {}\n",
    "    for fold in range(4):\n",
    "        ckpt_path = checkpoint_dir / AUDIO_EXP_ID / f\"fold{fold}_best.ckpt\"\n",
    "        if ckpt_path.exists():\n",
    "            model = BaseMERTModel.load_from_checkpoint(ckpt_path)\n",
    "            model = model.to(device).eval()\n",
    "            models[fold] = model\n",
    "            print(f\"Loaded fold {fold} model\")\n",
    "    \n",
    "    if not models:\n",
    "        raise RuntimeError(\"No model checkpoints found. Cannot generate predictions.\")\n",
    "    \n",
    "    # Generate predictions for each fold's validation set\n",
    "    for fold_id in range(4):\n",
    "        if fold_id not in models:\n",
    "            continue\n",
    "        model = models[fold_id]\n",
    "        fold_keys = fold_assignments.get(f\"fold_{fold_id}\", [])\n",
    "        \n",
    "        for key in fold_keys:\n",
    "            embed_path = mert_cache / f\"{key}.pt\"\n",
    "            if not embed_path.exists():\n",
    "                continue\n",
    "            \n",
    "            embeddings = torch.load(embed_path, weights_only=True)\n",
    "            if embeddings.shape[0] > 1000:\n",
    "                embeddings = embeddings[:1000]\n",
    "            \n",
    "            embeddings = embeddings.unsqueeze(0).to(device)\n",
    "            attention_mask = torch.ones(1, embeddings.shape[1], dtype=torch.bool, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred = model(embeddings, attention_mask)\n",
    "            \n",
    "            predictions[key] = pred.squeeze(0).cpu().numpy().tolist()\n",
    "        \n",
    "        print(f\"Fold {fold_id}: {len([k for k in fold_keys if k in predictions])} predictions\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Ensure checkpoints are available locally (download from GDrive if needed)\n",
    "local_ckpt_dir = CHECKPOINT_ROOT / AUDIO_EXP_ID\n",
    "if not local_ckpt_dir.exists() or len(list(local_ckpt_dir.glob('*.ckpt'))) < 4:\n",
    "    print(\"Downloading audio model checkpoints from GDrive...\")\n",
    "    local_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Note: checkpoints are stored under checkpoints/ subdirectory on GDrive\n",
    "    gdrive_ckpt_path = f'{GDRIVE_RESULTS}/checkpoints/{AUDIO_EXP_ID}'\n",
    "    result = subprocess.run(\n",
    "        ['rclone', 'copy', gdrive_ckpt_path, str(local_ckpt_dir)],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    ckpts = list(local_ckpt_dir.glob('*.ckpt'))\n",
    "    print(f\"Downloaded {len(ckpts)} checkpoints\")\n",
    "    if len(ckpts) == 0:\n",
    "        print(f\"Warning: No checkpoints found at {gdrive_ckpt_path}\")\n",
    "        if result.stderr:\n",
    "            print(f\"rclone error: {result.stderr[:200]}\")\n",
    "\n",
    "# Generate or load audio predictions (with validation)\n",
    "audio_preds_file = DATA_ROOT / 'audio_predictions.json'\n",
    "expected_samples = len(ALL_KEYS)  # Should match MERT embeddings count\n",
    "\n",
    "need_generation = True\n",
    "if audio_preds_file.exists():\n",
    "    with open(audio_preds_file) as f:\n",
    "        audio_predictions = json.load(f)\n",
    "    if len(audio_predictions) >= expected_samples * 0.9:  # Allow 10% tolerance\n",
    "        print(f\"Loaded {len(audio_predictions)} audio predictions (valid)\")\n",
    "        need_generation = False\n",
    "    else:\n",
    "        print(f\"Found {len(audio_predictions)} predictions but expected ~{expected_samples}, regenerating...\")\n",
    "\n",
    "if need_generation:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    audio_predictions = generate_audio_predictions(\n",
    "        FOLD_ASSIGNMENTS_RAW, CHECKPOINT_ROOT, MERT_CACHE, device\n",
    "    )\n",
    "    with open(audio_preds_file, 'w') as f:\n",
    "        json.dump(audio_predictions, f)\n",
    "    print(f\"Saved {len(audio_predictions)} audio predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for PercePiano symbolic model\n",
    "from types import ModuleType\n",
    "\n",
    "PERCEPIANO_ROOT = Path('/tmp/PercePiano')\n",
    "if not PERCEPIANO_ROOT.exists():\n",
    "    !git clone https://github.com/JonghoKimSNU/PercePiano.git /tmp/PercePiano\n",
    "\n",
    "PERCEPIANO_PATH = PERCEPIANO_ROOT / 'virtuoso' / 'virtuoso'\n",
    "!pip install omegaconf --quiet\n",
    "\n",
    "# Patch numpy 2.0 compatibility\n",
    "if not hasattr(np.lib, 'arraysetops'):\n",
    "    arraysetops = ModuleType('numpy.lib.arraysetops')\n",
    "    arraysetops.isin = np.isin\n",
    "    sys.modules['numpy.lib.arraysetops'] = arraysetops\n",
    "    np.lib.arraysetops = arraysetops\n",
    "\n",
    "sys.path.insert(0, str(PERCEPIANO_PATH / 'pyScoreParser'))\n",
    "sys.path.insert(0, str(PERCEPIANO_PATH))\n",
    "print(f\"PercePiano path: {PERCEPIANO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PercePiano data and checkpoints\n",
    "PP_DATA_ROOT = DATA_ROOT / 'percepiano_data'\n",
    "PP_CKPT_ROOT = DATA_ROOT / 'percepiano_ckpts'\n",
    "PP_DATA_ROOT.mkdir(exist_ok=True)\n",
    "PP_CKPT_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "rclone(['rclone', 'copy', 'gdrive:crescendai_data/percepiano_original', str(PP_DATA_ROOT)], \"PercePiano data\")\n",
    "rclone(['rclone', 'copy', GDRIVE_SYMBOLIC_CKPTS, str(PP_CKPT_ROOT)], \"PercePiano checkpoints\")\n",
    "\n",
    "print(f\"PercePiano folds: {len(list(PP_DATA_ROOT.glob('fold*')))}\")\n",
    "print(f\"PercePiano checkpoints: {len(list(PP_CKPT_ROOT.glob('*.pt')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate symbolic predictions (correctly aligned with PercePiano folds)\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from model_m2pf import VirtuosoNetMultiLevel\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "\n",
    "def extract_label_key(filename):\n",
    "    name = filename.replace('.pkl', '').replace('.mid', '')\n",
    "    if name.startswith('all_2rounds_'):\n",
    "        name = name[len('all_2rounds_'):]\n",
    "    return name\n",
    "\n",
    "def load_sample(pkl_path, max_notes=5000):\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    x = torch.tensor(data['input'], dtype=torch.float32)\n",
    "    if len(x) > max_notes:\n",
    "        x = x[:max_notes]\n",
    "    note_locations = {\n",
    "        'beat': torch.tensor(data['note_location']['beat'][:len(x)], dtype=torch.long),\n",
    "        'measure': torch.tensor(data['note_location']['measure'][:len(x)], dtype=torch.long),\n",
    "        'voice': torch.tensor(data['note_location']['voice'][:len(x)], dtype=torch.long),\n",
    "        'section': torch.tensor(data['note_location']['section'][:len(x)], dtype=torch.long),\n",
    "    }\n",
    "    return x, note_locations\n",
    "\n",
    "def predict_single(model, x, note_locations, device, sigmoid):\n",
    "    batch_x = pack_sequence([x], enforce_sorted=True).to(device)\n",
    "    note_locs = {k: v.unsqueeze(0).to(device) for k, v in note_locations.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_x, None, None, note_locs)\n",
    "        pred = sigmoid(outputs[-1]).squeeze(0).cpu().numpy()\n",
    "    return pred\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate symbolic predictions\n",
    "symbolic_preds_file = DATA_ROOT / 'symbolic_predictions.json'\n",
    "\n",
    "if not symbolic_preds_file.exists():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    # Load config\n",
    "    CONFIG_PATH = PERCEPIANO_PATH.parent / 'ymls' / 'shared' / 'label19' / 'han_measnote_nomask_bigger256.yml'\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    net_param = OmegaConf.create(config['nn_params'])\n",
    "    net_param.graph_keys = []\n",
    "    \n",
    "    symbolic_predictions = {}\n",
    "    \n",
    "    for fold_id in range(4):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FOLD {fold_id}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        # Load fold checkpoint\n",
    "        checkpoint_path = PP_CKPT_ROOT / f'fold{fold_id}_best.pt'\n",
    "        if not checkpoint_path.exists():\n",
    "            print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Load fold stats\n",
    "        fold_path = PP_DATA_ROOT / f'fold{fold_id}'\n",
    "        with open(fold_path / 'train' / 'stat.pkl', 'rb') as f:\n",
    "            fold_stats = pickle.load(f)\n",
    "        \n",
    "        # Update input size\n",
    "        net_param.input_size = max(v[1] for v in fold_stats['key_to_dim']['input'].values())\n",
    "        \n",
    "        # Load model\n",
    "        model = VirtuosoNetMultiLevel(net_param, fold_stats, multi_level=\"total_note_cat\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Loaded model (R2={checkpoint['r2']:.4f}, epoch {checkpoint['epoch']})\")\n",
    "        \n",
    "        # KEY FIX: Use PercePiano fold's VALID directory, not audio fold assignments!\n",
    "        valid_dir = fold_path / 'valid'\n",
    "        valid_files = [f for f in valid_dir.glob('*.pkl') if f.name != 'stat.pkl']\n",
    "        \n",
    "        print(f\"Generating predictions for {len(valid_files)} validation samples...\")\n",
    "        \n",
    "        count = 0\n",
    "        for pkl_file in valid_files:\n",
    "            key = extract_label_key(pkl_file.name)\n",
    "            if key not in LABELS:\n",
    "                continue\n",
    "            \n",
    "            x, note_locations = load_sample(pkl_file)\n",
    "            pred = predict_single(model, x, note_locations, device, sigmoid)\n",
    "            symbolic_predictions[key] = pred.tolist()\n",
    "            count += 1\n",
    "        \n",
    "        print(f\"Generated {count} predictions for fold {fold_id}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    with open(symbolic_preds_file, 'w') as f:\n",
    "        json.dump(symbolic_predictions, f)\n",
    "    print(f\"\\nSaved {len(symbolic_predictions)} symbolic predictions\")\n",
    "else:\n",
    "    with open(symbolic_preds_file) as f:\n",
    "        symbolic_predictions = json.load(f)\n",
    "    print(f\"Loaded {len(symbolic_predictions)} symbolic predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align predictions and labels\n",
    "# Convert fold assignments to sample_key -> fold_id mapping\n",
    "FOLD_ASSIGNMENTS = {}\n",
    "for fold_id in range(4):\n",
    "    for key in FOLD_ASSIGNMENTS_RAW.get(f\"fold_{fold_id}\", []):\n",
    "        FOLD_ASSIGNMENTS[key] = fold_id\n",
    "\n",
    "# Get aligned sample keys (have audio, symbolic, and labels)\n",
    "SAMPLE_KEYS = sorted(\n",
    "    set(audio_predictions.keys()) & \n",
    "    set(symbolic_predictions.keys()) & \n",
    "    set(LABELS.keys()) &\n",
    "    set(FOLD_ASSIGNMENTS.keys())\n",
    ")\n",
    "\n",
    "print(f\"Audio predictions: {len(audio_predictions)}\")\n",
    "print(f\"Symbolic predictions: {len(symbolic_predictions)}\")\n",
    "print(f\"Aligned samples: {len(SAMPLE_KEYS)}\")\n",
    "\n",
    "# Create arrays\n",
    "LABELS_ARR = np.array([LABELS[k][:19] for k in SAMPLE_KEYS])\n",
    "AUDIO_ARR = np.array([audio_predictions[k] for k in SAMPLE_KEYS])\n",
    "SYMBOLIC_ARR = np.array([symbolic_predictions[k] for k in SAMPLE_KEYS])\n",
    "\n",
    "print(f\"Shapes: labels={LABELS_ARR.shape}, audio={AUDIO_ARR.shape}, symbolic={SYMBOLIC_ARR.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUSION_EXPERIMENT_IDS = [\n",
    "    'S0_bootstrap', 'S1_paired_tests', 'S2_multiple_correction',\n",
    "    'F0_simple', 'F1_weighted', 'F2_ridge', 'F3_confidence',\n",
    "    'A0_stability', 'A1_category', 'A2_error_corr',\n",
    "]\n",
    "\n",
    "FUSION_COMPLETED = get_completed_experiments(GDRIVE_RESULTS)\n",
    "print_experiment_status(FUSION_EXPERIMENT_IDS, FUSION_COMPLETED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S0: Bootstrap CIs\n",
    "if should_run_experiment('S0_bootstrap', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['S0_bootstrap'] = run_bootstrap_experiment(\n",
    "        'S0_bootstrap', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR, n_bootstrap=10000\n",
    "    )\n",
    "    save_fusion_experiment('S0_bootstrap', ALL_RESULTS['S0_bootstrap'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('S0_bootstrap', ALL_RESULTS['S0_bootstrap'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S1: Paired Tests\n",
    "if should_run_experiment('S1_paired_tests', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['S1_paired_tests'] = run_paired_tests_experiment(\n",
    "        'S1_paired_tests', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR\n",
    "    )\n",
    "    save_fusion_experiment('S1_paired_tests', ALL_RESULTS['S1_paired_tests'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('S1_paired_tests', ALL_RESULTS['S1_paired_tests'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S2: Multiple Correction\n",
    "if should_run_experiment('S2_multiple_correction', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    if 'S1_paired_tests' not in ALL_RESULTS:\n",
    "        with open(RESULTS_DIR / 'S1_paired_tests.json') as f:\n",
    "            ALL_RESULTS['S1_paired_tests'] = json.load(f)\n",
    "    \n",
    "    ALL_RESULTS['S2_multiple_correction'] = run_multiple_correction_experiment(\n",
    "        'S2_multiple_correction', ALL_RESULTS['S1_paired_tests']\n",
    "    )\n",
    "    save_fusion_experiment('S2_multiple_correction', ALL_RESULTS['S2_multiple_correction'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('S2_multiple_correction', ALL_RESULTS['S2_multiple_correction'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Fusion Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F0: Simple Average\n",
    "if should_run_experiment('F0_simple', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['F0_simple'] = run_simple_fusion_experiment(\n",
    "        'F0_simple', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR\n",
    "    )\n",
    "    save_fusion_experiment('F0_simple', ALL_RESULTS['F0_simple'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F0_simple', ALL_RESULTS['F0_simple'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1: Weighted Fusion (CV)\n",
    "if should_run_experiment('F1_weighted', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['F1_weighted'] = run_weighted_fusion_experiment(\n",
    "        'F1_weighted', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR,\n",
    "        FOLD_ASSIGNMENTS, SAMPLE_KEYS\n",
    "    )\n",
    "    save_fusion_experiment('F1_weighted', ALL_RESULTS['F1_weighted'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F1_weighted', ALL_RESULTS['F1_weighted'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F2: Ridge Stacking\n",
    "if should_run_experiment('F2_ridge', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['F2_ridge'] = run_ridge_fusion_experiment(\n",
    "        'F2_ridge', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR,\n",
    "        FOLD_ASSIGNMENTS, SAMPLE_KEYS\n",
    "    )\n",
    "    save_fusion_experiment('F2_ridge', ALL_RESULTS['F2_ridge'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F2_ridge', ALL_RESULTS['F2_ridge'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F3: Confidence Weighted\n",
    "if should_run_experiment('F3_confidence', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['F3_confidence'] = run_confidence_fusion_experiment(\n",
    "        'F3_confidence', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR\n",
    "    )\n",
    "    save_fusion_experiment('F3_confidence', ALL_RESULTS['F3_confidence'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F3_confidence', ALL_RESULTS['F3_confidence'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0: Weight Stability\n",
    "if should_run_experiment('A0_stability', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    if 'F1_weighted' not in ALL_RESULTS:\n",
    "        with open(RESULTS_DIR / 'F1_weighted.json') as f:\n",
    "            ALL_RESULTS['F1_weighted'] = json.load(f)\n",
    "    \n",
    "    ALL_RESULTS['A0_stability'] = run_weight_stability_experiment(\n",
    "        'A0_stability', ALL_RESULTS['F1_weighted']['fold_weights']\n",
    "    )\n",
    "    save_fusion_experiment('A0_stability', ALL_RESULTS['A0_stability'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('A0_stability', ALL_RESULTS['A0_stability'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1: Category Fusion\n",
    "if should_run_experiment('A1_category', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['A1_category'] = run_category_fusion_experiment(\n",
    "        'A1_category', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR,\n",
    "        FOLD_ASSIGNMENTS, SAMPLE_KEYS\n",
    "    )\n",
    "    save_fusion_experiment('A1_category', ALL_RESULTS['A1_category'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('A1_category', ALL_RESULTS['A1_category'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2: Error Correlation\n",
    "if should_run_experiment('A2_error_corr', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    ALL_RESULTS['A2_error_corr'] = run_error_correlation_experiment(\n",
    "        'A2_error_corr', AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR\n",
    "    )\n",
    "    save_fusion_experiment('A2_error_corr', ALL_RESULTS['A2_error_corr'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('A2_error_corr', ALL_RESULTS['A2_error_corr'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results from disk\n",
    "for exp_id in FUSION_EXPERIMENT_IDS:\n",
    "    if exp_id not in ALL_RESULTS:\n",
    "        result_file = RESULTS_DIR / f\"{exp_id}.json\"\n",
    "        if result_file.exists():\n",
    "            with open(result_file) as f:\n",
    "                ALL_RESULTS[exp_id] = json.load(f)\n",
    "\n",
    "# Print summary\n",
    "print(\"=\"*80)\n",
    "print(\"ALIGNED FUSION RESULTS (PercePiano Folds)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Baselines\n",
    "audio_r2 = ALL_RESULTS.get('S0_bootstrap', {}).get('audio', {}).get('overall', {}).get('r2', 0)\n",
    "symbolic_r2 = ALL_RESULTS.get('S0_bootstrap', {}).get('symbolic', {}).get('overall', {}).get('r2', 0)\n",
    "best_single = max(audio_r2, symbolic_r2)\n",
    "\n",
    "print(f\"\\n{'Model':<25} {'R2':>10} {'95% CI':>25} {'vs Best':>12}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "if 'S0_bootstrap' in ALL_RESULTS:\n",
    "    s0 = ALL_RESULTS['S0_bootstrap']\n",
    "    a = s0['audio']['overall']\n",
    "    s = s0['symbolic']['overall']\n",
    "    print(f\"{'Audio (MERT L7-12)':<25} {a['r2']:>10.4f} [{a['ci_lower']:.3f}, {a['ci_upper']:.3f}] {'---':>12}\")\n",
    "    print(f\"{'Symbolic (PercePiano)':<25} {s['r2']:>10.4f} [{s['ci_lower']:.3f}, {s['ci_upper']:.3f}] {'---':>12}\")\n",
    "\n",
    "print(\"-\"*75)\n",
    "\n",
    "fusion_exps = [('F0_simple', 'Simple Average'), ('F1_weighted', 'Weighted CV'),\n",
    "               ('F2_ridge', 'Ridge Stacking'), ('F3_confidence', 'Confidence')]\n",
    "for exp_id, name in fusion_exps:\n",
    "    if exp_id in ALL_RESULTS:\n",
    "        r = ALL_RESULTS[exp_id]\n",
    "        r2 = r['overall_r2']\n",
    "        b = r['bootstrap']['overall']\n",
    "        diff = r2 - best_single\n",
    "        print(f\"{name:<25} {r2:>10.4f} [{b['ci_lower']:.3f}, {b['ci_upper']:.3f}] {diff:>+12.4f}\")\n",
    "\n",
    "print(\"=\"*75)\n",
    "\n",
    "# Key comparison\n",
    "print(f\"\\nKEY FINDING:\")\n",
    "if audio_r2 > symbolic_r2:\n",
    "    print(f\"  Audio beats Symbolic by {audio_r2 - symbolic_r2:.4f} R2\")\n",
    "else:\n",
    "    print(f\"  Symbolic beats Audio by {symbolic_r2 - audio_r2:.4f} R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final sync\n",
    "with open(RESULTS_DIR / 'aligned_fusion_all_results.json', 'w') as f:\n",
    "    json.dump(ALL_RESULTS, f, indent=2, default=str)\n",
    "\n",
    "subprocess.run(['rclone', 'copy', str(RESULTS_DIR), GDRIVE_RESULTS], capture_output=True)\n",
    "print(f\"Done! Results at: {GDRIVE_RESULTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mrzntcvqcg",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Learned Fusion Models\n",
    "\n",
    "Advanced fusion strategies that train neural networks to combine modalities:\n",
    "- **F4**: Modality Dropout - randomly drops modalities during training to force unique representations\n",
    "- **F5**: Orthogonality Loss - penalizes correlated audio/symbolic representations\n",
    "- **F6**: Residual Fusion - symbolic branch predicts audio's residual errors\n",
    "- **F7**: Dimension-Weighted - learns per-dimension routing between modalities\n",
    "\n",
    "These address the 0.77 error correlation by encouraging complementary representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bt9lip93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned fusion model imports\n",
    "from audio_experiments.models import (\n",
    "    ModalityDropoutFusion,\n",
    "    OrthogonalityFusion,\n",
    "    ResidualFusion,\n",
    "    DimensionWeightedFusion,\n",
    ")\n",
    "from audio_experiments.training import bootstrap_r2_extended\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FusionDataset(Dataset):\n",
    "    \"\"\"Dataset for learned fusion model training.\"\"\"\n",
    "    def __init__(self, audio_preds, symbolic_preds, labels, sample_keys, fold_assignments, fold_id, mode='train'):\n",
    "        self.audio_preds = audio_preds\n",
    "        self.symbolic_preds = symbolic_preds\n",
    "        self.labels = labels\n",
    "\n",
    "        # Get indices for this fold\n",
    "        fold_ids = np.array([fold_assignments[k] for k in sample_keys])\n",
    "        if mode == 'train':\n",
    "            self.indices = np.where(fold_ids != fold_id)[0]\n",
    "        else:\n",
    "            self.indices = np.where(fold_ids == fold_id)[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.indices[idx]\n",
    "        return {\n",
    "            'audio_repr': torch.tensor(self.audio_preds[i], dtype=torch.float32),\n",
    "            'symbolic_repr': torch.tensor(self.symbolic_preds[i], dtype=torch.float32),\n",
    "            'labels': torch.tensor(self.labels[i], dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "# Add learned fusion experiments to tracking\n",
    "LEARNED_FUSION_IDS = ['F4_modality_dropout', 'F5_orthogonality', 'F6_residual', 'F7_dim_weighted']\n",
    "FUSION_EXPERIMENT_IDS.extend(LEARNED_FUSION_IDS)\n",
    "\n",
    "print_experiment_status(LEARNED_FUSION_IDS, FUSION_COMPLETED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "avn0tif4ckr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F4: Modality Dropout Fusion\n",
    "if should_run_experiment('F4_modality_dropout', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"F4: Modality Dropout Fusion (p=0.25)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    all_preds, all_labels_list = [], []\n",
    "\n",
    "    for fold_id in range(4):\n",
    "        print(f\"\\nFold {fold_id}...\")\n",
    "        train_ds = FusionDataset(AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR, SAMPLE_KEYS, FOLD_ASSIGNMENTS, fold_id, 'train')\n",
    "        val_ds = FusionDataset(AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR, SAMPLE_KEYS, FOLD_ASSIGNMENTS, fold_id, 'val')\n",
    "\n",
    "        train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "        val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "        model = ModalityDropoutFusion(\n",
    "            audio_dim=19, symbolic_dim=19, hidden_dim=128,\n",
    "            modality_dropout=0.25, learning_rate=1e-3, max_epochs=100,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=100,\n",
    "            callbacks=[pl.callbacks.EarlyStopping(monitor='val_r2', mode='max', patience=15)],\n",
    "            accelerator='auto', devices=1, enable_progress_bar=True, logger=False,\n",
    "        )\n",
    "        trainer.fit(model, train_dl, val_dl)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                pred = model(batch['audio_repr'], batch['symbolic_repr'])\n",
    "                all_preds.append(pred.numpy())\n",
    "                all_labels_list.append(batch['labels'].numpy())\n",
    "\n",
    "    fused_preds = np.vstack(all_preds)\n",
    "    fused_labels = np.vstack(all_labels_list)\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(fused_labels, fused_preds)\n",
    "    bootstrap = bootstrap_r2_extended(fused_labels, fused_preds, 10000)\n",
    "\n",
    "    ALL_RESULTS['F4_modality_dropout'] = {\n",
    "        'exp_id': 'F4_modality_dropout',\n",
    "        'overall_r2': float(r2),\n",
    "        'bootstrap': bootstrap,\n",
    "        'modality_dropout_p': 0.25,\n",
    "    }\n",
    "    print(f\"\\nModality Dropout Fusion R2: {r2:.4f} [{bootstrap['overall']['ci_lower']:.3f}, {bootstrap['overall']['ci_upper']:.3f}]\")\n",
    "    save_fusion_experiment('F4_modality_dropout', ALL_RESULTS['F4_modality_dropout'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F4_modality_dropout', ALL_RESULTS['F4_modality_dropout'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jxpwzb685dn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F5: Orthogonality Loss Fusion\n",
    "if should_run_experiment('F5_orthogonality', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"F5: Orthogonality Loss Fusion (lambda=0.1)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    all_preds, all_labels_list = [], []\n",
    "\n",
    "    for fold_id in range(4):\n",
    "        print(f\"\\nFold {fold_id}...\")\n",
    "        train_ds = FusionDataset(AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR, SAMPLE_KEYS, FOLD_ASSIGNMENTS, fold_id, 'train')\n",
    "        val_ds = FusionDataset(AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR, SAMPLE_KEYS, FOLD_ASSIGNMENTS, fold_id, 'val')\n",
    "\n",
    "        train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "        val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "        model = OrthogonalityFusion(\n",
    "            audio_dim=19, symbolic_dim=19, hidden_dim=128,\n",
    "            orthogonality_lambda=0.1, learning_rate=1e-3, max_epochs=100,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=100,\n",
    "            callbacks=[pl.callbacks.EarlyStopping(monitor='val_r2', mode='max', patience=15)],\n",
    "            accelerator='auto', devices=1, enable_progress_bar=True, logger=False,\n",
    "        )\n",
    "        trainer.fit(model, train_dl, val_dl)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                pred, _, _ = model(batch['audio_repr'], batch['symbolic_repr'])\n",
    "                all_preds.append(pred.numpy())\n",
    "                all_labels_list.append(batch['labels'].numpy())\n",
    "\n",
    "    fused_preds = np.vstack(all_preds)\n",
    "    fused_labels = np.vstack(all_labels_list)\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(fused_labels, fused_preds)\n",
    "    bootstrap = bootstrap_r2_extended(fused_labels, fused_preds, 10000)\n",
    "\n",
    "    ALL_RESULTS['F5_orthogonality'] = {\n",
    "        'exp_id': 'F5_orthogonality',\n",
    "        'overall_r2': float(r2),\n",
    "        'bootstrap': bootstrap,\n",
    "        'orthogonality_lambda': 0.1,\n",
    "    }\n",
    "    print(f\"\\nOrthogonality Fusion R2: {r2:.4f} [{bootstrap['overall']['ci_lower']:.3f}, {bootstrap['overall']['ci_upper']:.3f}]\")\n",
    "    save_fusion_experiment('F5_orthogonality', ALL_RESULTS['F5_orthogonality'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F5_orthogonality', ALL_RESULTS['F5_orthogonality'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdgr8wqmn29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F6: Residual Fusion (symbolic predicts audio's errors)\n",
    "if should_run_experiment('F6_residual', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"F6: Residual Fusion (symbolic predicts audio errors)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    all_preds, all_labels_list = [], []\n",
    "\n",
    "    for fold_id in range(4):\n",
    "        print(f\"\\nFold {fold_id}...\")\n",
    "        train_ds = FusionDataset(AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR, SAMPLE_KEYS, FOLD_ASSIGNMENTS, fold_id, 'train')\n",
    "        val_ds = FusionDataset(AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR, SAMPLE_KEYS, FOLD_ASSIGNMENTS, fold_id, 'val')\n",
    "\n",
    "        train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "        val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "        model = ResidualFusion(\n",
    "            audio_dim=19, symbolic_dim=19, hidden_dim=128,\n",
    "            learning_rate=1e-3, max_epochs=100,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=100,\n",
    "            callbacks=[pl.callbacks.EarlyStopping(monitor='val_r2', mode='max', patience=15)],\n",
    "            accelerator='auto', devices=1, enable_progress_bar=True, logger=False,\n",
    "        )\n",
    "        trainer.fit(model, train_dl, val_dl)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                pred, _, _ = model(batch['audio_repr'], batch['symbolic_repr'])\n",
    "                all_preds.append(pred.numpy())\n",
    "                all_labels_list.append(batch['labels'].numpy())\n",
    "\n",
    "    fused_preds = np.vstack(all_preds)\n",
    "    fused_labels = np.vstack(all_labels_list)\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(fused_labels, fused_preds)\n",
    "    bootstrap = bootstrap_r2_extended(fused_labels, fused_preds, 10000)\n",
    "\n",
    "    # Get learned residual scale\n",
    "    residual_scale = float(model.residual_scale.item())\n",
    "\n",
    "    ALL_RESULTS['F6_residual'] = {\n",
    "        'exp_id': 'F6_residual',\n",
    "        'overall_r2': float(r2),\n",
    "        'bootstrap': bootstrap,\n",
    "        'learned_residual_scale': residual_scale,\n",
    "    }\n",
    "    print(f\"\\nResidual Fusion R2: {r2:.4f} [{bootstrap['overall']['ci_lower']:.3f}, {bootstrap['overall']['ci_upper']:.3f}]\")\n",
    "    print(f\"Learned residual scale: {residual_scale:.4f}\")\n",
    "    save_fusion_experiment('F6_residual', ALL_RESULTS['F6_residual'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F6_residual', ALL_RESULTS['F6_residual'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebl5zevcrc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F7: Dimension-Weighted Fusion (learnable per-dim routing)\n",
    "if should_run_experiment('F7_dim_weighted', CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, FUSION_COMPLETED):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"F7: Dimension-Weighted Fusion (learnable per-dim routing)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    all_preds, all_labels_list = [], []\n",
    "    all_weights = []\n",
    "\n",
    "    for fold_id in range(4):\n",
    "        print(f\"\\nFold {fold_id}...\")\n",
    "        train_ds = FusionDataset(AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR, SAMPLE_KEYS, FOLD_ASSIGNMENTS, fold_id, 'train')\n",
    "        val_ds = FusionDataset(AUDIO_ARR, SYMBOLIC_ARR, LABELS_ARR, SAMPLE_KEYS, FOLD_ASSIGNMENTS, fold_id, 'val')\n",
    "\n",
    "        train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "        val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "        model = DimensionWeightedFusion(\n",
    "            audio_dim=19, symbolic_dim=19, hidden_dim=128,\n",
    "            learning_rate=1e-3, max_epochs=100,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=100,\n",
    "            callbacks=[pl.callbacks.EarlyStopping(monitor='val_r2', mode='max', patience=15)],\n",
    "            accelerator='auto', devices=1, enable_progress_bar=True, logger=False,\n",
    "        )\n",
    "        trainer.fit(model, train_dl, val_dl)\n",
    "\n",
    "        # Get learned weights\n",
    "        weights = model.get_learned_weights()\n",
    "        all_weights.append(weights)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                pred, _, _ = model(batch['audio_repr'], batch['symbolic_repr'])\n",
    "                all_preds.append(pred.numpy())\n",
    "                all_labels_list.append(batch['labels'].numpy())\n",
    "\n",
    "    fused_preds = np.vstack(all_preds)\n",
    "    fused_labels = np.vstack(all_labels_list)\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(fused_labels, fused_preds)\n",
    "    bootstrap = bootstrap_r2_extended(fused_labels, fused_preds, 10000)\n",
    "\n",
    "    # Average weights across folds\n",
    "    avg_audio_weights = np.mean([w['audio'] for w in all_weights], axis=0)\n",
    "    dim_weights = {dim: float(avg_audio_weights[i]) for i, dim in enumerate(PERCEPIANO_DIMENSIONS)}\n",
    "\n",
    "    ALL_RESULTS['F7_dim_weighted'] = {\n",
    "        'exp_id': 'F7_dim_weighted',\n",
    "        'overall_r2': float(r2),\n",
    "        'bootstrap': bootstrap,\n",
    "        'learned_audio_weights': dim_weights,\n",
    "    }\n",
    "    print(f\"\\nDimension-Weighted Fusion R2: {r2:.4f} [{bootstrap['overall']['ci_lower']:.3f}, {bootstrap['overall']['ci_upper']:.3f}]\")\n",
    "    print(\"\\nLearned audio weights (higher = more audio):\")\n",
    "    for dim, w in sorted(dim_weights.items(), key=lambda x: -x[1])[:5]:\n",
    "        print(f\"  {dim}: {w:.3f}\")\n",
    "    save_fusion_experiment('F7_dim_weighted', ALL_RESULTS['F7_dim_weighted'], RESULTS_DIR, ALL_RESULTS)\n",
    "    sync_experiment_to_gdrive('F7_dim_weighted', ALL_RESULTS['F7_dim_weighted'], RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m32yjup3wi",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete Fusion Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l3h6bjd6i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Fusion Results (including learned models)\n",
    "for exp_id in FUSION_EXPERIMENT_IDS:\n",
    "    if exp_id not in ALL_RESULTS:\n",
    "        result_file = RESULTS_DIR / f\"{exp_id}.json\"\n",
    "        if result_file.exists():\n",
    "            with open(result_file) as f:\n",
    "                ALL_RESULTS[exp_id] = json.load(f)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETE FUSION RESULTS (All Strategies)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Baselines\n",
    "audio_r2 = ALL_RESULTS.get('S0_bootstrap', {}).get('audio', {}).get('overall', {}).get('r2', 0)\n",
    "symbolic_r2 = ALL_RESULTS.get('S0_bootstrap', {}).get('symbolic', {}).get('overall', {}).get('r2', 0)\n",
    "best_single = max(audio_r2, symbolic_r2)\n",
    "\n",
    "print(f\"\\n{'Model':<30} {'R2':>10} {'95% CI':>22} {'vs Best':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if 'S0_bootstrap' in ALL_RESULTS:\n",
    "    s0 = ALL_RESULTS['S0_bootstrap']\n",
    "    a = s0['audio']['overall']\n",
    "    s = s0['symbolic']['overall']\n",
    "    print(f\"{'Audio (MERT L7-12)':<30} {a['r2']:>10.4f} [{a['ci_lower']:.3f}, {a['ci_upper']:.3f}] {'---':>10}\")\n",
    "    print(f\"{'Symbolic (PercePiano)':<30} {s['r2']:>10.4f} [{s['ci_lower']:.3f}, {s['ci_upper']:.3f}] {'---':>10}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"Post-hoc Fusion:\")\n",
    "\n",
    "posthoc = [('F0_simple', 'Simple Average'), ('F1_weighted', 'Weighted CV'),\n",
    "           ('F2_ridge', 'Ridge Stacking'), ('F3_confidence', 'Confidence')]\n",
    "for exp_id, name in posthoc:\n",
    "    if exp_id in ALL_RESULTS:\n",
    "        r = ALL_RESULTS[exp_id]\n",
    "        r2 = r['overall_r2']\n",
    "        b = r['bootstrap']['overall']\n",
    "        diff = r2 - best_single\n",
    "        print(f\"{name:<30} {r2:>10.4f} [{b['ci_lower']:.3f}, {b['ci_upper']:.3f}] {diff:>+10.4f}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"Learned Fusion:\")\n",
    "\n",
    "learned = [('F4_modality_dropout', 'Modality Dropout (p=0.25)'),\n",
    "           ('F5_orthogonality', 'Orthogonality (lambda=0.1)'),\n",
    "           ('F6_residual', 'Residual Fusion'),\n",
    "           ('F7_dim_weighted', 'Dimension-Weighted')]\n",
    "for exp_id, name in learned:\n",
    "    if exp_id in ALL_RESULTS:\n",
    "        r = ALL_RESULTS[exp_id]\n",
    "        r2 = r['overall_r2']\n",
    "        b = r['bootstrap']['overall']\n",
    "        diff = r2 - best_single\n",
    "        print(f\"{name:<30} {r2:>10.4f} [{b['ci_lower']:.3f}, {b['ci_upper']:.3f}] {diff:>+10.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best overall\n",
    "all_fusion = [(ALL_RESULTS.get(exp_id, {}).get('overall_r2', 0), exp_id) \n",
    "              for exp_id in ['F0_simple', 'F1_weighted', 'F2_ridge', 'F3_confidence',\n",
    "                            'F4_modality_dropout', 'F5_orthogonality', 'F6_residual', 'F7_dim_weighted']\n",
    "              if exp_id in ALL_RESULTS]\n",
    "if all_fusion:\n",
    "    best = max(all_fusion)\n",
    "    print(f\"\\nBest Fusion: {best[1]} (R2={best[0]:.4f})\")\n",
    "    print(f\"Improvement over best single modality: {best[0] - best_single:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5gwsx5h9th",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final sync (all experiments)\n",
    "with open(RESULTS_DIR / 'aligned_fusion_all_results.json', 'w') as f:\n",
    "    json.dump(ALL_RESULTS, f, indent=2, default=str)\n",
    "\n",
    "subprocess.run(['rclone', 'copy', str(RESULTS_DIR), GDRIVE_RESULTS], capture_output=True)\n",
    "print(f\"Done! All results synced to: {GDRIVE_RESULTS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}