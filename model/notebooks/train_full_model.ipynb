{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Piano Performance Evaluation - 3-Way Model Comparison (Colab)\n\nTrains 3 models to prove multi-modal fusion advantage:\n1. Audio-Only (MERT only)\n2. MIDI-Only (MIDIBert only)\n3. Fusion (MERT + MIDIBert)\n\n**Dimensions**: 3 core (note_accuracy, rhythmic_precision, tone_quality)\n**Sample size**: 114,246 training samples (full dataset)\n**Expected time**: 3-4 hours training + 10-15 min setup\n**Goal**: Prove fusion beats both baselines by 15-20%"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.pop(\"HF_TOKEN\", None)\n",
    "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
    "\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "try:\n",
    "    import getpass as gp\n",
    "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
    "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
    "    if not isinstance(token, str):\n",
    "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
    "    token = token.strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"Empty token provided\")\n",
    "    login(token=token, add_to_git_credential=False)\n",
    "    who = HfApi().whoami(token=token)\n",
    "    print(f\"✓ Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "except Exception as e:\n",
    "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
    "    print(\"Falling back to interactive login widget...\")\n",
    "    login()\n",
    "    try:\n",
    "        who = HfApi().whoami()\n",
    "        print(f\"✓ Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"[HF Login] Verification skipped: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "CHECKPOINT_ROOT = '/content/drive/MyDrive/crescendai_checkpoints'\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Checkpoint directory ready: {CHECKPOINT_ROOT}\")\n",
    "print(\"\\nNote: Training data will be downloaded from Hugging Face Hub\")\n",
    "print(\"      (No longer using Drive for data - much faster!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"\\n⚠️  NO GPU! Enable GPU: Runtime → Change runtime type → T4 GPU\")\n",
    "    raise RuntimeError(\"GPU required\")\n",
    "\n",
    "print(f\"\\n✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"✓ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "!rm -rf /content/crescendai\n",
    "!git clone https://github.com/Jai-Dhiman/crescendai.git /content/crescendai\n",
    "%cd /content/crescendai/model\n",
    "!git log -1 --oneline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv (fast Python package manager)\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add to PATH for this session\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "print(\"\\n✓ uv installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!uv pip install --system -e .\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore', message='divide by zero')\nwarnings.filterwarnings('ignore', category=SyntaxWarning)  # pydub regex warnings\nwarnings.filterwarnings('ignore', category=UserWarning, module='torchaudio')\nwarnings.filterwarnings('ignore', category=UserWarning, module='torchmetrics')\n\nimport torch\nimport pytorch_lightning as pl\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Lightning: {pl.__version__}\")\nprint(\"✓ Dependencies installed\")\n\n!python scripts/setup_colab_environment.py"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MERT model (cached after first download)\n",
    "from transformers import AutoModel\n",
    "\n",
    "print(\"Downloading MERT-95M (~380MB)...\")\n",
    "model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\n",
    "print(\"✓ MERT-95M cached\")\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data from Hugging Face Hub\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 2: DOWNLOAD DATA FROM HUGGING FACE HUB\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThis is 10-100x faster and more reliable than Google Drive!\")\n",
    "print(\"Download time: 7-15 minutes (one-time per session)\\n\")\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "HF_REPO_ID = \"Jai-D/crescendai-data\"\n",
    "\n",
    "print(f\"Downloading from: {HF_REPO_ID}\")\n",
    "print(\"Archive size: ~20-25 GB compressed\\n\")\n",
    "\n",
    "# Download archive\n",
    "print(\"1. Downloading archive...\")\n",
    "archive_path = hf_hub_download(\n",
    "    repo_id=HF_REPO_ID,\n",
    "    filename=\"crescendai_data.tar.gz\",\n",
    "    repo_type=\"model\",\n",
    "    local_dir=\"/tmp/\",\n",
    "    local_dir_use_symlinks=False,\n",
    ")\n",
    "print(f\"   ✓ Downloaded to: {archive_path}\")\n",
    "\n",
    "# Extract\n",
    "print(\"\\n2. Extracting archive...\")\n",
    "with tarfile.open(archive_path, 'r:gz') as tar:\n",
    "    members = tar.getmembers()\n",
    "    print(f\"   Extracting {len(members):,} files...\")\n",
    "    tar.extractall('/tmp/crescendai_data/')\n",
    "\n",
    "print(\"   ✓ Extracted to: /tmp/crescendai_data/\")\n",
    "\n",
    "# Clean up archive to save space\n",
    "print(\"\\n3. Cleaning up...\")\n",
    "os.remove(archive_path)\n",
    "print(\"   ✓ Removed archive file\")\n",
    "\n",
    "# Verify structure\n",
    "print(\"\\n4. Verifying data structure...\")\n",
    "expected_paths = [\n",
    "    '/tmp/crescendai_data/data/all_segments',\n",
    "    '/tmp/crescendai_data/data/annotations',\n",
    "]\n",
    "\n",
    "all_good = True\n",
    "for path in expected_paths:\n",
    "    if os.path.exists(path):\n",
    "        if 'all_segments' in path:\n",
    "            num_files = len([f for f in os.listdir(path) if f.endswith('.wav')])\n",
    "            print(f\"   ✓ {path}: {num_files:,} audio files\")\n",
    "        else:\n",
    "            num_files = len([f for f in os.listdir(path) if f.endswith('.jsonl')])\n",
    "            print(f\"   ✓ {path}: {num_files} annotation files\")\n",
    "    else:\n",
    "        print(f\"   ✗ {path}: NOT FOUND\")\n",
    "        all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ DATA DOWNLOAD COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nData ready at: /tmp/crescendai_data/\")\n",
    "    print(\"Training will be 10-30x faster than reading from Drive!\")\n",
    "else:\n",
    "    print(\"\\n✗ Data structure verification failed!\")\n",
    "    print(\"   Check that your archive has the correct structure\")\n",
    "    raise RuntimeError(\"Data download verification failed\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Alternative: Extract MAESTRO from Google Drive\nprint(\"=\"*70)\nprint(\"ALTERNATIVE: EXTRACT MAESTRO FROM GOOGLE DRIVE\")\nprint(\"=\"*70)\n\nimport zipfile\nfrom pathlib import Path\nimport shutil\n\n# UPDATE THIS PATH to where you uploaded maestro-v3.0.0.zip in your Drive\nMAESTRO_ZIP_PATH = \"/content/drive/MyDrive/crescendai_data/maestro-v3.0.0.zip\"\n\n# Check if zip file exists\nif not Path(MAESTRO_ZIP_PATH).exists():\n    print(f\"\\n✗ MAESTRO zip not found at: {MAESTRO_ZIP_PATH}\")\n    print(\"\\nPlease:\")\n    print(\"1. Download MAESTRO v3.0.0 from https://magenta.tensorflow.org/datasets/maestro#v300\")\n    print(\"2. Upload maestro-v3.0.0.zip to your Google Drive\")\n    print(\"3. Update MAESTRO_ZIP_PATH in this cell\")\n    raise FileNotFoundError(f\"MAESTRO zip not found: {MAESTRO_ZIP_PATH}\")\n\nprint(f\"\\n✓ Found MAESTRO zip: {MAESTRO_ZIP_PATH}\")\nprint(f\"   Size: {Path(MAESTRO_ZIP_PATH).stat().st_size / 1e9:.1f} GB\")\n\n# Extract to /tmp for fast access\nextract_dir = Path(\"/tmp/maestro-v3.0.0\")\nif extract_dir.exists():\n    print(f\"\\nRemoving existing extraction at {extract_dir}...\")\n    shutil.rmtree(extract_dir)\n\nprint(f\"\\nExtracting MAESTRO to /tmp (this may take 5-10 minutes)...\")\nwith zipfile.ZipFile(MAESTRO_ZIP_PATH, 'r') as zip_ref:\n    zip_ref.extractall(\"/tmp/\")\n\nprint(f\"✓ Extracted to: {extract_dir}\")\n\n# Verify extraction\naudio_files = list(extract_dir.glob(\"**/*.wav\"))\nmidi_files = list(extract_dir.glob(\"**/*.midi\"))\ncsv_file = extract_dir / \"maestro-v3.0.0.csv\"\n\nprint(f\"\\nVerifying extraction:\")\nprint(f\"  Audio files: {len(audio_files):,}\")\nprint(f\"  MIDI files: {len(midi_files):,}\")\nprint(f\"  CSV file: {'✓' if csv_file.exists() else '✗'}\")\n\nif len(audio_files) == 0 or len(midi_files) == 0 or not csv_file.exists():\n    raise RuntimeError(\"MAESTRO extraction incomplete!\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"✓ MAESTRO DATASET READY\")\nprint(\"=\"*70)\nprint(f\"\\nDataset location: {extract_dir}\")\nprint(\"\\nNote: You'll need to run the preprocessing scripts to create\")\nprint(\"      annotation files and segments from this raw MAESTRO data.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Alternative: Use Original MAESTRO Dataset from Google Drive\n\n**Skip this cell if you're using the HuggingFace Hub data above**\n\nThis alternative approach uses the original MAESTRO v3.0.0 dataset that you can upload to Google Drive. This avoids any corrupted MIDI files from the processed archive.\n\n**Setup Instructions:**\n1. Download MAESTRO v3.0.0 from https://magenta.tensorflow.org/datasets/maestro#v300\n2. Upload `maestro-v3.0.0.zip` to your Google Drive (in a folder like `crescendai_data/`)\n3. Update the path below to point to your uploaded zip file\n4. Run this cell instead of the HuggingFace download cell above",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Fix annotation paths (Drive → Local SSD)\nprint(\"=\"*70)\nprint(\"STEP 2.5: FIX ANNOTATION PATHS\")\nprint(\"=\"*70)\nprint(\"\\nUpdating annotation files to use local SSD paths...\\n\")\n\n!python scripts/fix_annotation_paths.py\n\nprint(\"\\n✓ Annotation paths updated for local SSD access\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Quick verification - check a few sample paths\nprint(\"Verifying data paths...\")\n\nimport json\nfrom pathlib import Path\n\n# Check a sample annotation\nwith open('/tmp/crescendai_data/data/annotations/synthetic_train.jsonl') as f:\n    sample = json.loads(f.readline())\n    \nprint(f\"\\nSample annotation:\")\nprint(f\"  Audio: {sample['audio_path']}\")\nprint(f\"  MIDI:  {sample['midi_path']}\")\n\n# Verify files exist\naudio_exists = Path(sample['audio_path']).exists()\nmidi_exists = Path(sample['midi_path']).exists() if sample['midi_path'] else False\n\nprint(f\"\\nFile existence check:\")\nprint(f\"  Audio exists: {'✓' if audio_exists else '✗'}\")\nprint(f\"  MIDI exists:  {'✓' if midi_exists else '✗ (may be OK if path is None)'}\")\n\nif not audio_exists:\n    print(f\"\\n⚠️  WARNING: Audio file not found!\")\n    print(f\"     Check that data extraction completed correctly\")\n    print(f\"     Expected: {sample['audio_path']}\")\nelif not midi_exists and sample['midi_path']:\n    print(f\"\\n⚠️  WARNING: MIDI file not found!\")\n    print(f\"     Expected: {sample['midi_path']}\")\nelse:\n    print(f\"\\n✓ Sample files verified - data structure looks correct!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preflight Check\nprint(\"=\"*70)\nprint(\"STEP 3: PREFLIGHT CHECK\")\nprint(\"=\"*70)\nprint(\"\\nVerifying training environment and data...\\n\")\n\n!python scripts/preflight_check.py --config configs/experiment_full.yaml\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment 1: Audio-Only (~60-80 min)\n\nTraining with audio features only (MERT-95M encoder)"
  },
  {
   "cell_type": "code",
   "source": "%%time\n!python evaluate_audio_only.py \\\n  --checkpoint /content/drive/MyDrive/crescendai_checkpoints/audio_full/audio-epoch=02-val_loss=7.9935.ckpt \\\n  --test-data /tmp/crescendai_data/data/annotations/synthetic_test.jsonl \\\n  --batch-size 16 \\\n  --num-workers 4",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluate Audio-Only Model\n\nTest the completed audio-only checkpoint",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n!python train.py --config configs/experiment_full.yaml --mode audio"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment 2: MIDI-Only (~45-60 min)\n\nTraining with MIDI features only (MIDIBert encoder)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n!python train.py --config configs/experiment_full.yaml --mode midi"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment 3: Fusion (~90-120 min)\n\nTraining with both audio and MIDI features (multi-modal fusion)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n!python train.py --config configs/experiment_full.yaml --mode fusion"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results\n",
    "\n",
    "Load all 3 trained models and compare performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pytorch_lightning as pl\nfrom src.models.lightning_module import PerformanceEvaluationModel\nfrom src.data.dataset import create_dataloaders\nfrom pathlib import Path\n\n# Load all 3 models\nmodels = {}\nfor mode in ['audio', 'midi', 'fusion']:\n    ckpt_dir = Path(f'/content/drive/MyDrive/crescendai_checkpoints/{mode}_full')\n    ckpts = list(ckpt_dir.glob('*.ckpt'))\n    if ckpts:\n        latest = sorted(ckpts)[-1]\n        print(f\"Loading {mode}: {latest.name}\")\n        models[mode] = PerformanceEvaluationModel.load_from_checkpoint(str(latest))\n        models[mode].eval()\n        models[mode] = models[mode].cuda()\n    else:\n        print(f\"⚠️  No checkpoint found for {mode}\")\n\n# Create test dataloader (using local SSD paths)\n_, _, test_loader = create_dataloaders(\n    train_annotation_path='/tmp/crescendai_data/data/annotations/synthetic_train.jsonl',\n    val_annotation_path='/tmp/crescendai_data/data/annotations/synthetic_val.jsonl',\n    test_annotation_path='/tmp/crescendai_data/data/annotations/synthetic_test.jsonl',\n    dimension_names=['note_accuracy', 'rhythmic_precision', 'tone_quality'],\n    batch_size=8,\n    num_workers=4,  # Can use parallel loading with local data\n    augmentation_config=None,\n    audio_sample_rate=24000,\n    max_audio_length=240000,\n    max_midi_events=512,\n)\n\n# Evaluate each model\ntrainer = pl.Trainer(accelerator='auto', devices='auto', precision=16)\nresults = {}\n\nfor mode, model in models.items():\n    print(f\"\\nEvaluating {mode}...\")\n    test_results = trainer.test(model, dataloaders=test_loader, verbose=False)\n    results[mode] = test_results[0]\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPARISON\")\nprint(\"=\"*70)\nprint(f\"{'Dimension':<25} {'Audio r':<12} {'MIDI r':<12} {'Fusion r':<12} {'Gain'}\")\nprint(\"-\"*70)\n\nfor dim in ['note_accuracy', 'rhythmic_precision', 'tone_quality']:\n    audio_r = results.get('audio', {}).get(f'test_pearson_{dim}', 0)\n    midi_r = results.get('midi', {}).get(f'test_pearson_{dim}', 0)\n    fusion_r = results.get('fusion', {}).get(f'test_pearson_{dim}', 0)\n    gain = fusion_r - max(audio_r, midi_r)\n    \n    print(f\"{dim:<25} {audio_r:>11.3f} {midi_r:>11.3f} {fusion_r:>11.3f} {gain:>+11.3f}\")\n\navg_gain = sum(\n    results.get('fusion', {}).get(f'test_pearson_{dim}', 0) - \n    max(results.get('audio', {}).get(f'test_pearson_{dim}', 0),\n        results.get('midi', {}).get(f'test_pearson_{dim}', 0))\n    for dim in ['note_accuracy', 'rhythmic_precision', 'tone_quality']\n) / 3\n\nprint(\"-\"*70)\nprint(f\"Average fusion gain: {avg_gain:+.3f} ({avg_gain*100:+.1f}%)\")\nprint(\"=\"*70)\n\nif avg_gain > 0.05:\n    print(\"\\n✓ SUCCESS: Fusion shows clear multi-modal advantage!\")\nelse:\n    print(\"\\n⚠️  WARNING: Fusion gain is marginal. Check fusion implementation.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}