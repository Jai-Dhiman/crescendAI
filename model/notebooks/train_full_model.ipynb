{
 "cells": [
  {
   "cell_type": "code",
   "source": "print(\"=\"*70)\nprint(\"EXTRACTING MAESTRO WITH VARIANCE DATASET\")\nprint(\"=\"*70)\n\nimport os\nimport tarfile\nfrom pathlib import Path\n\n# Extract tar.gz to /tmp/ (local SSD for fast access)\ntarball_path = Path(\"/content/maestro_with_variance.tar.gz\")  # Adjust if uploaded elsewhere\n\nif not tarball_path.exists():\n    print(f\"✗ ERROR: {tarball_path} not found!\")\n    print(\"\\nPlease upload maestro_with_variance.tar.gz to /content/\")\n    print(\"You can create it by running:\")\n    print(\"  python scripts/prepare_maestro_for_upload.py --maestro_zip ~/Downloads/maestro-v3.0.0.zip\")\n    raise FileNotFoundError(f\"{tarball_path} not found\")\n\nextract_dir = Path(\"/tmp/maestro_data\")\nextract_dir.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Extracting {tarball_path.name}...\")\nprint(f\"Size: {tarball_path.stat().st_size / (1024**3):.2f} GB\")\nprint(f\"Destination: {extract_dir}\\n\")\n\nwith tarfile.open(tarball_path, \"r:gz\") as tar:\n    tar.extractall(extract_dir)\n\nprint(\"✓ Extraction complete!\")\nprint(f\"\\nDataset structure:\")\nprint(f\"  Audio: {extract_dir}/audio/\")\nprint(f\"  MIDI: {extract_dir}/midi/\")\nprint(f\"  Annotations: {extract_dir}/annotations/\")\n\n# Verify files\naudio_files = list((extract_dir / \"audio\").glob(\"*.wav\"))\nmidi_files = list((extract_dir / \"midi\").glob(\"*.mid\"))\nannotation_files = list((extract_dir / \"annotations\").glob(\"*.jsonl\"))\n\nprint(f\"\\nFiles extracted:\")\nprint(f\"  Audio files: {len(audio_files):,}\")\nprint(f\"  MIDI files: {len(midi_files):,}\")\nprint(f\"  Annotation files: {len(annotation_files)}\")\n\n# Read sample annotation to verify\nif annotation_files:\n    import json\n    with open(annotation_files[0]) as f:\n        sample = json.loads(f.readline())\n    print(f\"\\nSample annotation:\")\n    print(f\"  Dimensions: {list(sample['labels'].keys())}\")\n    print(f\"  Quality tier: {sample.get('quality_tier', 'N/A')}\")\n    print(f\"  Quality score: {sample.get('quality_score', 'N/A')}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"✓ DATASET READY FOR TRAINING\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Piano Performance Evaluation - TRAINING_PLAN_v2.md Phase 2\n\nTrains 3 models with controlled quality variance:\n1. Audio-Only (MERT only)\n2. MIDI-Only (MIDIBert only)\n3. Fusion (MERT + MIDIBert)\n\n**Updates from v1**:\n- Dimensions: 8 (6→8: added musical_expression, overall_interpretation)\n- Quality variance: 4 tiers (Pristine/Good/Moderate/Poor)\n- Diagnostics: Attention entropy, cross-modal alignment, quality tier analysis\n- Sample size: ~450K training samples (4x quality tiers)\n\n**Expected time**: 4-5 hours training + 15-20 min setup\n**Goal**: Validate fusion architecture learns quality (not complexity) before Phase 3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installation status unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q huggingface_hub\n",
    "\n",
    "import os\n",
    "os.environ.pop(\"HF_TOKEN\", None)\n",
    "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
    "\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "try:\n",
    "    import getpass as gp\n",
    "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
    "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
    "    if not isinstance(token, str):\n",
    "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
    "    token = token.strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"Empty token provided\")\n",
    "    login(token=token, add_to_git_credential=False)\n",
    "    who = HfApi().whoami(token=token)\n",
    "    print(f\"✓ Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "except Exception as e:\n",
    "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
    "    print(\"Falling back to interactive login widget...\")\n",
    "    login()\n",
    "    try:\n",
    "        who = HfApi().whoami()\n",
    "        print(f\"✓ Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"[HF Login] Verification skipped: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COPYING MODELS AND CHECKPOINTS FROM GOOGLE DRIVE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if rclone is configured\n",
    "import subprocess\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "if 'gdrive:' not in result.stdout:\n",
    "    print(\"\\n⚠️  rclone not configured!\")\n",
    "    print(\"Run 'rclone config' in terminal to set up 'gdrive' remote\")\n",
    "    print(\"Follow the OAuth flow for remote server authentication\")\n",
    "    raise RuntimeError(\"rclone gdrive remote not configured\")\n",
    "\n",
    "# 1. Copy MERT model from Google Drive\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1. MERT-95M MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "HF_CACHE_ROOT = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "MERT_CACHE_DIR = os.path.join(HF_CACHE_ROOT, \"models--m-a-p--MERT-v1-95M\")\n",
    "MERT_REFS_DIR = os.path.join(MERT_CACHE_DIR, \"refs\")\n",
    "MERT_SNAPSHOTS_DIR = os.path.join(MERT_CACHE_DIR, \"snapshots\")\n",
    "MERT_SNAPSHOT_MAIN = os.path.join(MERT_SNAPSHOTS_DIR, \"main\")\n",
    "\n",
    "if os.path.exists(MERT_SNAPSHOT_MAIN) and os.listdir(MERT_SNAPSHOT_MAIN):\n",
    "    print(f\"✓ MERT-95M already cached at: {MERT_CACHE_DIR}\")\n",
    "    print(f\"\\nCached files:\")\n",
    "    !ls -lh {MERT_SNAPSHOT_MAIN}/\n",
    "else:\n",
    "    print(\"Copying MERT-95M from Google Drive (~380MB)...\")\n",
    "    \n",
    "    # Create directory structure\n",
    "    os.makedirs(MERT_SNAPSHOT_MAIN, exist_ok=True)\n",
    "    os.makedirs(MERT_REFS_DIR, exist_ok=True)\n",
    "    \n",
    "    # Copy model files to snapshot directory\n",
    "    print(\"\\nCopying model files...\")\n",
    "    !rclone copy gdrive:MERT-v1-95M/ {MERT_SNAPSHOT_MAIN}/ -P --transfers 4\n",
    "    \n",
    "    # Create refs/main file pointing to the snapshot\n",
    "    with open(os.path.join(MERT_REFS_DIR, \"main\"), 'w') as f:\n",
    "        f.write(\"main\")\n",
    "    \n",
    "    # Create a minimal .no_exist marker file\n",
    "    Path(MERT_CACHE_DIR, \".no_exist\").touch()\n",
    "    \n",
    "    print(\"\\n✓ MERT-95M copied and cached\")\n",
    "    print(f\"   Cache location: {MERT_CACHE_DIR}\")\n",
    "    print(f\"   Snapshot: {MERT_SNAPSHOT_MAIN}\")\n",
    "    \n",
    "    # List what was copied\n",
    "    print(\"\\nCopied files:\")\n",
    "    !ls -lh {MERT_SNAPSHOT_MAIN}/\n",
    "\n",
    "# 2. Copy training checkpoints from Google Drive\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. TRAINING CHECKPOINTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "CHECKPOINT_ROOT = '/tmp/crescendai_checkpoints'\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"\\nCopying checkpoints from Google Drive...\")\n",
    "print(\"This may take a few minutes depending on checkpoint size...\\n\")\n",
    "\n",
    "for mode in ['audio_full', 'midi_full', 'fusion_full']:\n",
    "    print(f\"Copying {mode}...\")\n",
    "    !rclone copy gdrive:crescendai_checkpoints/{mode} {CHECKPOINT_ROOT}/{mode} -P --transfers 4\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ ALL FILES COPIED FROM GOOGLE DRIVE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify what was copied\n",
    "print(\"\\nCheckpoint contents:\")\n",
    "!ls -lh {CHECKPOINT_ROOT}/*/*.ckpt 2>/dev/null || echo \"No .ckpt files found\"\n",
    "\n",
    "print(f\"\\nMERT model cache: {MERT_CACHE_DIR}\")\n",
    "print(f\"MERT snapshot: {MERT_SNAPSHOT_MAIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "\n",
    "!rm -rf /tmp/crescendai\n",
    "!git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "%cd /tmp/crescendai/model\n",
    "!git log -1 --oneline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add to PATH for this session\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "print(\"\\n✓ uv installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --system -e .\n",
    "\n",
    "# Install optional GPU dependencies\n",
    "!uv pip install --system nnAudio torchcodec\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")\n",
    "print(\"✓ Dependencies installed\")\n",
    "\n",
    "!python scripts/setup_colab_environment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"VERIFYING MERT-95M MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from transformers import AutoModel, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Use the snapshot directory directly instead of relying on HF cache resolution\n",
    "MERT_SNAPSHOT_DIR = os.path.join(\n",
    "    os.path.expanduser(\"~/.cache/huggingface/hub\"),\n",
    "    \"models--m-a-p--MERT-v1-95M/snapshots/main\"\n",
    ")\n",
    "\n",
    "print(f\"\\nChecking for model files in: {MERT_SNAPSHOT_DIR}\")\n",
    "if not os.path.exists(MERT_SNAPSHOT_DIR):\n",
    "    raise RuntimeError(f\"MERT snapshot directory not found: {MERT_SNAPSHOT_DIR}\")\n",
    "\n",
    "print(\"Model files:\")\n",
    "!ls -lh {MERT_SNAPSHOT_DIR}/\n",
    "\n",
    "try:\n",
    "    print(\"\\nLoading model from local snapshot...\")\n",
    "    # Load directly from the snapshot directory (bypasses HF cache lookup)\n",
    "    model = AutoModel.from_pretrained(\n",
    "        MERT_SNAPSHOT_DIR,  # Use directory path directly\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    print(\"Loading feature extractor from local snapshot...\")\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "        MERT_SNAPSHOT_DIR,  # Use directory path directly\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Model type: {model.config.model_type}\")\n",
    "    print(f\"✓ Model loaded from: {MERT_SNAPSHOT_DIR}\")\n",
    "    print(\"✓ Feature extractor loaded\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del processor\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ MERT-95M VERIFIED AND READY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Verification failed: {e}\")\n",
    "    print(\"\\nDebug info:\")\n",
    "    print(f\"  Snapshot dir exists: {os.path.exists(MERT_SNAPSHOT_DIR)}\")\n",
    "    if os.path.exists(MERT_SNAPSHOT_DIR):\n",
    "        print(f\"  Files in snapshot: {os.listdir(MERT_SNAPSHOT_DIR)}\")\n",
    "    print(\"\\nMake sure cell-3 completed successfully!\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## STEP 2: EXTRACT UPLOADED DATASET\n\nExtract maestro_with_variance.tar.gz (uploaded to runtime)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Verifying data paths...\")\n\nimport json\nfrom pathlib import Path\n\n# Updated paths for extracted dataset\ntrain_path = '/tmp/maestro_data/annotations/train.jsonl'\nval_path = '/tmp/maestro_data/annotations/val.jsonl'\ntest_path = '/tmp/maestro_data/annotations/test.jsonl'\n\n# Check a sample annotation\nwith open(train_path) as f:\n    sample = json.loads(f.readline())\n    \nprint(f\"\\nSample annotation:\")\nprint(f\"  Audio: {sample['audio_path']}\")\nprint(f\"  MIDI:  {sample['midi_path']}\")\nprint(f\"  Quality tier: {sample.get('quality_tier', 'N/A')}\")\nprint(f\"  Quality score: {sample.get('quality_score', 'N/A'):.1f}\")\n\n# Verify files exist\naudio_exists = Path(sample['audio_path']).exists()\nmidi_exists = Path(sample['midi_path']).exists() if sample['midi_path'] else False\n\nprint(f\"\\nFile existence check:\")\nprint(f\"  Audio exists: {'✓' if audio_exists else '✗'}\")\nprint(f\"  MIDI exists:  {'✓' if midi_exists else '✗ (may be OK if path is None)'}\")\n\nif not audio_exists:\n    print(f\"\\n⚠️  WARNING: Audio file not found!\")\n    print(f\"     Check that data extraction completed correctly\")\n    print(f\"     Expected: {sample['audio_path']}\")\nelif not midi_exists and sample['midi_path']:\n    print(f\"\\n⚠️  WARNING: MIDI file not found!\")\n    print(f\"     Expected: {sample['midi_path']}\")\nelse:\n    print(f\"\\n✓ Sample files verified - data structure looks correct!\")\n\n# Preflight Check\nprint(\"=\"*70)\nprint(\"STEP 3: PREFLIGHT CHECK\")\nprint(\"=\"*70)\nprint(\"\\nVerifying training environment and data with 8 dimensions...\\n\")\n\n# Updated dimension list (8 dimensions from TRAINING_PLAN_v2.md)\nDIMENSIONS = [\n    'note_accuracy',\n    'rhythmic_stability', \n    'articulation_clarity',\n    'pedal_technique',\n    'tone_quality',\n    'dynamic_range',\n    'musical_expression',\n    'overall_interpretation'\n]\n\nprint(f\"Dimensions ({len(DIMENSIONS)}): {DIMENSIONS}\")\n\n# Verify sample has all dimensions\nmissing_dims = [d for d in DIMENSIONS if d not in sample['labels']]\nif missing_dims:\n    print(f\"\\n⚠️  WARNING: Sample missing dimensions: {missing_dims}\")\n    print(\"This may indicate the dataset was created with old labeling functions\")\nelse:\n    print(\"\\n✓ All 8 dimensions present in annotations\")\n\nprint(f\"\\nAnnotation paths:\")\nprint(f\"  Train: {train_path}\")\nprint(f\"  Val:   {val_path}\")  \nprint(f\"  Test:  {test_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 2.5: FIX ANNOTATION PATHS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUpdating annotation files to use local SSD paths...\\n\")\n",
    "\n",
    "!python scripts/fix_annotation_paths.py\n",
    "\n",
    "print(\"\\n✓ Annotation paths updated for local SSD access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings('ignore', message='divide by zero')\nwarnings.filterwarnings('ignore', category=SyntaxWarning)  # pydub regex warnings\nwarnings.filterwarnings('ignore', category=UserWarning, module='torchaudio')\nwarnings.filterwarnings('ignore', category=UserWarning, module='torchmetrics')\n\n%%time\n# Updated to use 8 dimensions and new paths\n!python train.py \\\n    --train-path /tmp/maestro_data/annotations/train.jsonl \\\n    --val-path /tmp/maestro_data/annotations/val.jsonl \\\n    --test-path /tmp/maestro_data/annotations/test.jsonl \\\n    --dimensions note_accuracy rhythmic_stability articulation_clarity pedal_technique tone_quality dynamic_range musical_expression overall_interpretation \\\n    --mode audio \\\n    --epochs 5 \\\n    --batch-size 16 \\\n    --learning-rate 3e-5 \\\n    --checkpoint-dir /content/drive/MyDrive/crescendai_checkpoints/audio_full"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Audio-Only\n",
    "\n",
    "Training with audio features only (MERT-95M encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings('ignore', message='divide by zero')\nwarnings.filterwarnings('ignore', category=SyntaxWarning)  # pydub regex warnings\nwarnings.filterwarnings('ignore', category=UserWarning, module='torchaudio')\nwarnings.filterwarnings('ignore', category=UserWarning, module='torchmetrics')\n\n%%time\n# Updated to use 8 dimensions and new paths\n!python train.py \\\n    --train-path /tmp/maestro_data/annotations/train.jsonl \\\n    --val-path /tmp/maestro_data/annotations/val.jsonl \\\n    --test-path /tmp/maestro_data/annotations/test.jsonl \\\n    --dimensions note_accuracy rhythmic_stability articulation_clarity pedal_technique tone_quality dynamic_range musical_expression overall_interpretation \\\n    --mode midi \\\n    --epochs 5 \\\n    --batch-size 16 \\\n    --learning-rate 3e-5 \\\n    --checkpoint-dir /content/drive/MyDrive/crescendai_checkpoints/midi_full"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: MIDI-Only\n",
    "\n",
    "Training with MIDI features only (MIDIBert encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings('ignore', message='divide by zero')\nwarnings.filterwarnings('ignore', category=SyntaxWarning)  # pydub regex warnings\nwarnings.filterwarnings('ignore', category=UserWarning, module='torchaudio')\nwarnings.filterwarnings('ignore', category=UserWarning, module='torchmetrics')\n\n%%time\n# Updated to use 8 dimensions and new paths\n!python train.py \\\n    --train-path /tmp/maestro_data/annotations/train.jsonl \\\n    --val-path /tmp/maestro_data/annotations/val.jsonl \\\n    --test-path /tmp/maestro_data/annotations/test.jsonl \\\n    --dimensions note_accuracy rhythmic_stability articulation_clarity pedal_technique tone_quality dynamic_range musical_expression overall_interpretation \\\n    --mode fusion \\\n    --epochs 5 \\\n    --batch-size 16 \\\n    --learning-rate 3e-5 \\\n    --checkpoint-dir /content/drive/MyDrive/crescendai_checkpoints/fusion_full"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Fusion\n",
    "\n",
    "Training with both audio and MIDI features (multi-modal fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pytorch_lightning as pl\nfrom src.models.lightning_module import PerformanceEvaluationModel\nfrom src.data.dataset import create_dataloaders\nfrom pathlib import Path\nimport numpy as np\nimport torch\nfrom scipy import stats\n\nprint(\"=\"*80)\nprint(\"COMPREHENSIVE 3-WAY MODEL EVALUATION - TRAINING_PLAN_v2.md\")\nprint(\"=\"*80)\n\n# Updated dimensions (8 from v2)\ndimensions = [\n    'note_accuracy',\n    'rhythmic_stability',\n    'articulation_clarity', \n    'pedal_technique',\n    'tone_quality',\n    'dynamic_range',\n    'musical_expression',\n    'overall_interpretation'\n]\n\n# Updated paths\ntrain_path = '/tmp/maestro_data/annotations/train.jsonl'\nval_path = '/tmp/maestro_data/annotations/val.jsonl'\ntest_path = '/tmp/maestro_data/annotations/test.jsonl'\n\n# Load all 3 models\nCHECKPOINT_ROOT = '/content/drive/MyDrive/crescendai_checkpoints'\nmodels = {}\nfor mode in ['audio', 'midi', 'fusion']:\n    ckpt_dir = Path(f'{CHECKPOINT_ROOT}/{mode}_full')\n    ckpts = list(ckpt_dir.glob('*.ckpt'))\n    if ckpts:\n        latest = sorted(ckpts)[-1]\n        print(f\"Loading {mode}: {latest.name}\")\n        models[mode] = PerformanceEvaluationModel.load_from_checkpoint(str(latest))\n        models[mode].eval()\n        models[mode] = models[mode].cuda()\n    else:\n        print(f\"⚠️  No checkpoint found for {mode}\")\n\nprint(f\"\\nLoaded {len(models)}/3 models\")\n\n# Create test dataloader\n_, _, test_loader = create_dataloaders(\n    train_annotation_path=train_path,\n    val_annotation_path=val_path,\n    test_annotation_path=test_path,\n    dimension_names=dimensions,\n    batch_size=8,\n    num_workers=4,\n    augmentation_config=None,\n    audio_sample_rate=24000,\n    max_audio_length=240000,\n    max_midi_events=512,\n)\n\nprint(f\"Test set size: {len(test_loader.dataset)} samples\")\n\n# Evaluate each model\ntrainer = pl.Trainer(accelerator='auto', devices='auto', precision=16)\nresults = {}\npredictions = {}\n\nfor mode, model in models.items():\n    print(f\"\\nEvaluating {mode}...\")\n    test_results = trainer.test(model, dataloaders=test_loader, verbose=False)\n    results[mode] = test_results[0]\n    \n    # Collect predictions for deeper analysis\n    model = model.cuda()\n    model.eval()\n    all_preds = []\n    all_targets = []\n    all_quality_tiers = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            audio_waveform = batch['audio_waveform'].cuda()\n            midi_tokens = batch.get('midi_tokens', None)\n            if midi_tokens is not None:\n                midi_tokens = midi_tokens.cuda()\n            targets = batch['labels'].cuda()\n            \n            # Forward pass\n            output = model(\n                audio_waveform=audio_waveform,\n                midi_tokens=midi_tokens,\n            )\n            \n            if output is None:\n                continue\n                \n            preds = output['scores']\n            all_preds.append(preds.cpu().numpy())\n            all_targets.append(targets.cpu().numpy())\n            \n            # Track quality tiers if available\n            if 'quality_tier' in batch:\n                all_quality_tiers.extend(batch['quality_tier'])\n    \n    predictions[mode] = {\n        'preds': np.concatenate(all_preds, axis=0),\n        'targets': np.concatenate(all_targets, axis=0),\n        'quality_tiers': all_quality_tiers if all_quality_tiers else None\n    }\n    print(f\"  Collected {len(predictions[mode]['preds'])} predictions\")\n\n# Find common sample count\nmin_samples = min(len(predictions['audio']['preds']), \n                  len(predictions['midi']['preds']), \n                  len(predictions['fusion']['preds']))\nprint(f\"\\nUsing {min_samples} samples for analysis\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. PER-DIMENSION CORRELATION (Pearson r)\")\nprint(\"=\"*80)\nprint(f\"{'Dimension':<28} {'Audio':<12} {'MIDI':<12} {'Fusion':<12} {'Best':<12} {'Improvement'}\")\nprint(\"-\"*80)\n\nfor dim_idx, dim in enumerate(dimensions):\n    audio_r = results.get('audio', {}).get(f'test_pearson_{dim}', 0)\n    midi_r = results.get('midi', {}).get(f'test_pearson_{dim}', 0)\n    fusion_r = results.get('fusion', {}).get(f'test_pearson_{dim}', 0)\n    \n    best_single = max(audio_r, midi_r)\n    best_modality = 'Audio' if audio_r >= midi_r else 'MIDI'\n    improvement = ((fusion_r - best_single) / best_single * 100) if best_single > 0 else 0\n    \n    print(f\"{dim:<28} {audio_r:>11.3f} {midi_r:>11.3f} {fusion_r:>11.3f} {best_modality:<12} {improvement:>+6.1f}%\")\n\nprint(\"-\"*80)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. OVERALL PERFORMANCE\")\nprint(\"=\"*80)\n\naudio_mean_r = np.mean([results['audio'][f'test_pearson_{d}'] for d in dimensions])\nmidi_mean_r = np.mean([results['midi'][f'test_pearson_{d}'] for d in dimensions])\nfusion_mean_r = np.mean([results['fusion'][f'test_pearson_{d}'] for d in dimensions])\n\nprint(f\"{'Metric':<35} {'Audio':<12} {'MIDI':<12} {'Fusion':<12} {'Winner'}\")\nprint(\"-\"*80)\nbest_r = max(audio_mean_r, midi_mean_r, fusion_mean_r)\nwinner_r = 'Audio' if audio_mean_r == best_r else ('MIDI' if midi_mean_r == best_r else 'Fusion')\nprint(f\"{'Mean Pearson Correlation':<35} {audio_mean_r:>11.3f} {midi_mean_r:>11.3f} {fusion_mean_r:>11.3f} {winner_r}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. PHASE 2 SUCCESS CRITERIA (TRAINING_PLAN_v2.md)\")\nprint(\"=\"*80)\n\nbest_single_r = max(audio_mean_r, midi_mean_r)\nfusion_improvement = ((fusion_mean_r - best_single_r) / best_single_r * 100) if best_single_r > 0 else 0\n\nprint(f\"\\nFusion improvement over best single-modal: {fusion_improvement:+.1f}%\")\nprint(f\"Target: ≥10% improvement\")\n\nif fusion_improvement >= 10:\n    print(\"\\n✓ PASS: Fusion beats single-modal by ≥10%\")\n    print(\"→ GO TO PHASE 3: Proceed with contrastive pre-training\")\nelse:\n    print(\"\\n✗ FAIL: Fusion improvement < 10% threshold\")\n    print(\"→ NO-GO: Debug fusion architecture before Phase 3\")\n\n# Display diagnostics if available\nif 'val_attention_entropy' in results.get('fusion', {}):\n    print(\"\\n\" + \"=\"*80)\n    print(\"4. FUSION DIAGNOSTICS\")\n    print(\"=\"*80)\n    \n    diag = results['fusion']\n    print(f\"Attention Entropy:       {diag.get('val_attention_entropy', 'N/A')}\")\n    print(f\"Attention Sparsity:      {diag.get('val_attention_sparsity', 'N/A')}\")\n    print(f\"Cross-Modal Alignment:   {diag.get('val_cross_modal_alignment', 'N/A')}\")\n    print(f\"Audio Feature Diversity: {diag.get('val_audio_feature_diversity', 'N/A')}\")\n    print(f\"MIDI Feature Diversity:  {diag.get('val_midi_feature_diversity', 'N/A')}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EVALUATION COMPLETE\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results\n",
    "\n",
    "Load all 3 trained models and compare performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pytorch_lightning as pl\nfrom src.models.lightning_module import PerformanceEvaluationModel\nfrom src.data.dataset import create_dataloaders\nfrom pathlib import Path\nimport numpy as np\nimport torch\nfrom scipy import stats\n\nprint(\"=\"*80)\nprint(\"COMPREHENSIVE 3-WAY MODEL EVALUATION\")\nprint(\"=\"*80)\n\n# Load all 3 models (use CHECKPOINT_ROOT from earlier cell)\nmodels = {}\nfor mode in ['audio', 'midi', 'fusion']:\n    ckpt_dir = Path(f'{CHECKPOINT_ROOT}/{mode}_full')\n    ckpts = list(ckpt_dir.glob('*.ckpt'))\n    if ckpts:\n        latest = sorted(ckpts)[-1]\n        print(f\"Loading {mode}: {latest.name}\")\n        models[mode] = PerformanceEvaluationModel.load_from_checkpoint(str(latest))\n        models[mode].eval()\n        models[mode] = models[mode].cuda()\n    else:\n        print(f\"⚠️  No checkpoint found for {mode}\")\n\nprint(f\"\\nLoaded {len(models)}/3 models\")\n\n# Create test dataloader (using local SSD paths)\n_, _, test_loader = create_dataloaders(\n    train_annotation_path='/tmp/crescendai_data/data/annotations/synthetic_train.jsonl',\n    val_annotation_path='/tmp/crescendai_data/data/annotations/synthetic_val.jsonl',\n    test_annotation_path='/tmp/crescendai_data/data/annotations/synthetic_test.jsonl',\n    dimension_names=['note_accuracy', 'rhythmic_precision', 'tone_quality', 'dynamics_control', 'articulation', 'pedaling'],\n    batch_size=8,\n    num_workers=4,\n    augmentation_config=None,\n    audio_sample_rate=24000,\n    max_audio_length=240000,\n    max_midi_events=512,\n)\n\nprint(f\"Test set size: {len(test_loader.dataset)} samples\")\n\n# Evaluate each model\ntrainer = pl.Trainer(accelerator='auto', devices='auto', precision=16)\nresults = {}\npredictions = {}\n\nfor mode, model in models.items():\n    print(f\"\\nEvaluating {mode}...\")\n    test_results = trainer.test(model, dataloaders=test_loader, verbose=False)\n    results[mode] = test_results[0]\n    \n    # Collect predictions for deeper analysis\n    model = model.cuda()  # Ensure model is on GPU\n    model.eval()\n    all_preds = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            # Move batch to GPU\n            audio_waveform = batch['audio_waveform'].cuda()\n            midi_tokens = batch.get('midi_tokens', None)\n            if midi_tokens is not None:\n                midi_tokens = midi_tokens.cuda()\n            targets = batch['labels'].cuda()\n            \n            # Forward pass with proper arguments\n            output = model(\n                audio_waveform=audio_waveform,\n                midi_tokens=midi_tokens,\n            )\n            \n            # Skip if batch was None (all MIDI failed in MIDI-only mode)\n            if output is None:\n                continue\n                \n            preds = output['scores']\n            all_preds.append(preds.cpu().numpy())\n            all_targets.append(targets.cpu().numpy())\n    \n    predictions[mode] = {\n        'preds': np.concatenate(all_preds, axis=0),\n        'targets': np.concatenate(all_targets, axis=0)\n    }\n    print(f\"  Collected {len(predictions[mode]['preds'])} predictions\")\n\ndimensions = ['note_accuracy', 'rhythmic_precision', 'tone_quality', 'dynamics_control', 'articulation', 'pedaling']\n\n# Find common sample count (some batches may have been skipped in MIDI mode)\nmin_samples = min(len(predictions['audio']['preds']), \n                  len(predictions['midi']['preds']), \n                  len(predictions['fusion']['preds']))\nprint(f\"\\nUsing {min_samples} samples for analysis (minimum across all models)\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"1. PER-DIMENSION CORRELATION (Pearson r)\")\nprint(\"=\"*80)\nprint(f\"{'Dimension':<25} {'Audio':<12} {'MIDI':<12} {'Fusion':<12} {'Best':<12} {'Improvement'}\")\nprint(\"-\"*80)\n\nfor dim_idx, dim in enumerate(dimensions):\n    audio_r = results.get('audio', {}).get(f'test_pearson_{dim}', 0)\n    midi_r = results.get('midi', {}).get(f'test_pearson_{dim}', 0)\n    fusion_r = results.get('fusion', {}).get(f'test_pearson_{dim}', 0)\n    \n    best_single = max(audio_r, midi_r)\n    best_modality = 'Audio' if audio_r >= midi_r else 'MIDI'\n    improvement = ((fusion_r - best_single) / best_single * 100) if best_single > 0 else 0\n    \n    print(f\"{dim:<25} {audio_r:>11.3f} {midi_r:>11.3f} {fusion_r:>11.3f} {best_modality:<12} {improvement:>+6.1f}%\")\n\nprint(\"-\"*80)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"2. PER-DIMENSION MAE (Mean Absolute Error, 0-100 scale)\")\nprint(\"=\"*80)\nprint(f\"{'Dimension':<25} {'Audio':<12} {'MIDI':<12} {'Fusion':<12} {'Best':<12} {'Reduction'}\")\nprint(\"-\"*80)\n\nfor dim_idx, dim in enumerate(dimensions):\n    audio_mae = np.mean(np.abs(predictions['audio']['preds'][:min_samples, dim_idx] - predictions['audio']['targets'][:min_samples, dim_idx]))\n    midi_mae = np.mean(np.abs(predictions['midi']['preds'][:min_samples, dim_idx] - predictions['midi']['targets'][:min_samples, dim_idx]))\n    fusion_mae = np.mean(np.abs(predictions['fusion']['preds'][:min_samples, dim_idx] - predictions['fusion']['targets'][:min_samples, dim_idx]))\n    \n    best_single_mae = min(audio_mae, midi_mae)\n    best_modality = 'Audio' if audio_mae <= midi_mae else 'MIDI'\n    reduction = ((best_single_mae - fusion_mae) / best_single_mae * 100) if best_single_mae > 0 else 0\n    \n    print(f\"{dim:<25} {audio_mae:>11.2f} {midi_mae:>11.2f} {fusion_mae:>11.2f} {best_modality:<12} {reduction:>+6.1f}%\")\n\nprint(\"-\"*80)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"3. PER-DIMENSION RMSE (Root Mean Squared Error, 0-100 scale)\")\nprint(\"=\"*80)\nprint(f\"{'Dimension':<25} {'Audio':<12} {'MIDI':<12} {'Fusion':<12} {'Best':<12} {'Reduction'}\")\nprint(\"-\"*80)\n\nfor dim_idx, dim in enumerate(dimensions):\n    audio_rmse = np.sqrt(np.mean((predictions['audio']['preds'][:min_samples, dim_idx] - predictions['audio']['targets'][:min_samples, dim_idx])**2))\n    midi_rmse = np.sqrt(np.mean((predictions['midi']['preds'][:min_samples, dim_idx] - predictions['midi']['targets'][:min_samples, dim_idx])**2))\n    fusion_rmse = np.sqrt(np.mean((predictions['fusion']['preds'][:min_samples, dim_idx] - predictions['fusion']['targets'][:min_samples, dim_idx])**2))\n    \n    best_single_rmse = min(audio_rmse, midi_rmse)\n    best_modality = 'Audio' if audio_rmse <= midi_rmse else 'MIDI'\n    reduction = ((best_single_rmse - fusion_rmse) / best_single_rmse * 100) if best_single_rmse > 0 else 0\n    \n    print(f\"{dim:<25} {audio_rmse:>11.2f} {midi_rmse:>11.2f} {fusion_rmse:>11.2f} {best_modality:<12} {reduction:>+6.1f}%\")\n\nprint(\"-\"*80)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"4. OVERALL PERFORMANCE (Averaged Across All Dimensions)\")\nprint(\"=\"*80)\nprint(f\"{'Metric':<35} {'Audio':<12} {'MIDI':<12} {'Fusion':<12} {'Winner'}\")\nprint(\"-\"*80)\n\n# Mean Pearson\naudio_mean_r = np.mean([results['audio'][f'test_pearson_{d}'] for d in dimensions])\nmidi_mean_r = np.mean([results['midi'][f'test_pearson_{d}'] for d in dimensions])\nfusion_mean_r = np.mean([results['fusion'][f'test_pearson_{d}'] for d in dimensions])\nbest_r = max(audio_mean_r, midi_mean_r, fusion_mean_r)\nwinner_r = 'Audio' if audio_mean_r == best_r else ('MIDI' if midi_mean_r == best_r else 'Fusion')\nprint(f\"{'Mean Pearson Correlation':<35} {audio_mean_r:>11.3f} {midi_mean_r:>11.3f} {fusion_mean_r:>11.3f} {winner_r}\")\n\n# Mean MAE (using min_samples for fair comparison)\naudio_mean_mae = np.mean([np.mean(np.abs(predictions['audio']['preds'][:min_samples, i] - predictions['audio']['targets'][:min_samples, i])) for i in range(len(dimensions))])\nmidi_mean_mae = np.mean([np.mean(np.abs(predictions['midi']['preds'][:min_samples, i] - predictions['midi']['targets'][:min_samples, i])) for i in range(len(dimensions))])\nfusion_mean_mae = np.mean([np.mean(np.abs(predictions['fusion']['preds'][:min_samples, i] - predictions['fusion']['targets'][:min_samples, i])) for i in range(len(dimensions))])\nbest_mae = min(audio_mean_mae, midi_mean_mae, fusion_mean_mae)\nwinner_mae = 'Audio' if audio_mean_mae == best_mae else ('MIDI' if midi_mean_mae == best_mae else 'Fusion')\nprint(f\"{'Mean Absolute Error':<35} {audio_mean_mae:>11.2f} {midi_mean_mae:>11.2f} {fusion_mean_mae:>11.2f} {winner_mae}\")\n\n# Mean RMSE (using min_samples for fair comparison)\naudio_mean_rmse = np.mean([np.sqrt(np.mean((predictions['audio']['preds'][:min_samples, i] - predictions['audio']['targets'][:min_samples, i])**2)) for i in range(len(dimensions))])\nmidi_mean_rmse = np.mean([np.sqrt(np.mean((predictions['midi']['preds'][:min_samples, i] - predictions['midi']['targets'][:min_samples, i])**2)) for i in range(len(dimensions))])\nfusion_mean_rmse = np.mean([np.sqrt(np.mean((predictions['fusion']['preds'][:min_samples, i] - predictions['fusion']['targets'][:min_samples, i])**2)) for i in range(len(dimensions))])\nbest_rmse = min(audio_mean_rmse, midi_mean_rmse, fusion_mean_rmse)\nwinner_rmse = 'Audio' if audio_mean_rmse == best_rmse else ('MIDI' if midi_mean_rmse == best_rmse else 'Fusion')\nprint(f\"{'Root Mean Squared Error':<35} {audio_mean_rmse:>11.2f} {midi_mean_rmse:>11.2f} {fusion_mean_rmse:>11.2f} {winner_rmse}\")\n\nprint(\"-\"*80)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"5. FUSION PERFORMANCE GAIN\")\nprint(\"=\"*80)\n\nbest_single_r = max(audio_mean_r, midi_mean_r)\nr_improvement = ((fusion_mean_r - best_single_r) / best_single_r * 100) if best_single_r > 0 else 0\nprint(f\"Pearson r improvement:  {r_improvement:+.1f}% over best single-modal\")\n\nbest_single_mae = min(audio_mean_mae, midi_mean_mae)\nmae_reduction = ((best_single_mae - fusion_mean_mae) / best_single_mae * 100) if best_single_mae > 0 else 0\nprint(f\"MAE reduction:          {mae_reduction:+.1f}% over best single-modal\")\n\nbest_single_rmse = min(audio_mean_rmse, midi_mean_rmse)\nrmse_reduction = ((best_single_rmse - fusion_mean_rmse) / best_single_rmse * 100) if best_single_rmse > 0 else 0\nprint(f\"RMSE reduction:         {rmse_reduction:+.1f}% over best single-modal\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"6. STATISTICAL SIGNIFICANCE (Fusion vs Best Single-Modal)\")\nprint(\"=\"*80)\nprint(f\"{'Dimension':<25} {'Best Single':<15} {'p-value':<12} {'Significant?'}\")\nprint(\"-\"*80)\n\nfor dim_idx, dim in enumerate(dimensions):\n    # Use min_samples to ensure equal length arrays for paired t-test\n    audio_errors = np.abs(predictions['audio']['preds'][:min_samples, dim_idx] - \n                         predictions['audio']['targets'][:min_samples, dim_idx])\n    midi_errors = np.abs(predictions['midi']['preds'][:min_samples, dim_idx] - \n                        predictions['midi']['targets'][:min_samples, dim_idx])\n    fusion_errors = np.abs(predictions['fusion']['preds'][:min_samples, dim_idx] - \n                          predictions['fusion']['targets'][:min_samples, dim_idx])\n    \n    # Compare fusion vs best single modal (paired t-test on MAE)\n    best_single_errors = audio_errors if np.mean(audio_errors) <= np.mean(midi_errors) else midi_errors\n    best_single_name = 'Audio' if np.mean(audio_errors) <= np.mean(midi_errors) else 'MIDI'\n    \n    t_stat, p_value = stats.ttest_rel(best_single_errors, fusion_errors)\n    is_significant = p_value < 0.05 and np.mean(fusion_errors) < np.mean(best_single_errors)\n    \n    print(f\"{dim:<25} {best_single_name:<15} {p_value:>11.4f} {'Yes' if is_significant else 'No':>12}\")\n\nprint(\"-\"*80)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"7. DIMENSION LEARNABILITY CATEGORIZATION\")\nprint(\"=\"*80)\n\nstrong = [d for d in dimensions if max(results.get('audio', {}).get(f'test_pearson_{d}', 0), \n                                        results.get('midi', {}).get(f'test_pearson_{d}', 0)) > 0.4]\nmoderate = [d for d in dimensions if 0.25 <= max(results.get('audio', {}).get(f'test_pearson_{d}', 0),\n                                                   results.get('midi', {}).get(f'test_pearson_{d}', 0)) <= 0.4]\nweak = [d for d in dimensions if max(results.get('audio', {}).get(f'test_pearson_{d}', 0),\n                                      results.get('midi', {}).get(f'test_pearson_{d}', 0)) < 0.25]\n\nprint(f\"Strong learners (r > 0.4):     {', '.join(strong) if strong else 'None'}\")\nprint(f\"Moderate learners (0.25-0.4):  {', '.join(moderate) if moderate else 'None'}\")\nprint(f\"Weak learners (r < 0.25):      {', '.join(weak) if weak else 'None'}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"8. MVP TARGET ASSESSMENT\")\nprint(\"=\"*80)\nprint(\"Technical dimension target: r = 0.50-0.65 (Pearson with expert)\")\nprint(\"Interpretive dimension target: r = 0.35-0.50\")\nprint(\"MAE target: 10-15 points on 0-100 scale\\n\")\n\ntechnical_dims = dimensions  # All 6 are technical in this experiment\ntechnical_r_values = [results.get('fusion', {}).get(f'test_pearson_{d}', 0) for d in technical_dims]\ntechnical_mean_r = np.mean(technical_r_values)\n\nmeets_r_target = technical_mean_r >= 0.50\nmeets_mae_target = fusion_mean_mae <= 15\n\nprint(f\"Fusion technical r:     {technical_mean_r:.3f} {'(PASS)' if meets_r_target else '(FAIL - below 0.50 target)'}\")\nprint(f\"Fusion overall MAE:     {fusion_mean_mae:.2f} {'(PASS)' if meets_mae_target else '(FAIL - above 15 target)'}\")\n\nif meets_r_target and meets_mae_target:\n    print(\"\\nMVP TARGETS MET - Ready to proceed with expert annotation\")\nelse:\n    print(\"\\nMVP TARGETS NOT MET - Consider architecture improvements or data augmentation\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"9. EXPERT ANNOTATION RECOMMENDATION\")\nprint(\"=\"*80)\nprint(f\"Include in expert labels:  {', '.join(strong + moderate)}\")\nprint(f\"Consider skipping:         {', '.join(weak)}\")\nprint(f\"\\nEstimated cost savings: ${len(weak) * 3000:,} by excluding weak dimensions\")\nprint(f\"Recommended budget:     ${len(strong + moderate) * 3000:,} for {len(strong + moderate)} dimensions\")\nprint(\"=\"*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}