{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piano Performance Evaluation - 3-Way Model Comparison (Thunder Compute)\n",
    "\n",
    "Trains 3 models to prove multi-modal fusion advantage:\n",
    "1. Audio-Only (MERT only)\n",
    "2. MIDI-Only (MIDIBert only)\n",
    "3. Fusion (MERT + MIDIBert)\n",
    "\n",
    "**Dimensions**: 6 technical (note_accuracy, rhythmic_precision, tone_quality, dynamics_control, articulation, pedaling)\n",
    "**Sample size**: 114,246 training samples (full dataset)\n",
    "**Expected time**: 3-4 hours training + 10-15 min setup\n",
    "**Goal**: Identify which dimensions are learnable to guide expert annotation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q huggingface_hub\n",
    "\n",
    "import os\n",
    "os.environ.pop(\"HF_TOKEN\", None)\n",
    "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
    "\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "try:\n",
    "    import getpass as gp\n",
    "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
    "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
    "    if not isinstance(token, str):\n",
    "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
    "    token = token.strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"Empty token provided\")\n",
    "    login(token=token, add_to_git_credential=False)\n",
    "    who = HfApi().whoami(token=token)\n",
    "    print(f\"✓ Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "except Exception as e:\n",
    "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
    "    print(\"Falling back to interactive login widget...\")\n",
    "    login()\n",
    "    try:\n",
    "        who = HfApi().whoami()\n",
    "        print(f\"✓ Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"[HF Login] Verification skipped: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Installing rclone if needed...\")\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installation status unknown\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COPYING CHECKPOINTS FROM GOOGLE DRIVE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Local checkpoint directory\n",
    "CHECKPOINT_ROOT = '/tmp/crescendai_checkpoints'\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "\n",
    "# Check if rclone is configured\n",
    "import subprocess\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "if 'gdrive:' not in result.stdout:\n",
    "    print(\"\\n⚠️  rclone not configured!\")\n",
    "    print(\"Run 'rclone config' in terminal to set up 'gdrive' remote\")\n",
    "    print(\"Follow the OAuth flow for remote server authentication\")\n",
    "    raise RuntimeError(\"rclone gdrive remote not configured\")\n",
    "\n",
    "print(\"\\nCopying checkpoints from Google Drive...\")\n",
    "print(\"This may take a few minutes depending on checkpoint size...\\n\")\n",
    "\n",
    "# Copy each model's checkpoints\n",
    "for mode in ['audio_full', 'midi_full', 'fusion_full']:\n",
    "    print(f\"Copying {mode}...\")\n",
    "    !rclone copy gdrive:crescendai_checkpoints/{mode} {CHECKPOINT_ROOT}/{mode} -P --transfers 4\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"✓ Checkpoints copied to: {CHECKPOINT_ROOT}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# List what was copied\n",
    "print(\"\\nCheckpoint contents:\")\n",
    "!ls -lh {CHECKPOINT_ROOT}/*/*.ckpt 2>/dev/null || echo \"No .ckpt files found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/crescendai\n",
    "!git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "%cd /tmp/crescendai/model\n",
    "!git log -1 --oneline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add to PATH for this session\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "print(\"\\n✓ uv installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --system -e .\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")\n",
    "print(\"✓ Dependencies installed\")\n",
    "\n",
    "!python scripts/setup_colab_environment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "MERT_CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface/hub/models--m-a-p--MERT-v1-95M\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DOWNLOADING MERT-95M MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if os.path.exists(MERT_CACHE_DIR):\n",
    "    print(f\"\\n✓ MERT-95M already cached at: {MERT_CACHE_DIR}\")\n",
    "else:\n",
    "    print(\"\\nDownloading MERT-95M (~380MB)...\")\n",
    "    print(\"This may take 2-5 minutes...\\n\")\n",
    "    \n",
    "    # Install git-lfs if needed\n",
    "    !command -v git-lfs >/dev/null 2>&1 || (curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash && sudo apt-get install -y git-lfs)\n",
    "    !git lfs install\n",
    "    \n",
    "    # Clone the model\n",
    "    os.makedirs(os.path.dirname(MERT_CACHE_DIR), exist_ok=True)\n",
    "    !git clone https://huggingface.co/m-a-p/MERT-v1-95M {MERT_CACHE_DIR}\n",
    "    \n",
    "    print(\"\\n✓ MERT-95M downloaded and cached\")\n",
    "\n",
    "# Verify the model can be loaded\n",
    "print(\"\\nVerifying model...\")\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True, local_files_only=True)\n",
    "print(f\"✓ Model loaded successfully: {model.config.model_type}\")\n",
    "\n",
    "del model\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ MERT-95M READY\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 2: DOWNLOAD DATA FROM HUGGING FACE HUB\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThis is 10-100x faster and more reliable than Google Drive!\")\n",
    "print(\"Download time: 7-15 minutes (one-time per session)\\n\")\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "HF_REPO_ID = \"Jai-D/crescendai-data\"\n",
    "\n",
    "print(f\"Downloading from: {HF_REPO_ID}\")\n",
    "print(\"Archive size: ~20-25 GB compressed\\n\")\n",
    "\n",
    "# Download archive\n",
    "print(\"1. Downloading archive...\")\n",
    "archive_path = hf_hub_download(\n",
    "    repo_id=HF_REPO_ID,\n",
    "    filename=\"crescendai_data.tar.gz\",\n",
    "    repo_type=\"model\",\n",
    "    local_dir=\"/tmp/\",\n",
    "    local_dir_use_symlinks=False,\n",
    ")\n",
    "print(f\"   ✓ Downloaded to: {archive_path}\")\n",
    "\n",
    "# Extract\n",
    "print(\"\\n2. Extracting archive...\")\n",
    "with tarfile.open(archive_path, 'r:gz') as tar:\n",
    "    members = tar.getmembers()\n",
    "    print(f\"   Extracting {len(members):,} files...\")\n",
    "    tar.extractall('/tmp/crescendai_data/')\n",
    "\n",
    "print(\"   ✓ Extracted to: /tmp/crescendai_data/\")\n",
    "\n",
    "# Clean up archive to save space\n",
    "print(\"\\n3. Cleaning up...\")\n",
    "os.remove(archive_path)\n",
    "print(\"   ✓ Removed archive file\")\n",
    "\n",
    "# Verify structure\n",
    "print(\"\\n4. Verifying data structure...\")\n",
    "expected_paths = [\n",
    "    '/tmp/crescendai_data/data/all_segments',\n",
    "    '/tmp/crescendai_data/data/annotations',\n",
    "]\n",
    "\n",
    "all_good = True\n",
    "for path in expected_paths:\n",
    "    if os.path.exists(path):\n",
    "        if 'all_segments' in path:\n",
    "            num_files = len([f for f in os.listdir(path) if f.endswith('.wav')])\n",
    "            print(f\"   ✓ {path}: {num_files:,} audio files\")\n",
    "        else:\n",
    "            num_files = len([f for f in os.listdir(path) if f.endswith('.jsonl')])\n",
    "            print(f\"   ✓ {path}: {num_files} annotation files\")\n",
    "    else:\n",
    "        print(f\"   ✗ {path}: NOT FOUND\")\n",
    "        all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ DATA DOWNLOAD COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nData ready at: /tmp/crescendai_data/\")\n",
    "    print(\"Training will be 10-30x faster than reading from Drive!\")\n",
    "else:\n",
    "    print(\"\\n✗ Data structure verification failed!\")\n",
    "    print(\"   Check that your archive has the correct structure\")\n",
    "    raise RuntimeError(\"Data download verification failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 2.5: FIX ANNOTATION PATHS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUpdating annotation files to use local SSD paths...\\n\")\n",
    "\n",
    "!python scripts/fix_annotation_paths.py\n",
    "\n",
    "print(\"\\n✓ Annotation paths updated for local SSD access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying data paths...\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check a sample annotation\n",
    "with open('/tmp/crescendai_data/data/annotations/synthetic_train.jsonl') as f:\n",
    "    sample = json.loads(f.readline())\n",
    "    \n",
    "print(f\"\\nSample annotation:\")\n",
    "print(f\"  Audio: {sample['audio_path']}\")\n",
    "print(f\"  MIDI:  {sample['midi_path']}\")\n",
    "\n",
    "# Verify files exist\n",
    "audio_exists = Path(sample['audio_path']).exists()\n",
    "midi_exists = Path(sample['midi_path']).exists() if sample['midi_path'] else False\n",
    "\n",
    "print(f\"\\nFile existence check:\")\n",
    "print(f\"  Audio exists: {'✓' if audio_exists else '✗'}\")\n",
    "print(f\"  MIDI exists:  {'✓' if midi_exists else '✗ (may be OK if path is None)'}\")\n",
    "\n",
    "if not audio_exists:\n",
    "    print(f\"\\n⚠️  WARNING: Audio file not found!\")\n",
    "    print(f\"     Check that data extraction completed correctly\")\n",
    "    print(f\"     Expected: {sample['audio_path']}\")\n",
    "elif not midi_exists and sample['midi_path']:\n",
    "    print(f\"\\n⚠️  WARNING: MIDI file not found!\")\n",
    "    print(f\"     Expected: {sample['midi_path']}\")\n",
    "else:\n",
    "    print(f\"\\n✓ Sample files verified - data structure looks correct!\")\n",
    "\n",
    "# Preflight Check\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 3: PREFLIGHT CHECK\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nVerifying training environment and data...\\n\")\n",
    "\n",
    "!python scripts/preflight_check.py --config configs/experiment_full.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Audio-Only\n",
    "\n",
    "Training with audio features only (MERT-95M encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='divide by zero')\n",
    "warnings.filterwarnings('ignore', category=SyntaxWarning)  # pydub regex warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torchaudio')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torchmetrics')\n",
    "\n",
    "%%time\n",
    "!python train.py --config configs/experiment_full.yaml --mode audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: MIDI-Only\n",
    "\n",
    "Training with MIDI features only (MIDIBert encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='divide by zero')\n",
    "warnings.filterwarnings('ignore', category=SyntaxWarning)  # pydub regex warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torchaudio')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torchmetrics')\n",
    "\n",
    "%%time\n",
    "!python train.py --config configs/experiment_full.yaml --mode midi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Fusion\n",
    "\n",
    "Training with both audio and MIDI features (multi-modal fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='divide by zero')\n",
    "warnings.filterwarnings('ignore', category=SyntaxWarning)  # pydub regex warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torchaudio')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torchmetrics')\n",
    "\n",
    "%%time\n",
    "!python train.py --config configs/experiment_full.yaml --mode fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results\n",
    "\n",
    "Load all 3 trained models and compare performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from src.models.lightning_module import PerformanceEvaluationModel\n",
    "from src.data.dataset import create_dataloaders\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE 3-WAY MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load all 3 models (use CHECKPOINT_ROOT from earlier cell)\n",
    "models = {}\n",
    "for mode in ['audio', 'midi', 'fusion']:\n",
    "    ckpt_dir = Path(f'{CHECKPOINT_ROOT}/{mode}_full')\n",
    "    ckpts = list(ckpt_dir.glob('*.ckpt'))\n",
    "    if ckpts:\n",
    "        latest = sorted(ckpts)[-1]\n",
    "        print(f\"Loading {mode}: {latest.name}\")\n",
    "        models[mode] = PerformanceEvaluationModel.load_from_checkpoint(str(latest))\n",
    "        models[mode].eval()\n",
    "        models[mode] = models[mode].cuda()\n",
    "    else:\n",
    "        print(f\"⚠️  No checkpoint found for {mode}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(models)}/3 models\")\n",
    "\n",
    "# Create test dataloader (using local SSD paths)\n",
    "_, _, test_loader = create_dataloaders(\n",
    "    train_annotation_path='/tmp/crescendai_data/data/annotations/synthetic_train.jsonl',\n",
    "    val_annotation_path='/tmp/crescendai_data/data/annotations/synthetic_val.jsonl',\n",
    "    test_annotation_path='/tmp/crescendai_data/data/annotations/synthetic_test.jsonl',\n",
    "    dimension_names=['note_accuracy', 'rhythmic_precision', 'tone_quality', 'dynamics_control', 'articulation', 'pedaling'],\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "    augmentation_config=None,\n",
    "    audio_sample_rate=24000,\n",
    "    max_audio_length=240000,\n",
    "    max_midi_events=512,\n",
    ")\n",
    "\n",
    "print(f\"Test set size: {len(test_loader.dataset)} samples\")\n",
    "\n",
    "# Evaluate each model\n",
    "trainer = pl.Trainer(accelerator='auto', devices='auto', precision=16)\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "for mode, model in models.items():\n",
    "    print(f\"\\nEvaluating {mode}...\")\n",
    "    test_results = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    results[mode] = test_results[0]\n",
    "    \n",
    "    # Collect predictions for deeper analysis\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Move batch to GPU\n",
    "            audio_waveform = batch['audio_waveform'].cuda()\n",
    "            midi_tokens = batch.get('midi_tokens', None)\n",
    "            if midi_tokens is not None:\n",
    "                midi_tokens = midi_tokens.cuda()\n",
    "            targets = batch['scores'].cuda()\n",
    "            \n",
    "            # Forward pass with proper arguments\n",
    "            output = model(\n",
    "                audio_waveform=audio_waveform,\n",
    "                midi_tokens=midi_tokens,\n",
    "            )\n",
    "            \n",
    "            # Skip if batch was None (all MIDI failed in MIDI-only mode)\n",
    "            if output is None:\n",
    "                continue\n",
    "                \n",
    "            preds = output['scores']\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    predictions[mode] = {\n",
    "        'preds': np.concatenate(all_preds, axis=0),\n",
    "        'targets': np.concatenate(all_targets, axis=0)\n",
    "    }\n",
    "\n",
    "dimensions = ['note_accuracy', 'rhythmic_precision', 'tone_quality', 'dynamics_control', 'articulation', 'pedaling']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. PER-DIMENSION CORRELATION (Pearson r)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Dimension':<25} {'Audio':<12} {'MIDI':<12} {'Fusion':<12} {'Best':<12} {'Improvement'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for dim_idx, dim in enumerate(dimensions):\n",
    "    audio_r = results.get('audio', {}).get(f'test_pearson_{dim}', 0)\n",
    "    midi_r = results.get('midi', {}).get(f'test_pearson_{dim}', 0)\n",
    "    fusion_r = results.get('fusion', {}).get(f'test_pearson_{dim}', 0)\n",
    "    \n",
    "    best_single = max(audio_r, midi_r)\n",
    "    best_modality = 'Audio' if audio_r >= midi_r else 'MIDI'\n",
    "    improvement = ((fusion_r - best_single) / best_single * 100) if best_single > 0 else 0\n",
    "    \n",
    "    print(f\"{dim:<25} {audio_r:>11.3f} {midi_r:>11.3f} {fusion_r:>11.3f} {best_modality:<12} {improvement:>+6.1f}%\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. PER-DIMENSION MAE (Mean Absolute Error, 0-100 scale)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Dimension':<25} {'Audio':<12} {'MIDI':<12} {'Fusion':<12} {'Best':<12} {'Reduction'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for dim_idx, dim in enumerate(dimensions):\n",
    "    audio_mae = np.mean(np.abs(predictions['audio']['preds'][:, dim_idx] - predictions['audio']['targets'][:, dim_idx]))\n",
    "    midi_mae = np.mean(np.abs(predictions['midi']['preds'][:, dim_idx] - predictions['midi']['targets'][:, dim_idx]))\n",
    "    fusion_mae = np.mean(np.abs(predictions['fusion']['preds'][:, dim_idx] - predictions['fusion']['targets'][:, dim_idx]))\n",
    "    \n",
    "    best_single_mae = min(audio_mae, midi_mae)\n",
    "    best_modality = 'Audio' if audio_mae <= midi_mae else 'MIDI'\n",
    "    reduction = ((best_single_mae - fusion_mae) / best_single_mae * 100) if best_single_mae > 0 else 0\n",
    "    \n",
    "    print(f\"{dim:<25} {audio_mae:>11.2f} {midi_mae:>11.2f} {fusion_mae:>11.2f} {best_modality:<12} {reduction:>+6.1f}%\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. PER-DIMENSION RMSE (Root Mean Squared Error, 0-100 scale)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Dimension':<25} {'Audio':<12} {'MIDI':<12} {'Fusion':<12} {'Best':<12} {'Reduction'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for dim_idx, dim in enumerate(dimensions):\n",
    "    audio_rmse = np.sqrt(np.mean((predictions['audio']['preds'][:, dim_idx] - predictions['audio']['targets'][:, dim_idx])**2))\n",
    "    midi_rmse = np.sqrt(np.mean((predictions['midi']['preds'][:, dim_idx] - predictions['midi']['targets'][:, dim_idx])**2))\n",
    "    fusion_rmse = np.sqrt(np.mean((predictions['fusion']['preds'][:, dim_idx] - predictions['fusion']['targets'][:, dim_idx])**2))\n",
    "    \n",
    "    best_single_rmse = min(audio_rmse, midi_rmse)\n",
    "    best_modality = 'Audio' if audio_rmse <= midi_rmse else 'MIDI'\n",
    "    reduction = ((best_single_rmse - fusion_rmse) / best_single_rmse * 100) if best_single_rmse > 0 else 0\n",
    "    \n",
    "    print(f\"{dim:<25} {audio_rmse:>11.2f} {midi_rmse:>11.2f} {fusion_rmse:>11.2f} {best_modality:<12} {reduction:>+6.1f}%\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. OVERALL PERFORMANCE (Averaged Across All Dimensions)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Metric':<35} {'Audio':<12} {'MIDI':<12} {'Fusion':<12} {'Winner'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Mean Pearson\n",
    "audio_mean_r = np.mean([results['audio'][f'test_pearson_{d}'] for d in dimensions])\n",
    "midi_mean_r = np.mean([results['midi'][f'test_pearson_{d}'] for d in dimensions])\n",
    "fusion_mean_r = np.mean([results['fusion'][f'test_pearson_{d}'] for d in dimensions])\n",
    "best_r = max(audio_mean_r, midi_mean_r, fusion_mean_r)\n",
    "winner_r = 'Audio' if audio_mean_r == best_r else ('MIDI' if midi_mean_r == best_r else 'Fusion')\n",
    "print(f\"{'Mean Pearson Correlation':<35} {audio_mean_r:>11.3f} {midi_mean_r:>11.3f} {fusion_mean_r:>11.3f} {winner_r}\")\n",
    "\n",
    "# Mean MAE\n",
    "audio_mean_mae = np.mean([np.mean(np.abs(predictions['audio']['preds'][:, i] - predictions['audio']['targets'][:, i])) for i in range(len(dimensions))])\n",
    "midi_mean_mae = np.mean([np.mean(np.abs(predictions['midi']['preds'][:, i] - predictions['midi']['targets'][:, i])) for i in range(len(dimensions))])\n",
    "fusion_mean_mae = np.mean([np.mean(np.abs(predictions['fusion']['preds'][:, i] - predictions['fusion']['targets'][:, i])) for i in range(len(dimensions))])\n",
    "best_mae = min(audio_mean_mae, midi_mean_mae, fusion_mean_mae)\n",
    "winner_mae = 'Audio' if audio_mean_mae == best_mae else ('MIDI' if midi_mean_mae == best_mae else 'Fusion')\n",
    "print(f\"{'Mean Absolute Error':<35} {audio_mean_mae:>11.2f} {midi_mean_mae:>11.2f} {fusion_mean_mae:>11.2f} {winner_mae}\")\n",
    "\n",
    "# Mean RMSE\n",
    "audio_mean_rmse = np.mean([np.sqrt(np.mean((predictions['audio']['preds'][:, i] - predictions['audio']['targets'][:, i])**2)) for i in range(len(dimensions))])\n",
    "midi_mean_rmse = np.mean([np.sqrt(np.mean((predictions['midi']['preds'][:, i] - predictions['midi']['targets'][:, i])**2)) for i in range(len(dimensions))])\n",
    "fusion_mean_rmse = np.mean([np.sqrt(np.mean((predictions['fusion']['preds'][:, i] - predictions['fusion']['targets'][:, i])**2)) for i in range(len(dimensions))])\n",
    "best_rmse = min(audio_mean_rmse, midi_mean_rmse, fusion_mean_rmse)\n",
    "winner_rmse = 'Audio' if audio_mean_rmse == best_rmse else ('MIDI' if midi_mean_rmse == best_rmse else 'Fusion')\n",
    "print(f\"{'Root Mean Squared Error':<35} {audio_mean_rmse:>11.2f} {midi_mean_rmse:>11.2f} {fusion_mean_rmse:>11.2f} {winner_rmse}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. FUSION PERFORMANCE GAIN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_single_r = max(audio_mean_r, midi_mean_r)\n",
    "r_improvement = ((fusion_mean_r - best_single_r) / best_single_r * 100) if best_single_r > 0 else 0\n",
    "print(f\"Pearson r improvement:  {r_improvement:+.1f}% over best single-modal\")\n",
    "\n",
    "best_single_mae = min(audio_mean_mae, midi_mean_mae)\n",
    "mae_reduction = ((best_single_mae - fusion_mean_mae) / best_single_mae * 100) if best_single_mae > 0 else 0\n",
    "print(f\"MAE reduction:          {mae_reduction:+.1f}% over best single-modal\")\n",
    "\n",
    "best_single_rmse = min(audio_mean_rmse, midi_mean_rmse)\n",
    "rmse_reduction = ((best_single_rmse - fusion_mean_rmse) / best_single_rmse * 100) if best_single_rmse > 0 else 0\n",
    "print(f\"RMSE reduction:         {rmse_reduction:+.1f}% over best single-modal\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. STATISTICAL SIGNIFICANCE (Fusion vs Best Single-Modal)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Dimension':<25} {'Best Single':<15} {'p-value':<12} {'Significant?'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for dim_idx, dim in enumerate(dimensions):\n",
    "    audio_errors = np.abs(predictions['audio']['preds'][:, dim_idx] - predictions['audio']['targets'][:, dim_idx])\n",
    "    midi_errors = np.abs(predictions['midi']['preds'][:, dim_idx] - predictions['midi']['targets'][:, dim_idx])\n",
    "    fusion_errors = np.abs(predictions['fusion']['preds'][:, dim_idx] - predictions['fusion']['targets'][:, dim_idx])\n",
    "    \n",
    "    # Compare fusion vs best single modal (paired t-test on MAE)\n",
    "    best_single_errors = audio_errors if np.mean(audio_errors) <= np.mean(midi_errors) else midi_errors\n",
    "    best_single_name = 'Audio' if np.mean(audio_errors) <= np.mean(midi_errors) else 'MIDI'\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_rel(best_single_errors, fusion_errors)\n",
    "    is_significant = p_value < 0.05 and np.mean(fusion_errors) < np.mean(best_single_errors)\n",
    "    \n",
    "    print(f\"{dim:<25} {best_single_name:<15} {p_value:>11.4f} {'Yes' if is_significant else 'No':>12}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. DIMENSION LEARNABILITY CATEGORIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "strong = [d for d in dimensions if max(results.get('audio', {}).get(f'test_pearson_{d}', 0), \n",
    "                                        results.get('midi', {}).get(f'test_pearson_{d}', 0)) > 0.4]\n",
    "moderate = [d for d in dimensions if 0.25 <= max(results.get('audio', {}).get(f'test_pearson_{d}', 0),\n",
    "                                                   results.get('midi', {}).get(f'test_pearson_{d}', 0)) <= 0.4]\n",
    "weak = [d for d in dimensions if max(results.get('audio', {}).get(f'test_pearson_{d}', 0),\n",
    "                                      results.get('midi', {}).get(f'test_pearson_{d}', 0)) < 0.25]\n",
    "\n",
    "print(f\"Strong learners (r > 0.4):     {', '.join(strong) if strong else 'None'}\")\n",
    "print(f\"Moderate learners (0.25-0.4):  {', '.join(moderate) if moderate else 'None'}\")\n",
    "print(f\"Weak learners (r < 0.25):      {', '.join(weak) if weak else 'None'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. MVP TARGET ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "print(\"Technical dimension target: r = 0.50-0.65 (Pearson with expert)\")\n",
    "print(\"Interpretive dimension target: r = 0.35-0.50\")\n",
    "print(\"MAE target: 10-15 points on 0-100 scale\\n\")\n",
    "\n",
    "technical_dims = dimensions  # All 6 are technical in this experiment\n",
    "technical_r_values = [results.get('fusion', {}).get(f'test_pearson_{d}', 0) for d in technical_dims]\n",
    "technical_mean_r = np.mean(technical_r_values)\n",
    "\n",
    "meets_r_target = technical_mean_r >= 0.50\n",
    "meets_mae_target = fusion_mean_mae <= 15\n",
    "\n",
    "print(f\"Fusion technical r:     {technical_mean_r:.3f} {'(PASS)' if meets_r_target else '(FAIL - below 0.50 target)'}\")\n",
    "print(f\"Fusion overall MAE:     {fusion_mean_mae:.2f} {'(PASS)' if meets_mae_target else '(FAIL - above 15 target)'}\")\n",
    "\n",
    "if meets_r_target and meets_mae_target:\n",
    "    print(\"\\nMVP TARGETS MET - Ready to proceed with expert annotation\")\n",
    "else:\n",
    "    print(\"\\nMVP TARGETS NOT MET - Consider architecture improvements or data augmentation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. EXPERT ANNOTATION RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Include in expert labels:  {', '.join(strong + moderate)}\")\n",
    "print(f\"Consider skipping:         {', '.join(weak)}\")\n",
    "print(f\"\\nEstimated cost savings: ${len(weak) * 3000:,} by excluding weak dimensions\")\n",
    "print(f\"Recommended budget:     ${len(strong + moderate) * 3000:,} for {len(strong + moderate)} dimensions\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
