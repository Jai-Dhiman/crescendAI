{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piano Performance Evaluation - 3-Way Model Comparison (Colab)\n",
    "\n",
    "Trains 3 models to prove multi-modal fusion advantage:\n",
    "1. Audio-Only (MERT only)\n",
    "2. MIDI-Only (MIDIBert only)\n",
    "3. Fusion (MERT + MIDIBert)\n",
    "\n",
    "**Dimensions**: 3 core (note_accuracy, rhythmic_precision, tone_quality)\n",
    "**Sample size**: 10,000 training samples\n",
    "**Expected time**: 6-7 hours total (2h + 1.5h + 2.5h)\n",
    "**Goal**: Prove fusion beats both baselines by 15-20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive Structure\n",
    "\n",
    "```\n",
    "MyDrive/\n",
    "  crescendai_data/\n",
    "    all_segments/              # Audio segments\n",
    "      *.wav\n",
    "      midi_segments/\n",
    "        *.mid\n",
    "    annotations/\n",
    "      synthetic_train_filtered.jsonl    # 91,865 samples\n",
    "      synthetic_val_filtered.jsonl\n",
    "      synthetic_test_filtered.jsonl\n",
    "\n",
    "  crescendai_checkpoints/\n",
    "    audio_10k/                 # Audio-only checkpoints\n",
    "    midi_10k/                  # MIDI-only checkpoints  \n",
    "    fusion_10k/                # Fusion checkpoints\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged in as: Jai-D\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Login\n",
    "import os\n",
    "os.environ.pop(\"HF_TOKEN\", None)\n",
    "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
    "\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "try:\n",
    "    import getpass as gp\n",
    "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
    "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
    "    if not isinstance(token, str):\n",
    "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
    "    token = token.strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"Empty token provided\")\n",
    "    login(token=token, add_to_git_credential=False)\n",
    "    who = HfApi().whoami(token=token)\n",
    "    print(f\"✓ Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "except Exception as e:\n",
    "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
    "    print(\"Falling back to interactive login widget...\")\n",
    "    login()\n",
    "    try:\n",
    "        who = HfApi().whoami()\n",
    "        print(f\"✓ Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"[HF Login] Verification skipped: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify data exists\n",
    "import os\n",
    "ANNOTATIONS_ROOT = '/content/drive/MyDrive/crescendai_data/annotations'\n",
    "\n",
    "required_files = [\n",
    "    f'{ANNOTATIONS_ROOT}/synthetic_train_filtered.jsonl',\n",
    "    f'{ANNOTATIONS_ROOT}/synthetic_val_filtered.jsonl',\n",
    "    f'{ANNOTATIONS_ROOT}/synthetic_test_filtered.jsonl',\n",
    "]\n",
    "\n",
    "print(\"Checking for data files...\")\n",
    "for f in required_files:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"✓ {os.path.basename(f)}\")\n",
    "    else:\n",
    "        print(f\"✗ MISSING: {f}\")\n",
    "        raise FileNotFoundError(f\"Required file not found: {f}\")\n",
    "\n",
    "print(\"\\n✓ All data files present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "!rm -rf /content/crescendai\n",
    "!git clone https://github.com/Jai-Dhiman/crescendai.git /content/crescendai\n",
    "%cd /content/crescendai/model\n",
    "!git log -1 --oneline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv (fast Python package manager)\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add to PATH for this session\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "print(\"\\n✓ uv installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!uv pip install --system -e .\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='divide by zero')\n",
    "warnings.filterwarnings('ignore', category=SyntaxWarning)  # pydub regex warnings\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"\\n⚠️  NO GPU! Enable GPU: Runtime → Change runtime type → T4 GPU\")\n",
    "    raise RuntimeError(\"GPU required\")\n",
    "\n",
    "print(f\"\\n✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"✓ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MERT model (cached after first download)\n",
    "from transformers import AutoModel\n",
    "\n",
    "print(\"Downloading MERT-95M (~380MB)...\")\n",
    "model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\n",
    "print(\"✓ MERT-95M cached\")\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport shutil\nimport random\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# Create local data directories\nLOCAL_DATA = Path('/tmp/training_data')\nLOCAL_AUDIO = Path('/tmp/audio_segments')\nLOCAL_MIDI = Path('/tmp/midi_segments')\n\nLOCAL_DATA.mkdir(exist_ok=True)\nLOCAL_AUDIO.mkdir(exist_ok=True)\nLOCAL_MIDI.mkdir(exist_ok=True)\n\ndef copy_files_and_update_annotations(input_jsonl, output_jsonl, n_samples=10000, seed=42):\n    \"\"\"\n    Copy only the files needed for this subset to /tmp/ and update paths\n    \"\"\"\n    # Load annotations\n    with open(input_jsonl) as f:\n        data = [json.loads(line) for line in f if line.strip()]\n    \n    print(f\"Original: {len(data):,} samples\")\n    \n    # Subsample if needed\n    if n_samples < len(data):\n        random.seed(seed)\n        data = random.sample(data, n_samples)\n        print(f\"Subsampled to {len(data):,} samples ({len(data)/len(data)*100:.1f}%)\")\n    \n    # Copy files and update paths\n    updated_data = []\n    copied_audio = set()\n    copied_midi = set()\n    \n    print(f\"Copying {len(data):,} audio/MIDI files to /tmp/...\")\n    for item in tqdm(data, desc=\"Copying files\"):\n        # Copy audio\n        audio_src = Path(item['audio_path'])\n        audio_dst = LOCAL_AUDIO / audio_src.name\n        \n        if audio_src.name not in copied_audio:\n            if audio_src.exists():\n                shutil.copy2(audio_src, audio_dst)\n                copied_audio.add(audio_src.name)\n            else:\n                print(f\"\\n  Warning: Audio not found: {audio_src}\")\n        \n        # Copy MIDI\n        midi_src = Path(item['midi_path'])\n        midi_dst = LOCAL_MIDI / midi_src.name\n        \n        if midi_src.name not in copied_midi:\n            if midi_src.exists():\n                shutil.copy2(midi_src, midi_dst)\n                copied_midi.add(midi_src.name)\n            else:\n                print(f\"\\n  Warning: MIDI not found: {midi_src}\")\n        \n        # Update paths to point to /tmp/\n        item['audio_path'] = str(audio_dst)\n        item['midi_path'] = str(midi_dst)\n        updated_data.append(item)\n    \n    # Save updated annotations\n    with open(output_jsonl, 'w') as f:\n        for item in updated_data:\n            f.write(json.dumps(item) + '\\n')\n    \n    print(f\"Copied {len(copied_audio):,} unique audio files\")\n    print(f\"Copied {len(copied_midi):,} unique MIDI files\")\n    \n    return len(updated_data)\n\nprint(\"=\"*70)\nprint(\"COPYING DATA TO LOCAL STORAGE\")\nprint(\"=\"*70)\nprint(\"\\nThis will copy ~10K audio + MIDI files (~20-30GB) to /tmp/\")\nprint(\"Expected time: 5-10 minutes\")\nprint()\n\n# Copy training data (10K samples)\nprint(\"Training set:\")\nn_train = copy_files_and_update_annotations(\n    f'{ANNOTATIONS_ROOT}/synthetic_train_filtered.jsonl',\n    LOCAL_DATA / 'synthetic_train_filtered.jsonl',\n    n_samples=10000\n)\n\nprint(\"\\nValidation set:\")\nn_val = copy_files_and_update_annotations(\n    f'{ANNOTATIONS_ROOT}/synthetic_val_filtered.jsonl',\n    LOCAL_DATA / 'synthetic_val_filtered.jsonl',\n    n_samples=999999  # Copy all\n)\n\nprint(\"\\nTest set:\")\nn_test = copy_files_and_update_annotations(\n    f'{ANNOTATIONS_ROOT}/synthetic_test_filtered.jsonl',\n    LOCAL_DATA / 'synthetic_test_filtered.jsonl',\n    n_samples=999999  # Copy all\n)\n\n# Check disk usage\nimport subprocess\nresult = subprocess.run(['df', '-h', '/tmp'], capture_output=True, text=True)\nprint(\"\\n\" + \"=\"*70)\nprint(\"DISK USAGE\")\nprint(\"=\"*70)\nprint(result.stdout)\n\nprint(\"=\"*70)\nprint(\"✓ DATA COPIED TO LOCAL STORAGE\")\nprint(\"=\"*70)\nprint(f\"\\nAnnotation files (updated paths to /tmp/):\")\nprint(f\"  Train: {n_train:,} samples\")\nprint(f\"  Val:   {n_val:,} samples\")\nprint(f\"  Test:  {n_test:,} samples\")\nprint(f\"\\nAll files now on fast local SSD\")\nprint(f\"Expected speedup: 10-20× faster than Google Drive\")\n!ls -lh /tmp/training_data/\n!echo \"\"\n!du -sh /tmp/audio_segments /tmp/midi_segments"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preflight Check\n",
    "\n",
    "Verify data and test audio/MIDI loading before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\nprint(\"=\"*70)\nprint(\"PREFLIGHT CHECK\")\nprint(\"=\"*70)\n\n# 1. Check data files exist and are readable\nprint(\"\\n1. Checking data files...\")\nfor split in ['train', 'val', 'test']:\n    path = f'/tmp/training_data/synthetic_{split}_filtered.jsonl'\n    if not Path(path).exists():\n        print(f\"  ✗ {split}: FILE NOT FOUND\")\n        raise FileNotFoundError(f\"Missing: {path}\")\n    \n    # Load first line\n    with open(path) as f:\n        first_line = f.readline()\n        sample = json.loads(first_line)\n    \n    # Check required fields\n    required = ['audio_path', 'midi_path', 'labels']\n    missing = [f for f in required if f not in sample]\n    if missing:\n        print(f\"  ✗ {split}: Missing fields {missing}\")\n        raise ValueError(f\"Invalid annotation format in {path}\")\n    \n    print(f\"  ✓ {split}: {Path(path).stat().st_size / 1024 / 1024:.1f} MB\")\n\n# 2. Test ACTUAL audio/MIDI loading (1 sample from Drive - will be slow)\nprint(\"\\n2. Testing actual data loading (from Google Drive)...\")\nprint(\"   This will be slow (~30-60 seconds) - validating Drive access works\")\n\nfrom src.data.audio_processing import load_audio, normalize_audio\nfrom src.data.midi_processing import load_midi, align_midi_to_audio, encode_octuple_midi\n\naudio_path = sample['audio_path']\nmidi_path = sample['midi_path']\n\ntry:\n    # Load audio\n    audio, sr = load_audio(audio_path, sr=24000)\n    audio = normalize_audio(audio)\n    print(f\"  ✓ Audio loaded: {len(audio)} samples @ {sr} Hz from Drive\")\n    \n    # Load MIDI\n    midi = load_midi(midi_path)\n    audio_duration = len(audio) / sr\n    midi = align_midi_to_audio(midi, audio_duration)\n    tokens = encode_octuple_midi(midi)\n    print(f\"  ✓ MIDI loaded: {len(tokens)} tokens from Drive\")\n    \nexcept Exception as e:\n    print(f\"  ✗ Data loading failed: {e}\")\n    print(f\"     Audio path: {audio_path}\")\n    print(f\"     MIDI path: {midi_path}\")\n    raise\n\n# 3. Test config file\nprint(\"\\n3. Testing config file...\")\ntry:\n    import yaml\n    with open('configs/experiment_10k.yaml') as f:\n        config = yaml.safe_load(f)\n    print(f\"  ✓ Config loaded\")\n    print(f\"    Dimensions: {config['data']['dimensions']}\")\n    print(f\"    Batch size: {config['data']['batch_size']}\")\n    print(f\"    Num workers: {config['data']['num_workers']} (must be 0 for Drive)\")\nexcept Exception as e:\n    print(f\"  ✗ Config loading failed: {e}\")\n    raise\n\n# 4. Test model instantiation\nprint(\"\\n4. Testing model instantiation...\")\ntry:\n    from src.models.lightning_module import PerformanceEvaluationModel\n    \n    # Test each mode\n    for mode in ['audio', 'midi', 'fusion']:\n        model_config = config['model'].copy()\n        mode_overrides = config['modes'][mode]\n        model_config.update(mode_overrides)\n        \n        model = PerformanceEvaluationModel(\n            dimension_names=config['data']['dimensions'],\n            **model_config\n        )\n        params = sum(p.numel() for p in model.parameters()) / 1e6\n        print(f\"  ✓ {mode}: {params:.1f}M params\")\n        del model\n    \n    torch.cuda.empty_cache()\nexcept Exception as e:\n    print(f\"  ✗ Model instantiation failed: {e}\")\n    raise\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"✓ ALL PREFLIGHT CHECKS PASSED\")\nprint(\"=\"*70)\nprint(\"\\nGoogle Drive access verified - training will be slow but functional\")\nprint(\"Expected: 2-3 hours per epoch with num_workers=0\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Audio-Only (~2 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!python train.py --config configs/experiment_10k.yaml --mode audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: MIDI-Only (~1.5 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!python train.py --config configs/experiment_10k.yaml --mode midi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Fusion (~2.5 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!python train.py --config configs/experiment_10k.yaml --mode fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from src.models.lightning_module import PerformanceEvaluationModel\n",
    "from src.data.dataset import create_dataloaders\n",
    "from pathlib import Path\n",
    "\n",
    "# Load all 3 models\n",
    "models = {}\n",
    "for mode in ['audio', 'midi', 'fusion']:\n",
    "    ckpt_dir = Path(f'/content/drive/MyDrive/crescendai_checkpoints/{mode}_10k')\n",
    "    ckpts = list(ckpt_dir.glob('*.ckpt'))\n",
    "    if ckpts:\n",
    "        latest = sorted(ckpts)[-1]\n",
    "        print(f\"Loading {mode}: {latest.name}\")\n",
    "        models[mode] = PerformanceEvaluationModel.load_from_checkpoint(str(latest))\n",
    "        models[mode].eval()\n",
    "        models[mode] = models[mode].cuda()\n",
    "    else:\n",
    "        print(f\"⚠️  No checkpoint found for {mode}\")\n",
    "\n",
    "# Create test dataloader\n",
    "_, _, test_loader = create_dataloaders(\n",
    "    train_annotation_path='/tmp/training_data/synthetic_train_filtered.jsonl',\n",
    "    val_annotation_path='/tmp/training_data/synthetic_val_filtered.jsonl',\n",
    "    test_annotation_path='/tmp/training_data/synthetic_test_filtered.jsonl',\n",
    "    dimension_names=['note_accuracy', 'rhythmic_precision', 'tone_quality'],\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    "    augmentation_config=None,\n",
    "    audio_sample_rate=24000,\n",
    "    max_audio_length=240000,\n",
    "    max_midi_events=512,\n",
    ")\n",
    "\n",
    "# Evaluate each model\n",
    "trainer = pl.Trainer(accelerator='auto', devices='auto', precision=16)\n",
    "results = {}\n",
    "\n",
    "for mode, model in models.items():\n",
    "    print(f\"\\nEvaluating {mode}...\")\n",
    "    test_results = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    results[mode] = test_results[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Dimension':<25} {'Audio r':<12} {'MIDI r':<12} {'Fusion r':<12} {'Gain'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for dim in ['note_accuracy', 'rhythmic_precision', 'tone_quality']:\n",
    "    audio_r = results.get('audio', {}).get(f'test_pearson_{dim}', 0)\n",
    "    midi_r = results.get('midi', {}).get(f'test_pearson_{dim}', 0)\n",
    "    fusion_r = results.get('fusion', {}).get(f'test_pearson_{dim}', 0)\n",
    "    gain = fusion_r - max(audio_r, midi_r)\n",
    "    \n",
    "    print(f\"{dim:<25} {audio_r:>11.3f} {midi_r:>11.3f} {fusion_r:>11.3f} {gain:>+11.3f}\")\n",
    "\n",
    "avg_gain = sum(\n",
    "    results.get('fusion', {}).get(f'test_pearson_{dim}', 0) - \n",
    "    max(results.get('audio', {}).get(f'test_pearson_{dim}', 0),\n",
    "        results.get('midi', {}).get(f'test_pearson_{dim}', 0))\n",
    "    for dim in ['note_accuracy', 'rhythmic_precision', 'tone_quality']\n",
    ") / 3\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(f\"Average fusion gain: {avg_gain:+.3f} ({avg_gain*100:+.1f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if avg_gain > 0.05:\n",
    "    print(\"\\n✓ SUCCESS: Fusion shows clear multi-modal advantage!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  WARNING: Fusion gain is marginal. Check fusion implementation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from src.models.lightning_module import PerformanceEvaluationModel\n",
    "from src.data.dataset import create_dataloaders\n",
    "from pathlib import Path\n",
    "\n",
    "# Load all 3 models\n",
    "models = {}\n",
    "for mode in ['audio', 'midi', 'fusion']:\n",
    "    ckpt_dir = Path(f'/content/drive/MyDrive/crescendai_checkpoints/{mode}_10k')\n",
    "    ckpts = list(ckpt_dir.glob('*.ckpt'))\n",
    "    if ckpts:\n",
    "        latest = sorted(ckpts)[-1]\n",
    "        print(f\"Loading {mode}: {latest.name}\")\n",
    "        models[mode] = PerformanceEvaluationModel.load_from_checkpoint(str(latest))\n",
    "        models[mode].eval()\n",
    "        models[mode] = models[mode].cuda()\n",
    "    else:\n",
    "        print(f\"⚠️  No checkpoint found for {mode}\")\n",
    "\n",
    "# Create test dataloader\n",
    "_, _, test_loader = create_dataloaders(\n",
    "    train_annotation_path='/tmp/training_data/synthetic_train_filtered.jsonl',\n",
    "    val_annotation_path='/tmp/training_data/synthetic_val_filtered.jsonl',\n",
    "    test_annotation_path='/tmp/training_data/synthetic_test_filtered.jsonl',\n",
    "    dimension_names=['note_accuracy', 'rhythmic_precision', 'tone_quality'],\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    "    augmentation_config=None,\n",
    "    audio_sample_rate=24000,\n",
    "    max_audio_length=240000,\n",
    "    max_midi_events=512,\n",
    ")\n",
    "\n",
    "# Evaluate each model\n",
    "trainer = pl.Trainer(accelerator='auto', devices='auto', precision=16)\n",
    "results = {}\n",
    "\n",
    "for mode, model in models.items():\n",
    "    print(f\"\\nEvaluating {mode}...\")\n",
    "    test_results = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    results[mode] = test_results[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Dimension':<25} {'Audio r':<12} {'MIDI r':<12} {'Fusion r':<12} {'Gain'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for dim in ['note_accuracy', 'rhythmic_precision', 'tone_quality']:\n",
    "    audio_r = results.get('audio', {}).get(f'test_pearson_{dim}', 0)\n",
    "    midi_r = results.get('midi', {}).get(f'test_pearson_{dim}', 0)\n",
    "    fusion_r = results.get('fusion', {}).get(f'test_pearson_{dim}', 0)\n",
    "    gain = fusion_r - max(audio_r, midi_r)\n",
    "    \n",
    "    print(f\"{dim:<25} {audio_r:>11.3f} {midi_r:>11.3f} {fusion_r:>11.3f} {gain:>+11.3f}\")\n",
    "\n",
    "avg_gain = sum(\n",
    "    results.get('fusion', {}).get(f'test_pearson_{dim}', 0) - \n",
    "    max(results.get('audio', {}).get(f'test_pearson_{dim}', 0),\n",
    "        results.get('midi', {}).get(f'test_pearson_{dim}', 0))\n",
    "    for dim in ['note_accuracy', 'rhythmic_precision', 'tone_quality']\n",
    ") / 3\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(f\"Average fusion gain: {avg_gain:+.3f} ({avg_gain*100:+.1f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if avg_gain > 0.05:\n",
    "    print(\"\\n✓ SUCCESS: Fusion shows clear multi-modal advantage!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  WARNING: Fusion gain is marginal. Check fusion implementation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}