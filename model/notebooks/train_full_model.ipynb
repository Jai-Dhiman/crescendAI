{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Piano Performance Evaluation - 3-Way Model Comparison (Colab)\n\nTrains 3 models to prove multi-modal fusion advantage:\n1. Audio-Only (MERT only)\n2. MIDI-Only (MIDIBert only)\n3. Fusion (MERT + MIDIBert)\n\n**Dimensions**: 3 core (note_accuracy, rhythmic_precision, tone_quality)\n**Sample size**: 10,000 training samples\n**Expected time**: 6-7 hours total (2h + 1.5h + 2.5h)\n**Goal**: Prove fusion beats both baselines by 15-20%"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Google Drive Structure\n\n```\nMyDrive/\n  crescendai_data/\n    all_segments/              # Audio segments\n      *.wav\n      midi_segments/\n        *.mid\n    annotations/\n      synthetic_train_filtered.jsonl    # 91,865 samples\n      synthetic_val_filtered.jsonl\n      synthetic_test_filtered.jsonl\n\n  crescendai_checkpoints/\n    audio_10k/                 # Audio-only checkpoints\n    midi_10k/                  # MIDI-only checkpoints  \n    fusion_10k/                # Fusion checkpoints\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged in as: Jai-D\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Login\n",
    "import os\n",
    "os.environ.pop(\"HF_TOKEN\", None)\n",
    "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
    "\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "try:\n",
    "    import getpass as gp\n",
    "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
    "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
    "    if not isinstance(token, str):\n",
    "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
    "    token = token.strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"Empty token provided\")\n",
    "    login(token=token, add_to_git_credential=False)\n",
    "    who = HfApi().whoami(token=token)\n",
    "    print(f\"✓ Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "except Exception as e:\n",
    "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
    "    print(\"Falling back to interactive login widget...\")\n",
    "    login()\n",
    "    try:\n",
    "        who = HfApi().whoami()\n",
    "        print(f\"✓ Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"[HF Login] Verification skipped: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Verify data exists\nimport os\nANNOTATIONS_ROOT = '/content/drive/MyDrive/crescendai_data/annotations'\n\nrequired_files = [\n    f'{ANNOTATIONS_ROOT}/synthetic_train_filtered.jsonl',\n    f'{ANNOTATIONS_ROOT}/synthetic_val_filtered.jsonl',\n    f'{ANNOTATIONS_ROOT}/synthetic_test_filtered.jsonl',\n]\n\nprint(\"Checking for data files...\")\nfor f in required_files:\n    if os.path.exists(f):\n        print(f\"✓ {os.path.basename(f)}\")\n    else:\n        print(f\"✗ MISSING: {f}\")\n        raise FileNotFoundError(f\"Required file not found: {f}\")\n\nprint(\"\\n✓ All data files present\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone repo\n!rm -rf /content/crescendai\n!git clone https://github.com/Jai-Dhiman/crescendai.git /content/crescendai\n%cd /content/crescendai/model\n!git log -1 --oneline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv (fast Python package manager)\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add to PATH for this session\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "print(\"\\n✓ uv installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!uv pip install --system -e .\n\n# Suppress MIDI warnings\nimport warnings\nwarnings.filterwarnings('ignore', message='divide by zero')\n\nimport torch\nimport pytorch_lightning as pl\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Lightning: {pl.__version__}\")\nprint(\"✓ Dependencies installed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## GPU Check"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!nvidia-smi\n\nimport torch\nif not torch.cuda.is_available():\n    print(\"\\n⚠️  NO GPU! Enable GPU: Runtime → Change runtime type → T4 GPU\")\n    raise RuntimeError(\"GPU required\")\n\nprint(f\"\\n✓ GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"✓ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download MERT model (cached after first download)\nfrom transformers import AutoModel\n\nprint(\"Downloading MERT-95M (~380MB)...\")\nmodel = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\nprint(\"✓ MERT-95M cached\")\n\ndel model\ntorch.cuda.empty_cache()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Optimization (CRITICAL)\n\nCopy data from Google Drive → local SSD for 5-10× speedup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport random\nfrom pathlib import Path\n\n# Create local data directory\nLOCAL_DATA = Path('/tmp/training_data')\nLOCAL_DATA.mkdir(exist_ok=True)\n\n# Function to subsample dataset\ndef subsample_jsonl(input_path, output_path, n_samples=10000, seed=42):\n    \"\"\"Create random subset of JSONL file\"\"\"\n    with open(input_path) as f:\n        data = [json.loads(line) for line in f if line.strip()]\n    \n    print(f\"Original: {len(data):,} samples\")\n    \n    if n_samples >= len(data):\n        # Just copy\n        with open(output_path, 'w') as f:\n            for item in data:\n                f.write(json.dumps(item) + '\\n')\n        print(f\"Copied all {len(data):,} samples\")\n    else:\n        # Random subsample\n        random.seed(seed)\n        subset = random.sample(data, n_samples)\n        with open(output_path, 'w') as f:\n            for item in subset:\n                f.write(json.dumps(item) + '\\n')\n        print(f\"Subsampled to {len(subset):,} samples ({len(subset)/len(data)*100:.1f}%)\")\n\n# Copy train set (10K subsample)\nprint(\"Creating 10K training subset...\")\nsubsample_jsonl(\n    f'{ANNOTATIONS_ROOT}/synthetic_train_filtered.jsonl',\n    LOCAL_DATA / 'synthetic_train_filtered.jsonl',\n    n_samples=10000\n)\n\n# Copy val/test (full)\nprint(\"\\nCopying validation set...\")\n!cp {ANNOTATIONS_ROOT}/synthetic_val_filtered.jsonl /tmp/training_data/\n\nprint(\"Copying test set...\")\n!cp {ANNOTATIONS_ROOT}/synthetic_test_filtered.jsonl /tmp/training_data/\n\nprint(\"\\n✓ Data copied to /tmp/training_data/ (fast local SSD)\")\nprint(\"\\nThis data is TEMPORARY - wiped when runtime disconnects\")\nprint(\"Checkpoints still save to Google Drive (persistent)\")\n!ls -lh /tmp/training_data/"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment 1: Audio-Only (~2 hours)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n!python train.py --config configs/experiment_10k.yaml --mode audio"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment 2: MIDI-Only (~1.5 hours)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n!python train.py --config configs/experiment_10k.yaml --mode midi"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Experiment 3: Fusion (~2.5 hours)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n!python train.py --config configs/experiment_10k.yaml --mode fusion"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Compare Results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import pytorch_lightning as pl\nfrom src.models.lightning_module import PerformanceEvaluationModel\nfrom src.data.dataset import create_dataloaders\nfrom pathlib import Path\n\n# Load all 3 models\nmodels = {}\nfor mode in ['audio', 'midi', 'fusion']:\n    ckpt_dir = Path(f'/content/drive/MyDrive/crescendai_checkpoints/{mode}_10k')\n    ckpts = list(ckpt_dir.glob('*.ckpt'))\n    if ckpts:\n        latest = sorted(ckpts)[-1]\n        print(f\"Loading {mode}: {latest.name}\")\n        models[mode] = PerformanceEvaluationModel.load_from_checkpoint(str(latest))\n        models[mode].eval()\n        models[mode] = models[mode].cuda()\n    else:\n        print(f\"⚠️  No checkpoint found for {mode}\")\n\n# Create test dataloader\n_, _, test_loader = create_dataloaders(\n    train_annotation_path='/tmp/training_data/synthetic_train_filtered.jsonl',\n    val_annotation_path='/tmp/training_data/synthetic_val_filtered.jsonl',\n    test_annotation_path='/tmp/training_data/synthetic_test_filtered.jsonl',\n    dimension_names=['note_accuracy', 'rhythmic_precision', 'tone_quality'],\n    batch_size=8,\n    num_workers=0,\n    augmentation_config=None,\n    audio_sample_rate=24000,\n    max_audio_length=240000,\n    max_midi_events=512,\n)\n\n# Evaluate each model\ntrainer = pl.Trainer(accelerator='auto', devices='auto', precision=16)\nresults = {}\n\nfor mode, model in models.items():\n    print(f\"\\nEvaluating {mode}...\")\n    test_results = trainer.test(model, dataloaders=test_loader, verbose=False)\n    results[mode] = test_results[0]\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPARISON\")\nprint(\"=\"*70)\nprint(f\"{'Dimension':<25} {'Audio r':<12} {'MIDI r':<12} {'Fusion r':<12} {'Gain'}\")\nprint(\"-\"*70)\n\nfor dim in ['note_accuracy', 'rhythmic_precision', 'tone_quality']:\n    audio_r = results.get('audio', {}).get(f'test_pearson_{dim}', 0)\n    midi_r = results.get('midi', {}).get(f'test_pearson_{dim}', 0)\n    fusion_r = results.get('fusion', {}).get(f'test_pearson_{dim}', 0)\n    gain = fusion_r - max(audio_r, midi_r)\n    \n    print(f\"{dim:<25} {audio_r:>11.3f} {midi_r:>11.3f} {fusion_r:>11.3f} {gain:>+11.3f}\")\n\navg_gain = sum(\n    results.get('fusion', {}).get(f'test_pearson_{dim}', 0) - \n    max(results.get('audio', {}).get(f'test_pearson_{dim}', 0),\n        results.get('midi', {}).get(f'test_pearson_{dim}', 0))\n    for dim in ['note_accuracy', 'rhythmic_precision', 'tone_quality']\n) / 3\n\nprint(\"-\"*70)\nprint(f\"Average fusion gain: {avg_gain:+.3f} ({avg_gain*100:+.1f}%)\")\nprint(\"=\"*70)\n\nif avg_gain > 0.05:\n    print(\"\\n✓ SUCCESS: Fusion shows clear multi-modal advantage!\")\nelse:\n    print(\"\\n⚠️  WARNING: Fusion gain is marginal. Check fusion implementation.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Session Disconnected\n",
    "- **Don't worry!** Checkpoints are saved directly to Google Drive (persistent)\n",
    "- Re-run setup cells (Google Drive mount, git clone, install dependencies)\n",
    "- Re-run the training cell - will **automatically resume** from latest checkpoint\n",
    "- Check Google Drive → MyDrive → crescendai_checkpoints for saved checkpoints\n",
    "\n",
    "### Out of Memory (OOM)\n",
    "- Reduce batch size: `config['data']['batch_size'] = 4`\n",
    "- Increase gradient accumulation: `config['training']['accumulate_grad_batches'] = 8`\n",
    "- This keeps effective batch size = 4 × 8 = 32\n",
    "\n",
    "### I/O Errors During Training\n",
    "- **Already fixed!** `num_workers=0` prevents Google Drive concurrent access issues\n",
    "- If you still see errors, check Google Drive sync status\n",
    "- Try remounting Drive: restart runtime and re-run cells\n",
    "\n",
    "### Slow Training\n",
    "- Verify you have T4 or better GPU (not K80)\n",
    "- Check `num_workers=0` in config (required for Google Drive)\n",
    "- Sequential loading is normal and expected with Google Drive\n",
    "\n",
    "### MIDI Divide-by-Zero Warnings\n",
    "- **Harmless** - occurs when MIDI files have 0 duration\n",
    "- Already suppressed in the notebook\n",
    "- Does not affect training\n",
    "\n",
    "### VSCode Extension Limitations\n",
    "- The VSCode Colab extension only keeps your **notebook file** local\n",
    "- Source code is cloned from GitHub on the Colab runtime\n",
    "- Data must be in Google Drive (local files are NOT synced to runtime)\n",
    "- You get VSCode editing experience with Colab's compute power\n",
    "\n",
    "### Target Performance (MVP Goals)\n",
    "**Technical dimensions** (current):\n",
    "- Pearson r: 0.50-0.65\n",
    "- MAE: 10-15 points (0-100 scale)\n",
    "\n",
    "**Interpretive dimensions** (after expert labels):\n",
    "- Pearson r: 0.35-0.50\n",
    "- MAE: 12-18 points\n",
    "\n",
    "If you achieve these targets, the architecture is validated!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}