{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Training on Expanded Dataset (Real + Pseudo Labels)\n",
    "\n",
    "**Goal**: Improve model performance by training on expanded dataset with pseudo-labeled MAESTRO.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Trained teacher model** from `train_percepiano_replica.ipynb` (R^2 >= 0.25)\n",
    "2. **Pseudo-labeled MAESTRO** from `scripts/pseudo_label_maestro.py`\n",
    "\n",
    "## Dataset Composition\n",
    "\n",
    "| Source | Samples | Labels |\n",
    "|--------|---------|--------|\n",
    "| PercePiano (real) | 955 | Expert annotations |\n",
    "| MAESTRO (pseudo) | ~5000+ | Teacher predictions |\n",
    "| **Total** | ~6000+ | Mixed |\n",
    "\n",
    "## Training Strategy\n",
    "\n",
    "1. **Sample weighting**: Real labels weighted 1.0, pseudo labels weighted by confidence\n",
    "2. **Confidence filtering**: Only include pseudo samples with confidence >= 0.5\n",
    "3. **Validation on real only**: Evaluate on PercePiano val/test (real labels)\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "With expanded data, we expect:\n",
    "- Reduced overfitting (more data)\n",
    "- Potentially higher R^2 on test set\n",
    "- Better generalization to unseen performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rclone\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv and clone repository\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "\n",
    "if not os.path.exists('/tmp/crescendai'):\n",
    "    !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "\n",
    "%cd /tmp/crescendai/model\n",
    "!git pull\n",
    "!git log -1 --oneline\n",
    "\n",
    "!uv pip install --system -e .\n",
    "!pip install tensorboard rich\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_ROOT = '/tmp/checkpoints/expanded_training'\n",
    "GDRIVE_CHECKPOINT_PATH = 'gdrive:crescendai_checkpoints/expanded_training'\n",
    "GDRIVE_DATA_PATH = 'gdrive:percepiano_data'\n",
    "GDRIVE_PSEUDO_PATH = 'gdrive:crescendai_checkpoints/pseudo_labels'\n",
    "DATA_ROOT = Path('/tmp/percepiano_data')\n",
    "PSEUDO_ROOT = Path('/tmp/pseudo_labels')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPANDED DATASET TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(\"Training on: PercePiano (real) + MAESTRO (pseudo)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "PSEUDO_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check rclone\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "RCLONE_AVAILABLE = 'gdrive:' in result.stdout\n",
    "print(f\"rclone available: {RCLONE_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Download PercePiano data\n",
    "if not (DATA_ROOT / 'percepiano_train.json').exists():\n",
    "    print(\"Downloading PercePiano data...\")\n",
    "    subprocess.run(['rclone', 'copy', GDRIVE_DATA_PATH, str(DATA_ROOT), '--progress'])\n",
    "else:\n",
    "    print(\"PercePiano data already exists\")\n",
    "\n",
    "# Download pseudo labels\n",
    "pseudo_file = PSEUDO_ROOT / 'maestro_pseudo_train.json'\n",
    "if not pseudo_file.exists():\n",
    "    print(\"\\nDownloading pseudo labels...\")\n",
    "    subprocess.run(['rclone', 'copy', GDRIVE_PSEUDO_PATH, str(PSEUDO_ROOT), '--progress'])\n",
    "else:\n",
    "    print(\"Pseudo labels already exist\")\n",
    "\n",
    "# Verify data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Real labels\n",
    "for split in ['train', 'val', 'test']:\n",
    "    path = DATA_ROOT / f'percepiano_{split}.json'\n",
    "    if path.exists():\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"PercePiano {split}: {len(data)} samples\")\n",
    "\n",
    "# Pseudo labels\n",
    "if pseudo_file.exists():\n",
    "    with open(pseudo_file) as f:\n",
    "        pseudo_data = json.load(f)\n",
    "    print(f\"\\nMAESTRO pseudo: {len(pseudo_data)} samples\")\n",
    "    \n",
    "    # Confidence distribution\n",
    "    confidences = [s.get('confidence', 0.5) for s in pseudo_data]\n",
    "    print(f\"  Mean confidence: {sum(confidences)/len(confidences):.3f}\")\n",
    "    print(f\"  Min confidence: {min(confidences):.3f}\")\n",
    "    print(f\"  Max confidence: {max(confidences):.3f}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Pseudo labels not found!\")\n",
    "    print(\"Run pseudo_label_maestro.py first, or continue with real labels only\")\n",
    "    pseudo_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update paths for Thunder Compute\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "MIDI_DIR = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'all_2rounds'\n",
    "SCORE_DIR = DATA_ROOT / 'PercePiano' / 'virtuoso' / 'data' / 'score_xml'\n",
    "\n",
    "PERCEPIANO_DIMENSIONS = [\n",
    "    \"timing\", \"articulation_length\", \"articulation_touch\",\n",
    "    \"pedal_amount\", \"pedal_clarity\", \"timbre_variety\",\n",
    "    \"timbre_depth\", \"timbre_brightness\", \"timbre_loudness\",\n",
    "    \"dynamic_range\", \"tempo\", \"space\", \"balance\", \"drama\",\n",
    "    \"mood_valence\", \"mood_energy\", \"mood_imagination\",\n",
    "    \"sophistication\", \"interpretation\",\n",
    "]\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    path = DATA_ROOT / f'percepiano_{split}.json'\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    for sample in data:\n",
    "        filename = Path(sample['midi_path']).name\n",
    "        sample['midi_path'] = str(MIDI_DIR / filename)\n",
    "        \n",
    "        if 'percepiano_scores' in sample:\n",
    "            pp_scores = sample['percepiano_scores'][:19]\n",
    "            sample['scores'] = {\n",
    "                dim: pp_scores[i] for i, dim in enumerate(PERCEPIANO_DIMENSIONS)\n",
    "            }\n",
    "    \n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"Updated {split}\")\n",
    "\n",
    "print(\"\\nPaths updated for Thunder Compute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 3: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'data_dir': str(DATA_ROOT),\n",
    "    'score_dir': str(SCORE_DIR),\n",
    "    'pseudo_data_path': str(pseudo_file) if pseudo_file else None,\n",
    "    \n",
    "    # Pseudo-label settings\n",
    "    'pseudo_weight': 0.5,           # Weight for pseudo labels (real = 1.0)\n",
    "    'min_pseudo_confidence': 0.5,   # Minimum confidence to include\n",
    "    'use_weighted_sampling': True,  # Sample by weight\n",
    "    \n",
    "    # Model (PercePiano replica architecture)\n",
    "    'hidden_size': 256,\n",
    "    'note_layers': 2,\n",
    "    'voice_layers': 2,\n",
    "    'beat_layers': 2,\n",
    "    'measure_layers': 1,\n",
    "    'num_attention_heads': 8,\n",
    "    'final_hidden': 128,\n",
    "    \n",
    "    # Training\n",
    "    'learning_rate': 2.5e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 8,\n",
    "    'max_epochs': 100,\n",
    "    'early_stopping_patience': 20,\n",
    "    'gradient_clip_val': 1.0,\n",
    "    'precision': '16-mixed',\n",
    "    \n",
    "    # Sequence limits\n",
    "    'max_score_notes': 1024,\n",
    "    'max_midi_seq_length': 512,\n",
    "    \n",
    "    # Checkpoints\n",
    "    'checkpoint_dir': CHECKPOINT_ROOT,\n",
    "    'gdrive_checkpoint': GDRIVE_CHECKPOINT_PATH,\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPANDED TRAINING CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 4: Create Mixed DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.data.mixed_dataset import create_mixed_dataloaders\n",
    "\n",
    "train_loader, val_loader, test_loader = create_mixed_dataloaders(\n",
    "    real_data_dir=Path(CONFIG['data_dir']),\n",
    "    pseudo_data_path=Path(CONFIG['pseudo_data_path']) if CONFIG['pseudo_data_path'] else None,\n",
    "    score_dir=Path(CONFIG['score_dir']),\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    max_midi_seq_length=CONFIG['max_midi_seq_length'],\n",
    "    max_score_notes=CONFIG['max_score_notes'],\n",
    "    pseudo_weight=CONFIG['pseudo_weight'],\n",
    "    min_pseudo_confidence=CONFIG['min_pseudo_confidence'],\n",
    "    num_workers=4,\n",
    "    use_weighted_sampling=CONFIG['use_weighted_sampling'],\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader sizes:\")\n",
    "print(f\"  Train: {len(train_loader)} batches\")\n",
    "print(f\"  Val: {len(val_loader)} batches\")\n",
    "print(f\"  Test: {len(test_loader)} batches\")\n",
    "\n",
    "# Test batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch contents:\")\n",
    "print(f\"  score_note_features: {batch['score_note_features'].shape}\")\n",
    "print(f\"  scores: {batch['scores'].shape}\")\n",
    "print(f\"  is_pseudo_label: {batch['is_pseudo_label'].sum().item()}/{len(batch['is_pseudo_label'])} pseudo\")\n",
    "print(f\"  sample_weights: {batch['sample_weight'].mean().item():.3f} (mean)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 5: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.percepiano_replica import PercePianoReplicaModule\n",
    "\n",
    "model = PercePianoReplicaModule(\n",
    "    score_note_features=20,\n",
    "    score_global_features=12,\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    note_layers=CONFIG['note_layers'],\n",
    "    voice_layers=CONFIG['voice_layers'],\n",
    "    beat_layers=CONFIG['beat_layers'],\n",
    "    measure_layers=CONFIG['measure_layers'],\n",
    "    num_attention_heads=CONFIG['num_attention_heads'],\n",
    "    final_hidden=CONFIG['final_hidden'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    dropout=CONFIG['dropout'],\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Architecture: PercePiano Replica (Bi-LSTM + HAN)\")\n",
    "print(f\"Parameters: {model.count_parameters():,}\")\n",
    "print(f\"Dimensions: {len(model.dimensions)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 6: Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CONFIG['checkpoint_dir'],\n",
    "    filename='expanded-{epoch:02d}-{val_mean_r2:.4f}',\n",
    "    monitor='val/mean_r2',\n",
    "    mode='max',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val/mean_r2',\n",
    "    patience=CONFIG['early_stopping_patience'],\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir='/tmp/logs',\n",
    "    name='expanded_training',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['max_epochs'],\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    precision=CONFIG['precision'],\n",
    "    gradient_clip_val=CONFIG['gradient_clip_val'],\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_monitor],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.5,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 7: Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ON EXPANDED DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Real samples: {len(train_loader.dataset.real_samples)}\")\n",
    "print(f\"Pseudo samples: {len(train_loader.dataset.pseudo_samples)}\")\n",
    "print(f\"Total: {len(train_loader.dataset)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints\n",
    "if RCLONE_AVAILABLE:\n",
    "    print(\"Syncing checkpoints to Google Drive...\")\n",
    "    subprocess.run(['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 8: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with best checkpoint\n",
    "best_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best checkpoint: {best_path}\")\n",
    "\n",
    "if best_path:\n",
    "    test_results = trainer.test(model, test_loader, ckpt_path=best_path)\n",
    "    print(\"\\nTest Results:\")\n",
    "    for k, v in test_results[0].items():\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from src.models.percepiano_replica import PercePianoReplicaModule\n",
    "\n",
    "# Load best model\n",
    "best_model = PercePianoReplicaModule.load_from_checkpoint(best_path)\n",
    "best_model.eval()\n",
    "best_model.cuda()\n",
    "\n",
    "# Collect predictions\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        note_locations = {\n",
    "            'beat': batch['note_locations_beat'],\n",
    "            'measure': batch['note_locations_measure'],\n",
    "            'voice': batch['note_locations_voice'],\n",
    "        }\n",
    "        outputs = best_model(\n",
    "            batch['score_note_features'],\n",
    "            batch['score_global_features'],\n",
    "            batch['score_tempo_curve'],\n",
    "            note_locations,\n",
    "        )\n",
    "        all_preds.append(outputs['predictions'].cpu())\n",
    "        all_targets.append(batch['scores'].cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "dimensions = best_model.dimensions\n",
    "\n",
    "print(f\"Collected {len(all_preds)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import (\n",
    "    compute_all_metrics,\n",
    "    compare_to_sota,\n",
    "    format_comparison_table,\n",
    ")\n",
    "\n",
    "metrics = compute_all_metrics(\n",
    "    predictions=all_preds,\n",
    "    targets=all_targets,\n",
    "    dimension_names=list(dimensions),\n",
    ")\n",
    "\n",
    "our_r2 = metrics['r2'].value\n",
    "per_dim_r2 = metrics['r2'].per_dimension\n",
    "\n",
    "comparison = compare_to_sota(\n",
    "    model_r2=our_r2,\n",
    "    model_name=\"Expanded Training (Real + Pseudo)\",\n",
    "    split_type=\"piece\",\n",
    "    per_dimension_r2=per_dim_r2,\n",
    ")\n",
    "\n",
    "print(format_comparison_table(comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*70)\n",
    "print(\"EXPANDED TRAINING - RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. DATASET\")\n",
    "print(f\"   Real samples (PercePiano): {len(train_loader.dataset.real_samples)}\")\n",
    "print(f\"   Pseudo samples (MAESTRO): {len(train_loader.dataset.pseudo_samples)}\")\n",
    "print(f\"   Expansion factor: {len(train_loader.dataset) / len(train_loader.dataset.real_samples):.1f}x\")\n",
    "\n",
    "print(f\"\\n2. PERFORMANCE\")\n",
    "print(f\"   Mean R^2: {our_r2:.4f}\")\n",
    "print(f\"   Target (0.35-0.40): {'ACHIEVED' if our_r2 >= 0.35 else 'CLOSE' if our_r2 >= 0.30 else 'NOT YET'}\")\n",
    "\n",
    "print(f\"\\n3. COMPARISON\")\n",
    "print(f\"   Teacher model R^2: {train_loader.dataset.pseudo_samples[0].get('teacher_r2', 'N/A') if train_loader.dataset.pseudo_samples else 'N/A'}\")\n",
    "print(f\"   Expanded model R^2: {our_r2:.4f}\")\n",
    "print(f\"   Improvement: {'+' if our_r2 > 0.35 else ''}{(our_r2 - 0.35)*100:.1f}% vs target\")\n",
    "\n",
    "print(f\"\\n4. TOP 5 DIMENSIONS\")\n",
    "sorted_dims = sorted(per_dim_r2.items(), key=lambda x: x[1], reverse=True)\n",
    "for dim, r2 in sorted_dims[:5]:\n",
    "    print(f\"   {dim}: {r2:.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Step 9: Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Save final model\n",
    "final_path = Path(CONFIG['checkpoint_dir']) / 'expanded_final.pt'\n",
    "\n",
    "torch.save({\n",
    "    'state_dict': best_model.state_dict(),\n",
    "    'hparams': dict(best_model.hparams),\n",
    "    'dimensions': list(dimensions),\n",
    "    'metrics': {\n",
    "        'r2': our_r2,\n",
    "        'per_dimension_r2': per_dim_r2,\n",
    "    },\n",
    "    'training_info': {\n",
    "        'real_samples': len(train_loader.dataset.real_samples),\n",
    "        'pseudo_samples': len(train_loader.dataset.pseudo_samples),\n",
    "        'pseudo_weight': CONFIG['pseudo_weight'],\n",
    "        'min_pseudo_confidence': CONFIG['min_pseudo_confidence'],\n",
    "    },\n",
    "}, final_path)\n",
    "\n",
    "print(f\"Saved final model to {final_path}\")\n",
    "\n",
    "# Sync to GDrive\n",
    "if RCLONE_AVAILABLE:\n",
    "    subprocess.run(['rclone', 'copy', CONFIG['checkpoint_dir'], CONFIG['gdrive_checkpoint'], '--progress'])\n",
    "    print(f\"Synced to {CONFIG['gdrive_checkpoint']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If R^2 improved:\n",
    "1. **Iterate**: Use this model as new teacher, pseudo-label more data\n",
    "2. **Noisy Student**: Add augmentation for student training\n",
    "3. **Scale up**: Try larger model now that we have more data\n",
    "\n",
    "If R^2 didn't improve:\n",
    "1. Reduce pseudo_weight (try 0.3)\n",
    "2. Increase min_pseudo_confidence (try 0.7)\n",
    "3. Check if teacher model was good enough (R^2 >= 0.25)\n",
    "\n",
    "---\n",
    "\n",
    "**Attribution**: Based on PercePiano (Park et al., ISMIR/Nature 2024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
