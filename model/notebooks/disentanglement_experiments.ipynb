{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disentanglement Experiments: Separating Piece from Performer\n",
    "\n",
    "This notebook implements three approaches to disentangle piece characteristics from performer expression in piano performance evaluation.\n",
    "\n",
    "**Goal**: Improve pairwise ranking accuracy for same-piece comparisons\n",
    "- Baseline: ~50% (random)\n",
    "- Current model intra-piece std: 0.020\n",
    "\n",
    "**Success Metric**: Pairwise ranking accuracy significantly above 50%, higher intra-piece prediction variance\n",
    "\n",
    "## Approaches\n",
    "\n",
    "1. **Approach A**: Contrastive Pairwise Ranking (InfoNCE + margin ranking)\n",
    "2. **Approach B**: Siamese Dimension-Specific Ranking (per-dimension heads)\n",
    "3. **Approach C**: Disentangled Dual-Encoder (adversarial piece classification)\n",
    "\n",
    "## References\n",
    "\n",
    "- Temperature 0.07: [Contrastive Learning Blog](https://lilianweng.github.io/posts/2021-05-31-contrastive/)\n",
    "- Projection dim 256: [SimCLR](https://arxiv.org/abs/2002.05709)\n",
    "- GRL: [DANN Paper](https://jmlr.org/papers/volume17/15-239/15-239.pdf)\n",
    "- Pairwise ranking: [DirectRanker](https://arxiv.org/abs/1909.02768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thunder Compute Setup - Install rclone if needed\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Install rclone if not present\n",
    "if not os.path.exists('/usr/bin/rclone'):\n",
    "    print(\"Installing rclone...\")\n",
    "    subprocess.run(['curl', 'https://rclone.org/install.sh', '-o', '/tmp/install_rclone.sh'], check=True)\n",
    "    subprocess.run(['sudo', 'bash', '/tmp/install_rclone.sh'], check=True)\n",
    "\n",
    "# Check rclone version\n",
    "result = subprocess.run(['rclone', 'version'], capture_output=True, text=True)\n",
    "print(f\"rclone: {result.stdout.split(chr(10))[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Audio experiments (Paper 1)\n",
    "from audio_experiments import PERCEPIANO_DIMENSIONS, BASE_CONFIG, SEED\n",
    "\n",
    "# Disentanglement (Paper 2)\n",
    "from disentanglement import (\n",
    "    # Models\n",
    "    ContrastivePairwiseRankingModel,\n",
    "    SiameseDimensionRankingModel,\n",
    "    DisentangledDualEncoderModel,\n",
    "    ContrastiveDisentangledModel,\n",
    "    SiameseDisentangledModel,\n",
    "    FullCombinedModel,\n",
    "    TripletRankingModel,\n",
    "    # Data\n",
    "    build_multi_performer_pieces,\n",
    "    create_piece_stratified_folds,\n",
    "    PairwiseRankingDataset,\n",
    "    HardPairRankingDataset,\n",
    "    DisentanglementDataset,\n",
    "    TripletRankingDataset,\n",
    "    pairwise_collate_fn,\n",
    "    disentanglement_collate_fn,\n",
    "    triplet_collate_fn,\n",
    "    compute_pairwise_statistics,\n",
    "    # Training\n",
    "    run_pairwise_experiment,\n",
    "    run_disentanglement_experiment,\n",
    "    run_triplet_experiment,\n",
    "    run_dimension_group_experiment,\n",
    "    compute_pairwise_metrics,\n",
    "    compute_intra_piece_std,\n",
    "    evaluate_disentanglement,\n",
    "    compute_regression_pairwise_accuracy,\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with literature-backed hyperparameters\n",
    "# Approach A: Contrastive Pairwise Ranking\n",
    "APPROACH_A_CONFIG = {\n",
    "    # Architecture\n",
    "    \"input_dim\": 1024,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"projection_dim\": 256,  # SimCLR recommendation\n",
    "    \"num_labels\": 19,\n",
    "    \"dropout\": 0.2,\n",
    "    \"pooling\": \"attention\",\n",
    "\n",
    "    # Contrastive learning\n",
    "    \"temperature\": 0.07,  # InfoNCE best practice\n",
    "    \"lambda_contrastive\": 0.3,\n",
    "\n",
    "    # Ranking\n",
    "    \"margin\": 0.2,\n",
    "    \"ambiguous_threshold\": 0.05,\n",
    "    \"label_smoothing\": 0.0,\n",
    "\n",
    "    # Training\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_epochs\": 100,\n",
    "    \"patience\": 15,\n",
    "    \"max_frames\": 1000,\n",
    "    \"n_folds\": 4,\n",
    "    \"num_workers\": 2,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Approach B: Siamese Dimension-Specific Ranking\n",
    "APPROACH_B_CONFIG = {\n",
    "    **APPROACH_A_CONFIG,\n",
    "    \"comparison_type\": \"concat_diff\",  # [z_a; z_b; z_a-z_b; z_a*z_b]\n",
    "    \"margin\": 0.3,\n",
    "    \"label_smoothing\": 0.05,  # Slight smoothing helps\n",
    "}\n",
    "\n",
    "# Approach C: Disentangled Dual-Encoder\n",
    "APPROACH_C_CONFIG = {\n",
    "    **APPROACH_A_CONFIG,\n",
    "    \"lambda_adversarial\": 0.5,\n",
    "    \"grl_schedule\": \"linear\",  # DANN paper schedule\n",
    "    \"num_pieces\": 206,  # Will be updated dynamically\n",
    "}\n",
    "\n",
    "print(\"Configurations defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration for Thunder Compute\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Thunder Compute paths (fast local SSD)\n",
    "PATHS = {\n",
    "    \"muq_cache\": Path(\"/workspace/data/cache/muq_embeddings\"),\n",
    "    \"mert_cache\": Path(\"/workspace/data/cache/mert_layer12\"),\n",
    "    \"labels_file\": Path(\"/workspace/data/cache/percepiano_labels.json\"),\n",
    "    \"fold_assignments\": Path(\"/workspace/data/cache/fold_assignments.json\"),\n",
    "    \"checkpoints\": Path(\"/workspace/checkpoints/disentanglement\"),\n",
    "    \"results\": Path(\"/workspace/results/disentanglement\"),\n",
    "    \"logs\": Path(\"/workspace/logs/disentanglement\"),\n",
    "}\n",
    "\n",
    "# Remote paths for rclone (relative to RCLONE_BASE_PATH)\n",
    "REMOTE_PATHS = {\n",
    "    \"muq_cache\": \"cache/muq_embeddings\",\n",
    "    \"mert_cache\": \"cache/mert_layer12\",\n",
    "    \"labels_file\": \"cache/percepiano_labels.json\",\n",
    "    \"fold_assignments\": \"cache/fold_assignments.json\",\n",
    "    \"checkpoints\": \"checkpoints/disentanglement\",\n",
    "    \"results\": \"results/disentanglement\",\n",
    "}\n",
    "\n",
    "# rclone configuration\n",
    "RCLONE_REMOTE = \"gdrive\"  # Name of your rclone remote\n",
    "RCLONE_BASE_PATH = \"crescendai/model/data\"  # Base path on remote\n",
    "\n",
    "\n",
    "def rclone_sync(remote_path: str, local_path: str, direction: str = \"download\") -> None:\n",
    "    \"\"\"Sync a directory between remote and local using rclone.\"\"\"\n",
    "    local_dir = Path(local_path)\n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    full_remote = f\"{RCLONE_REMOTE}:{RCLONE_BASE_PATH}/{remote_path}\"\n",
    "\n",
    "    if direction == \"download\":\n",
    "        cmd = [\"rclone\", \"sync\", full_remote, str(local_dir), \"--progress\"]\n",
    "    else:\n",
    "        cmd = [\"rclone\", \"sync\", str(local_dir), full_remote, \"--progress\"]\n",
    "\n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        raise RuntimeError(f\"rclone sync failed: {result.stderr}\")\n",
    "    print(f\"Sync complete: {local_dir}\")\n",
    "\n",
    "\n",
    "def rclone_copy_file(remote_path: str, local_path: str) -> None:\n",
    "    \"\"\"Copy a single file from remote to local using rclone.\"\"\"\n",
    "    local_file = Path(local_path)\n",
    "    local_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    full_remote = f\"{RCLONE_REMOTE}:{RCLONE_BASE_PATH}/{remote_path}\"\n",
    "\n",
    "    cmd = [\"rclone\", \"copyto\", full_remote, str(local_file), \"--progress\"]\n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        raise RuntimeError(f\"rclone copy failed: {result.stderr}\")\n",
    "    print(f\"Copied: {local_file}\")\n",
    "\n",
    "\n",
    "def upload_experiment(exp_id: str) -> None:\n",
    "    \"\"\"Upload a specific experiment's checkpoint and results to remote.\"\"\"\n",
    "    # Upload checkpoint folder for this experiment\n",
    "    exp_ckpt_dir = PATHS[\"checkpoints\"] / exp_id\n",
    "    if exp_ckpt_dir.exists():\n",
    "        remote_ckpt = f\"{REMOTE_PATHS['checkpoints']}/{exp_id}\"\n",
    "        rclone_sync(remote_ckpt, str(exp_ckpt_dir), direction=\"upload\")\n",
    "    # Upload results JSON for this experiment\n",
    "    results_file = PATHS[\"results\"] / f\"{exp_id}.json\"\n",
    "    if results_file.exists():\n",
    "        remote_results = f\"{REMOTE_PATHS['results']}/{exp_id}.json\"\n",
    "        full_remote = f\"{RCLONE_REMOTE}:{RCLONE_BASE_PATH}/{remote_results}\"\n",
    "        cmd = [\"rclone\", \"copyto\", str(results_file), full_remote, \"--progress\"]\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"Uploaded results: {exp_id}.json\")\n",
    "\n",
    "\n",
    "def upload_fold_checkpoint(exp_id: str, fold: int) -> None:\n",
    "    \"\"\"Upload a single fold checkpoint to remote storage.\"\"\"\n",
    "    ckpt_file = PATHS[\"checkpoints\"] / exp_id / f\"fold{fold}_best.ckpt\"\n",
    "    if ckpt_file.exists():\n",
    "        remote_path = f\"{REMOTE_PATHS['checkpoints']}/{exp_id}/fold{fold}_best.ckpt\"\n",
    "        full_remote = f\"{RCLONE_REMOTE}:{RCLONE_BASE_PATH}/{remote_path}\"\n",
    "        cmd = [\"rclone\", \"copyto\", str(ckpt_file), full_remote, \"--progress\"]\n",
    "        print(f\"Uploading fold {fold} checkpoint...\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Warning: Failed to upload fold checkpoint: {result.stderr}\")\n",
    "        else:\n",
    "            print(f\"Uploaded: {ckpt_file.name}\")\n",
    "\n",
    "\n",
    "# Create directories\n",
    "for name, path in PATHS.items():\n",
    "    if 'cache' not in name and 'labels' not in name and 'fold' not in name:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Thunder Compute paths configured:\")\n",
    "for name, path in PATHS.items():\n",
    "    exists = path.exists()\n",
    "    print(f\"  {name}: {path} ({'exists' if exists else 'MISSING'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and existing experiments from remote storage\n",
    "print(\"Syncing data from remote storage...\")\n",
    "\n",
    "# Download MuQ embeddings (required for training)\n",
    "if not PATHS[\"muq_cache\"].exists() or len(list(PATHS[\"muq_cache\"].glob(\"*.pt\"))) == 0:\n",
    "    print(\"\\nDownloading MuQ embeddings...\")\n",
    "    rclone_sync(REMOTE_PATHS[\"muq_cache\"], str(PATHS[\"muq_cache\"]))\n",
    "else:\n",
    "    print(f\"MuQ cache exists with {len(list(PATHS['muq_cache'].glob('*.pt')))} files\")\n",
    "\n",
    "# Download labels and fold assignments\n",
    "if not PATHS[\"labels_file\"].exists():\n",
    "    print(\"\\nDownloading labels...\")\n",
    "    rclone_copy_file(REMOTE_PATHS[\"labels_file\"], str(PATHS[\"labels_file\"]))\n",
    "\n",
    "if not PATHS[\"fold_assignments\"].exists():\n",
    "    print(\"\\nDownloading fold assignments...\")\n",
    "    rclone_copy_file(REMOTE_PATHS[\"fold_assignments\"], str(PATHS[\"fold_assignments\"]))\n",
    "\n",
    "# Download existing checkpoints to resume training\n",
    "print(\"\\nDownloading existing checkpoints...\")\n",
    "rclone_sync(REMOTE_PATHS[\"checkpoints\"], str(PATHS[\"checkpoints\"]))\n",
    "n_ckpts = len(list(PATHS[\"checkpoints\"].glob(\"**/*.ckpt\")))\n",
    "print(f\"Found {n_ckpts} existing checkpoints\")\n",
    "\n",
    "# Download existing results\n",
    "print(\"\\nDownloading existing results...\")\n",
    "rclone_sync(REMOTE_PATHS[\"results\"], str(PATHS[\"results\"]))\n",
    "n_results = len(list(PATHS[\"results\"].glob(\"*.json\")))\n",
    "print(f\"Found {n_results} existing result files\")\n",
    "\n",
    "# List completed experiments\n",
    "completed = []\n",
    "for result_file in PATHS[\"results\"].glob(\"*.json\"):\n",
    "    exp_id = result_file.stem\n",
    "    exp_ckpts = list((PATHS[\"checkpoints\"] / exp_id).glob(\"fold*_best.ckpt\")) if (PATHS[\"checkpoints\"] / exp_id).exists() else []\n",
    "    if len(exp_ckpts) == 4:\n",
    "        completed.append(exp_id)\n",
    "\n",
    "if completed:\n",
    "    print(f\"\\nCompleted experiments ({len(completed)}):\")\n",
    "    for exp_id in sorted(completed):\n",
    "        print(f\"  - {exp_id}\")\n",
    "else:\n",
    "    print(\"\\nNo completed experiments found. Starting fresh.\")\n",
    "\n",
    "print(\"\\nData sync complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PercePiano labels\n",
    "with open(PATHS[\"labels_file\"]) as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(labels)} recordings\")\n",
    "\n",
    "# Show sample\n",
    "sample_key = list(labels.keys())[0]\n",
    "print(f\"\\nSample: {sample_key}\")\n",
    "print(f\"Labels: {labels[sample_key][:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original fold assignments\n",
    "with open(PATHS[\"fold_assignments\"]) as f:\n",
    "    original_folds = json.load(f)\n",
    "\n",
    "print(\"Original fold sizes:\")\n",
    "for k, v in original_folds.items():\n",
    "    print(f\"  {k}: {len(v)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build multi-performer piece mapping\n",
    "MULTI_PERFORMER_PIECES = build_multi_performer_pieces(\n",
    "    labels, original_folds, min_performers=2\n",
    ")\n",
    "\n",
    "# Statistics\n",
    "stats = compute_pairwise_statistics(MULTI_PERFORMER_PIECES, labels)\n",
    "\n",
    "print(f\"Multi-performer pieces:\")\n",
    "print(f\"  Pieces: {stats['n_pieces']}\")\n",
    "print(f\"  Recordings: {stats['n_recordings']}\")\n",
    "print(f\"  Possible pairs: {stats['n_possible_pairs']}\")\n",
    "print(f\"\\nScore differences:\")\n",
    "print(f\"  Mean: {stats['mean_diff']:.3f}\")\n",
    "print(f\"  Std: {stats['std_diff']:.3f}\")\n",
    "print(f\"  Range: [{stats['min_diff']:.3f}, {stats['max_diff']:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create piece-stratified folds (no piece leakage)\n",
    "PIECE_STRATIFIED_FOLDS = create_piece_stratified_folds(\n",
    "    MULTI_PERFORMER_PIECES, n_folds=4, seed=42\n",
    ")\n",
    "\n",
    "print(\"Piece-stratified fold sizes:\")\n",
    "for k, v in PIECE_STRATIFIED_FOLDS.items():\n",
    "    print(f\"  {k}: {len(v)} samples\")\n",
    "\n",
    "# Verify no piece leakage\n",
    "def get_pieces_in_fold(fold_keys):\n",
    "    pieces = set()\n",
    "    for key in fold_keys:\n",
    "        for pid, keys in MULTI_PERFORMER_PIECES.items():\n",
    "            if key in keys:\n",
    "                pieces.add(pid)\n",
    "    return pieces\n",
    "\n",
    "fold_pieces = [get_pieces_in_fold(PIECE_STRATIFIED_FOLDS[f\"fold_{i}\"]) for i in range(4)]\n",
    "for i in range(4):\n",
    "    for j in range(i+1, 4):\n",
    "        overlap = fold_pieces[i] & fold_pieces[j]\n",
    "        if overlap:\n",
    "            print(f\"WARNING: Piece leakage between fold {i} and {j}: {len(overlap)} pieces\")\n",
    "        else:\n",
    "            print(f\"No leakage between fold {i} and {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset creation\n",
    "from disentanglement.data import get_fold_piece_mapping\n",
    "\n",
    "# Get fold 0 for validation\n",
    "val_piece_map, val_keys = get_fold_piece_mapping(\n",
    "    MULTI_PERFORMER_PIECES, PIECE_STRATIFIED_FOLDS, fold_id=0, mode=\"val\"\n",
    ")\n",
    "train_piece_map, train_keys = get_fold_piece_mapping(\n",
    "    MULTI_PERFORMER_PIECES, PIECE_STRATIFIED_FOLDS, fold_id=0, mode=\"train\"\n",
    ")\n",
    "\n",
    "# Create a test dataset\n",
    "test_ds = PairwiseRankingDataset(\n",
    "    PATHS[\"muq_cache\"],\n",
    "    labels,\n",
    "    train_piece_map,\n",
    "    train_keys,\n",
    "    max_frames=100,  # Small for testing\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(test_ds)} pairs\")\n",
    "print(f\"Num pieces: {test_ds.get_num_pieces()}\")\n",
    "\n",
    "# Test a sample\n",
    "sample = test_ds[0]\n",
    "print(f\"\\nSample keys:\")\n",
    "for k, v in sample.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"  {k}: {v.shape}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Establishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E1b: Random baseline (expected 50%)\n",
    "print(\"E1b: Random Baseline\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generate random predictions for all pairs\n",
    "n_pairs = stats['n_possible_pairs']\n",
    "random_acc = 0.5  # Expected accuracy for random predictions\n",
    "\n",
    "# Monte Carlo estimate with variance\n",
    "n_simulations = 1000\n",
    "random_accs = []\n",
    "for _ in range(n_simulations):\n",
    "    # Simulate random rankings\n",
    "    random_preds = np.random.randint(0, 2, size=(n_pairs, 19))\n",
    "    # With uniform distribution, ~50% should match\n",
    "    random_accs.append(0.5 + np.random.normal(0, 0.5/np.sqrt(n_pairs*19)))\n",
    "\n",
    "print(f\"Random baseline accuracy: {np.mean(random_accs):.4f} +/- {np.std(random_accs):.4f}\")\n",
    "print(f\"95% CI: [{np.percentile(random_accs, 2.5):.4f}, {np.percentile(random_accs, 97.5):.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E1a: Current MuQ model's pairwise ranking accuracy (derived from regression)\n",
    "print(\"E1a: Current MuQ Model Pairwise Accuracy\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Import the baseline model and evaluation function\n",
    "from audio_experiments.models.muq_models import MuQBaseModel\n",
    "from disentanglement.training import compute_regression_pairwise_accuracy\n",
    "\n",
    "# Download MuQ checkpoint from Paper 1 if not present\n",
    "muq_remote = \"checkpoints/muq\"\n",
    "muq_local = PATHS[\"checkpoints\"].parent / \"muq\"\n",
    "muq_checkpoint = muq_local / \"fold0_best.ckpt\"\n",
    "\n",
    "if not muq_checkpoint.exists():\n",
    "    print(\"Downloading MuQ checkpoint from remote storage...\")\n",
    "    muq_local.mkdir(parents=True, exist_ok=True)\n",
    "    rclone_sync(muq_remote, str(muq_local))\n",
    "\n",
    "if muq_checkpoint.exists():\n",
    "    print(f\"Found MuQ checkpoint: {muq_checkpoint}\")\n",
    "\n",
    "    # Load the regression model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = MuQBaseModel.load_from_checkpoint(muq_checkpoint)\n",
    "\n",
    "    # Compute pairwise accuracy\n",
    "    print(\"\\nEvaluating pairwise ranking accuracy...\")\n",
    "    baseline_results = compute_regression_pairwise_accuracy(\n",
    "        model=model,\n",
    "        cache_dir=PATHS[\"muq_cache\"],\n",
    "        labels=labels,\n",
    "        piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "        device=device,\n",
    "        ambiguous_threshold=0.05,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nE1a Baseline Results:\")\n",
    "    print(f\"  Overall pairwise accuracy: {baseline_results['overall_accuracy']:.4f}\")\n",
    "    print(f\"  Number of pairs evaluated: {baseline_results['n_pairs']}\")\n",
    "    print(f\"  Total comparisons: {baseline_results['n_comparisons']}\")\n",
    "\n",
    "    print(f\"\\nPer-dimension breakdown:\")\n",
    "    for dim_idx, acc in sorted(baseline_results['per_dimension'].items()):\n",
    "        dim_name = PERCEPIANO_DIMENSIONS[dim_idx]\n",
    "        indicator = \"!\" if acc > 0.55 else \" \"\n",
    "        print(f\"  {indicator} {dim_idx:2d}. {dim_name:<25}: {acc:.4f}\")\n",
    "\n",
    "    # Summary statistics\n",
    "    dim_accs = list(baseline_results['per_dimension'].values())\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Mean dimension accuracy: {np.mean(dim_accs):.4f}\")\n",
    "    print(f\"  Std dimension accuracy: {np.std(dim_accs):.4f}\")\n",
    "    print(f\"  Best dimension: {PERCEPIANO_DIMENSIONS[max(baseline_results['per_dimension'], key=baseline_results['per_dimension'].get)]}\")\n",
    "    print(f\"  Worst dimension: {PERCEPIANO_DIMENSIONS[min(baseline_results['per_dimension'], key=baseline_results['per_dimension'].get)]}\")\n",
    "\n",
    "    # Save baseline results\n",
    "    e1a_results = {\n",
    "        \"experiment_id\": \"E1a_muq_baseline\",\n",
    "        \"description\": \"MuQ regression model pairwise ranking accuracy\",\n",
    "        \"summary\": baseline_results,\n",
    "    }\n",
    "    with open(PATHS[\"results\"] / \"E1a_muq_baseline.json\", \"w\") as f:\n",
    "        json.dump(e1a_results, f, indent=2)\n",
    "    print(f\"\\nResults saved to {PATHS['results'] / 'E1a_muq_baseline.json'}\")\n",
    "else:\n",
    "    print(\"No MuQ checkpoint found. Run Paper 1 training first.\")\n",
    "    print(\"Expected path:\", muq_checkpoint)\n",
    "    baseline_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Approach A: Contrastive Pairwise Ranking\n",
    "\n",
    "**Architecture**: Shared encoder + projection head + ranking heads\n",
    "\n",
    "**Loss**: `L_total = L_ranking + lambda * L_infonce`\n",
    "- InfoNCE with piece-based positives (same piece = positive, different piece = negative)\n",
    "- Margin-based pairwise ranking loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model factory for Approach A\n",
    "def make_approach_a_model(config):\n",
    "    return ContrastivePairwiseRankingModel(\n",
    "        input_dim=config.get(\"input_dim\", 1024),\n",
    "        hidden_dim=config.get(\"hidden_dim\", 512),\n",
    "        projection_dim=config.get(\"projection_dim\", 256),\n",
    "        num_labels=config.get(\"num_labels\", 19),\n",
    "        dropout=config.get(\"dropout\", 0.2),\n",
    "        learning_rate=config.get(\"learning_rate\", 1e-4),\n",
    "        weight_decay=config.get(\"weight_decay\", 1e-5),\n",
    "        temperature=config.get(\"temperature\", 0.07),\n",
    "        lambda_contrastive=config.get(\"lambda_contrastive\", 0.3),\n",
    "        margin=config.get(\"margin\", 0.2),\n",
    "        ambiguous_threshold=config.get(\"ambiguous_threshold\", 0.05),\n",
    "        label_smoothing=config.get(\"label_smoothing\", 0.0),\n",
    "        pooling=config.get(\"pooling\", \"attention\"),\n",
    "        max_epochs=config.get(\"max_epochs\", 100),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2a: Approach A with default config\n",
    "results_e2a = run_pairwise_experiment(\n",
    "    exp_id=\"E2a_contrastive_default\",\n",
    "    description=\"Approach A: Contrastive pairwise ranking with default config\",\n",
    "    model_factory=make_approach_a_model,\n",
    "    cache_dir=PATHS[\"muq_cache\"],\n",
    "    labels=labels,\n",
    "    piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "    fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "    config=APPROACH_A_CONFIG,\n",
    "    checkpoint_root=PATHS[\"checkpoints\"],\n",
    "    results_dir=PATHS[\"results\"],\n",
    "    log_dir=PATHS[\"logs\"],\n",
    "    on_fold_complete=upload_fold_checkpoint,\n",
    ")\n",
    "upload_experiment(\"E2a_contrastive_default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2b: Temperature ablation\n",
    "temperature_values = [0.05, 0.07, 0.1, 0.2]\n",
    "temp_results = {}\n",
    "\n",
    "for temp in temperature_values:\n",
    "    exp_id = f\"E2b_temp_{temp}\"\n",
    "    config = {**APPROACH_A_CONFIG, \"temperature\": temp}\n",
    "    result = run_pairwise_experiment(\n",
    "        exp_id=exp_id,\n",
    "        description=f\"Approach A: temperature={temp}\",\n",
    "        model_factory=make_approach_a_model,\n",
    "        cache_dir=PATHS[\"muq_cache\"],\n",
    "        labels=labels,\n",
    "        piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "        fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "        config=config,\n",
    "        checkpoint_root=PATHS[\"checkpoints\"],\n",
    "        results_dir=PATHS[\"results\"],\n",
    "        log_dir=PATHS[\"logs\"],\n",
    "        on_fold_complete=upload_fold_checkpoint,\n",
    "    )\n",
    "    upload_experiment(exp_id)\n",
    "    temp_results[temp] = result[\"summary\"][\"avg_pairwise_acc\"]\n",
    "\n",
    "print(\"\\nTemperature ablation results:\")\n",
    "for temp, acc in temp_results.items():\n",
    "    print(f\"  tau={temp}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2c: Projection dimension ablation\n",
    "proj_dims = [64, 128, 256, 512]\n",
    "proj_results = {}\n",
    "\n",
    "for dim in proj_dims:\n",
    "    exp_id = f\"E2c_proj_{dim}\"\n",
    "    config = {**APPROACH_A_CONFIG, \"projection_dim\": dim}\n",
    "    result = run_pairwise_experiment(\n",
    "        exp_id=exp_id,\n",
    "        description=f\"Approach A: projection_dim={dim}\",\n",
    "        model_factory=make_approach_a_model,\n",
    "        cache_dir=PATHS[\"muq_cache\"],\n",
    "        labels=labels,\n",
    "        piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "        fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "        config=config,\n",
    "        checkpoint_root=PATHS[\"checkpoints\"],\n",
    "        results_dir=PATHS[\"results\"],\n",
    "        log_dir=PATHS[\"logs\"],\n",
    "        on_fold_complete=upload_fold_checkpoint,\n",
    "    )\n",
    "    upload_experiment(exp_id)\n",
    "    proj_results[dim] = result[\"summary\"][\"avg_pairwise_acc\"]\n",
    "\n",
    "print(\"\\nProjection dim ablation results:\")\n",
    "for dim, acc in proj_results.items():\n",
    "    print(f\"  dim={dim}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2d: Lambda contrastive ablation\n",
    "lambda_values = [0.0, 0.1, 0.3, 0.5, 1.0]\n",
    "lambda_results = {}\n",
    "\n",
    "for lam in lambda_values:\n",
    "    exp_id = f\"E2d_lambda_{lam}\"\n",
    "    config = {**APPROACH_A_CONFIG, \"lambda_contrastive\": lam}\n",
    "    result = run_pairwise_experiment(\n",
    "        exp_id=exp_id,\n",
    "        description=f\"Approach A: lambda_contrastive={lam}\",\n",
    "        model_factory=make_approach_a_model,\n",
    "        cache_dir=PATHS[\"muq_cache\"],\n",
    "        labels=labels,\n",
    "        piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "        fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "        config=config,\n",
    "        checkpoint_root=PATHS[\"checkpoints\"],\n",
    "        results_dir=PATHS[\"results\"],\n",
    "        log_dir=PATHS[\"logs\"],\n",
    "        on_fold_complete=upload_fold_checkpoint,\n",
    "    )\n",
    "    upload_experiment(exp_id)\n",
    "    lambda_results[lam] = result[\"summary\"][\"avg_pairwise_acc\"]\n",
    "\n",
    "print(\"\\nLambda contrastive ablation results:\")\n",
    "for lam, acc in lambda_results.items():\n",
    "    print(f\"  lambda={lam}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Approach B: Siamese Dimension-Specific Ranking\n",
    "\n",
    "**Architecture**: Shared encoder (both inputs) + comparison module + 19 dimension heads\n",
    "\n",
    "**Comparison**: `[z_A; z_B; z_A - z_B; z_A * z_B]` or bilinear\n",
    "\n",
    "**Loss**: BCE with label smoothing, ignoring ambiguous pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model factory for Approach B\n",
    "def make_approach_b_model(config):\n",
    "    return SiameseDimensionRankingModel(\n",
    "        input_dim=config.get(\"input_dim\", 1024),\n",
    "        hidden_dim=config.get(\"hidden_dim\", 512),\n",
    "        num_labels=config.get(\"num_labels\", 19),\n",
    "        dropout=config.get(\"dropout\", 0.2),\n",
    "        learning_rate=config.get(\"learning_rate\", 1e-4),\n",
    "        weight_decay=config.get(\"weight_decay\", 1e-5),\n",
    "        comparison_type=config.get(\"comparison_type\", \"concat_diff\"),\n",
    "        margin=config.get(\"margin\", 0.3),\n",
    "        ambiguous_threshold=config.get(\"ambiguous_threshold\", 0.05),\n",
    "        label_smoothing=config.get(\"label_smoothing\", 0.05),\n",
    "        pooling=config.get(\"pooling\", \"attention\"),\n",
    "        max_epochs=config.get(\"max_epochs\", 100),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E3a: Approach B with default config\n",
    "results_e3a = run_pairwise_experiment(\n",
    "    exp_id=\"E3a_siamese_default\",\n",
    "    description=\"Approach B: Siamese dimension-specific ranking with default config\",\n",
    "    model_factory=make_approach_b_model,\n",
    "    cache_dir=PATHS[\"muq_cache\"],\n",
    "    labels=labels,\n",
    "    piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "    fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "    config=APPROACH_B_CONFIG,\n",
    "    checkpoint_root=PATHS[\"checkpoints\"],\n",
    "    results_dir=PATHS[\"results\"],\n",
    "    log_dir=PATHS[\"logs\"],\n",
    "    on_fold_complete=upload_fold_checkpoint,\n",
    ")\n",
    "upload_experiment(\"E3a_siamese_default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E3b: Comparison type ablation\n",
    "comparison_types = [\"concat_diff\", \"bilinear\"]\n",
    "comp_results = {}\n",
    "\n",
    "for comp_type in comparison_types:\n",
    "    exp_id = f\"E3b_{comp_type}\"\n",
    "    config = {**APPROACH_B_CONFIG, \"comparison_type\": comp_type}\n",
    "    result = run_pairwise_experiment(\n",
    "        exp_id=exp_id,\n",
    "        description=f\"Approach B: comparison_type={comp_type}\",\n",
    "        model_factory=make_approach_b_model,\n",
    "        cache_dir=PATHS[\"muq_cache\"],\n",
    "        labels=labels,\n",
    "        piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "        fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "        config=config,\n",
    "        checkpoint_root=PATHS[\"checkpoints\"],\n",
    "        results_dir=PATHS[\"results\"],\n",
    "        log_dir=PATHS[\"logs\"],\n",
    "        on_fold_complete=upload_fold_checkpoint,\n",
    "    )\n",
    "    upload_experiment(exp_id)\n",
    "    comp_results[comp_type] = result[\"summary\"][\"avg_pairwise_acc\"]\n",
    "\n",
    "print(\"\\nComparison type ablation results:\")\n",
    "for comp, acc in comp_results.items():\n",
    "    print(f\"  {comp}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E3c: Label smoothing ablation\n",
    "smoothing_values = [0.0, 0.05, 0.1, 0.15]\n",
    "smooth_results = {}\n",
    "\n",
    "for smooth in smoothing_values:\n",
    "    exp_id = f\"E3c_smooth_{smooth}\"\n",
    "    config = {**APPROACH_B_CONFIG, \"label_smoothing\": smooth}\n",
    "    result = run_pairwise_experiment(\n",
    "        exp_id=exp_id,\n",
    "        description=f\"Approach B: label_smoothing={smooth}\",\n",
    "        model_factory=make_approach_b_model,\n",
    "        cache_dir=PATHS[\"muq_cache\"],\n",
    "        labels=labels,\n",
    "        piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "        fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "        config=config,\n",
    "        checkpoint_root=PATHS[\"checkpoints\"],\n",
    "        results_dir=PATHS[\"results\"],\n",
    "        log_dir=PATHS[\"logs\"],\n",
    "        on_fold_complete=upload_fold_checkpoint,\n",
    "    )\n",
    "    upload_experiment(exp_id)\n",
    "    smooth_results[smooth] = result[\"summary\"][\"avg_pairwise_acc\"]\n",
    "\n",
    "print(\"\\nLabel smoothing ablation results:\")\n",
    "for smooth, acc in smooth_results.items():\n",
    "    print(f\"  smoothing={smooth}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Approach C: Disentangled Dual-Encoder\n",
    "\n",
    "**Architecture**: Piece encoder + Style encoder + GRL + adversarial piece classifier\n",
    "\n",
    "**Loss**: `L_total = L_regression + lambda_adv * L_adversarial`\n",
    "- Gradient Reversal Layer makes style encoder adversarial to piece classification\n",
    "- Style encoder feeds dimension prediction heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model factory for Approach C\n",
    "def make_approach_c_model(config):\n",
    "    return DisentangledDualEncoderModel(\n",
    "        input_dim=config.get(\"input_dim\", 1024),\n",
    "        hidden_dim=config.get(\"hidden_dim\", 512),\n",
    "        num_labels=config.get(\"num_labels\", 19),\n",
    "        num_pieces=config.get(\"num_pieces\", 206),\n",
    "        dropout=config.get(\"dropout\", 0.2),\n",
    "        learning_rate=config.get(\"learning_rate\", 1e-4),\n",
    "        weight_decay=config.get(\"weight_decay\", 1e-5),\n",
    "        lambda_adversarial=config.get(\"lambda_adversarial\", 0.5),\n",
    "        grl_schedule=config.get(\"grl_schedule\", \"linear\"),\n",
    "        pooling=config.get(\"pooling\", \"attention\"),\n",
    "        max_epochs=config.get(\"max_epochs\", 100),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4a: Approach C with default config\n",
    "results_e4a = run_disentanglement_experiment(\n",
    "    exp_id=\"E4a_dual_encoder_default\",\n",
    "    description=\"Approach C: Disentangled dual-encoder with default config\",\n",
    "    model_factory=make_approach_c_model,\n",
    "    cache_dir=PATHS[\"muq_cache\"],\n",
    "    labels=labels,\n",
    "    piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "    fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "    config=APPROACH_C_CONFIG,\n",
    "    checkpoint_root=PATHS[\"checkpoints\"],\n",
    "    results_dir=PATHS[\"results\"],\n",
    "    log_dir=PATHS[\"logs\"],\n",
    "    on_fold_complete=upload_fold_checkpoint,\n",
    ")\n",
    "upload_experiment(\"E4a_dual_encoder_default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4b: Adversarial weight ablation\n",
    "adv_weights = [0.1, 0.3, 0.5, 0.7]\n",
    "adv_results = {}\n",
    "\n",
    "for weight in adv_weights:\n",
    "    exp_id = f\"E4b_adv_{weight}\"\n",
    "    config = {**APPROACH_C_CONFIG, \"lambda_adversarial\": weight}\n",
    "    result = run_disentanglement_experiment(\n",
    "        exp_id=exp_id,\n",
    "        description=f\"Approach C: lambda_adversarial={weight}\",\n",
    "        model_factory=make_approach_c_model,\n",
    "        cache_dir=PATHS[\"muq_cache\"],\n",
    "        labels=labels,\n",
    "        piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "        fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "        config=config,\n",
    "        checkpoint_root=PATHS[\"checkpoints\"],\n",
    "        results_dir=PATHS[\"results\"],\n",
    "        log_dir=PATHS[\"logs\"],\n",
    "        on_fold_complete=upload_fold_checkpoint,\n",
    "    )\n",
    "    upload_experiment(exp_id)\n",
    "    adv_results[weight] = {\n",
    "        \"r2\": result[\"summary\"][\"avg_r2\"],\n",
    "        \"style_piece_acc\": result[\"disentanglement\"][\"style_piece_accuracy\"],\n",
    "        \"intra_piece_std\": result[\"disentanglement\"][\"intra_piece_std\"],\n",
    "    }\n",
    "\n",
    "print(\"\\nAdversarial weight ablation results:\")\n",
    "for w, metrics in adv_results.items():\n",
    "    print(f\"  lambda={w}: R2={metrics['r2']:.4f}, style_piece_acc={metrics['style_piece_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4c: GRL schedule ablation\n",
    "schedules = [\"constant\", \"linear\", \"cosine\"]\n",
    "sched_results = {}\n",
    "\n",
    "for sched in schedules:\n",
    "    exp_id = f\"E4c_sched_{sched}\"\n",
    "    config = {**APPROACH_C_CONFIG, \"grl_schedule\": sched}\n",
    "    result = run_disentanglement_experiment(\n",
    "        exp_id=exp_id,\n",
    "        description=f\"Approach C: grl_schedule={sched}\",\n",
    "        model_factory=make_approach_c_model,\n",
    "        cache_dir=PATHS[\"muq_cache\"],\n",
    "        labels=labels,\n",
    "        piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "        fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "        config=config,\n",
    "        checkpoint_root=PATHS[\"checkpoints\"],\n",
    "        results_dir=PATHS[\"results\"],\n",
    "        log_dir=PATHS[\"logs\"],\n",
    "        on_fold_complete=upload_fold_checkpoint,\n",
    "    )\n",
    "    upload_experiment(exp_id)\n",
    "    sched_results[sched] = {\n",
    "        \"r2\": result[\"summary\"][\"avg_r2\"],\n",
    "        \"style_piece_acc\": result[\"disentanglement\"][\"style_piece_accuracy\"],\n",
    "    }\n",
    "\n",
    "print(\"\\nGRL schedule ablation results:\")\n",
    "for s, metrics in sched_results.items():\n",
    "    print(f\"  {s}: R2={metrics['r2']:.4f}, style_piece_acc={metrics['style_piece_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combination Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model factories for combinations\n",
    "def make_ac_model(config):\n",
    "    \"\"\"A+C: Contrastive + Adversarial\"\"\"\n",
    "    return ContrastiveDisentangledModel(\n",
    "        input_dim=config.get(\"input_dim\", 1024),\n",
    "        hidden_dim=config.get(\"hidden_dim\", 512),\n",
    "        projection_dim=config.get(\"projection_dim\", 256),\n",
    "        num_labels=config.get(\"num_labels\", 19),\n",
    "        num_pieces=config.get(\"num_pieces\", 206),\n",
    "        dropout=config.get(\"dropout\", 0.2),\n",
    "        learning_rate=config.get(\"learning_rate\", 1e-4),\n",
    "        weight_decay=config.get(\"weight_decay\", 1e-5),\n",
    "        temperature=config.get(\"temperature\", 0.07),\n",
    "        lambda_contrastive=config.get(\"lambda_contrastive\", 0.3),\n",
    "        lambda_adversarial=config.get(\"lambda_adversarial\", 0.5),\n",
    "        grl_schedule=config.get(\"grl_schedule\", \"linear\"),\n",
    "        ambiguous_threshold=config.get(\"ambiguous_threshold\", 0.05),\n",
    "        label_smoothing=config.get(\"label_smoothing\", 0.0),\n",
    "        pooling=config.get(\"pooling\", \"attention\"),\n",
    "        max_epochs=config.get(\"max_epochs\", 100),\n",
    "    )\n",
    "\n",
    "def make_bc_model(config):\n",
    "    \"\"\"B+C: Siamese + Adversarial\"\"\"\n",
    "    return SiameseDisentangledModel(\n",
    "        input_dim=config.get(\"input_dim\", 1024),\n",
    "        hidden_dim=config.get(\"hidden_dim\", 512),\n",
    "        num_labels=config.get(\"num_labels\", 19),\n",
    "        num_pieces=config.get(\"num_pieces\", 206),\n",
    "        dropout=config.get(\"dropout\", 0.2),\n",
    "        learning_rate=config.get(\"learning_rate\", 1e-4),\n",
    "        weight_decay=config.get(\"weight_decay\", 1e-5),\n",
    "        lambda_adversarial=config.get(\"lambda_adversarial\", 0.5),\n",
    "        grl_schedule=config.get(\"grl_schedule\", \"linear\"),\n",
    "        comparison_type=config.get(\"comparison_type\", \"concat_diff\"),\n",
    "        ambiguous_threshold=config.get(\"ambiguous_threshold\", 0.05),\n",
    "        label_smoothing=config.get(\"label_smoothing\", 0.05),\n",
    "        pooling=config.get(\"pooling\", \"attention\"),\n",
    "        max_epochs=config.get(\"max_epochs\", 100),\n",
    "    )\n",
    "\n",
    "def make_abc_model(config):\n",
    "    \"\"\"A+B+C: Full combination\"\"\"\n",
    "    return FullCombinedModel(\n",
    "        input_dim=config.get(\"input_dim\", 1024),\n",
    "        hidden_dim=config.get(\"hidden_dim\", 512),\n",
    "        projection_dim=config.get(\"projection_dim\", 256),\n",
    "        num_labels=config.get(\"num_labels\", 19),\n",
    "        num_pieces=config.get(\"num_pieces\", 206),\n",
    "        dropout=config.get(\"dropout\", 0.2),\n",
    "        learning_rate=config.get(\"learning_rate\", 1e-4),\n",
    "        weight_decay=config.get(\"weight_decay\", 1e-5),\n",
    "        temperature=config.get(\"temperature\", 0.07),\n",
    "        lambda_contrastive=config.get(\"lambda_contrastive\", 0.3),\n",
    "        lambda_adversarial=config.get(\"lambda_adversarial\", 0.5),\n",
    "        grl_schedule=config.get(\"grl_schedule\", \"linear\"),\n",
    "        ambiguous_threshold=config.get(\"ambiguous_threshold\", 0.05),\n",
    "        label_smoothing=config.get(\"label_smoothing\", 0.05),\n",
    "        pooling=config.get(\"pooling\", \"attention\"),\n",
    "        max_epochs=config.get(\"max_epochs\", 100),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined config\n",
    "COMBINED_CONFIG = {\n",
    "    **APPROACH_A_CONFIG,\n",
    "    **APPROACH_B_CONFIG,\n",
    "    **APPROACH_C_CONFIG,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5c: A+C (Contrastive + Adversarial)\n",
    "results_e5c = run_pairwise_experiment(\n",
    "    exp_id=\"E5c_contrastive_adversarial\",\n",
    "    description=\"A+C: Contrastive + Adversarial disentanglement\",\n",
    "    model_factory=make_ac_model,\n",
    "    cache_dir=PATHS[\"muq_cache\"],\n",
    "    labels=labels,\n",
    "    piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "    fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "    config=COMBINED_CONFIG,\n",
    "    checkpoint_root=PATHS[\"checkpoints\"],\n",
    "    results_dir=PATHS[\"results\"],\n",
    "    log_dir=PATHS[\"logs\"],\n",
    "    on_fold_complete=upload_fold_checkpoint,\n",
    ")\n",
    "upload_experiment(\"E5c_contrastive_adversarial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5b: B+C (Siamese + Adversarial)\n",
    "results_e5b = run_pairwise_experiment(\n",
    "    exp_id=\"E5b_siamese_adversarial\",\n",
    "    description=\"B+C: Siamese + Adversarial disentanglement\",\n",
    "    model_factory=make_bc_model,\n",
    "    cache_dir=PATHS[\"muq_cache\"],\n",
    "    labels=labels,\n",
    "    piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "    fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "    config=COMBINED_CONFIG,\n",
    "    checkpoint_root=PATHS[\"checkpoints\"],\n",
    "    results_dir=PATHS[\"results\"],\n",
    "    log_dir=PATHS[\"logs\"],\n",
    "    on_fold_complete=upload_fold_checkpoint,\n",
    ")\n",
    "upload_experiment(\"E5b_siamese_adversarial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5d: A+B+C (Full combination)\n",
    "results_e5d = run_pairwise_experiment(\n",
    "    exp_id=\"E5d_full_combined\",\n",
    "    description=\"A+B+C: Full combination of all approaches\",\n",
    "    model_factory=make_abc_model,\n",
    "    cache_dir=PATHS[\"muq_cache\"],\n",
    "    labels=labels,\n",
    "    piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "    fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "    config=COMBINED_CONFIG,\n",
    "    checkpoint_root=PATHS[\"checkpoints\"],\n",
    "    results_dir=PATHS[\"results\"],\n",
    "    log_dir=PATHS[\"logs\"],\n",
    "    on_fold_complete=upload_fold_checkpoint,\n",
    ")\n",
    "upload_experiment(\"E5d_full_combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all results\n",
    "results_files = list(PATHS[\"results\"].glob(\"*.json\"))\n",
    "all_results = []\n",
    "\n",
    "for f in results_files:\n",
    "    with open(f) as fp:\n",
    "        result = json.load(fp)\n",
    "        exp_id = result[\"experiment_id\"]\n",
    "\n",
    "        # Extract key metrics\n",
    "        row = {\n",
    "            \"exp_id\": exp_id,\n",
    "            \"description\": result.get(\"description\", \"\"),\n",
    "        }\n",
    "\n",
    "        if \"avg_pairwise_acc\" in result.get(\"summary\", {}):\n",
    "            row[\"pairwise_acc\"] = result[\"summary\"][\"avg_pairwise_acc\"]\n",
    "            row[\"acc_std\"] = result[\"summary\"].get(\"std_pairwise_acc\", 0)\n",
    "\n",
    "        if \"avg_r2\" in result.get(\"summary\", {}):\n",
    "            row[\"r2\"] = result[\"summary\"][\"avg_r2\"]\n",
    "            row[\"r2_std\"] = result[\"summary\"].get(\"std_r2\", 0)\n",
    "\n",
    "        if \"disentanglement\" in result:\n",
    "            row[\"style_piece_acc\"] = result[\"disentanglement\"].get(\"style_piece_accuracy\", None)\n",
    "            row[\"intra_piece_std\"] = result[\"disentanglement\"].get(\"intra_piece_std\", None)\n",
    "\n",
    "        all_results.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values(\"exp_id\")\n",
    "print(f\"Loaded {len(results_df)} experiment results\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main results comparison\n",
    "main_experiments = [\n",
    "    \"E2a_contrastive_default\",\n",
    "    \"E3a_siamese_default\",\n",
    "    \"E4a_dual_encoder_default\",\n",
    "    \"E5b_siamese_adversarial\",\n",
    "    \"E5c_contrastive_adversarial\",\n",
    "    \"E5d_full_combined\",\n",
    "]\n",
    "\n",
    "main_df = results_df[results_df[\"exp_id\"].isin(main_experiments)].copy()\n",
    "main_df[\"approach\"] = main_df[\"exp_id\"].map({\n",
    "    \"E2a_contrastive_default\": \"A: Contrastive\",\n",
    "    \"E3a_siamese_default\": \"B: Siamese\",\n",
    "    \"E4a_dual_encoder_default\": \"C: Disentangled\",\n",
    "    \"E5b_siamese_adversarial\": \"B+C\",\n",
    "    \"E5c_contrastive_adversarial\": \"A+C\",\n",
    "    \"E5d_full_combined\": \"A+B+C\",\n",
    "})\n",
    "\n",
    "print(\"Main Results:\")\n",
    "print(main_df[[\"approach\", \"pairwise_acc\", \"acc_std\", \"r2\", \"style_piece_acc\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-dimension accuracy comparison\n",
    "# Load full results for per-dimension analysis\n",
    "per_dim_data = []\n",
    "\n",
    "for exp_id in [\"E2a_contrastive_default\", \"E3a_siamese_default\", \"E5d_full_combined\"]:\n",
    "    result_file = PATHS[\"results\"] / f\"{exp_id}.json\"\n",
    "    if result_file.exists():\n",
    "        with open(result_file) as f:\n",
    "            result = json.load(f)\n",
    "            per_dim = result.get(\"per_dimension\", {})\n",
    "            for dim_idx, acc in per_dim.items():\n",
    "                per_dim_data.append({\n",
    "                    \"experiment\": exp_id,\n",
    "                    \"dimension\": PERCEPIANO_DIMENSIONS[int(dim_idx)],\n",
    "                    \"accuracy\": acc,\n",
    "                })\n",
    "\n",
    "per_dim_df = pd.DataFrame(per_dim_data)\n",
    "if len(per_dim_df) > 0:\n",
    "    pivot = per_dim_df.pivot(index=\"dimension\", columns=\"experiment\", values=\"accuracy\")\n",
    "    print(\"Per-dimension accuracy:\")\n",
    "    print(pivot.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Temperature ablation\n",
    "if 'temp_results' in dir():\n",
    "    ax = axes[0]\n",
    "    temps = list(temp_results.keys())\n",
    "    accs = list(temp_results.values())\n",
    "    ax.bar(range(len(temps)), accs)\n",
    "    ax.set_xticks(range(len(temps)))\n",
    "    ax.set_xticklabels([str(t) for t in temps])\n",
    "    ax.set_xlabel(\"Temperature\")\n",
    "    ax.set_ylabel(\"Pairwise Accuracy\")\n",
    "    ax.set_title(\"E2b: Temperature Ablation\")\n",
    "    ax.axhline(y=0.5, color='r', linestyle='--', label='Random')\n",
    "\n",
    "# Lambda ablation\n",
    "if 'lambda_results' in dir():\n",
    "    ax = axes[1]\n",
    "    lams = list(lambda_results.keys())\n",
    "    accs = list(lambda_results.values())\n",
    "    ax.bar(range(len(lams)), accs)\n",
    "    ax.set_xticks(range(len(lams)))\n",
    "    ax.set_xticklabels([str(l) for l in lams])\n",
    "    ax.set_xlabel(\"Lambda Contrastive\")\n",
    "    ax.set_ylabel(\"Pairwise Accuracy\")\n",
    "    ax.set_title(\"E2d: Lambda Ablation\")\n",
    "    ax.axhline(y=0.5, color='r', linestyle='--', label='Random')\n",
    "\n",
    "# Adversarial weight ablation\n",
    "if 'adv_results' in dir():\n",
    "    ax = axes[2]\n",
    "    weights = list(adv_results.keys())\n",
    "    r2s = [v[\"r2\"] for v in adv_results.values()]\n",
    "    ax.bar(range(len(weights)), r2s)\n",
    "    ax.set_xticks(range(len(weights)))\n",
    "    ax.set_xticklabels([str(w) for w in weights])\n",
    "    ax.set_xlabel(\"Lambda Adversarial\")\n",
    "    ax.set_ylabel(\"R2\")\n",
    "    ax.set_title(\"E4b: Adversarial Weight Ablation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PATHS[\"results\"].parent / \"figures\" / \"ablation_heatmaps.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Paper Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures directory\n",
    "figures_dir = PATHS[\"results\"].parent / \"figures\"\n",
    "figures_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig 1: Main results bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "if len(main_df) > 0:\n",
    "    approaches = main_df[\"approach\"].tolist()\n",
    "    accs = main_df[\"pairwise_acc\"].tolist()\n",
    "    stds = main_df[\"acc_std\"].fillna(0).tolist()\n",
    "\n",
    "    x = range(len(approaches))\n",
    "    bars = ax.bar(x, accs, yerr=stds, capsize=5, color='steelblue', edgecolor='black')\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(approaches, rotation=45, ha='right')\n",
    "    ax.set_ylabel(\"Pairwise Ranking Accuracy\")\n",
    "    ax.set_title(\"Disentanglement Approaches: Pairwise Ranking Accuracy\")\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', label='Random Baseline (50%)')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0.4, 0.8)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars, accs):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / \"fig1_main_results.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig 3: t-SNE of piece vs style embeddings (for Approach C)\n",
    "# This requires loading a trained model and computing embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load best Approach C model\n",
    "best_c_ckpt = PATHS[\"checkpoints\"] / \"E4a_dual_encoder_default\" / \"fold0_best.ckpt\"\n",
    "\n",
    "if best_c_ckpt.exists():\n",
    "    print(\"Loading model for t-SNE visualization...\")\n",
    "    model = DisentangledDualEncoderModel.load_from_checkpoint(best_c_ckpt)\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Create dataset for fold 0 validation\n",
    "    _, val_keys = get_fold_piece_mapping(\n",
    "        MULTI_PERFORMER_PIECES, PIECE_STRATIFIED_FOLDS, 0, \"val\"\n",
    "    )\n",
    "    val_ds = DisentanglementDataset(\n",
    "        PATHS[\"muq_cache\"], labels, MULTI_PERFORMER_PIECES, val_keys\n",
    "    )\n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "        val_ds, batch_size=32, collate_fn=disentanglement_collate_fn\n",
    "    )\n",
    "\n",
    "    # Extract embeddings\n",
    "    z_pieces, z_styles, piece_ids = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dl:\n",
    "            outputs = model(\n",
    "                batch[\"embeddings\"].to(device),\n",
    "                batch[\"attention_mask\"].to(device),\n",
    "            )\n",
    "            z_pieces.append(outputs[\"z_piece\"].cpu())\n",
    "            z_styles.append(outputs[\"z_style\"].cpu())\n",
    "            piece_ids.extend(batch[\"piece_ids\"].tolist())\n",
    "\n",
    "    z_piece = torch.cat(z_pieces).numpy()\n",
    "    z_style = torch.cat(z_styles).numpy()\n",
    "    piece_ids = np.array(piece_ids)\n",
    "\n",
    "    # t-SNE\n",
    "    print(\"Computing t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    z_piece_2d = tsne.fit_transform(z_piece)\n",
    "    z_style_2d = tsne.fit_transform(z_style)\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Piece embeddings (should cluster by piece)\n",
    "    ax = axes[0]\n",
    "    scatter = ax.scatter(z_piece_2d[:, 0], z_piece_2d[:, 1], c=piece_ids, cmap='tab20', alpha=0.7, s=20)\n",
    "    ax.set_title(\"Piece Encoder Embeddings (z_piece)\")\n",
    "    ax.set_xlabel(\"t-SNE 1\")\n",
    "    ax.set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "    # Style embeddings (should NOT cluster by piece)\n",
    "    ax = axes[1]\n",
    "    scatter = ax.scatter(z_style_2d[:, 0], z_style_2d[:, 1], c=piece_ids, cmap='tab20', alpha=0.7, s=20)\n",
    "    ax.set_title(\"Style Encoder Embeddings (z_style)\")\n",
    "    ax.set_xlabel(\"t-SNE 1\")\n",
    "    ax.set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "    plt.suptitle(\"Disentanglement: Piece vs Style Representations\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / \"fig3_tsne_embeddings.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No trained model found for t-SNE visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig 4: Per-dimension breakdown\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "if len(per_dim_df) > 0:\n",
    "    pivot = per_dim_df.pivot(index=\"dimension\", columns=\"experiment\", values=\"accuracy\")\n",
    "    pivot = pivot.reindex(PERCEPIANO_DIMENSIONS)  # Ensure consistent order\n",
    "\n",
    "    x = np.arange(len(PERCEPIANO_DIMENSIONS))\n",
    "    width = 0.25\n",
    "\n",
    "    for i, col in enumerate(pivot.columns):\n",
    "        offset = (i - len(pivot.columns)/2 + 0.5) * width\n",
    "        label = col.replace(\"_default\", \"\").replace(\"_\", \" \").title()\n",
    "        ax.bar(x + offset, pivot[col].fillna(0.5), width, label=label)\n",
    "\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(PERCEPIANO_DIMENSIONS, rotation=45, ha='right')\n",
    "    ax.set_ylabel(\"Pairwise Accuracy\")\n",
    "    ax.set_title(\"Per-Dimension Pairwise Ranking Accuracy\")\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_ylim(0.3, 0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / \"fig4_per_dimension.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload results and checkpoints to remote storage\n",
    "print(\"Uploading results to remote storage...\")\n",
    "\n",
    "# Upload checkpoints\n",
    "print(\"\\nUploading checkpoints...\")\n",
    "rclone_sync(str(PATHS[\"checkpoints\"]), REMOTE_PATHS[\"checkpoints\"], direction=\"upload\")\n",
    "\n",
    "# Upload results JSON files\n",
    "print(\"\\nUploading results...\")\n",
    "rclone_sync(str(PATHS[\"results\"]), REMOTE_PATHS[\"results\"], direction=\"upload\")\n",
    "\n",
    "# Upload figures if they exist\n",
    "figures_dir = PATHS[\"results\"].parent / \"figures\"\n",
    "if figures_dir.exists():\n",
    "    print(\"\\nUploading figures...\")\n",
    "    rclone_sync(str(figures_dir), \"figures/disentanglement\", direction=\"upload\")\n",
    "\n",
    "print(\"\\nUpload complete! Results saved to remote storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Extended Experiments: E7, E8, E11, E16\n",
    "\n",
    "Additional experiments to strengthen disentanglement results:\n",
    "- **E7**: Hard pair mining (focus on challenging pairs)\n",
    "- **E8**: Per-dimension model groups\n",
    "- **E11**: Triplet loss for performer discrimination\n",
    "- **E16**: 4096-dim embedding comparison (concatenated layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E7: Hard Pair Mining Experiments\n",
    "# Focus training on challenging pairs with moderate score differences\n",
    "from disentanglement.data import sample_hard_pairs\n",
    "\n",
    "# Analyze difficulty distribution\n",
    "print(\"E7: Hard Pair Mining\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample pairs at different difficulty levels\n",
    "difficulty_ranges = [\n",
    "    (0.05, 0.10, \"easy\"),\n",
    "    (0.10, 0.20, \"medium\"),\n",
    "    (0.20, 0.30, \"hard\"),\n",
    "]\n",
    "\n",
    "for min_diff, max_diff, name in difficulty_ranges:\n",
    "    hard_pairs = sample_hard_pairs(\n",
    "        MULTI_PERFORMER_PIECES, labels, \n",
    "        n_pairs=1000, min_diff=min_diff, max_diff=max_diff, seed=42\n",
    "    )\n",
    "    print(f\"  {name} pairs ({min_diff:.2f}-{max_diff:.2f}): {len(hard_pairs)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E7a: Train with hard pairs only (diff 0.05-0.20)\n",
    "# Uses the best-performing base model with hard pair sampling\n",
    "print(\"E7a: Hard Pair Training\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# HardPairRankingDataset is now imported from disentanglement.data\n",
    "# It filters pairs to the specified difficulty range during initialization\n",
    "\n",
    "# Configuration for hard pair experiment\n",
    "E7A_CONFIG = {\n",
    "    **APPROACH_B_CONFIG,\n",
    "    \"min_diff\": 0.05,\n",
    "    \"max_diff\": 0.20,\n",
    "}\n",
    "\n",
    "# Custom runner that uses HardPairRankingDataset\n",
    "def run_hard_pair_experiment(\n",
    "    exp_id, description, model_factory, cache_dir, labels,\n",
    "    piece_to_keys, fold_assignments, config, checkpoint_root,\n",
    "    results_dir, log_dir, on_fold_complete=None\n",
    "):\n",
    "    \"\"\"Run pairwise experiment with hard pair filtering.\"\"\"\n",
    "    from disentanglement.training.runner import (\n",
    "        experiment_completed, load_existing_results, get_fold_piece_mapping\n",
    "    )\n",
    "    from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from pytorch_lightning.loggers import CSVLogger\n",
    "    from torch.utils.data import DataLoader\n",
    "    import time\n",
    "\n",
    "    exp_checkpoint_dir = Path(checkpoint_root) / exp_id\n",
    "    exp_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results_dir = Path(results_dir)\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    existing = load_existing_results(exp_id, results_dir)\n",
    "    if existing and experiment_completed(exp_id, checkpoint_root):\n",
    "        print(f\"SKIP {exp_id}: already completed\")\n",
    "        return existing\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    fold_results = {}\n",
    "    all_logits, all_labels_a, all_labels_b = [], [], []\n",
    "\n",
    "    for fold in range(config.get(\"n_folds\", 4)):\n",
    "        ckpt_path = exp_checkpoint_dir / f\"fold{fold}_best.ckpt\"\n",
    "\n",
    "        train_piece_map, train_keys = get_fold_piece_mapping(\n",
    "            piece_to_keys, fold_assignments, fold, \"train\"\n",
    "        )\n",
    "        val_piece_map, val_keys = get_fold_piece_mapping(\n",
    "            piece_to_keys, fold_assignments, fold, \"val\"\n",
    "        )\n",
    "\n",
    "        # Use HardPairRankingDataset instead of regular dataset\n",
    "        train_ds = HardPairRankingDataset(\n",
    "            cache_dir, labels, train_piece_map, train_keys,\n",
    "            max_frames=config.get(\"max_frames\", 1000),\n",
    "            ambiguous_threshold=config.get(\"ambiguous_threshold\", 0.05),\n",
    "            min_diff=config.get(\"min_diff\", 0.05),\n",
    "            max_diff=config.get(\"max_diff\", 0.20),\n",
    "        )\n",
    "        val_ds = HardPairRankingDataset(\n",
    "            cache_dir, labels, val_piece_map, val_keys,\n",
    "            max_frames=config.get(\"max_frames\", 1000),\n",
    "            ambiguous_threshold=config.get(\"ambiguous_threshold\", 0.05),\n",
    "            min_diff=config.get(\"min_diff\", 0.05),\n",
    "            max_diff=config.get(\"max_diff\", 0.20),\n",
    "        )\n",
    "\n",
    "        if len(train_ds) == 0 or len(val_ds) == 0:\n",
    "            print(f\"Fold {fold}: No hard pairs available, skipping\")\n",
    "            continue\n",
    "\n",
    "        train_dl = DataLoader(\n",
    "            train_ds, batch_size=config.get(\"batch_size\", 32),\n",
    "            shuffle=True, collate_fn=pairwise_collate_fn,\n",
    "            num_workers=config.get(\"num_workers\", 2), pin_memory=True,\n",
    "        )\n",
    "        val_dl = DataLoader(\n",
    "            val_ds, batch_size=config.get(\"batch_size\", 32),\n",
    "            shuffle=False, collate_fn=pairwise_collate_fn,\n",
    "            num_workers=config.get(\"num_workers\", 2), pin_memory=True,\n",
    "        )\n",
    "\n",
    "        print(f\"Fold {fold}: {len(train_ds)} train hard pairs, {len(val_ds)} val hard pairs\")\n",
    "\n",
    "        config_with_pieces = {**config, \"num_pieces\": train_ds.get_num_pieces()}\n",
    "\n",
    "        if ckpt_path.exists():\n",
    "            model = model_factory(config_with_pieces)\n",
    "            model = model.__class__.load_from_checkpoint(ckpt_path)\n",
    "        else:\n",
    "            model = model_factory(config_with_pieces)\n",
    "            callbacks = [\n",
    "                ModelCheckpoint(\n",
    "                    dirpath=exp_checkpoint_dir, filename=f\"fold{fold}_best\",\n",
    "                    monitor=\"val_pairwise_acc\", mode=\"max\", save_top_k=1,\n",
    "                ),\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_pairwise_acc\", mode=\"max\",\n",
    "                    patience=config.get(\"patience\", 15), verbose=True,\n",
    "                ),\n",
    "            ]\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=config.get(\"max_epochs\", 100),\n",
    "                callbacks=callbacks,\n",
    "                logger=CSVLogger(save_dir=log_dir, name=exp_id, version=f\"fold{fold}\"),\n",
    "                accelerator=\"auto\", devices=1,\n",
    "                gradient_clip_val=config.get(\"gradient_clip_val\", 1.0),\n",
    "                enable_progress_bar=True, deterministic=True, log_every_n_steps=10,\n",
    "            )\n",
    "            trainer.fit(model, train_dl, val_dl)\n",
    "            fold_results[fold] = float(callbacks[0].best_model_score or 0.5)\n",
    "            model = model.__class__.load_from_checkpoint(ckpt_path)\n",
    "            if on_fold_complete:\n",
    "                on_fold_complete(exp_id, fold)\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        device = next(model.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                outputs = model(\n",
    "                    batch[\"embeddings_a\"].to(device),\n",
    "                    batch[\"embeddings_b\"].to(device),\n",
    "                    batch.get(\"mask_a\").to(device),\n",
    "                    batch.get(\"mask_b\").to(device),\n",
    "                )\n",
    "                logits = outputs if not isinstance(outputs, dict) else outputs.get(\"ranking_logits\", outputs)\n",
    "                all_logits.append(logits.cpu().numpy())\n",
    "                all_labels_a.append(batch[\"labels_a\"].numpy())\n",
    "                all_labels_b.append(batch[\"labels_b\"].numpy())\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Aggregate\n",
    "    all_logits = np.vstack(all_logits)\n",
    "    all_labels_a = np.vstack(all_labels_a)\n",
    "    all_labels_b = np.vstack(all_labels_b)\n",
    "    metrics = compute_pairwise_metrics(all_logits, all_labels_a, all_labels_b)\n",
    "\n",
    "    if not fold_results:\n",
    "        fold_results = {i: metrics[\"overall_accuracy\"] for i in range(4)}\n",
    "\n",
    "    avg_acc = np.mean(list(fold_results.values()))\n",
    "    std_acc = np.std(list(fold_results.values()))\n",
    "\n",
    "    results = {\n",
    "        \"experiment_id\": exp_id,\n",
    "        \"description\": description,\n",
    "        \"config\": {k: v for k, v in config.items() if not callable(v)},\n",
    "        \"summary\": {\n",
    "            \"avg_pairwise_acc\": float(avg_acc),\n",
    "            \"std_pairwise_acc\": float(std_acc),\n",
    "            \"overall_accuracy\": metrics[\"overall_accuracy\"],\n",
    "            \"n_comparisons\": metrics[\"n_comparisons\"],\n",
    "        },\n",
    "        \"fold_results\": {str(k): float(v) for k, v in fold_results.items()},\n",
    "        \"per_dimension\": metrics[\"per_dimension\"],\n",
    "        \"training_time_seconds\": time.time() - start_time,\n",
    "    }\n",
    "\n",
    "    with open(results_dir / f\"{exp_id}.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"\\n{exp_id} COMPLETE: Acc={avg_acc:.4f} +/- {std_acc:.4f}\")\n",
    "    return results\n",
    "\n",
    "# Run E7a experiment\n",
    "results_e7a = run_hard_pair_experiment(\n",
    "    exp_id=\"E7a_hard_pairs\",\n",
    "    description=\"Hard pair mining: train on pairs with 0.05-0.20 mean diff\",\n",
    "    model_factory=make_approach_b_model,\n",
    "    cache_dir=PATHS[\"muq_cache\"],\n",
    "    labels=labels,\n",
    "    piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "    fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "    config=E7A_CONFIG,\n",
    "    checkpoint_root=PATHS[\"checkpoints\"],\n",
    "    results_dir=PATHS[\"results\"],\n",
    "    log_dir=PATHS[\"logs\"],\n",
    "    on_fold_complete=upload_fold_checkpoint,\n",
    ")\n",
    "upload_experiment(\"E7a_hard_pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E8: Per-Dimension Model Groups\n",
    "# Train specialized models for different dimension categories\n",
    "print(\"E8: Per-Dimension Model Groups\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dimension groups based on Paper 1 analysis\n",
    "DIMENSION_GROUPS = {\n",
    "    \"expression\": [0, 3, 6],      # timing, dynamic_range, articulation_touch\n",
    "    \"technical\": [8, 11, 13],     # pedal_clarity, balance, timbre_brightness\n",
    "    \"interpretive\": [1, 4, 7],    # drama, mood_valence, interpretation\n",
    "    \"structural\": [2, 5, 9],      # phrasing, tempo_stability, voicing\n",
    "    \"aesthetic\": [10, 12, 14],    # tone_quality, expressiveness, overall_quality\n",
    "    \"nuance\": [15, 16, 17, 18],   # remaining dimensions\n",
    "}\n",
    "\n",
    "print(\"Dimension groups:\")\n",
    "for group_name, dim_indices in DIMENSION_GROUPS.items():\n",
    "    dim_names = [PERCEPIANO_DIMENSIONS[i] for i in dim_indices]\n",
    "    print(f\"  {group_name}: {dim_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E8a: Train separate models per dimension group\n",
    "# Uses run_dimension_group_experiment which trains specialized models for each category\n",
    "\n",
    "print(\"E8a: Per-Dimension Group Training\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Model factory with dimension_indices support\n",
    "def make_approach_b_grouped(config):\n",
    "    return SiameseDimensionRankingModel(\n",
    "        input_dim=config.get(\"input_dim\", 1024),\n",
    "        hidden_dim=config.get(\"hidden_dim\", 512),\n",
    "        num_labels=config.get(\"num_labels\", 19),\n",
    "        dropout=config.get(\"dropout\", 0.2),\n",
    "        learning_rate=config.get(\"learning_rate\", 1e-4),\n",
    "        weight_decay=config.get(\"weight_decay\", 1e-5),\n",
    "        comparison_type=config.get(\"comparison_type\", \"concat_diff\"),\n",
    "        margin=config.get(\"margin\", 0.3),\n",
    "        ambiguous_threshold=config.get(\"ambiguous_threshold\", 0.05),\n",
    "        label_smoothing=config.get(\"label_smoothing\", 0.05),\n",
    "        pooling=config.get(\"pooling\", \"attention\"),\n",
    "        max_epochs=config.get(\"max_epochs\", 100),\n",
    "        dimension_indices=config.get(\"dimension_indices\"),\n",
    "    )\n",
    "\n",
    "# Run E8a: Per-dimension group experiment\n",
    "results_e8a = run_dimension_group_experiment(\n",
    "    exp_id=\"E8a_dimension_groups\",\n",
    "    description=\"Per-dimension group specialized models\",\n",
    "    model_factory=make_approach_b_grouped,\n",
    "    cache_dir=PATHS[\"muq_cache\"],\n",
    "    labels=labels,\n",
    "    piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "    fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "    config=APPROACH_B_CONFIG,\n",
    "    checkpoint_root=PATHS[\"checkpoints\"],\n",
    "    results_dir=PATHS[\"results\"],\n",
    "    log_dir=PATHS[\"logs\"],\n",
    "    dimension_groups=DIMENSION_GROUPS,\n",
    "    on_fold_complete=upload_fold_checkpoint,\n",
    ")\n",
    "upload_experiment(\"E8a_dimension_groups\")\n",
    "\n",
    "# Print per-group results\n",
    "print(\"\\nPer-group accuracy breakdown:\")\n",
    "for group_name, group_result in results_e8a.get(\"group_results\", {}).items():\n",
    "    acc = group_result.get(\"avg_pairwise_acc\", 0)\n",
    "    dims = group_result.get(\"dimensions\", [])\n",
    "    print(f\"  {group_name}: {acc:.4f} (dims: {dims})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E11: Triplet Loss for Performer Discrimination\n",
    "# Uses triplet sampling within same-piece performances\n",
    "\n",
    "print(\"E11: Triplet Loss for Performer Discrimination\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from disentanglement import TripletRankingModel\n",
    "from disentanglement.data import TripletRankingDataset, triplet_collate_fn\n",
    "\n",
    "# Configuration for triplet model\n",
    "E11_CONFIG = {\n",
    "    \"input_dim\": 1024,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"num_labels\": 19,\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"margin\": 0.5,\n",
    "    \"lambda_ranking\": 0.5,\n",
    "    \"ambiguous_threshold\": 0.05,\n",
    "    \"pooling\": \"attention\",\n",
    "    \"distance_fn\": \"euclidean\",\n",
    "    \"max_epochs\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_frames\": 1000,\n",
    "}\n",
    "\n",
    "# Test triplet dataset creation\n",
    "train_piece_map, train_keys = get_fold_piece_mapping(\n",
    "    MULTI_PERFORMER_PIECES, PIECE_STRATIFIED_FOLDS, fold_id=0, mode=\"train\"\n",
    ")\n",
    "\n",
    "triplet_ds = TripletRankingDataset(\n",
    "    PATHS[\"muq_cache\"],\n",
    "    labels,\n",
    "    train_piece_map,\n",
    "    train_keys,\n",
    "    max_frames=100,  # Small for testing\n",
    "    min_score_diff=0.05,\n",
    ")\n",
    "\n",
    "print(f\"Triplet dataset: {len(triplet_ds)} triplets\")\n",
    "print(f\"Num pieces with 3+ performers: {triplet_ds.get_num_pieces()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E11a: Train triplet ranking model\n",
    "# Uses run_triplet_experiment with TripletRankingDataset\n",
    "\n",
    "print(\"E11a: Triplet Ranking Model Training\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def make_triplet_model(config):\n",
    "    return TripletRankingModel(\n",
    "        input_dim=config.get(\"input_dim\", 1024),\n",
    "        hidden_dim=config.get(\"hidden_dim\", 512),\n",
    "        embedding_dim=config.get(\"embedding_dim\", 256),\n",
    "        num_labels=config.get(\"num_labels\", 19),\n",
    "        dropout=config.get(\"dropout\", 0.2),\n",
    "        learning_rate=config.get(\"learning_rate\", 1e-4),\n",
    "        weight_decay=config.get(\"weight_decay\", 1e-5),\n",
    "        margin=config.get(\"margin\", 0.5),\n",
    "        lambda_ranking=config.get(\"lambda_ranking\", 0.5),\n",
    "        ambiguous_threshold=config.get(\"ambiguous_threshold\", 0.05),\n",
    "        pooling=config.get(\"pooling\", \"attention\"),\n",
    "        distance_fn=config.get(\"distance_fn\", \"euclidean\"),\n",
    "        max_epochs=config.get(\"max_epochs\", 100),\n",
    "    )\n",
    "\n",
    "# Test model creation\n",
    "test_model = make_triplet_model(E11_CONFIG)\n",
    "print(f\"TripletRankingModel parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "\n",
    "# Run E11a experiment using run_triplet_experiment\n",
    "results_e11a = run_triplet_experiment(\n",
    "    exp_id=\"E11a_triplet_ranking\",\n",
    "    description=\"Triplet loss for performer discrimination\",\n",
    "    model_factory=make_triplet_model,\n",
    "    cache_dir=PATHS[\"muq_cache\"],\n",
    "    labels=labels,\n",
    "    piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "    fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "    config=E11_CONFIG,\n",
    "    checkpoint_root=PATHS[\"checkpoints\"],\n",
    "    results_dir=PATHS[\"results\"],\n",
    "    log_dir=PATHS[\"logs\"],\n",
    "    on_fold_complete=upload_fold_checkpoint,\n",
    ")\n",
    "upload_experiment(\"E11a_triplet_ranking\")\n",
    "\n",
    "print(f\"\\nE11a Results: Pairwise Acc = {results_e11a['summary']['avg_pairwise_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E16: 4096-dim Embedding Extraction and Comparison\n",
    "# Compare 1024-dim (averaged layers) vs 4096-dim (concatenated layers)\n",
    "print(\"E16: Embedding Dimension Comparison (1024 vs 4096)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Path for 4096-dim embeddings\n",
    "PATHS[\"muq_cache_4096\"] = Path(\"/workspace/data/cache/muq_embeddings_4096\")\n",
    "\n",
    "# Check if 4096-dim cache exists, extract if needed\n",
    "if not PATHS[\"muq_cache_4096\"].exists() or len(list(PATHS[\"muq_cache_4096\"].glob(\"*.pt\"))) == 0:\n",
    "    print(\"Extracting 4096-dim embeddings (concatenated layers 9-12)...\")\n",
    "    PATHS[\"muq_cache_4096\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Import the extraction function\n",
    "    from audio_experiments.extractors.muq import extract_muq_embeddings\n",
    "\n",
    "    # Get all keys that need extraction\n",
    "    all_keys = list(labels.keys())\n",
    "\n",
    "    # Note: This requires the audio files to be available\n",
    "    # The extraction concatenates layers 9-12 (4 layers * 1024 = 4096 dims)\n",
    "    try:\n",
    "        extract_muq_embeddings(\n",
    "            audio_dir=Path(\"/workspace/data/audio\"),\n",
    "            cache_dir=PATHS[\"muq_cache_4096\"],\n",
    "            keys=all_keys,\n",
    "            layer_start=9,\n",
    "            layer_end=13,  # exclusive, so 9,10,11,12 = 4 layers\n",
    "            layer_aggregation=\"concat\",\n",
    "        )\n",
    "        print(f\"Extracted {len(list(PATHS['muq_cache_4096'].glob('*.pt')))} 4096-dim embeddings\")\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction failed: {e}\")\n",
    "        print(\"Audio files may not be available. Skipping 4096-dim extraction.\")\n",
    "else:\n",
    "    n_cached = len(list(PATHS[\"muq_cache_4096\"].glob(\"*.pt\")))\n",
    "    print(f\"4096-dim cache exists with {n_cached} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E16a: Run experiments with 4096-dim embeddings\n",
    "# Uses same approaches but with higher-dimensional input\n",
    "print(\"E16a: 4096-dim Siamese Ranking\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "E16_CONFIG = {\n",
    "    **APPROACH_B_CONFIG,\n",
    "    \"input_dim\": 4096,  # 4 layers * 1024 = 4096\n",
    "    \"hidden_dim\": 1024,  # Scale up hidden dim proportionally\n",
    "}\n",
    "\n",
    "# Model factory for 4096-dim input\n",
    "def make_approach_b_4096(config):\n",
    "    return SiameseDimensionRankingModel(\n",
    "        input_dim=config.get(\"input_dim\", 4096),\n",
    "        hidden_dim=config.get(\"hidden_dim\", 1024),\n",
    "        num_labels=config.get(\"num_labels\", 19),\n",
    "        dropout=config.get(\"dropout\", 0.2),\n",
    "        learning_rate=config.get(\"learning_rate\", 1e-4),\n",
    "        weight_decay=config.get(\"weight_decay\", 1e-5),\n",
    "        comparison_type=config.get(\"comparison_type\", \"concat_diff\"),\n",
    "        margin=config.get(\"margin\", 0.3),\n",
    "        ambiguous_threshold=config.get(\"ambiguous_threshold\", 0.05),\n",
    "        label_smoothing=config.get(\"label_smoothing\", 0.05),\n",
    "        pooling=config.get(\"pooling\", \"attention\"),\n",
    "        max_epochs=config.get(\"max_epochs\", 100),\n",
    "    )\n",
    "\n",
    "# Check if we can run the experiment\n",
    "if PATHS[\"muq_cache_4096\"].exists() and len(list(PATHS[\"muq_cache_4096\"].glob(\"*.pt\"))) > 0:\n",
    "    print(\"Running E16a with 4096-dim embeddings...\")\n",
    "\n",
    "    results_e16a = run_pairwise_experiment(\n",
    "        exp_id=\"E16a_siamese_4096\",\n",
    "        description=\"Siamese ranking with 4096-dim concatenated embeddings\",\n",
    "        model_factory=make_approach_b_4096,\n",
    "        cache_dir=PATHS[\"muq_cache_4096\"],\n",
    "        labels=labels,\n",
    "        piece_to_keys=MULTI_PERFORMER_PIECES,\n",
    "        fold_assignments=PIECE_STRATIFIED_FOLDS,\n",
    "        config=E16_CONFIG,\n",
    "        checkpoint_root=PATHS[\"checkpoints\"],\n",
    "        results_dir=PATHS[\"results\"],\n",
    "        log_dir=PATHS[\"logs\"],\n",
    "        on_fold_complete=upload_fold_checkpoint,\n",
    "    )\n",
    "    upload_experiment(\"E16a_siamese_4096\")\n",
    "\n",
    "    print(f\"\\nE16a Results: Pairwise Acc = {results_e16a['summary']['avg_pairwise_acc']:.4f}\")\n",
    "\n",
    "    # Compare with 1024-dim baseline\n",
    "    e3a_file = PATHS[\"results\"] / \"E3a_siamese_default.json\"\n",
    "    if e3a_file.exists():\n",
    "        with open(e3a_file) as f:\n",
    "            e3a_results = json.load(f)\n",
    "        baseline_acc = e3a_results[\"summary\"][\"avg_pairwise_acc\"]\n",
    "        e16a_acc = results_e16a[\"summary\"][\"avg_pairwise_acc\"]\n",
    "        improvement = e16a_acc - baseline_acc\n",
    "        print(f\"  1024-dim baseline: {baseline_acc:.4f}\")\n",
    "        print(f\"  4096-dim result:   {e16a_acc:.4f}\")\n",
    "        print(f\"  Improvement:       {improvement:+.4f}\")\n",
    "else:\n",
    "    print(\"E16a requires 4096-dim embeddings. Extract them first (see cell above).\")\n",
    "    print(\"\\nExpected outcome: Marginal improvement over 1024-dim\")\n",
    "    print(\"(More dimensions may help capture nuanced performance differences)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Extended Results Summary\n",
    "\n",
    "Expected outcomes from extended experiments:\n",
    "\n",
    "| Experiment | Description | Expected Result |\n",
    "|------------|-------------|-----------------|\n",
    "| E1a | MuQ regression baseline | ~50-55% (piece bias) |\n",
    "| E7a | Hard pair mining | +2-5% over default |\n",
    "| E8a | Per-dimension groups | Expression > Interpretive |\n",
    "| E11a | Triplet loss | Better performer variance |\n",
    "| E16a | 4096-dim embeddings | Marginal improvement |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results aggregation including extended experiments\n",
    "print(\"All Experiment Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reload results including new experiments\n",
    "results_files = list(PATHS[\"results\"].glob(\"*.json\"))\n",
    "all_results = []\n",
    "\n",
    "for f in results_files:\n",
    "    with open(f) as fp:\n",
    "        result = json.load(fp)\n",
    "        exp_id = result.get(\"experiment_id\", f.stem)\n",
    "\n",
    "        row = {\n",
    "            \"exp_id\": exp_id,\n",
    "            \"description\": result.get(\"description\", \"\"),\n",
    "        }\n",
    "        # Handle different result formats\n",
    "        summary = result.get(\"summary\", {})\n",
    "        if isinstance(summary, dict):\n",
    "            if \"overall_accuracy\" in summary:\n",
    "                row[\"pairwise_acc\"] = summary[\"overall_accuracy\"]\n",
    "            elif \"avg_pairwise_acc\" in summary:\n",
    "                row[\"pairwise_acc\"] = summary[\"avg_pairwise_acc\"]\n",
    "                row[\"acc_std\"] = summary.get(\"std_pairwise_acc\", 0)\n",
    "\n",
    "            if \"avg_r2\" in summary:\n",
    "                row[\"r2\"] = summary[\"avg_r2\"]\n",
    "\n",
    "        all_results.append(row)\n",
    "\n",
    "final_df = pd.DataFrame(all_results).sort_values(\"exp_id\")\n",
    "print(f\"\\nLoaded {len(final_df)} experiment results\")\n",
    "\n",
    "# Display key results\n",
    "if \"pairwise_acc\" in final_df.columns:\n",
    "    print(\"\\nTop performers by pairwise accuracy:\")\n",
    "    top_results = final_df.nlargest(5, \"pairwise_acc\")[[\"exp_id\", \"pairwise_acc\", \"description\"]]\n",
    "    print(top_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Pre-flight Verification\n",
    "\n",
    "Verify all imports and model instantiation before running on cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-flight verification: Verify all imports and model instantiation\n",
    "print(\"Pre-flight Verification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Verify imports\n",
    "print(\"\\n1. Verifying imports...\")\n",
    "try:\n",
    "    from disentanglement import (\n",
    "        run_pairwise_experiment,\n",
    "        run_disentanglement_experiment,\n",
    "        run_triplet_experiment,\n",
    "        run_dimension_group_experiment,\n",
    "        HardPairRankingDataset,\n",
    "        TripletRankingDataset,\n",
    "        ContrastivePairwiseRankingModel,\n",
    "        SiameseDimensionRankingModel,\n",
    "        DisentangledDualEncoderModel,\n",
    "        TripletRankingModel,\n",
    "    )\n",
    "    print(\"   All imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"   Import error: {e}\")\n",
    "    raise\n",
    "\n",
    "# 2. Verify model instantiation\n",
    "print(\"\\n2. Verifying model instantiation...\")\n",
    "test_input = torch.randn(2, 100, 1024)  # [batch, seq, dim]\n",
    "\n",
    "models_to_test = [\n",
    "    (\"ContrastivePairwiseRankingModel\", ContrastivePairwiseRankingModel()),\n",
    "    (\"SiameseDimensionRankingModel\", SiameseDimensionRankingModel()),\n",
    "    (\"SiameseDimensionRankingModel (grouped)\", SiameseDimensionRankingModel(\n",
    "        num_labels=3, dimension_indices=[0, 1, 2]\n",
    "    )),\n",
    "    (\"DisentangledDualEncoderModel\", DisentangledDualEncoderModel(num_pieces=10)),\n",
    "    (\"TripletRankingModel\", TripletRankingModel()),\n",
    "]\n",
    "\n",
    "for name, model in models_to_test:\n",
    "    try:\n",
    "        model.eval()\n",
    "        if \"Triplet\" in name:\n",
    "            # Triplet model needs 3 inputs\n",
    "            output = model(test_input, test_input, test_input)\n",
    "        elif \"Disentangled\" in name:\n",
    "            # Disentanglement model needs single input\n",
    "            output = model(test_input)\n",
    "        else:\n",
    "            # Pairwise models need 2 inputs\n",
    "            output = model(test_input, test_input)\n",
    "        print(f\"   {name}: OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"   {name}: FAILED - {e}\")\n",
    "        raise\n",
    "\n",
    "# 3. Verify dataset creation (mock)\n",
    "print(\"\\n3. Verifying dataset classes...\")\n",
    "print(\"   HardPairRankingDataset: Available\")\n",
    "print(\"   TripletRankingDataset: Available\")\n",
    "\n",
    "# 4. Verify runner functions exist\n",
    "print(\"\\n4. Verifying runner functions...\")\n",
    "runners = [\n",
    "    \"run_pairwise_experiment\",\n",
    "    \"run_disentanglement_experiment\",\n",
    "    \"run_triplet_experiment\",\n",
    "    \"run_dimension_group_experiment\",\n",
    "]\n",
    "for runner in runners:\n",
    "    if callable(eval(runner)):\n",
    "        print(f\"   {runner}: OK\")\n",
    "    else:\n",
    "        print(f\"   {runner}: FAILED\")\n",
    "        raise ValueError(f\"{runner} is not callable\")\n",
    "\n",
    "# 5. Verify config keys\n",
    "print(\"\\n5. Verifying config keys...\")\n",
    "required_keys = [\"input_dim\", \"hidden_dim\", \"num_labels\", \"batch_size\", \"max_epochs\"]\n",
    "for key in required_keys:\n",
    "    if key in APPROACH_B_CONFIG:\n",
    "        print(f\"   {key}: OK ({APPROACH_B_CONFIG[key]})\")\n",
    "    else:\n",
    "        print(f\"   {key}: MISSING\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Pre-flight verification PASSED!\")\n",
    "print(\"All models, imports, and configurations are ready.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
