{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge, Validation, and Distillation\n",
    "\n",
    "Computes PercePiano composite labels, runs all 5 validation gates, and\n",
    "executes the LLM distillation pilot with calibration analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "COMPOSITE_DIR = Path(\"data/composite_labels\")\n",
    "PP_CACHE = Path(\"data/percepiano_cache\")\n",
    "MC_DIR = Path(\"data/masterclass_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load taxonomy and PercePiano labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_experiments.constants import PERCEPIANO_DIMENSIONS\n",
    "\n",
    "with open(COMPOSITE_DIR / \"dimension_definitions.json\") as f:\n",
    "    taxonomy = json.load(f)\n",
    "\n",
    "with open(PP_CACHE / \"labels.json\") as f:\n",
    "    raw_labels = json.load(f)\n",
    "\n",
    "pp_labels = {k: np.array(v[:19]) for k, v in raw_labels.items()}\n",
    "dim_index = {d: i for i, d in enumerate(PERCEPIANO_DIMENSIONS)}\n",
    "print(f\"Loaded {len(pp_labels)} PercePiano segments, {len(taxonomy['dimensions'])} taxonomy dims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dimension mapping and compute bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from masterclass_experiments.bridge import compute_weights, compute_composite_labels\n",
    "from masterclass_experiments.scoring import PERCEPIANO_MUQ_R2\n",
    "\n",
    "# Manual: map each taxonomy dimension to PercePiano dimensions\n",
    "# Based on config.rs percepiano_dims and cluster analysis\n",
    "DIM_MAPPING = {\n",
    "    # \"dynamics\": [\"dynamic_range\"],\n",
    "    # \"pedaling\": [\"pedal_amount\", \"pedal_clarity\"],\n",
    "    # ... fill in from taxonomy analysis\n",
    "}\n",
    "\n",
    "weights = compute_weights(DIM_MAPPING, PERCEPIANO_MUQ_R2)\n",
    "composites = compute_composite_labels(pp_labels, weights, dim_index)\n",
    "\n",
    "from masterclass_experiments.bridge import save_composite_labels\n",
    "save_composite_labels(composites, COMPOSITE_DIR / \"composite_labels.json\")\n",
    "print(f\"Computed composite labels for {len(composites)} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run validation gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from masterclass_experiments.validation import run_all_gates\n",
    "from masterclass_experiments.clustering import load_open_descriptions\n",
    "\n",
    "_, descriptions = load_open_descriptions(MC_DIR / \"open_moments.jsonl\")\n",
    "# Reload labels from taxonomy derivation\n",
    "# ... (load cluster labels from saved artifacts)\n",
    "\n",
    "# Build composite matrix [N_segments, N_dims] for independence check\n",
    "dim_names = list(DIM_MAPPING.keys())\n",
    "composite_matrix = np.array([\n",
    "    [composites[seg][d] for d in dim_names]\n",
    "    for seg in composites\n",
    "])\n",
    "\n",
    "# Load quote bank\n",
    "with open(COMPOSITE_DIR / \"quote_bank.json\") as f:\n",
    "    quote_bank = json.load(f)\n",
    "\n",
    "# STOP AUC from re-running masterclass evaluation with composite dims\n",
    "stop_auc = 0.0  # Fill in after running STOP experiment below\n",
    "\n",
    "report = run_all_gates(\n",
    "    labels=cluster_labels,  # from taxonomy notebook\n",
    "    stop_auc=stop_auc,\n",
    "    composite_matrix=composite_matrix,\n",
    "    quote_bank=quote_bank,\n",
    "    dim_names=dim_names,\n",
    ")\n",
    "\n",
    "print(f\"\\nAll gates passed: {report['all_passed']}\")\n",
    "for gate in report['gates']:\n",
    "    status = 'PASS' if gate['passed'] else 'FAIL'\n",
    "    print(f\"  [{status}] {gate['gate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP prediction with composite dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run STOP classifier using composite dimension scores instead of raw MuQ\n",
    "from masterclass_experiments.evaluation import leave_one_video_out_cv\n",
    "\n",
    "# Build feature matrix from composite labels of masterclass segments\n",
    "# ... (similar to existing masterclass experiment but using composite scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation pilot (if all gates pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from masterclass_experiments.distillation import (\n",
    "    build_rubric, build_scoring_prompt, parse_scores,\n",
    "    calibration_analysis, go_no_go,\n",
    ")\n",
    "\n",
    "with open(COMPOSITE_DIR / \"quote_bank.json\") as f:\n",
    "    quote_bank = json.load(f)\n",
    "\n",
    "# Build rubric\n",
    "rubric = build_rubric(taxonomy[\"dimensions\"], quote_bank)\n",
    "with open(COMPOSITE_DIR / \"teacher_rubric.json\", \"w\") as f:\n",
    "    json.dump(rubric, f, indent=2)\n",
    "print(\"Rubric built for dimensions:\", list(rubric.keys()))\n",
    "\n",
    "# Score T1 segments (requires API calls -- ~$36 at $0.03/segment)\n",
    "# See distillation.py for the scoring loop using openai SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After scoring all 1,202 T1 segments, run calibration\n",
    "# per_dim_results = {}\n",
    "# for dim in dim_names:\n",
    "#     teacher = np.array([scores[seg][dim] for seg in scores]) / 5.0  # normalize 1-5 to 0-1\n",
    "#     composite = np.array([composites[seg][dim] for seg in scores])\n",
    "#     per_dim_results[dim] = calibration_analysis(teacher, composite, dim)\n",
    "\n",
    "# decision = go_no_go(per_dim_results, stop_auc=..., spot_check_accuracy=...)\n",
    "# print(f\"Go/No-Go: {'GO' if decision['go'] else 'NO-GO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(COMPOSITE_DIR / \"validation_report.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "# if distillation ran:\n",
    "# with open(COMPOSITE_DIR / \"distillation_report.json\", \"w\") as f:\n",
    "#     json.dump({\"per_dim\": per_dim_results, \"decision\": decision}, f, indent=2)\n",
    "\n",
    "print(\"Reports saved to\", COMPOSITE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}