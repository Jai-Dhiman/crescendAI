{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priority Signal Validation Experiment\n",
    "\n",
    "Validates whether masterclass teaching moments (STOP/CONTINUE) can be predicted from audio.\n",
    "\n",
    "- **Model B:** Logistic regression on 19 PercePiano quality scores\n",
    "- **Model A:** Logistic regression on 2048-dim MuQ embeddings\n",
    "- **Evaluation:** Leave-one-video-out cross-validation (5 folds)\n",
    "\n",
    "Design doc: `docs/plans/2026-02-14-priority-signal-validation-design.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Add src to path\n",
    "MODEL_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(MODEL_ROOT / \"src\"))\n",
    "\n",
    "from masterclass_experiments.data import load_moments, identify_segments, extract_audio_segments\n",
    "from masterclass_experiments.features import extract_muq_features, extract_quality_scores, stats_pool\n",
    "from masterclass_experiments.evaluation import leave_one_video_out_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "REPO_ROOT = MODEL_ROOT.parent\nMOMENTS_PATH = REPO_ROOT / \"tools\" / \"masterclass-pipeline\" / \"all_moments.jsonl\"\nWAV_DIR = REPO_ROOT / \"tools\" / \"masterclass-pipeline\" / \"data\" / \"audio\"\nCACHE_DIR = MODEL_ROOT / \"data\" / \"masterclass_cache\"\nSEGMENT_DIR = CACHE_DIR / \"segments\"\nMUQ_CACHE_DIR = CACHE_DIR / \"muq_embeddings\"\nCHECKPOINT_PATH = MODEL_ROOT / \"data\" / \"checkpoints\" / \"percepiano\" / \"fold0_best.ckpt\"\n\nfor d in [SEGMENT_DIR, MUQ_CACHE_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Moments: {MOMENTS_PATH}\")\nprint(f\"WAV dir: {WAV_DIR}\")\nprint(f\"Checkpoint: {CHECKPOINT_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore moments\n",
    "moments = load_moments(MOMENTS_PATH)\n",
    "print(f\"Loaded {len(moments)} moments\")\n",
    "\n",
    "# Identify segments\n",
    "segments = identify_segments(moments)\n",
    "stop_segments = [s for s in segments if s.label == \"stop\"]\n",
    "cont_segments = [s for s in segments if s.label == \"continue\"]\n",
    "print(f\"STOP segments: {len(stop_segments)}\")\n",
    "print(f\"CONTINUE segments: {len(cont_segments)}\")\n",
    "\n",
    "# Duration stats\n",
    "for label, segs in [(\"STOP\", stop_segments), (\"CONTINUE\", cont_segments)]:\n",
    "    durations = [s.end_time - s.start_time for s in segs]\n",
    "    print(f\"{label}: mean={np.mean(durations):.1f}s, median={np.median(durations):.1f}s, \"\n",
    "          f\"min={np.min(durations):.1f}s, max={np.max(durations):.1f}s\")\n",
    "\n",
    "# Extract audio segments\n",
    "extract_audio_segments(segments, WAV_DIR, SEGMENT_DIR)\n",
    "print(f\"Audio segments saved to {SEGMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract raw MuQ embeddings once (used by both Model A and Model B)\nfrom audio_experiments.extractors.muq import MuQExtractor\n\nprint(\"Extracting MuQ embeddings...\")\nextractor = MuQExtractor(cache_dir=MUQ_CACHE_DIR)\nraw_embeddings = {}\nfor seg in segments:\n    wav_path = SEGMENT_DIR / f\"{seg.segment_id}.wav\"\n    raw_embeddings[seg.segment_id] = extractor.extract_from_file(wav_path)\nprint(f\"Extracted raw embeddings for {len(raw_embeddings)} segments\")\n\n# Stats pool for Model A features\nmuq_features = {sid: stats_pool(emb) for sid, emb in raw_embeddings.items()}\nprint(f\"Stats-pooled MuQ features: {next(iter(muq_features.values())).shape}\")\n\n# Run PercePiano model for Model B features\nprint(\"Running PercePiano inference...\")\nquality_scores = extract_quality_scores(raw_embeddings, CHECKPOINT_PATH)\nprint(f\"Extracted quality scores for {len(quality_scores)} segments\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_experiments.constants import PERCEPIANO_DIMENSIONS\n",
    "\n",
    "# Build feature matrices\n",
    "segment_ids = np.array([s.segment_id for s in segments])\n",
    "video_ids = np.array([s.video_id for s in segments])\n",
    "labels = np.array([1 if s.label == \"stop\" else 0 for s in segments])\n",
    "\n",
    "# Model B features: 19 quality scores\n",
    "X_quality = np.stack([quality_scores[sid].numpy() for sid in segment_ids])\n",
    "print(f\"Model B features: {X_quality.shape}\")\n",
    "\n",
    "# Model A features: 2048-dim MuQ embeddings\n",
    "X_muq = np.stack([muq_features[sid].numpy() for sid in segment_ids])\n",
    "print(f\"Model A features: {X_muq.shape}\")\n",
    "\n",
    "# Run leave-one-video-out CV\n",
    "print(\"\\n--- Model B (Quality Scores) ---\")\n",
    "results_b = leave_one_video_out_cv(X_quality, labels, video_ids, segment_ids)\n",
    "print(f\"AUC: {results_b['auc']:.3f}\")\n",
    "print(f\"Accuracy: {results_b['accuracy']:.3f}\")\n",
    "print(f\"Precision: {results_b['precision']:.3f}\")\n",
    "print(f\"Recall: {results_b['recall']:.3f}\")\n",
    "print(f\"Samples: {results_b['n_samples']} ({results_b['n_stop']} stop, {results_b['n_continue']} continue)\")\n",
    "\n",
    "print(\"\\n--- Model A (MuQ Embeddings) ---\")\n",
    "results_a = leave_one_video_out_cv(X_muq, labels, video_ids, segment_ids)\n",
    "print(f\"AUC: {results_a['auc']:.3f}\")\n",
    "print(f\"Accuracy: {results_a['accuracy']:.3f}\")\n",
    "print(f\"Precision: {results_a['precision']:.3f}\")\n",
    "print(f\"Recall: {results_a['recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nfrom masterclass_experiments.models import train_classifier\n\n# Comparison bar chart\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nmetrics = [\"auc\", \"accuracy\", \"precision\", \"recall\"]\nmodel_b_vals = [results_b[m] for m in metrics]\nmodel_a_vals = [results_a[m] for m in metrics]\n\nx = np.arange(len(metrics))\nwidth = 0.35\naxes[0].bar(x - width/2, model_b_vals, width, label=\"Model B (Quality)\")\naxes[0].bar(x + width/2, model_a_vals, width, label=\"Model A (MuQ)\")\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(metrics)\naxes[0].set_ylabel(\"Score\")\naxes[0].set_title(\"Model Comparison\")\naxes[0].legend()\naxes[0].axhline(y=0.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\naxes[0].set_ylim(0, 1)\n\n# Qualitative: worst false negatives (teacher stopped but model said continue)\nprint(\"\\n--- False Negatives (Model B) ---\")\nfn = [s for s in results_b[\"per_segment\"] if s[\"y_true\"] == 1 and s[\"y_pred_proba\"] < 0.5]\nfn.sort(key=lambda s: s[\"y_pred_proba\"])\nfor s in fn[:5]:\n    print(f\"  {s['segment_id']} (video {s['video_id']}): proba={s['y_pred_proba']:.3f}\")\n\nprint(\"\\n--- False Positives (Model B) ---\")\nfp = [s for s in results_b[\"per_segment\"] if s[\"y_true\"] == 0 and s[\"y_pred_proba\"] > 0.5]\nfp.sort(key=lambda s: -s[\"y_pred_proba\"])\nfor s in fp[:5]:\n    print(f\"  {s['segment_id']} (video {s['video_id']}): proba={s['y_pred_proba']:.3f}\")\n\n# Model B coefficient analysis (full-dataset refit for inspection only, not for reporting)\nprint(\"\\n--- Model B: Which quality dimensions predict STOP? ---\")\nfull_result = train_classifier(\n    X_quality, labels, list(range(len(labels))), list(range(len(labels)))\n)\ncoefs = full_result[\"coefficients\"]\nsorted_idx = np.argsort(np.abs(coefs))[::-1]\nfor i in sorted_idx:\n    print(f\"  {PERCEPIANO_DIMENSIONS[i]:25s}: {coefs[i]:+.3f}\")\n\naxes[1].barh(\n    range(19),\n    coefs[sorted_idx],\n    tick_label=[PERCEPIANO_DIMENSIONS[i] for i in sorted_idx],\n)\naxes[1].set_xlabel(\"Coefficient\")\naxes[1].set_title(\"Model B: Quality Dimension Importance\")\nplt.tight_layout()\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}