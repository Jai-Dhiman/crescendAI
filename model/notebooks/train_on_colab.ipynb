{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piano Performance Evaluation - Pseudo-Label Pre-training\n",
    "\n",
    "This notebook trains a baseline model on MAESTRO pseudo-labels.\n",
    "\n",
    "**Goal**: Production-ready baseline model to compare against future expert labels\n",
    "\n",
    "**Requirements:**\n",
    "- Colab Pro (recommended for T4/V100 GPU)\n",
    "- Google Drive for data and checkpoints\n",
    "- HuggingFace account for MERT model\n",
    "- Git repository pushed to GitHub\n",
    "\n",
    "**Note**: This notebook installs packages directly into Colab's system Python (no venv needed)\n",
    "\n",
    "---\n",
    "\n",
    "## Google Drive Setup\n",
    "\n",
    "Upload the MAESTRO dataset to your Google Drive in this structure:\n",
    "\n",
    "```\n",
    "MyDrive/\n",
    "  piano_eval_data/\n",
    "    maestro/                           # MAESTRO v3.0.0 dataset\n",
    "      maestro-v3.0.0.csv               # Metadata file (required!)\n",
    "      2004/\n",
    "        audio/\n",
    "        midi/\n",
    "      2006/\n",
    "        audio/\n",
    "        midi/\n",
    "      ...\n",
    "  piano_eval_checkpoints/              # Empty folder (checkpoints will be saved here)\n",
    "    pseudo_pretrain/\n",
    "```\n",
    "\n",
    "**Note**: Annotation files (JSONL) will be generated automatically in this notebook with correct paths. You only need the raw MAESTRO dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HF\n",
    "import os\n",
    "os.environ.pop(\"HF_TOKEN\", None)\n",
    "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
    "from huggingface_hub import login, HfApi\n",
    "try:\n",
    "    import getpass as gp\n",
    "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
    "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
    "    if not isinstance(token, str):\n",
    "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
    "    token = token.strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"Empty token provided\")\n",
    "    login(token=token, add_to_git_credential=False)\n",
    "    who = HfApi().whoami(token=token)\n",
    "    print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "except Exception as e:\n",
    "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
    "    print(\"Falling back to interactive login widget...\")\n",
    "    login()\n",
    "    try:\n",
    "        who = HfApi().whoami()\n",
    "        print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"[HF Login] Verification skipped: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for data and checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify data exists\n",
    "import os\n",
    "data_dir = '/content/drive/MyDrive/piano_eval_data'\n",
    "checkpoint_dir = '/content/drive/MyDrive/piano_eval_checkpoints'\n",
    "\n",
    "assert os.path.exists(data_dir), f\"Data directory not found: {data_dir}\"\n",
    "assert os.path.exists(checkpoint_dir), f\"Checkpoint directory not found: {checkpoint_dir}\"\n",
    "\n",
    "print(f\"✓ Data directory: {data_dir}\")\n",
    "print(f\"  Contents: {os.listdir(data_dir)}\")\n",
    "print(f\"✓ Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL = \"https://github.com/Jai-Dhiman/crescendai.git\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "# Remove old clone if exists\n",
    "!rm -rf /content/crescendai\n",
    "\n",
    "# Clone fresh\n",
    "!git clone --branch {BRANCH} {REPO_URL} /content/crescendai\n",
    "\n",
    "# Navigate to model directory\n",
    "%cd /content/crescendai/model\n",
    "\n",
    "# Show git status\n",
    "!git log -1 --oneline\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv (fast Python package manager)\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add to PATH for this session\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies directly into system Python (no venv needed in Colab)\n",
    "# Using uv pip for faster installation\n",
    "!uv pip install --system -e .\n",
    "\n",
    "# Verify installation\n",
    "import os\n",
    "os.environ['MPLBACKEND'] = 'Agg'\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning\n",
    "\n",
    "print(f\"Dependencies installed\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pytorch_lightning.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU and PyTorch setup\nimport torch\nimport pytorch_lightning as pl\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Lightning version: {pl.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(f\"\\n✓ GPU ready for training\")\nelse:\n    print(\"\\n\" + \"=\"*70)\n    print(\"⚠️  CRITICAL: NO GPU DETECTED!\")\n    print(\"=\"*70)\n    print(\"\\nTraining on CPU will be EXTREMELY SLOW (200x slower than GPU).\")\n    print(\"\\nTO ENABLE GPU:\")\n    print(\"1. Go to: Runtime → Change runtime type\")\n    print(\"2. Set 'Hardware accelerator' to: T4 GPU (or V100)\")\n    print(\"3. Click 'Save'\")\n    print(\"4. Re-run all cells from the beginning\")\n    print(\"\\nDO NOT proceed with training until GPU is enabled!\")\n    print(\"=\"*70)\n    raise RuntimeError(\"GPU required for training. Please enable GPU and restart.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MERT model download (this will cache the model)\n",
    "from transformers import AutoModel\n",
    "\n",
    "print(\"Downloading MERT-95M model (one-time, ~380MB)...\")\n",
    "model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\n",
    "print(f\"✓ MERT-95M loaded: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
    "del model  # Free memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify MAESTRO dataset exists in Google Drive\n",
    "import os\n",
    "\n",
    "maestro_root = f'{data_dir}/maestro'\n",
    "\n",
    "# Check MAESTRO dataset\n",
    "assert os.path.exists(maestro_root), f\"MAESTRO dataset not found at: {maestro_root}\"\n",
    "assert os.path.exists(f'{maestro_root}/metadata.csv'), f\"maestro-v3.0.0.csv not found! Make sure you uploaded the complete MAESTRO dataset.\"\n",
    "\n",
    "print(f\"✓ MAESTRO dataset found at: {maestro_root}\")\n",
    "print(f\"  Contents: {os.listdir(maestro_root)[:10]}\")  # Show first 10 items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Pseudo-Label Annotations\n",
    "\n",
    "This will create annotation files with correct Google Drive paths. Takes ~15-30 minutes to process 50 pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Output path for combined annotations (temporary)\n",
    "temp_annotations = '/tmp/maestro_pseudo_labels_all.jsonl'\n",
    "\n",
    "# Check if annotations already exist (skip regeneration if so)\n",
    "train_path = f'{data_dir}/maestro_pseudo_labels_train.jsonl'\n",
    "val_path = f'{data_dir}/maestro_pseudo_labels_val.jsonl'\n",
    "\n",
    "if os.path.exists(train_path) and os.path.exists(val_path):\n",
    "    print(\"✓ Annotation files already exist, skipping generation.\")\n",
    "    print(f\"  Train: {train_path}\")\n",
    "    print(f\"  Val: {val_path}\")\n",
    "    print(\"\\nIf you want to regenerate, delete these files from Google Drive and re-run this cell.\")\n",
    "else:\n",
    "    print(\"Generating pseudo-labels for MAESTRO dataset...\")\n",
    "    print(\"Processing ALL 238 pieces (will take ~2-3 hours).\\n\")\n",
    "    \n",
    "    # Generate pseudo-labels for ALL pieces (no limit)\n",
    "    !python scripts/generate_pseudo_labels.py \\\n",
    "      --maestro-root {maestro_root} \\\n",
    "      --output {temp_annotations} \\\n",
    "      --metadata-csv {maestro_root}/metadata.csv \\\n",
    "      --segment-duration 20.0 \\\n",
    "      --segment-overlap 5.0\n",
    "    \n",
    "    print(\"\\n✓ Pseudo-label generation complete!\")\n",
    "    \n",
    "    # Split annotations into train/val sets (80/20 split)\n",
    "    print(\"\\nSplitting annotations into train/val sets...\")\n",
    "    !python scripts/split_annotations.py \\\n",
    "      --input {temp_annotations} \\\n",
    "      --train-output {train_path} \\\n",
    "      --val-output {val_path} \\\n",
    "      --train-ratio 0.8 \\\n",
    "      --val-ratio 0.2 \\\n",
    "      --seed 42\n",
    "    \n",
    "    print(\"\\n✓ Train/val split complete!\")\n",
    "    print(f\"  Train: {train_path}\")\n",
    "    print(f\"  Val: {val_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training configuration for Colab\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Define config with Google Drive paths\n",
    "config = {\n",
    "    'data': {\n",
    "        'train_path': f'{data_dir}/maestro_pseudo_labels_train.jsonl',\n",
    "        'val_path': f'{data_dir}/maestro_pseudo_labels_val.jsonl',\n",
    "        'test_path': None,\n",
    "        'dimensions': [\n",
    "            'note_accuracy',\n",
    "            'rhythmic_precision',\n",
    "            'dynamics_control',\n",
    "            'articulation',\n",
    "            'pedaling',\n",
    "            'tone_quality'\n",
    "        ],\n",
    "        'audio_sample_rate': 24000,\n",
    "        'max_audio_length': 240000,\n",
    "        'max_midi_events': 512,\n",
    "        'batch_size': 8,\n",
    "        'num_workers': 2,\n",
    "        'pin_memory': True,\n",
    "        'augmentation': {\n",
    "            'enabled': True,\n",
    "            'pitch_shift': {\n",
    "                'enabled': True,\n",
    "                'probability': 0.3,\n",
    "                'min_semitones': -2,\n",
    "                'max_semitones': 2\n",
    "            },\n",
    "            'time_stretch': {\n",
    "                'enabled': True,\n",
    "                'probability': 0.3,\n",
    "                'min_rate': 0.85,\n",
    "                'max_rate': 1.15\n",
    "            },\n",
    "            'add_noise': {\n",
    "                'enabled': True,\n",
    "                'probability': 0.2,\n",
    "                'min_snr_db': 25,\n",
    "                'max_snr_db': 40\n",
    "            },\n",
    "            'room_acoustics': {\n",
    "                'enabled': True,\n",
    "                'probability': 0.2,\n",
    "                'num_room_types': 5\n",
    "            },\n",
    "            'compress_audio': {\n",
    "                'enabled': True,\n",
    "                'probability': 0.15,\n",
    "                'bitrates': [128, 192, 256, 320]\n",
    "            },\n",
    "            'gain_variation': {\n",
    "                'enabled': True,\n",
    "                'probability': 0.3,\n",
    "                'min_db': -6,\n",
    "                'max_db': 6\n",
    "            },\n",
    "            'max_transforms': 3\n",
    "        }\n",
    "    },\n",
    "    'model': {\n",
    "        'audio_dim': 768,\n",
    "        'midi_dim': 256,\n",
    "        'fusion_dim': 1024,\n",
    "        'aggregator_dim': 512,\n",
    "        'num_dimensions': 6,\n",
    "        'mert_model_name': 'm-a-p/MERT-v1-95M',\n",
    "        'freeze_audio_encoder': False,\n",
    "        'gradient_checkpointing': True,\n",
    "        'midi_hidden_size': 256,\n",
    "        'midi_num_layers': 6,\n",
    "        'midi_num_heads': 4,\n",
    "        'fusion_num_heads': 8,\n",
    "        'fusion_dropout': 0.1,\n",
    "        'lstm_hidden': 256,\n",
    "        'lstm_layers': 2,\n",
    "        'attention_heads': 4,\n",
    "        'aggregator_dropout': 0.2,\n",
    "        'shared_hidden': 256,\n",
    "        'task_hidden': 128,\n",
    "        'mtl_dropout': 0.1\n",
    "    },\n",
    "    'training': {\n",
    "        'max_epochs': 20,\n",
    "        'precision': 16,\n",
    "        'optimizer': 'AdamW',\n",
    "        'learning_rate': 1e-5,\n",
    "        'backbone_lr': 1e-5,\n",
    "        'heads_lr': 1e-4,\n",
    "        'weight_decay': 0.01,\n",
    "        'scheduler': 'cosine',\n",
    "        'warmup_steps': 500,\n",
    "        'min_lr': 1e-6,\n",
    "        'gradient_clip_val': 1.0,\n",
    "        'accumulate_grad_batches': 4,\n",
    "        'val_check_interval': 1.0,\n",
    "        'limit_val_batches': 1.0\n",
    "    },\n",
    "    'callbacks': {\n",
    "        'checkpoint': {\n",
    "            'monitor': 'val_loss',\n",
    "            'mode': 'min',\n",
    "            'save_top_k': 3,\n",
    "            'save_last': True,\n",
    "            'dirpath': f'{checkpoint_dir}/pseudo_pretrain',\n",
    "            'filename': 'pseudo-{epoch:02d}-{val_loss:.4f}'\n",
    "        },\n",
    "        'early_stopping': {\n",
    "            'monitor': 'val_loss',\n",
    "            'mode': 'min',\n",
    "            'patience': 5,\n",
    "            'min_delta': 0.001\n",
    "        },\n",
    "        'lr_monitor': {\n",
    "            'logging_interval': 'step'\n",
    "        }\n",
    "    },\n",
    "    'logging': {\n",
    "        'log_every_n_steps': 50,\n",
    "        'use_wandb': False,\n",
    "        'wandb_project': 'piano-eval-mvp',\n",
    "        'wandb_entity': None,\n",
    "        'wandb_run_name': 'pseudo-pretrain',\n",
    "        'use_tensorboard': True,\n",
    "        'tensorboard_logdir': f'{checkpoint_dir}/logs'\n",
    "    },\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Save config to temporary file\n",
    "colab_config_path = '/tmp/colab_config.yaml'\n",
    "with open(colab_config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"✓ Training configuration created: {colab_config_path}\")\n",
    "print(\"\\nConfiguration summary:\")\n",
    "print(f\"  Batch size: {config['data']['batch_size']}\")\n",
    "print(f\"  Gradient accumulation: {config['training']['accumulate_grad_batches']}\")\n",
    "print(f\"  Effective batch size: {config['data']['batch_size'] * config['training']['accumulate_grad_batches']}\")\n",
    "print(f\"  Max epochs: {config['training']['max_epochs']}\")\n",
    "print(f\"  Checkpoint dir: {config['callbacks']['checkpoint']['dirpath']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Run MIDI diagnostics on first 5 files to identify the issue\n!python scripts/diagnose_midi.py {data_dir}/maestro_pseudo_labels_train.jsonl --num 5",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## MIDI Diagnostics (Run if MIDI loading fails)\n\nIf you see many MIDI loading failures, run this diagnostic to identify the root cause:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pull latest code fixes (augmentation config parsing, MIDI loading, diagnostics)\n%cd /content/crescendai\n!git pull origin main\n%cd /content/crescendai/model\n\n# Show latest changes\n!git log -3 --oneline"
  },
  {
   "cell_type": "markdown",
   "source": "## Diagnostic Tools (Run if training issues occur)\n\nThese tools help diagnose training problems before running full training:\n- **Full diagnostics**: Checks data, model outputs, gradients, and parameter updates\n- **Single-dimension training**: Tests basic learning on just one dimension (5 epochs, ~30-60 min)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run full diagnostics to check for issues\n# This analyzes data, model outputs, gradients, and parameter updates WITHOUT running full training\nprint(\"Running comprehensive diagnostics...\")\nprint(\"This will take 2-3 minutes.\\n\")\n\n!python scripts/diagnose_training.py --config {colab_config_path}\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Review the diagnostics above. Look for:\")\nprint(\"  ✓ Label distributions look reasonable (mean ~50-70, std > 5)\")\nprint(\"  ✓ Predictions in [0, 100] range\")\nprint(\"  ✓ Gradients not too small (> 1e-6) or too large (< 100)\")\nprint(\"  ✓ Parameters are updating (changes > 1e-10)\")\nprint(\"\\nIf you see warnings, check DIAGNOSTICS.md for solutions\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Single-Dimension Diagnostic Training (Optional)\n\nIf diagnostics show issues or you want to test basic learning, run this simplified training:\n- Only trains on **note_accuracy** dimension (1 of 6)\n- Only 5 epochs (~30-60 min on T4)\n- Higher learning rates for faster convergence\n- No augmentation to isolate issues\n\n**Success criteria**:\n- Loss decreases over epochs\n- MAE < 15 by epoch 5\n- Pearson r > 0.3 by epoch 5",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create diagnostic config (single dimension, 5 epochs)\ndiagnostic_config = config.copy()\ndiagnostic_config['data']['dimensions'] = ['note_accuracy']  # Only one dimension\ndiagnostic_config['data']['augmentation'] = None  # Disable augmentation\ndiagnostic_config['model']['num_dimensions'] = 1\ndiagnostic_config['training']['max_epochs'] = 5\ndiagnostic_config['training']['backbone_lr'] = 2e-5  # Slightly higher for faster learning\ndiagnostic_config['training']['heads_lr'] = 2e-4\ndiagnostic_config['training']['warmup_steps'] = 100  # Shorter warmup\ndiagnostic_config['training']['accumulate_grad_batches'] = 2\ndiagnostic_config['callbacks']['checkpoint']['dirpath'] = f'{checkpoint_dir}/diagnostic_pretrain'\ndiagnostic_config['callbacks']['early_stopping']['patience'] = 3\ndiagnostic_config['logging']['tensorboard_logdir'] = f'{checkpoint_dir}/logs/diagnostic'\n\n# Save diagnostic config\ndiagnostic_config_path = '/tmp/diagnostic_config.yaml'\nwith open(diagnostic_config_path, 'w') as f:\n    yaml.dump(diagnostic_config, f, default_flow_style=False, sort_keys=False)\n\nprint(\"✓ Diagnostic training configuration created\")\nprint(\"\\nDiagnostic training settings:\")\nprint(f\"  Dimensions: {diagnostic_config['data']['dimensions']}\")\nprint(f\"  Epochs: {diagnostic_config['training']['max_epochs']}\")\nprint(f\"  Augmentation: Disabled\")\nprint(f\"  Backbone LR: {diagnostic_config['training']['backbone_lr']}\")\nprint(f\"  Heads LR: {diagnostic_config['training']['heads_lr']}\")\nprint(\"\\nRun diagnostic training (uncomment to execute):\")\nprint(f\"# !python train.py --config {diagnostic_config_path}\")\n\n# Uncomment the line below to run diagnostic training\n# !python train.py --config {diagnostic_config_path}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Note on MIDI Loading Warnings**\n\nYou may see warnings like \"Failed to load MIDI for ... all the input arrays must have same number of dimensions\". This is expected for a small number of malformed MIDI files in the MAESTRO dataset.\n\nThe system handles this gracefully:\n- Failed MIDI files are skipped (audio-only processing)\n- Fusion layer falls back to audio-only mode with zero-padded MIDI features\n- Training continues normally without interruption\n\nTypical failure rate: ~2-5 files out of 1,868 segments.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Training\n",
    "!python train.py --config {colab_config_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "import sys\n",
    "sys.path.insert(0, '/content/crescendai/model')\n",
    "\n",
    "from src.models.lightning_module import PerformanceEvaluationModel\n",
    "\n",
    "# Find best checkpoint\n",
    "checkpoint_path = f'{checkpoint_dir}/pseudo_pretrain'\n",
    "checkpoints = [f for f in os.listdir(checkpoint_path) if f.endswith('.ckpt') and not f.startswith('last')]\n",
    "best_ckpt = sorted(checkpoints)[0]  # First by name (lowest val_loss in filename)\n",
    "best_ckpt_path = os.path.join(checkpoint_path, best_ckpt)\n",
    "\n",
    "print(f\"Loading best checkpoint: {best_ckpt}\")\n",
    "model = PerformanceEvaluationModel.load_from_checkpoint(best_ckpt_path)\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "print(f\"\\nModel loaded successfully\")\n",
    "print(f\"  Dimensions: {model.dimension_names}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Session Disconnected\n",
    "- Re-run cells 1-2 (mount Drive, clone repo)\n",
    "- Re-run cell 4 (training) - will automatically resume from last checkpoint\n",
    "- All checkpoints are in Google Drive (persistent)\n",
    "\n",
    "### Out of Memory (OOM)\n",
    "- Reduce batch size in config: `config['data']['batch_size'] = 4`\n",
    "- Increase gradient accumulation: `config['training']['accumulate_grad_batches'] = 8`\n",
    "- This keeps effective batch size = 4 × 8 = 32\n",
    "\n",
    "### Slow Training\n",
    "- Check you have T4 or better GPU (not K80)\n",
    "- Verify data is in Google Drive (not Colab Files)\n",
    "- Check num_workers: `config['data']['num_workers'] = 2` (lower if I/O bottleneck)\n",
    "\n",
    "### MERT Download Fails\n",
    "- Verify HuggingFace authentication\n",
    "- Check internet connection\n",
    "- Try manual download: `huggingface-cli download m-a-p/MERT-v1-95M`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}