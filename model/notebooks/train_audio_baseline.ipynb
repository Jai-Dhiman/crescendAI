{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# Audio Baseline for PercePiano (MERT-330M)\n",
    "\n",
    "This notebook establishes the **first audio baseline** for piano performance evaluation using the PercePiano dataset with MERT embeddings.\n",
    "\n",
    "## Research Question\n",
    "\n",
    "> Can audio representations (MERT) capture perceptual dimensions that symbolic MIDI representations miss?\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "Based on our research (RESEARCH_v4.md), we expect audio to excel on:\n",
    "- **Timbre dimensions** (variety, depth, brightness) - MIDI cannot capture overtones/harmonics\n",
    "- **Pedal dimensions** (amount, clarity) - Pedal effects are acoustic phenomena\n",
    "- **Emotional dimensions** (mood_energy, mood_imagination) - Require timbral context\n",
    "\n",
    "## Attribution\n",
    "\n",
    "> **PercePiano: Piano Performance Evaluation Dataset with Multi-level Perceptual Features**  \n",
    "> Park, Kim et al., Nature Scientific Reports 2024  \n",
    "> Paper: https://pmc.ncbi.nlm.nih.gov/articles/PMC11450231/\n",
    "\n",
    "> **MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training**  \n",
    "> Li et al., ICLR 2024  \n",
    "> HuggingFace: https://huggingface.co/m-a-p/MERT-v1-330M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-goals",
   "metadata": {},
   "source": [
    "## Success Criteria and Targets\n",
    "\n",
    "### Phase 1: Audio Rendering\n",
    "| Metric | Target | Notes |\n",
    "|--------|--------|-------|\n",
    "| Files rendered | 1,202 | All PercePiano segments |\n",
    "| Audio quality | 44.1kHz, 16-bit | Standard CD quality |\n",
    "| Rendering time | < 2 hours | Batch processing |\n",
    "\n",
    "### Phase 2: MERT Extraction\n",
    "| Metric | Target | Notes |\n",
    "|--------|--------|-------|\n",
    "| Embeddings extracted | 1,202 | All segments |\n",
    "| Embedding dimension | 1024 | MERT-330M hidden size |\n",
    "| GPU memory | < 8GB | Fits on T4/consumer GPU |\n",
    "\n",
    "### Phase 3: Baseline Model\n",
    "| Metric | Target | Stretch | Notes |\n",
    "|--------|--------|---------|-------|\n",
    "| Overall R2 | >= 0.25 | >= 0.35 | Competitive with symbolic |\n",
    "| Timbre dims R2 | >= 0.30 | >= 0.40 | Audio should excel here |\n",
    "| Pedal dims R2 | >= 0.25 | >= 0.35 | Pedal is acoustic phenomenon |\n",
    "\n",
    "### Phase 4: Comparison to Symbolic\n",
    "| Dimension Category | Expected Winner | Hypothesis |\n",
    "|-------------------|-----------------|------------|\n",
    "| Timing, Articulation | Symbolic | Precise onset/offset in MIDI |\n",
    "| Timbre (4 dims) | **Audio** | MIDI has no timbre info |\n",
    "| Pedal (2 dims) | **Audio** | Acoustic resonance effects |\n",
    "| Dynamics | Tie | Both capture velocity/loudness |\n",
    "| Emotion/Mood | Audio | Requires timbral context |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gpu-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. MERT extraction will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab/remote\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install uv\n",
    "    !curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "    os.environ['PATH'] = f\"{os.environ['HOME']}/.cargo/bin:{os.environ['PATH']}\"\n",
    "    \n",
    "    # Clone repository\n",
    "    if not os.path.exists('/tmp/crescendai'):\n",
    "        !git clone https://github.com/Jai-Dhiman/crescendai.git /tmp/crescendai\n",
    "    %cd /tmp/crescendai/model\n",
    "    !git pull\n",
    "    !uv pip install --system -e .\n",
    "\n",
    "# Install audio-specific dependencies\n",
    "!pip install transformers librosa soundfile torchaudio rich\n",
    "\n",
    "# Check FluidSynth installation\n",
    "result = subprocess.run(['which', 'fluidsynth'], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"FluidSynth found: {result.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"FluidSynth not found. Install with:\")\n",
    "    print(\"  macOS: brew install fluidsynth\")\n",
    "    print(\"  Ubuntu: apt-get install fluidsynth\")\n",
    "    print(\"  Colab: apt-get install -y fluidsynth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "console = Console()\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-paths-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Detect environment\n",
    "if IN_COLAB:\n",
    "    PROJECT_ROOT = Path('/tmp/crescendai/model')\n",
    "else:\n",
    "    # Local development\n",
    "    PROJECT_ROOT = Path('.').resolve().parent\n",
    "    if not (PROJECT_ROOT / 'src').exists():\n",
    "        PROJECT_ROOT = Path('/Users/jdhiman/Documents/crescendai/model')\n",
    "\n",
    "# Data paths\n",
    "PERCEPIANO_ROOT = PROJECT_ROOT / 'data' / 'raw' / 'PercePiano'\n",
    "MIDI_DIR = PERCEPIANO_ROOT / 'virtuoso' / 'data' / 'all_2rounds'\n",
    "LABEL_FILE = PERCEPIANO_ROOT / 'label_2round_mean_reg_19_with0_rm_highstd0.json'\n",
    "\n",
    "# Audio output paths\n",
    "AUDIO_DIR = PROJECT_ROOT / 'data' / 'audio' / 'percepiano_rendered'\n",
    "MERT_CACHE_DIR = PROJECT_ROOT / 'data' / 'cache' / 'mert_embeddings'\n",
    "SOUNDFONT_PATH = PROJECT_ROOT / 'data' / 'soundfonts' / 'SalamanderGrandPiano.sf2'\n",
    "\n",
    "# Checkpoint paths\n",
    "CHECKPOINT_ROOT = Path('/tmp/checkpoints/audio_baseline')\n",
    "LOG_ROOT = Path('/tmp/logs/audio_baseline')\n",
    "\n",
    "# Symbolic baseline results (for comparison)\n",
    "SYMBOLIC_RESULTS_PATH = PROJECT_ROOT / 'data' / 'cache' / 'symbolic_baseline_results.json'\n",
    "\n",
    "# Create directories\n",
    "AUDIO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MERT_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "LOG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SOUNDFONT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AUDIO BASELINE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"MIDI source: {MIDI_DIR}\")\n",
    "print(f\"Audio output: {AUDIO_DIR}\")\n",
    "print(f\"MERT cache: {MERT_CACHE_DIR}\")\n",
    "print(f\"Soundfont: {SOUNDFONT_PATH}\")\n",
    "\n",
    "# Verify paths\n",
    "if MIDI_DIR.exists():\n",
    "    midi_count = len(list(MIDI_DIR.glob('*.mid')))\n",
    "    print(f\"\\nMIDI files found: {midi_count}\")\n",
    "else:\n",
    "    print(f\"\\n[ERROR] MIDI directory not found: {MIDI_DIR}\")\n",
    "\n",
    "if LABEL_FILE.exists():\n",
    "    with open(LABEL_FILE) as f:\n",
    "        labels = json.load(f)\n",
    "    print(f\"Labels found: {len(labels)} segments\")\n",
    "else:\n",
    "    print(f\"[ERROR] Label file not found: {LABEL_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-rendering-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Audio Rendering Pipeline\n",
    "\n",
    "### Goal: Render all 1,202 MIDI segments to high-quality audio\n",
    "\n",
    "We use FluidSynth with the Salamander Grand Piano soundfont:\n",
    "- **Why FluidSynth?** Free, scriptable, good quality for research\n",
    "- **Why Salamander?** Best free piano soundfont, recorded from Yamaha C5\n",
    "- **Alternative**: Pianoteq (better half-pedal, costs $$$)\n",
    "\n",
    "### Success Criteria\n",
    "- All 1,202 segments rendered\n",
    "- 44.1kHz, 16-bit WAV format\n",
    "- < 2 hours total rendering time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-download-soundfont",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Salamander Grand Piano soundfont if not present\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "SOUNDFONT_URL = \"https://freepats.zenvoid.org/Piano/SalamanderGrandPiano/SalamanderGrandPianoV3+20161209.tar.xz\"\n",
    "\n",
    "if not SOUNDFONT_PATH.exists():\n",
    "    print(\"Downloading Salamander Grand Piano soundfont...\")\n",
    "    print(\"(This is ~400MB, may take a few minutes)\")\n",
    "    \n",
    "    tar_path = SOUNDFONT_PATH.parent / \"salamander.tar.xz\"\n",
    "    \n",
    "    # Download\n",
    "    urllib.request.urlretrieve(SOUNDFONT_URL, tar_path)\n",
    "    print(f\"Downloaded to {tar_path}\")\n",
    "    \n",
    "    # Extract\n",
    "    print(\"Extracting...\")\n",
    "    subprocess.run(['tar', '-xf', str(tar_path), '-C', str(SOUNDFONT_PATH.parent)], check=True)\n",
    "    \n",
    "    # Find the .sf2 file\n",
    "    sf2_files = list(SOUNDFONT_PATH.parent.rglob('*.sf2'))\n",
    "    if sf2_files:\n",
    "        # Move to expected location\n",
    "        sf2_files[0].rename(SOUNDFONT_PATH)\n",
    "        print(f\"Soundfont ready: {SOUNDFONT_PATH}\")\n",
    "    else:\n",
    "        print(\"[ERROR] No .sf2 file found in archive\")\n",
    "    \n",
    "    # Cleanup\n",
    "    tar_path.unlink(missing_ok=True)\n",
    "else:\n",
    "    print(f\"Soundfont already exists: {SOUNDFONT_PATH}\")\n",
    "    print(f\"Size: {SOUNDFONT_PATH.stat().st_size / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-render-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def render_midi_to_wav(\n",
    "    midi_path: Path,\n",
    "    wav_path: Path,\n",
    "    soundfont_path: Path,\n",
    "    sample_rate: int = 44100,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Render a MIDI file to WAV using FluidSynth.\n",
    "    \n",
    "    Args:\n",
    "        midi_path: Path to input MIDI file\n",
    "        wav_path: Path to output WAV file\n",
    "        soundfont_path: Path to .sf2 soundfont\n",
    "        sample_rate: Output sample rate (default 44100)\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wav_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            [\n",
    "                'fluidsynth',\n",
    "                '-ni',                    # Non-interactive\n",
    "                str(soundfont_path),      # Soundfont\n",
    "                str(midi_path),           # MIDI file\n",
    "                '-F', str(wav_path),      # Output file\n",
    "                '-r', str(sample_rate),   # Sample rate\n",
    "                '-g', '0.8',              # Gain (avoid clipping)\n",
    "            ],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=60,  # 1 minute timeout per file\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error rendering {midi_path.name}: {result.stderr}\")\n",
    "            return False\n",
    "        \n",
    "        return wav_path.exists()\n",
    "    \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"Timeout rendering {midi_path.name}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Exception rendering {midi_path.name}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def batch_render_midi(\n",
    "    midi_dir: Path,\n",
    "    output_dir: Path,\n",
    "    soundfont_path: Path,\n",
    "    label_keys: List[str],\n",
    "    max_workers: int = 4,\n",
    "    skip_existing: bool = True,\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Batch render MIDI files to WAV.\n",
    "    \n",
    "    Args:\n",
    "        midi_dir: Directory containing MIDI files\n",
    "        output_dir: Directory for output WAV files\n",
    "        soundfont_path: Path to soundfont\n",
    "        label_keys: List of segment keys (to match labels)\n",
    "        max_workers: Number of parallel workers\n",
    "        skip_existing: Skip files that already exist\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (successful, failed) counts\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Build list of files to render\n",
    "    to_render = []\n",
    "    for key in label_keys:\n",
    "        midi_path = midi_dir / f\"{key}.mid\"\n",
    "        wav_path = output_dir / f\"{key}.wav\"\n",
    "        \n",
    "        if skip_existing and wav_path.exists():\n",
    "            continue\n",
    "        \n",
    "        if midi_path.exists():\n",
    "            to_render.append((midi_path, wav_path))\n",
    "        else:\n",
    "            print(f\"MIDI not found: {key}\")\n",
    "    \n",
    "    print(f\"Files to render: {len(to_render)}\")\n",
    "    print(f\"Already rendered: {len(label_keys) - len(to_render)}\")\n",
    "    \n",
    "    if not to_render:\n",
    "        return len(label_keys), 0\n",
    "    \n",
    "    # Render in parallel\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                render_midi_to_wav, midi_path, wav_path, soundfont_path\n",
    "            ): midi_path.stem\n",
    "            for midi_path, wav_path in to_render\n",
    "        }\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Rendering\"):\n",
    "            if future.result():\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "    \n",
    "    return successful, failed\n",
    "\n",
    "print(\"Rendering functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-render-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render all MIDI files\n",
    "print(\"=\"*60)\n",
    "print(\"AUDIO RENDERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get all labeled segment keys\n",
    "with open(LABEL_FILE) as f:\n",
    "    labels = json.load(f)\n",
    "label_keys = list(labels.keys())\n",
    "print(f\"Total labeled segments: {len(label_keys)}\")\n",
    "\n",
    "# Check soundfont\n",
    "if not SOUNDFONT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Soundfont not found: {SOUNDFONT_PATH}\")\n",
    "\n",
    "# Check FluidSynth\n",
    "result = subprocess.run(['which', 'fluidsynth'], capture_output=True)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\"FluidSynth not installed. Run: brew install fluidsynth\")\n",
    "\n",
    "# Render\n",
    "successful, failed = batch_render_midi(\n",
    "    midi_dir=MIDI_DIR,\n",
    "    output_dir=AUDIO_DIR,\n",
    "    soundfont_path=SOUNDFONT_PATH,\n",
    "    label_keys=label_keys,\n",
    "    max_workers=4,\n",
    "    skip_existing=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RENDERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Successful: {successful}\")\n",
    "print(f\"Failed: {failed}\")\n",
    "print(f\"Total WAV files: {len(list(AUDIO_DIR.glob('*.wav')))}\")\n",
    "\n",
    "if failed > 0:\n",
    "    print(f\"\\n[WARNING] {failed} files failed to render. Check logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-mert-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: MERT Feature Extraction\n",
    "\n",
    "### Goal: Extract MERT-330M embeddings for all audio segments\n",
    "\n",
    "MERT (Music Understanding Model) is pretrained on 160K hours of music with:\n",
    "- **Acoustic teacher**: RVQ-VAE for acoustic features\n",
    "- **Musical teacher**: CQT for pitch/harmonic structure\n",
    "\n",
    "We extract embeddings from layers 12-24 (higher layers capture performance quality).\n",
    "\n",
    "### Success Criteria\n",
    "- All 1,202 embeddings extracted and cached\n",
    "- GPU memory usage < 8GB\n",
    "- Extraction time < 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-mert-extractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "class MERTExtractor:\n",
    "    \"\"\"\n",
    "    Extract embeddings from MERT-330M for audio segments.\n",
    "    \n",
    "    Uses weighted sum of layers 12-24 following SUPERB/MARBLE protocols.\n",
    "    Caches embeddings to disk to avoid re-extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"m-a-p/MERT-v1-330M\",\n",
    "        device: str = \"auto\",\n",
    "        cache_dir: Optional[Path] = None,\n",
    "        target_sr: int = 24000,  # MERT native sample rate\n",
    "        use_layers: Tuple[int, int] = (12, 25),  # Layers 12-24 (0-indexed)\n",
    "    ):\n",
    "        self.target_sr = target_sr\n",
    "        self.use_layers = use_layers\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        # Device selection\n",
    "        if device == \"auto\":\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        print(f\"Loading MERT model: {model_name}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        \n",
    "        # Load model and processor\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            output_hidden_states=True,\n",
    "            trust_remote_code=True,\n",
    "        ).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Get hidden size\n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "        print(f\"Hidden size: {self.hidden_size}\")\n",
    "        print(f\"Using layers: {use_layers[0]}-{use_layers[1]-1}\")\n",
    "    \n",
    "    def load_audio(self, audio_path: Path) -> torch.Tensor:\n",
    "        \"\"\"Load and resample audio to target sample rate.\"\"\"\n",
    "        audio, sr = librosa.load(audio_path, sr=self.target_sr, mono=True)\n",
    "        return torch.from_numpy(audio).float()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract MERT embeddings from audio.\n",
    "        \n",
    "        Args:\n",
    "            audio: Audio tensor [num_samples] at target_sr\n",
    "        \n",
    "        Returns:\n",
    "            Embeddings tensor [num_frames, hidden_size]\n",
    "        \"\"\"\n",
    "        # Process audio\n",
    "        inputs = self.processor(\n",
    "            audio.numpy(),\n",
    "            sampling_rate=self.target_sr,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(**inputs)\n",
    "        \n",
    "        # Get hidden states from specified layers\n",
    "        hidden_states = outputs.hidden_states[self.use_layers[0]:self.use_layers[1]]\n",
    "        \n",
    "        # Stack and average across layers\n",
    "        stacked = torch.stack(hidden_states, dim=0)  # [num_layers, B, T, H]\n",
    "        embeddings = stacked.mean(dim=0).squeeze(0)  # [T, H]\n",
    "        \n",
    "        return embeddings.cpu()\n",
    "    \n",
    "    def extract_from_file(\n",
    "        self,\n",
    "        audio_path: Path,\n",
    "        use_cache: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract embeddings from audio file, with optional caching.\n",
    "        \"\"\"\n",
    "        # Check cache\n",
    "        if use_cache and self.cache_dir is not None:\n",
    "            cache_path = self.cache_dir / f\"{audio_path.stem}.pt\"\n",
    "            if cache_path.exists():\n",
    "                return torch.load(cache_path)\n",
    "        \n",
    "        # Load and extract\n",
    "        audio = self.load_audio(audio_path)\n",
    "        embeddings = self.extract(audio)\n",
    "        \n",
    "        # Cache\n",
    "        if use_cache and self.cache_dir is not None:\n",
    "            self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(embeddings, cache_path)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "print(\"MERTExtractor class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-extract-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MERT embeddings for all audio files\n",
    "print(\"=\"*60)\n",
    "print(\"MERT FEATURE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = MERTExtractor(\n",
    "    model_name=\"m-a-p/MERT-v1-330M\",\n",
    "    cache_dir=MERT_CACHE_DIR,\n",
    ")\n",
    "\n",
    "# Get audio files\n",
    "audio_files = sorted(AUDIO_DIR.glob('*.wav'))\n",
    "print(f\"\\nAudio files found: {len(audio_files)}\")\n",
    "\n",
    "# Check cache\n",
    "cached = len(list(MERT_CACHE_DIR.glob('*.pt')))\n",
    "print(f\"Already cached: {cached}\")\n",
    "\n",
    "# Extract embeddings\n",
    "successful = 0\n",
    "failed = []\n",
    "\n",
    "for audio_path in tqdm(audio_files, desc=\"Extracting MERT embeddings\"):\n",
    "    try:\n",
    "        embeddings = extractor.extract_from_file(audio_path, use_cache=True)\n",
    "        successful += 1\n",
    "    except Exception as e:\n",
    "        failed.append((audio_path.stem, str(e)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Successful: {successful}\")\n",
    "print(f\"Failed: {len(failed)}\")\n",
    "\n",
    "if failed:\n",
    "    print(\"\\nFailed files:\")\n",
    "    for name, error in failed[:10]:\n",
    "        print(f\"  {name}: {error}\")\n",
    "\n",
    "# Verify cache\n",
    "cached_files = list(MERT_CACHE_DIR.glob('*.pt'))\n",
    "print(f\"\\nCached embeddings: {len(cached_files)}\")\n",
    "\n",
    "# Sample embedding stats\n",
    "if cached_files:\n",
    "    sample = torch.load(cached_files[0])\n",
    "    print(f\"Embedding shape: {sample.shape}\")\n",
    "    print(f\"Embedding dtype: {sample.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-dataset-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Dataset and DataLoader\n",
    "\n",
    "### Goal: Create dataset class that loads MERT embeddings with PercePiano labels\n",
    "\n",
    "Uses the same k-fold splits as the symbolic baseline for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 19 PercePiano dimensions (same order as symbolic baseline)\n",
    "PERCEPIANO_DIMENSIONS = [\n",
    "    \"timing\", \"articulation_length\", \"articulation_touch\",\n",
    "    \"pedal_amount\", \"pedal_clarity\",\n",
    "    \"timbre_variety\", \"timbre_depth\", \"timbre_brightness\", \"timbre_loudness\",\n",
    "    \"dynamic_range\", \"tempo\", \"space\", \"balance\", \"drama\",\n",
    "    \"mood_valence\", \"mood_energy\", \"mood_imagination\",\n",
    "    \"sophistication\", \"interpretation\",\n",
    "]\n",
    "\n",
    "# Dimension categories for analysis\n",
    "DIMENSION_CATEGORIES = {\n",
    "    \"timing\": [\"timing\"],\n",
    "    \"articulation\": [\"articulation_length\", \"articulation_touch\"],\n",
    "    \"pedal\": [\"pedal_amount\", \"pedal_clarity\"],\n",
    "    \"timbre\": [\"timbre_variety\", \"timbre_depth\", \"timbre_brightness\", \"timbre_loudness\"],\n",
    "    \"dynamics\": [\"dynamic_range\"],\n",
    "    \"tempo_space\": [\"tempo\", \"space\", \"balance\", \"drama\"],\n",
    "    \"emotion\": [\"mood_valence\", \"mood_energy\", \"mood_imagination\"],\n",
    "    \"interpretation\": [\"sophistication\", \"interpretation\"],\n",
    "}\n",
    "\n",
    "\n",
    "class AudioPercePianoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for audio-based PercePiano evaluation.\n",
    "    \n",
    "    Loads pre-extracted MERT embeddings and PercePiano labels.\n",
    "    Supports k-fold cross-validation with the same splits as symbolic baseline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        mert_cache_dir: Path,\n",
    "        label_file: Path,\n",
    "        fold_assignments: Optional[Dict] = None,\n",
    "        fold_id: int = 0,\n",
    "        mode: str = \"train\",  # \"train\", \"val\", \"test\"\n",
    "        max_frames: int = 1000,  # Max MERT frames (truncate if longer)\n",
    "    ):\n",
    "        self.mert_cache_dir = Path(mert_cache_dir)\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "        # Load labels\n",
    "        with open(label_file) as f:\n",
    "            all_labels = json.load(f)\n",
    "        \n",
    "        # Filter to segments with cached embeddings\n",
    "        available_keys = {p.stem for p in self.mert_cache_dir.glob('*.pt')}\n",
    "        self.samples = []\n",
    "        \n",
    "        for key, label_values in all_labels.items():\n",
    "            if key in available_keys:\n",
    "                # Labels are [19 values, 0] - last value is unused\n",
    "                labels = torch.tensor(label_values[:19], dtype=torch.float32)\n",
    "                self.samples.append((key, labels))\n",
    "        \n",
    "        print(f\"Total samples with embeddings: {len(self.samples)}\")\n",
    "        \n",
    "        # Apply fold filtering if provided\n",
    "        if fold_assignments is not None:\n",
    "            self._apply_fold_filter(fold_assignments, fold_id, mode)\n",
    "    \n",
    "    def _apply_fold_filter(self, fold_assignments: Dict, fold_id: int, mode: str):\n",
    "        \"\"\"Filter samples based on fold assignment.\"\"\"\n",
    "        if mode == \"test\":\n",
    "            valid_keys = set(fold_assignments.get(\"test\", []))\n",
    "        elif mode == \"val\":\n",
    "            valid_keys = set(fold_assignments.get(f\"fold_{fold_id}\", []))\n",
    "        else:  # train\n",
    "            valid_keys = set()\n",
    "            for i in range(4):  # Assuming 4 folds\n",
    "                if i != fold_id:\n",
    "                    valid_keys.update(fold_assignments.get(f\"fold_{i}\", []))\n",
    "        \n",
    "        self.samples = [(k, l) for k, l in self.samples if k in valid_keys]\n",
    "        print(f\"{mode} samples (fold {fold_id}): {len(self.samples)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        key, labels = self.samples[idx]\n",
    "        \n",
    "        # Load cached embeddings\n",
    "        embed_path = self.mert_cache_dir / f\"{key}.pt\"\n",
    "        embeddings = torch.load(embed_path)  # [T, 1024]\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if embeddings.shape[0] > self.max_frames:\n",
    "            embeddings = embeddings[:self.max_frames]\n",
    "        \n",
    "        return {\n",
    "            \"embeddings\": embeddings,\n",
    "            \"labels\": labels,\n",
    "            \"key\": key,\n",
    "            \"length\": embeddings.shape[0],\n",
    "        }\n",
    "\n",
    "\n",
    "def audio_collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate function for variable-length MERT embeddings.\n",
    "    \n",
    "    Pads embeddings to max length in batch and creates attention mask.\n",
    "    \"\"\"\n",
    "    embeddings = [item[\"embeddings\"] for item in batch]\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    lengths = torch.tensor([item[\"length\"] for item in batch])\n",
    "    keys = [item[\"key\"] for item in batch]\n",
    "    \n",
    "    # Pad embeddings\n",
    "    padded_embeddings = pad_sequence(embeddings, batch_first=True)  # [B, T, H]\n",
    "    \n",
    "    # Create attention mask (1 for valid, 0 for padding)\n",
    "    max_len = padded_embeddings.shape[1]\n",
    "    attention_mask = torch.arange(max_len).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "    \n",
    "    return {\n",
    "        \"embeddings\": padded_embeddings,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"lengths\": lengths,\n",
    "        \"keys\": keys,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Dataset classes defined.\")\n",
    "print(f\"Dimensions: {len(PERCEPIANO_DIMENSIONS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-create-folds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or load fold assignments\n",
    "# Use same splits as symbolic baseline for fair comparison\n",
    "\n",
    "FOLD_FILE = MERT_CACHE_DIR.parent / 'audio_fold_assignments.json'\n",
    "\n",
    "# Try to load existing symbolic fold assignments\n",
    "symbolic_fold_file = Path('/tmp/percepiano_vnet_84dim/fold_assignments.json')\n",
    "\n",
    "if symbolic_fold_file.exists():\n",
    "    print(\"Loading fold assignments from symbolic baseline...\")\n",
    "    with open(symbolic_fold_file) as f:\n",
    "        fold_assignments = json.load(f)\n",
    "    print(f\"Loaded {len(fold_assignments)} fold groups\")\n",
    "elif FOLD_FILE.exists():\n",
    "    print(f\"Loading existing fold assignments from {FOLD_FILE}\")\n",
    "    with open(FOLD_FILE) as f:\n",
    "        fold_assignments = json.load(f)\n",
    "else:\n",
    "    print(\"Creating new piece-based fold assignments...\")\n",
    "    \n",
    "    # Group by piece\n",
    "    from collections import defaultdict\n",
    "    piece_to_keys = defaultdict(list)\n",
    "    \n",
    "    with open(LABEL_FILE) as f:\n",
    "        labels = json.load(f)\n",
    "    \n",
    "    for key in labels.keys():\n",
    "        # Extract piece name (everything before _Xbars_)\n",
    "        parts = key.split('_')\n",
    "        for i, part in enumerate(parts):\n",
    "            if 'bars' in part:\n",
    "                piece = '_'.join(parts[:i])\n",
    "                break\n",
    "        else:\n",
    "            piece = key\n",
    "        piece_to_keys[piece].append(key)\n",
    "    \n",
    "    print(f\"Found {len(piece_to_keys)} unique pieces\")\n",
    "    \n",
    "    # Assign pieces to folds (round-robin for simplicity)\n",
    "    pieces = sorted(piece_to_keys.keys())\n",
    "    fold_assignments = {\"test\": [], \"fold_0\": [], \"fold_1\": [], \"fold_2\": [], \"fold_3\": []}\n",
    "    \n",
    "    # First ~15% to test\n",
    "    test_count = 0\n",
    "    target_test = len(labels) * 0.15\n",
    "    test_pieces = []\n",
    "    \n",
    "    for piece in pieces:\n",
    "        if test_count < target_test:\n",
    "            fold_assignments[\"test\"].extend(piece_to_keys[piece])\n",
    "            test_count += len(piece_to_keys[piece])\n",
    "            test_pieces.append(piece)\n",
    "    \n",
    "    # Remaining to folds\n",
    "    remaining_pieces = [p for p in pieces if p not in test_pieces]\n",
    "    for i, piece in enumerate(remaining_pieces):\n",
    "        fold_idx = i % 4\n",
    "        fold_assignments[f\"fold_{fold_idx}\"].extend(piece_to_keys[piece])\n",
    "    \n",
    "    # Save\n",
    "    with open(FOLD_FILE, 'w') as f:\n",
    "        json.dump(fold_assignments, f, indent=2)\n",
    "    print(f\"Saved to {FOLD_FILE}\")\n",
    "\n",
    "# Print fold statistics\n",
    "print(\"\\nFold statistics:\")\n",
    "for fold_name, keys in fold_assignments.items():\n",
    "    print(f\"  {fold_name}: {len(keys)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "print(\"Testing dataset...\")\n",
    "\n",
    "test_ds = AudioPercePianoDataset(\n",
    "    mert_cache_dir=MERT_CACHE_DIR,\n",
    "    label_file=LABEL_FILE,\n",
    "    fold_assignments=fold_assignments,\n",
    "    fold_id=2,\n",
    "    mode=\"val\",\n",
    ")\n",
    "\n",
    "# Test single sample\n",
    "sample = test_ds[0]\n",
    "print(f\"\\nSample:\")\n",
    "print(f\"  Key: {sample['key']}\")\n",
    "print(f\"  Embeddings shape: {sample['embeddings'].shape}\")\n",
    "print(f\"  Labels shape: {sample['labels'].shape}\")\n",
    "print(f\"  Labels range: [{sample['labels'].min():.3f}, {sample['labels'].max():.3f}]\")\n",
    "\n",
    "# Test dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=audio_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "print(f\"\\nBatch:\")\n",
    "print(f\"  Embeddings: {batch['embeddings'].shape}\")\n",
    "print(f\"  Attention mask: {batch['attention_mask'].shape}\")\n",
    "print(f\"  Labels: {batch['labels'].shape}\")\n",
    "print(f\"  Lengths: {batch['lengths']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-model-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Model Architecture\n",
    "\n",
    "### Goal: Simple MERT baseline model\n",
    "\n",
    "We start with the simplest possible architecture to establish a floor:\n",
    "1. **MERT embeddings** (frozen, pre-extracted)\n",
    "2. **Mean pooling** across time\n",
    "3. **Linear layers** for regression\n",
    "\n",
    "This is intentionally simple. If it works, we can add:\n",
    "- Attention pooling (Phase 2)\n",
    "- Bi-LSTM on MERT frames (Phase 2)\n",
    "- Fine-tuning MERT (Phase 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "class MERTBaselineModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Simple MERT baseline for PercePiano evaluation.\n",
    "    \n",
    "    Architecture:\n",
    "        MERT embeddings [B, T, 1024]\n",
    "            -> Mean pooling [B, 1024]\n",
    "            -> Linear(1024, 512) + GELU + Dropout\n",
    "            -> Linear(512, 19) + Sigmoid\n",
    "    \n",
    "    This is the simplest possible model. We use this to establish\n",
    "    a baseline before adding complexity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 1024,\n",
    "        hidden_dim: int = 512,\n",
    "        num_labels: int = 19,\n",
    "        dropout: float = 0.2,\n",
    "        learning_rate: float = 1e-4,\n",
    "        weight_decay: float = 1e-5,\n",
    "        pooling: str = \"mean\",  # \"mean\", \"max\", \"attention\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_labels = num_labels\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        # Optional attention pooling\n",
    "        if pooling == \"attention\":\n",
    "            self.attention = nn.Sequential(\n",
    "                nn.Linear(input_dim, 256),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(256, 1),\n",
    "            )\n",
    "        \n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_labels),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        # Metrics storage\n",
    "        self.validation_outputs = []\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        embeddings: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: [B, T, input_dim]\n",
    "            attention_mask: [B, T] (1 for valid, 0 for padding)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: [B, num_labels]\n",
    "        \"\"\"\n",
    "        # Pool across time dimension\n",
    "        if self.pooling == \"mean\":\n",
    "            if attention_mask is not None:\n",
    "                # Masked mean pooling\n",
    "                mask = attention_mask.unsqueeze(-1).float()  # [B, T, 1]\n",
    "                pooled = (embeddings * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "            else:\n",
    "                pooled = embeddings.mean(dim=1)\n",
    "        \n",
    "        elif self.pooling == \"max\":\n",
    "            if attention_mask is not None:\n",
    "                embeddings = embeddings.masked_fill(~attention_mask.unsqueeze(-1), float('-inf'))\n",
    "            pooled = embeddings.max(dim=1).values\n",
    "        \n",
    "        elif self.pooling == \"attention\":\n",
    "            # Attention weights\n",
    "            scores = self.attention(embeddings).squeeze(-1)  # [B, T]\n",
    "            if attention_mask is not None:\n",
    "                scores = scores.masked_fill(~attention_mask, float('-inf'))\n",
    "            weights = torch.softmax(scores, dim=-1).unsqueeze(-1)  # [B, T, 1]\n",
    "            pooled = (embeddings * weights).sum(dim=1)  # [B, H]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling: {self.pooling}\")\n",
    "        \n",
    "        # Classify\n",
    "        predictions = self.classifier(pooled)\n",
    "        return predictions\n",
    "    \n",
    "    def training_step(self, batch: Dict, batch_idx: int) -> torch.Tensor:\n",
    "        predictions = self(batch[\"embeddings\"], batch[\"attention_mask\"])\n",
    "        loss = self.loss_fn(predictions, batch[\"labels\"])\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Dict, batch_idx: int):\n",
    "        predictions = self(batch[\"embeddings\"], batch[\"attention_mask\"])\n",
    "        loss = self.loss_fn(predictions, batch[\"labels\"])\n",
    "        \n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        # Store for epoch-end metrics\n",
    "        self.validation_outputs.append({\n",
    "            \"predictions\": predictions.detach().cpu(),\n",
    "            \"labels\": batch[\"labels\"].detach().cpu(),\n",
    "        })\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.validation_outputs:\n",
    "            return\n",
    "        \n",
    "        # Aggregate predictions and labels\n",
    "        all_preds = torch.cat([x[\"predictions\"] for x in self.validation_outputs])\n",
    "        all_labels = torch.cat([x[\"labels\"] for x in self.validation_outputs])\n",
    "        \n",
    "        # Compute R2\n",
    "        r2 = r2_score(all_labels.numpy(), all_preds.numpy())\n",
    "        self.log(\"val_r2\", r2, prog_bar=True)\n",
    "        \n",
    "        # Per-dimension R2\n",
    "        per_dim_r2 = {}\n",
    "        for i, dim_name in enumerate(PERCEPIANO_DIMENSIONS):\n",
    "            dim_r2 = r2_score(all_labels[:, i].numpy(), all_preds[:, i].numpy())\n",
    "            per_dim_r2[dim_name] = dim_r2\n",
    "            self.log(f\"val_r2_{dim_name}\", dim_r2)\n",
    "        \n",
    "        self.validation_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=100,\n",
    "            eta_min=1e-6,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"MERTBaselineModel defined.\")\n",
    "print(f\"Pooling options: mean, max, attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-training-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'input_dim': 1024,         # MERT-330M hidden size\n",
    "    'hidden_dim': 512,         # Classifier hidden dim\n",
    "    'num_labels': 19,          # PercePiano dimensions\n",
    "    'dropout': 0.2,\n",
    "    'pooling': 'mean',         # Start simple: mean pooling\n",
    "    \n",
    "    # Training\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'batch_size': 16,          # Can be larger since MERT is frozen\n",
    "    'max_epochs': 100,\n",
    "    'early_stopping_patience': 15,\n",
    "    'gradient_clip_val': 1.0,\n",
    "    \n",
    "    # Data\n",
    "    'max_frames': 1000,        # Max MERT frames per segment\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    # Fold\n",
    "    'fold_id': 2,              # Same as best symbolic fold for comparison\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-train-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, RichProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Set seed\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Create datasets\n",
    "train_ds = AudioPercePianoDataset(\n",
    "    mert_cache_dir=MERT_CACHE_DIR,\n",
    "    label_file=LABEL_FILE,\n",
    "    fold_assignments=fold_assignments,\n",
    "    fold_id=CONFIG['fold_id'],\n",
    "    mode=\"train\",\n",
    "    max_frames=CONFIG['max_frames'],\n",
    ")\n",
    "\n",
    "val_ds = AudioPercePianoDataset(\n",
    "    mert_cache_dir=MERT_CACHE_DIR,\n",
    "    label_file=LABEL_FILE,\n",
    "    fold_assignments=fold_assignments,\n",
    "    fold_id=CONFIG['fold_id'],\n",
    "    mode=\"val\",\n",
    "    max_frames=CONFIG['max_frames'],\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=audio_collate_fn,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=audio_collate_fn,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = MERTBaselineModel(\n",
    "    input_dim=CONFIG['input_dim'],\n",
    "    hidden_dim=CONFIG['hidden_dim'],\n",
    "    num_labels=CONFIG['num_labels'],\n",
    "    dropout=CONFIG['dropout'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    pooling=CONFIG['pooling'],\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=CHECKPOINT_ROOT / f\"fold_{CONFIG['fold_id']}\",\n",
    "        filename=\"best-{epoch:02d}-{val_r2:.4f}\",\n",
    "        monitor=\"val_r2\",\n",
    "        mode=\"max\",\n",
    "        save_top_k=3,\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_r2\",\n",
    "        mode=\"max\",\n",
    "        patience=CONFIG['early_stopping_patience'],\n",
    "        verbose=True,\n",
    "    ),\n",
    "    RichProgressBar(),\n",
    "]\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=LOG_ROOT,\n",
    "    name=f\"fold_{CONFIG['fold_id']}\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['max_epochs'],\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    gradient_clip_val=CONFIG['gradient_clip_val'],\n",
    "    precision='32',\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best checkpoint: {callbacks[0].best_model_path}\")\n",
    "print(f\"Best val R2: {callbacks[0].best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-eval-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Evaluation and Diagnostics\n",
    "\n",
    "### Goals:\n",
    "1. Per-dimension R2 analysis\n",
    "2. Comparison to symbolic baseline\n",
    "3. Category-level analysis (timbre, pedal, etc.)\n",
    "4. Identify where audio excels vs. fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eval-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "best_ckpt = callbacks[0].best_model_path\n",
    "print(f\"Loading best checkpoint: {best_ckpt}\")\n",
    "\n",
    "model = MERTBaselineModel.load_from_checkpoint(best_ckpt)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eval-predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation predictions\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_keys = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        embeddings = batch[\"embeddings\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        predictions = model(embeddings, attention_mask)\n",
    "        \n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_labels.append(batch[\"labels\"])\n",
    "        all_keys.extend(batch[\"keys\"])\n",
    "\n",
    "all_predictions = torch.cat(all_predictions).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "print(f\"Predictions shape: {all_predictions.shape}\")\n",
    "print(f\"Labels shape: {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eval-per-dim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-dimension R2 analysis\n",
    "print(\"=\"*80)\n",
    "print(\"PER-DIMENSION R2 ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute per-dimension R2\n",
    "audio_r2 = {}\n",
    "for i, dim_name in enumerate(PERCEPIANO_DIMENSIONS):\n",
    "    r2 = r2_score(all_labels[:, i], all_predictions[:, i])\n",
    "    audio_r2[dim_name] = r2\n",
    "\n",
    "# Sort by R2\n",
    "sorted_dims = sorted(audio_r2.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display table\n",
    "table = Table(title=\"Audio Baseline Per-Dimension R2\")\n",
    "table.add_column(\"Dimension\", style=\"cyan\")\n",
    "table.add_column(\"R2\", justify=\"right\")\n",
    "table.add_column(\"Status\", justify=\"center\")\n",
    "\n",
    "for dim_name, r2 in sorted_dims:\n",
    "    if r2 >= 0.30:\n",
    "        status = \"[green]Strong[/green]\"\n",
    "    elif r2 >= 0.15:\n",
    "        status = \"[yellow]OK[/yellow]\"\n",
    "    elif r2 >= 0:\n",
    "        status = \"[orange3]Weak[/orange3]\"\n",
    "    else:\n",
    "        status = \"[red]Failed[/red]\"\n",
    "    \n",
    "    table.add_row(dim_name, f\"{r2:+.4f}\", status)\n",
    "\n",
    "console.print(table)\n",
    "\n",
    "# Overall R2\n",
    "overall_r2 = r2_score(all_labels, all_predictions)\n",
    "print(f\"\\nOverall R2: {overall_r2:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eval-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-level analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CATEGORY-LEVEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "category_r2 = {}\n",
    "for category, dims in DIMENSION_CATEGORIES.items():\n",
    "    cat_r2s = [audio_r2[d] for d in dims]\n",
    "    category_r2[category] = np.mean(cat_r2s)\n",
    "\n",
    "# Display\n",
    "table = Table(title=\"Category Average R2\")\n",
    "table.add_column(\"Category\", style=\"cyan\")\n",
    "table.add_column(\"Dimensions\", justify=\"right\")\n",
    "table.add_column(\"Mean R2\", justify=\"right\")\n",
    "table.add_column(\"Expected Winner\", justify=\"center\")\n",
    "\n",
    "expected_audio = [\"timbre\", \"pedal\", \"emotion\"]\n",
    "expected_symbolic = [\"timing\", \"articulation\"]\n",
    "\n",
    "for category in sorted(category_r2.keys(), key=lambda x: category_r2[x], reverse=True):\n",
    "    r2 = category_r2[category]\n",
    "    n_dims = len(DIMENSION_CATEGORIES[category])\n",
    "    \n",
    "    if category in expected_audio:\n",
    "        expected = \"[green]Audio[/green]\"\n",
    "    elif category in expected_symbolic:\n",
    "        expected = \"[blue]Symbolic[/blue]\"\n",
    "    else:\n",
    "        expected = \"Tie\"\n",
    "    \n",
    "    table.add_row(category, str(n_dims), f\"{r2:+.4f}\", expected)\n",
    "\n",
    "console.print(table)\n",
    "\n",
    "# Hypothesis validation\n",
    "print(\"\\nHypothesis Validation:\")\n",
    "timbre_r2 = category_r2[\"timbre\"]\n",
    "pedal_r2 = category_r2[\"pedal\"]\n",
    "timing_r2 = category_r2[\"timing\"]\n",
    "\n",
    "print(f\"  Timbre R2: {timbre_r2:+.4f} (target: >= 0.30)\")\n",
    "print(f\"  Pedal R2: {pedal_r2:+.4f} (target: >= 0.25)\")\n",
    "print(f\"  Timing R2: {timing_r2:+.4f} (expected to be lower than symbolic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-symbolic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to symbolic baseline (if available)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON TO SYMBOLIC BASELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load symbolic results if available\n",
    "# These would come from the PercePiano replica training\n",
    "# For now, use paper-reported values as reference\n",
    "\n",
    "SYMBOLIC_REFERENCE = {\n",
    "    \"overall\": 0.397,  # Bi-LSTM + SA + HAN (paper SOTA)\n",
    "    \"baseline\": 0.185,  # Bi-LSTM baseline\n",
    "    # Per-dimension values would come from our replica training\n",
    "}\n",
    "\n",
    "print(f\"\\nSymbolic baseline (paper): R2 = {SYMBOLIC_REFERENCE['baseline']:.3f}\")\n",
    "print(f\"Symbolic SOTA (paper): R2 = {SYMBOLIC_REFERENCE['overall']:.3f}\")\n",
    "print(f\"Audio baseline (ours): R2 = {overall_r2:.3f}\")\n",
    "\n",
    "if overall_r2 > SYMBOLIC_REFERENCE['baseline']:\n",
    "    print(f\"\\n[SUCCESS] Audio baseline beats symbolic baseline!\")\n",
    "    print(f\"  Improvement: +{overall_r2 - SYMBOLIC_REFERENCE['baseline']:.3f}\")\n",
    "else:\n",
    "    print(f\"\\n[INFO] Audio baseline below symbolic baseline\")\n",
    "    print(f\"  Gap: {overall_r2 - SYMBOLIC_REFERENCE['baseline']:.3f}\")\n",
    "\n",
    "print(\"\\nNote: Per-dimension comparison requires symbolic replica results.\")\n",
    "print(\"See train_percepiano_replica.ipynb for symbolic per-dimension R2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-diagnostics-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Diagnostics and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction statistics\n",
    "print(\"=\"*80)\n",
    "print(\"PREDICTION DIAGNOSTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"  Mean: {all_predictions.mean():.4f}\")\n",
    "print(f\"  Std: {all_predictions.std():.4f}\")\n",
    "print(f\"  Min: {all_predictions.min():.4f}\")\n",
    "print(f\"  Max: {all_predictions.max():.4f}\")\n",
    "\n",
    "print(f\"\\nLabel statistics:\")\n",
    "print(f\"  Mean: {all_labels.mean():.4f}\")\n",
    "print(f\"  Std: {all_labels.std():.4f}\")\n",
    "print(f\"  Min: {all_labels.min():.4f}\")\n",
    "print(f\"  Max: {all_labels.max():.4f}\")\n",
    "\n",
    "# Check for prediction collapse\n",
    "pred_std = all_predictions.std(axis=0)\n",
    "label_std = all_labels.std(axis=0)\n",
    "\n",
    "print(f\"\\nPer-dimension prediction std vs label std:\")\n",
    "collapsed = []\n",
    "for i, dim_name in enumerate(PERCEPIANO_DIMENSIONS):\n",
    "    ratio = pred_std[i] / label_std[i] if label_std[i] > 0 else 0\n",
    "    status = \"OK\" if ratio > 0.5 else \"COLLAPSED\"\n",
    "    if status == \"COLLAPSED\":\n",
    "        collapsed.append(dim_name)\n",
    "        print(f\"  {dim_name}: pred_std={pred_std[i]:.4f}, label_std={label_std[i]:.4f}, ratio={ratio:.2f} [{status}]\")\n",
    "\n",
    "if collapsed:\n",
    "    print(f\"\\n[WARNING] {len(collapsed)} dimensions show prediction collapse:\")\n",
    "    for dim in collapsed:\n",
    "        print(f\"  - {dim}\")\n",
    "else:\n",
    "    print(f\"\\n[OK] No prediction collapse detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-worst-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst performing samples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORST PERFORMING SAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute per-sample MSE\n",
    "sample_mse = np.mean((all_predictions - all_labels) ** 2, axis=1)\n",
    "worst_indices = np.argsort(sample_mse)[-10:][::-1]\n",
    "\n",
    "print(\"\\nTop 10 worst predictions:\")\n",
    "for idx in worst_indices:\n",
    "    key = all_keys[idx]\n",
    "    mse = sample_mse[idx]\n",
    "    print(f\"  {key}: MSE={mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"AUDIO BASELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. OVERALL PERFORMANCE\")\n",
    "print(f\"   Audio Baseline R2: {overall_r2:+.4f}\")\n",
    "print(f\"   Symbolic Baseline (paper): 0.185\")\n",
    "print(f\"   Symbolic SOTA (paper): 0.397\")\n",
    "\n",
    "print(f\"\\n2. HYPOTHESIS VALIDATION\")\n",
    "print(f\"   Timbre dimensions: {category_r2['timbre']:+.4f} (expected audio advantage)\")\n",
    "print(f\"   Pedal dimensions: {category_r2['pedal']:+.4f} (expected audio advantage)\")\n",
    "print(f\"   Timing dimension: {category_r2['timing']:+.4f} (expected symbolic advantage)\")\n",
    "\n",
    "print(f\"\\n3. STRONGEST DIMENSIONS (Audio)\")\n",
    "for dim, r2 in sorted_dims[:5]:\n",
    "    print(f\"   {dim}: {r2:+.4f}\")\n",
    "\n",
    "print(f\"\\n4. WEAKEST DIMENSIONS (Audio)\")\n",
    "for dim, r2 in sorted_dims[-5:]:\n",
    "    print(f\"   {dim}: {r2:+.4f}\")\n",
    "\n",
    "print(f\"\\n5. NEXT STEPS\")\n",
    "print(f\"   [ ] Compare per-dimension with symbolic baseline from replica training\")\n",
    "print(f\"   [ ] Try attention pooling instead of mean pooling\")\n",
    "print(f\"   [ ] Add Bi-LSTM on MERT frames\")\n",
    "print(f\"   [ ] Explore multimodal fusion if audio/symbolic show complementary strengths\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for comparison\n",
    "results = {\n",
    "    \"config\": CONFIG,\n",
    "    \"overall_r2\": float(overall_r2),\n",
    "    \"per_dimension_r2\": {k: float(v) for k, v in audio_r2.items()},\n",
    "    \"category_r2\": {k: float(v) for k, v in category_r2.items()},\n",
    "    \"best_checkpoint\": str(best_ckpt),\n",
    "}\n",
    "\n",
    "results_path = CHECKPOINT_ROOT / \"audio_baseline_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
