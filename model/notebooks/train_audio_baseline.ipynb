{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# Audio Baseline for PercePiano (MERT-330M)\n",
    "\n",
    "Train audio baseline using MERT-330M embeddings on Thunder Compute.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. Download pre-rendered WAV files from Google Drive\n",
    "2. Extract MERT-330M embeddings (GPU required)\n",
    "3. Train 4-fold cross-validation\n",
    "4. Evaluate and analyze results\n",
    "\n",
    "## Target: R2 >= 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-header",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gpu-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. MERT extraction will be very slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-install-rclone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rclone\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers librosa soundfile torchaudio pytorch-lightning --quiet\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "if 'gdrive:' not in result.stdout:\n",
    "    raise RuntimeError(\"rclone not configured. Run 'rclone config' to set up gdrive remote.\")\n",
    "print(\"rclone 'gdrive' remote: CONFIGURED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import subprocess\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-paths-header",
   "metadata": {},
   "source": [
    "## Step 2: Download Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_ROOT = Path('/tmp/audio_baseline')\n",
    "AUDIO_DIR = DATA_ROOT / 'percepiano_rendered'\n",
    "MERT_CACHE_DIR = DATA_ROOT / 'mert_embeddings'\n",
    "CHECKPOINT_ROOT = DATA_ROOT / 'checkpoints'\n",
    "LABEL_DIR = DATA_ROOT / 'labels'\n",
    "\n",
    "# Google Drive paths\n",
    "GDRIVE_AUDIO = 'gdrive:crescendai_data/audio_baseline/percepiano_rendered'\n",
    "GDRIVE_LABELS = 'gdrive:crescendai_data/percepiano_labels'\n",
    "GDRIVE_FOLDS = 'gdrive:crescendai_data/audio_baseline/audio_fold_assignments.json'\n",
    "GDRIVE_CHECKPOINTS = 'gdrive:crescendai_data/checkpoints/audio_baseline'\n",
    "GDRIVE_MERT_CACHE = 'gdrive:crescendai_data/audio_baseline/mert_embeddings'\n",
    "\n",
    "# Create directories\n",
    "for d in [AUDIO_DIR, MERT_CACHE_DIR, CHECKPOINT_ROOT, LABEL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-download-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-rendered audio from Google Drive\n",
    "print(\"Downloading pre-rendered audio files...\")\n",
    "print(f\"  Source: {GDRIVE_AUDIO}\")\n",
    "print(f\"  Destination: {AUDIO_DIR}\")\n",
    "\n",
    "subprocess.run(\n",
    "    ['rclone', 'copy', GDRIVE_AUDIO, str(AUDIO_DIR), '--progress'],\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "wav_count = len(list(AUDIO_DIR.glob('*.wav')))\n",
    "print(f\"\\nDownloaded {wav_count} WAV files\")\n",
    "\n",
    "if wav_count == 0:\n",
    "    raise RuntimeError(\"No WAV files downloaded! Run prepare_audio_baseline.py locally first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-download-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download labels and fold assignments\n",
    "print(\"Downloading labels...\")\n",
    "subprocess.run(\n",
    "    ['rclone', 'copy', GDRIVE_LABELS, str(LABEL_DIR), '--progress'],\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "print(\"\\nDownloading fold assignments...\")\n",
    "subprocess.run(\n",
    "    ['rclone', 'copy', GDRIVE_FOLDS, str(DATA_ROOT), '--progress'],\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "# Verify\n",
    "LABEL_FILE = LABEL_DIR / 'label_2round_mean_reg_19_with0_rm_highstd0.json'\n",
    "FOLD_FILE = DATA_ROOT / 'audio_fold_assignments.json'\n",
    "\n",
    "if not LABEL_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Label file not found: {LABEL_FILE}\")\n",
    "\n",
    "with open(LABEL_FILE) as f:\n",
    "    labels = json.load(f)\n",
    "print(f\"Labels: {len(labels)} segments\")\n",
    "\n",
    "if not FOLD_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Fold file not found: {FOLD_FILE}\")\n",
    "\n",
    "with open(FOLD_FILE) as f:\n",
    "    fold_assignments = json.load(f)\n",
    "print(f\"\\nFold statistics:\")\n",
    "for fold_name, keys in fold_assignments.items():\n",
    "    print(f\"  {fold_name}: {len(keys)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-restore-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing MERT cache (resume capability)\n",
    "print(\"Checking for existing MERT cache on Google Drive...\")\n",
    "result = subprocess.run(\n",
    "    ['rclone', 'lsf', GDRIVE_MERT_CACHE],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0 and result.stdout.strip():\n",
    "    remote_files = [f for f in result.stdout.strip().split('\\n') if f.endswith('.pt')]\n",
    "    if remote_files:\n",
    "        print(f\"Found {len(remote_files)} cached embeddings. Restoring...\")\n",
    "        subprocess.run(\n",
    "            ['rclone', 'copy', GDRIVE_MERT_CACHE, str(MERT_CACHE_DIR), '--progress'],\n",
    "            capture_output=False\n",
    "        )\n",
    "        print(f\"Restored {len(list(MERT_CACHE_DIR.glob('*.pt')))} embeddings\")\n",
    "else:\n",
    "    print(\"No existing cache found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-mert-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: MERT Feature Extraction\n",
    "\n",
    "- Model: m-a-p/MERT-v1-330M (~8GB VRAM)\n",
    "- Layers: 12-24 averaged\n",
    "- Output: 1024-dim per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-mert-extractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "class MERT330MExtractor:\n",
    "    def __init__(self, cache_dir=None):\n",
    "        self.target_sr = 24000\n",
    "        self.use_layers = (12, 25)\n",
    "        self.cache_dir = Path(cache_dir) if cache_dir else None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        print(f\"Loading MERT-v1-330M on {self.device}...\")\n",
    "        self.processor = AutoProcessor.from_pretrained(\"m-a-p/MERT-v1-330M\", trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            \"m-a-p/MERT-v1-330M\",\n",
    "            output_hidden_states=True,\n",
    "            trust_remote_code=True,\n",
    "        ).to(self.device)\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded. Hidden size: {self.model.config.hidden_size}\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def extract_from_file(self, audio_path, use_cache=True):\n",
    "        audio_path = Path(audio_path)\n",
    "        \n",
    "        if use_cache and self.cache_dir:\n",
    "            cache_path = self.cache_dir / f\"{audio_path.stem}.pt\"\n",
    "            if cache_path.exists():\n",
    "                return torch.load(cache_path, weights_only=True)\n",
    "        \n",
    "        audio, _ = librosa.load(audio_path, sr=self.target_sr, mono=True)\n",
    "        inputs = self.processor(audio, sampling_rate=self.target_sr, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = self.model(**inputs)\n",
    "        hidden_states = outputs.hidden_states[self.use_layers[0]:self.use_layers[1]]\n",
    "        embeddings = torch.stack(hidden_states, dim=0).mean(dim=0).squeeze(0).cpu()\n",
    "        \n",
    "        if use_cache and self.cache_dir:\n",
    "            self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(embeddings, cache_path)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-extract-mert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MERT embeddings\n",
    "print(\"=\"*60)\n",
    "print(\"MERT FEATURE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "audio_files = sorted(AUDIO_DIR.glob('*.wav'))\n",
    "cached_files = set(f.stem for f in MERT_CACHE_DIR.glob('*.pt'))\n",
    "to_extract = [f for f in audio_files if f.stem not in cached_files]\n",
    "\n",
    "print(f\"Audio files: {len(audio_files)}\")\n",
    "print(f\"Already cached: {len(cached_files)}\")\n",
    "print(f\"To extract: {len(to_extract)}\")\n",
    "\n",
    "if to_extract:\n",
    "    extractor = MERT330MExtractor(cache_dir=MERT_CACHE_DIR)\n",
    "    \n",
    "    failed = []\n",
    "    for audio_path in tqdm(to_extract, desc=\"Extracting\"):\n",
    "        try:\n",
    "            extractor.extract_from_file(audio_path)\n",
    "        except Exception as e:\n",
    "            failed.append((audio_path.stem, str(e)))\n",
    "    \n",
    "    del extractor\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nExtracted: {len(to_extract) - len(failed)}\")\n",
    "    if failed:\n",
    "        print(f\"Failed: {len(failed)}\")\n",
    "else:\n",
    "    print(\"\\nAll embeddings cached!\")\n",
    "\n",
    "print(f\"Total cached: {len(list(MERT_CACHE_DIR.glob('*.pt')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-sync-mert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync MERT cache to Google Drive\n",
    "print(\"Syncing MERT cache to Google Drive...\")\n",
    "subprocess.run(\n",
    "    ['rclone', 'copy', str(MERT_CACHE_DIR), GDRIVE_MERT_CACHE, '--progress'],\n",
    "    capture_output=False\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-dataset-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCEPIANO_DIMENSIONS = [\n",
    "    \"timing\", \"articulation_length\", \"articulation_touch\",\n",
    "    \"pedal_amount\", \"pedal_clarity\",\n",
    "    \"timbre_variety\", \"timbre_depth\", \"timbre_brightness\", \"timbre_loudness\",\n",
    "    \"dynamic_range\", \"tempo\", \"space\", \"balance\", \"drama\",\n",
    "    \"mood_valence\", \"mood_energy\", \"mood_imagination\",\n",
    "    \"sophistication\", \"interpretation\",\n",
    "]\n",
    "\n",
    "DIMENSION_CATEGORIES = {\n",
    "    \"timing\": [\"timing\"],\n",
    "    \"articulation\": [\"articulation_length\", \"articulation_touch\"],\n",
    "    \"pedal\": [\"pedal_amount\", \"pedal_clarity\"],\n",
    "    \"timbre\": [\"timbre_variety\", \"timbre_depth\", \"timbre_brightness\", \"timbre_loudness\"],\n",
    "    \"dynamics\": [\"dynamic_range\"],\n",
    "    \"tempo_space\": [\"tempo\", \"space\", \"balance\", \"drama\"],\n",
    "    \"emotion\": [\"mood_valence\", \"mood_energy\", \"mood_imagination\"],\n",
    "    \"interpretation\": [\"sophistication\", \"interpretation\"],\n",
    "}\n",
    "\n",
    "\n",
    "class AudioPercePianoDataset(Dataset):\n",
    "    def __init__(self, mert_cache_dir, labels, fold_assignments, fold_id, mode, max_frames=1000):\n",
    "        self.mert_cache_dir = Path(mert_cache_dir)\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "        available = {p.stem for p in self.mert_cache_dir.glob('*.pt')}\n",
    "        \n",
    "        if mode == \"test\":\n",
    "            valid_keys = set(fold_assignments.get(\"test\", []))\n",
    "        elif mode == \"val\":\n",
    "            valid_keys = set(fold_assignments.get(f\"fold_{fold_id}\", []))\n",
    "        else:\n",
    "            valid_keys = set()\n",
    "            for i in range(4):\n",
    "                if i != fold_id:\n",
    "                    valid_keys.update(fold_assignments.get(f\"fold_{i}\", []))\n",
    "        \n",
    "        self.samples = [(k, torch.tensor(labels[k][:19], dtype=torch.float32))\n",
    "                        for k in valid_keys if k in available and k in labels]\n",
    "        print(f\"{mode} (fold {fold_id}): {len(self.samples)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        key, label = self.samples[idx]\n",
    "        emb = torch.load(self.mert_cache_dir / f\"{key}.pt\", weights_only=True)\n",
    "        if emb.shape[0] > self.max_frames:\n",
    "            emb = emb[:self.max_frames]\n",
    "        return {\"embeddings\": emb, \"labels\": label, \"key\": key, \"length\": emb.shape[0]}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    embs = [b[\"embeddings\"] for b in batch]\n",
    "    labels = torch.stack([b[\"labels\"] for b in batch])\n",
    "    lengths = torch.tensor([b[\"length\"] for b in batch])\n",
    "    padded = pad_sequence(embs, batch_first=True)\n",
    "    mask = torch.arange(padded.shape[1]).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "    return {\"embeddings\": padded, \"attention_mask\": mask, \"labels\": labels, \"keys\": [b[\"key\"] for b in batch]}\n",
    "\n",
    "\n",
    "print(f\"Dataset defined. {len(PERCEPIANO_DIMENSIONS)} dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPercePianoModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=512, num_labels=19, dropout=0.2,\n",
    "                 learning_rate=1e-4, weight_decay=1e-5, pooling=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = learning_rate\n",
    "        self.wd = weight_decay\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        if pooling == \"attention\":\n",
    "            self.attn = nn.Sequential(nn.Linear(input_dim, 256), nn.Tanh(), nn.Linear(256, 1))\n",
    "        \n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_labels), nn.Sigmoid(),\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.val_outputs = []\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        if self.pooling == \"mean\":\n",
    "            if mask is not None:\n",
    "                m = mask.unsqueeze(-1).float()\n",
    "                pooled = (x * m).sum(1) / m.sum(1).clamp(min=1)\n",
    "            else:\n",
    "                pooled = x.mean(1)\n",
    "        elif self.pooling == \"attention\":\n",
    "            scores = self.attn(x).squeeze(-1)\n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(~mask, float('-inf'))\n",
    "            w = torch.softmax(scores, dim=-1).unsqueeze(-1)\n",
    "            pooled = (x * w).sum(1)\n",
    "        else:\n",
    "            pooled = x.mean(1)\n",
    "        return self.clf(pooled)\n",
    "    \n",
    "    def training_step(self, batch, idx):\n",
    "        loss = self.loss_fn(self(batch[\"embeddings\"], batch[\"attention_mask\"]), batch[\"labels\"])\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, idx):\n",
    "        preds = self(batch[\"embeddings\"], batch[\"attention_mask\"])\n",
    "        self.log(\"val_loss\", self.loss_fn(preds, batch[\"labels\"]), prog_bar=True)\n",
    "        self.val_outputs.append({\"p\": preds.cpu(), \"l\": batch[\"labels\"].cpu()})\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.val_outputs:\n",
    "            p = torch.cat([x[\"p\"] for x in self.val_outputs]).numpy()\n",
    "            l = torch.cat([x[\"l\"] for x in self.val_outputs]).numpy()\n",
    "            self.log(\"val_r2\", r2_score(l, p), prog_bar=True)\n",
    "            self.val_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=100, eta_min=1e-6)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"interval\": \"epoch\"}}\n",
    "\n",
    "\n",
    "print(\"Model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-train-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "CONFIG = {\n",
    "    'input_dim': 1024,\n",
    "    'hidden_dim': 512,\n",
    "    'num_labels': 19,\n",
    "    'dropout': 0.2,\n",
    "    'pooling': 'mean',\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'batch_size': 16,\n",
    "    'max_epochs': 100,\n",
    "    'patience': 15,\n",
    "    'max_frames': 1000,\n",
    "    'n_folds': 4,\n",
    "}\n",
    "\n",
    "print(\"Config:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-restore-ckpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints\n",
    "print(\"Checking for existing checkpoints...\")\n",
    "result = subprocess.run(['rclone', 'lsf', GDRIVE_CHECKPOINTS], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0 and result.stdout.strip():\n",
    "    print(f\"Found checkpoints. Restoring...\")\n",
    "    subprocess.run(['rclone', 'copy', GDRIVE_CHECKPOINTS, str(CHECKPOINT_ROOT), '--progress'])\n",
    "else:\n",
    "    print(\"No existing checkpoints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"4-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fold_results = {}\n",
    "\n",
    "for fold in range(CONFIG['n_folds']):\n",
    "    ckpt_path = CHECKPOINT_ROOT / f'fold{fold}_best.ckpt'\n",
    "    \n",
    "    if ckpt_path.exists():\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "        r2 = ckpt.get('callbacks', {}).get('ModelCheckpoint', {}).get('best_model_score', 0)\n",
    "        fold_results[fold] = float(r2) if r2 else 0.0\n",
    "        print(f\"Fold {fold}: SKIP (exists) R2={fold_results[fold]:+.4f}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nFold {fold}: Training...\")\n",
    "    \n",
    "    train_ds = AudioPercePianoDataset(MERT_CACHE_DIR, labels, fold_assignments, fold, \"train\", CONFIG['max_frames'])\n",
    "    val_ds = AudioPercePianoDataset(MERT_CACHE_DIR, labels, fold_assignments, fold, \"val\", CONFIG['max_frames'])\n",
    "    \n",
    "    train_dl = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    model = AudioPercePianoModel(\n",
    "        CONFIG['input_dim'], CONFIG['hidden_dim'], CONFIG['num_labels'],\n",
    "        CONFIG['dropout'], CONFIG['learning_rate'], CONFIG['weight_decay'], CONFIG['pooling']\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(dirpath=CHECKPOINT_ROOT, filename=f'fold{fold}_best', monitor='val_r2', mode='max', save_top_k=1),\n",
    "        EarlyStopping(monitor='val_r2', mode='max', patience=CONFIG['patience'], verbose=True),\n",
    "    ]\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=CONFIG['max_epochs'],\n",
    "        callbacks=callbacks,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_dl, val_dl)\n",
    "    \n",
    "    fold_results[fold] = float(callbacks[0].best_model_score or 0)\n",
    "    print(f\"Fold {fold} Best R2: {fold_results[fold]:+.4f}\")\n",
    "    \n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "for f, r2 in sorted(fold_results.items()):\n",
    "    print(f\"  Fold {f}: {r2:+.4f}\")\n",
    "avg = np.mean(list(fold_results.values()))\n",
    "std = np.std(list(fold_results.values()))\n",
    "print(f\"  Average: {avg:+.4f} +/- {std:.4f}\")\n",
    "print(f\"  Target: >= 0.25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-sync-ckpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync checkpoints\n",
    "print(\"Syncing checkpoints to Google Drive...\")\n",
    "subprocess.run(['rclone', 'copy', str(CHECKPOINT_ROOT), GDRIVE_CHECKPOINTS, '--progress'])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-eval-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PER-DIMENSION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for fold in range(CONFIG['n_folds']):\n",
    "    ckpt_path = CHECKPOINT_ROOT / f'fold{fold}_best.ckpt'\n",
    "    if not ckpt_path.exists():\n",
    "        continue\n",
    "    \n",
    "    model = AudioPercePianoModel.load_from_checkpoint(ckpt_path).to(device).eval()\n",
    "    val_ds = AudioPercePianoDataset(MERT_CACHE_DIR, labels, fold_assignments, fold, \"val\", CONFIG['max_frames'])\n",
    "    val_dl = DataLoader(val_ds, batch_size=CONFIG['batch_size'], collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dl:\n",
    "            preds = model(batch[\"embeddings\"].to(device), batch[\"attention_mask\"].to(device))\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(batch[\"labels\"].numpy())\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_labels = np.vstack(all_labels)\n",
    "\n",
    "print(f\"\\nSamples: {len(all_preds)}\")\n",
    "print(f\"Overall R2: {r2_score(all_labels, all_preds):+.4f}\")\n",
    "\n",
    "print(\"\\nPer-dimension R2:\")\n",
    "dim_r2 = {}\n",
    "for i, d in enumerate(PERCEPIANO_DIMENSIONS):\n",
    "    r2 = r2_score(all_labels[:, i], all_preds[:, i])\n",
    "    dim_r2[d] = r2\n",
    "\n",
    "for d, r2 in sorted(dim_r2.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {d:<25} {r2:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCategory Analysis:\")\n",
    "for cat, dims in DIMENSION_CATEGORIES.items():\n",
    "    cat_r2 = np.mean([dim_r2[d] for d in dims])\n",
    "    print(f\"  {cat:<15} {cat_r2:+.4f}\")\n",
    "\n",
    "print(\"\\nHypothesis:\")\n",
    "print(f\"  Timbre (audio advantage): {np.mean([dim_r2[d] for d in DIMENSION_CATEGORIES['timbre']]):+.4f}\")\n",
    "print(f\"  Pedal (audio advantage):  {np.mean([dim_r2[d] for d in DIMENSION_CATEGORIES['pedal']]):+.4f}\")\n",
    "print(f\"  Timing (symbolic better): {dim_r2['timing']:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    \"fold_results\": fold_results,\n",
    "    \"avg_r2\": float(avg),\n",
    "    \"std_r2\": float(std),\n",
    "    \"per_dimension_r2\": {k: float(v) for k, v in dim_r2.items()},\n",
    "    \"overall_r2\": float(r2_score(all_labels, all_preds)),\n",
    "}\n",
    "\n",
    "with open(CHECKPOINT_ROOT / \"results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "subprocess.run(['rclone', 'copy', str(CHECKPOINT_ROOT / \"results.json\"), GDRIVE_CHECKPOINTS])\n",
    "print(\"Results saved and synced!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Average R2: {avg:+.4f} +/- {std:.4f}\")\n",
    "print(f\"Target: >= 0.25\")\n",
    "print(f\"Status: {'PASS' if avg >= 0.25 else 'BELOW TARGET'}\")\n",
    "\n",
    "if avg >= 0.20:\n",
    "    print(\"\\nPhase A validation PASSED. Proceed to Phase B (Pianoteq).\")\n",
    "else:\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  - Try attention pooling\")\n",
    "    print(\"  - Try LSTM on MERT frames\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
