{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Strongest Paper Experiments for ISMIR 2026\n",
    "\n",
    "This notebook contains all experiments needed to make the strongest possible paper for ISMIR 2026 submission.\n",
    "\n",
    "## Experiment Structure\n",
    "\n",
    "### PHASE A: Configuration Optimization (RUN FIRST)\n",
    "Determine the optimal training configuration. All subsequent experiments use the winning combination.\n",
    "\n",
    "- **A1: Fold Method Comparison** - piece-based vs performer-based vs stratified\n",
    "  - Piece-based: Same performer can appear in train/test (current approach)\n",
    "  - Performer-based: Each performer in one fold only (tests generalization to unseen performers)\n",
    "  - Stratified: Balanced by composer/difficulty (reduces variance)\n",
    "  \n",
    "- **A2: Audio Source Comparison** - Salamander vs Pianoteq ensemble\n",
    "  - Salamander: 1202 samples, single timbre\n",
    "  - Pianoteq ensemble: 7212 samples (6 soundfonts), varied timbres\n",
    "\n",
    "### PHASE B: Core Validation\n",
    "Using optimal config from Phase A:\n",
    "\n",
    "- **B1: Multi-Seed Stability** - 5 seeds x 4 folds, report mean +/- std\n",
    "- **B2: Cross-Soundfont Generalization** - Train on 5 soundfonts, test on held-out 6th\n",
    "- **B3: Statistical Rigor** - Bootstrap CIs, significance tests vs baselines\n",
    "\n",
    "### PHASE C: Cross-Dataset Transfer\n",
    "Test generalization to external datasets:\n",
    "\n",
    "- **C1: PSyllabus Difficulty** - Correlation with external piano difficulty ratings\n",
    "- **C2: ASAP Multi-Performer** - Same pieces played by different performers\n",
    "- **C3: MAESTRO Zero-Shot** - Transfer to MAESTRO piano dataset\n",
    "\n",
    "### PHASE D: Advanced Analysis\n",
    "- **D1: Per-Dimension Breakdown** - R2 for each of 19 PercePiano dimensions\n",
    "- **D2: Error Analysis** - By composer, difficulty level, segment position\n",
    "\n",
    "## Available Audio Sources\n",
    "- **Salamander Grand Piano**: 1202 samples (original renders)\n",
    "- **Pianoteq Renders** (6 soundfonts x 1202 = 7212 samples):\n",
    "  - `HB_Steinway_Model_D`, `YC5_Vintage`, `K2_Basic`\n",
    "  - `NY_Steinway_D_Honky_Tonk`, `NY_Steinway_D_Worn_Out`, `U4_Small`\n",
    "\n",
    "## Requirements\n",
    "- **Compute**: Thunder Compute A100 (80GB VRAM)\n",
    "- **Storage**: rclone configured with `gdrive:` remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: CUDA setup (must be before any CUDA operations)\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:   \n",
    "    raise RuntimeError(\"GPU required for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install rclone\n",
    "!curl -fsSL https://rclone.org/install.sh | sudo bash 2>&1 | grep -E \"(successfully|already)\" || echo \"rclone installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Install dependencies and clone repo\n",
    "!pip install transformers librosa soundfile pytorch_lightning nnAudio scipy scikit-learn muq requests tqdm --quiet\n",
    "\n",
    "import os\n",
    "REPO_DIR = '/tmp/crescendai'\n",
    "if os.path.exists(REPO_DIR):\n",
    "    !cd {REPO_DIR} && git pull origin main\n",
    "else:\n",
    "    !git clone https://github.com/jai-dhiman/crescendai.git {REPO_DIR}\n",
    "\n",
    "print(f\"Repo: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Imports\n",
    "import sys\n",
    "sys.path.insert(0, f'{REPO_DIR}/model/src')\n",
    "\n",
    "import json\n",
    "import subprocess\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from audio_experiments import PERCEPIANO_DIMENSIONS, DIMENSION_CATEGORIES, BASE_CONFIG, SEED\n",
    "from audio_experiments.extractors import extract_muq_embeddings, MuQExtractor\n",
    "from audio_experiments.models import MuQStatsModel, StatsPoolingModel\n",
    "from audio_experiments.data import MERTDataset, mert_collate_fn\n",
    "from audio_experiments.training import (\n",
    "    run_4fold_mert_experiment,\n",
    "    should_run_experiment,\n",
    "    sync_experiment_to_gdrive,\n",
    "    get_completed_experiments,\n",
    "    print_experiment_status,\n",
    "    bootstrap_r2_extended,\n",
    "    compute_comprehensive_metrics,\n",
    ")\n",
    "from audio_experiments.training.sync import numpy_serializer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Imports: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Path configuration\n",
    "DATA_ROOT = Path('/tmp/strongest_paper_experiments')\n",
    "AUDIO_DIR = DATA_ROOT / 'audio'\n",
    "LABEL_DIR = DATA_ROOT / 'labels'\n",
    "MUQ_CACHE_ROOT = DATA_ROOT / 'muq_cache'\n",
    "CHECKPOINT_ROOT = DATA_ROOT / 'checkpoints'\n",
    "RESULTS_DIR = DATA_ROOT / 'results'\n",
    "LOG_DIR = DATA_ROOT / 'logs'\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "\n",
    "# Cross-dataset directories\n",
    "MAESTRO_DIR = DATA_ROOT / 'maestro'\n",
    "\n",
    "# Pianoteq soundfont directories\n",
    "PIANOTEQ_ROOT = DATA_ROOT / 'pianoteq'\n",
    "PIANOTEQ_SOUNDFONTS = [\n",
    "    'HB_Steinway_Model_D',\n",
    "    'YC5_Vintage',\n",
    "    'K2_Basic',\n",
    "    'NY_Steinway_D_Honky_Tonk',\n",
    "    'NY_Steinway_D_Worn_Out',\n",
    "    'U4_Small',\n",
    "]\n",
    "\n",
    "# GDrive paths\n",
    "GDRIVE_AUDIO = 'gdrive:crescendai_data/audio_baseline/percepiano_rendered'\n",
    "GDRIVE_PIANOTEQ = 'gdrive:crescendai_data/audio_baseline/pianoteq_rendered'\n",
    "GDRIVE_LABELS = 'gdrive:crescendai_data/percepiano_labels'\n",
    "GDRIVE_FOLDS = 'gdrive:crescendai_data/percepiano_fold_assignments.json'\n",
    "GDRIVE_MUQ_CACHE = 'gdrive:crescendai_data/audio_baseline/muq_embeddings'\n",
    "GDRIVE_PHASE2_RESULTS = 'gdrive:crescendai_data/checkpoints/audio_phase2'\n",
    "GDRIVE_RESULTS = 'gdrive:crescendai_data/checkpoints/strongest_paper'\n",
    "\n",
    "for d in [AUDIO_DIR, LABEL_DIR, MUQ_CACHE_ROOT, CHECKPOINT_ROOT,\n",
    "          RESULTS_DIR, LOG_DIR, FIGURES_DIR, MAESTRO_DIR, PIANOTEQ_ROOT]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_rclone(cmd, desc=\"\"):\n",
    "    \"\"\"Run rclone command with error handling.\"\"\"\n",
    "    if desc:\n",
    "        print(f\"{desc}...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"rclone failed: {desc}\\nCommand: {' '.join(cmd)}\\nStderr: {result.stderr}\")\n",
    "    return result\n",
    "\n",
    "# Check rclone\n",
    "result = subprocess.run(['rclone', 'listremotes'], capture_output=True, text=True)\n",
    "if 'gdrive:' not in result.stdout:\n",
    "    raise RuntimeError(\"rclone 'gdrive' not configured\")\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"GDrive results: {GDRIVE_RESULTS}\")\n",
    "print(f\"Pianoteq soundfonts: {PIANOTEQ_SOUNDFONTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Download data\n",
    "run_rclone(['rclone', 'copy', GDRIVE_AUDIO, str(AUDIO_DIR), '--progress'], \"Downloading audio\")\n",
    "run_rclone(['rclone', 'copy', GDRIVE_LABELS, str(LABEL_DIR)], \"Downloading labels\")\n",
    "\n",
    "FOLD_FILE = DATA_ROOT / 'folds.json'\n",
    "run_rclone(['rclone', 'copyto', GDRIVE_FOLDS, str(FOLD_FILE)], \"Downloading folds\")\n",
    "\n",
    "# Load labels and folds\n",
    "LABEL_FILE = LABEL_DIR / 'label_2round_mean_reg_19_with0_rm_highstd0.json'\n",
    "with open(LABEL_FILE) as f:\n",
    "    LABELS = json.load(f)\n",
    "with open(FOLD_FILE) as f:\n",
    "    FOLD_ASSIGNMENTS = json.load(f)\n",
    "\n",
    "# Create key->fold_id mapping\n",
    "FOLD_BY_KEY = {}\n",
    "for fold_id in range(4):\n",
    "    for key in FOLD_ASSIGNMENTS.get(f\"fold_{fold_id}\", []):\n",
    "        FOLD_BY_KEY[key] = fold_id\n",
    "\n",
    "ALL_KEYS = sorted(FOLD_BY_KEY.keys())\n",
    "print(f\"Samples per fold: {[len(FOLD_ASSIGNMENTS.get(f'fold_{i}', [])) for i in range(4)]}\")\n",
    "print(f\"Total samples: {len(ALL_KEYS)}\")\n",
    "print(f\"Audio files: {len(list(AUDIO_DIR.glob('*.wav')))}\")\n",
    "\n",
    "# Initialize results tracking\n",
    "ALL_RESULTS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lwn5vri1lqk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Initialize results tracking and check completed experiments\n",
    "ALL_RESULTS = {}\n",
    "\n",
    "# Get completed experiments from GDrive to enable resume/skip\n",
    "print(\"Checking GDrive for completed experiments...\")\n",
    "COMPLETED_CACHE = get_completed_experiments(GDRIVE_RESULTS)\n",
    "print(f\"Found {len(COMPLETED_CACHE)} completed experiments on GDrive\")\n",
    "\n",
    "if COMPLETED_CACHE:\n",
    "    print(\"\\nCompleted experiments:\")\n",
    "    for exp_id, r2 in sorted(COMPLETED_CACHE.items()):\n",
    "        print(f\"  {exp_id}: R2={r2:.4f}\")\n",
    "\n",
    "# Define all experiment IDs for this notebook (in execution order)\n",
    "EXPERIMENT_IDS = [\n",
    "    # PHASE A: Configuration Optimization\n",
    "    'A1a_piece_fold',           # Piece-based folds (current)\n",
    "    'A1b_performer_fold',       # Performer-based folds\n",
    "    'A1c_stratified_fold',      # Stratified by composer/difficulty\n",
    "    'A2_pianoteq_ensemble',     # All 6 Pianoteq soundfonts\n",
    "    \n",
    "    # PHASE B: Core Validation\n",
    "    'B1_multi_seed_stability',  # 5 seeds x 4 folds\n",
    "    'B2_cross_soundfont',       # Leave-one-out soundfont\n",
    "    'B3_bootstrap_significance',# Bootstrap CIs + significance tests\n",
    "    \n",
    "    # PHASE C: Cross-Dataset Transfer\n",
    "    'C1_psyllabus_difficulty',  # PSyllabus difficulty correlation\n",
    "    'C2_asap_multiperformer',   # ASAP multi-performer analysis\n",
    "    'C3_maestro_transfer',      # MAESTRO zero-shot\n",
    "    \n",
    "    # PHASE D: Advanced Analysis\n",
    "    'D1_per_dimension',         # Per-dimension R2 breakdown\n",
    "    'D2_error_analysis',        # By composer, difficulty, position\n",
    "]\n",
    "\n",
    "print_experiment_status(EXPERIMENT_IDS, COMPLETED_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vddl8g5ay0d",
   "metadata": {},
   "source": [
    "---\n",
    "## PHASE A: Configuration Optimization\n",
    "\n",
    "**Run these experiments FIRST.** Results determine the optimal training configuration for all subsequent experiments.\n",
    "\n",
    "### A1: Fold Method Comparison\n",
    "\n",
    "Compare three fold assignment strategies on Salamander audio:\n",
    "\n",
    "| Method | Description | What it tests |\n",
    "|--------|-------------|---------------|\n",
    "| **Piece-based** | Random split by piece (current) | Standard CV |\n",
    "| **Performer-based** | Each performer in one fold only | Generalization to unseen performers |\n",
    "| **Stratified** | Balanced by composer + difficulty | Reduced variance |\n",
    "\n",
    "### A2: Audio Source Comparison\n",
    "\n",
    "Using the best fold method from A1, compare training data:\n",
    "\n",
    "| Source | Samples | Description |\n",
    "|--------|---------|-------------|\n",
    "| **Salamander** | 1,202 | Single soundfont baseline |\n",
    "| **Pianoteq ensemble** | 7,212 | All 6 soundfonts (implicit augmentation) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q1crjxp91bp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Download MuQ L9-12 embeddings\n",
    "MUQ_L9_12_DIR = MUQ_CACHE_ROOT / 'L9-12'\n",
    "MUQ_L9_12_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GDRIVE_MUQ_L9_12 = 'gdrive:crescendai_data/audio_baseline/muq_embeddings/L9-12'\n",
    "\n",
    "# Try to download cached embeddings\n",
    "result = subprocess.run(['rclone', 'lsf', GDRIVE_MUQ_L9_12], capture_output=True, text=True)\n",
    "if result.returncode == 0 and result.stdout.strip():\n",
    "    print(\"Downloading cached MuQ L9-12 embeddings...\")\n",
    "    run_rclone(['rclone', 'copy', GDRIVE_MUQ_L9_12, str(MUQ_L9_12_DIR), '--progress'],\n",
    "               \"Downloading MuQ L9-12 embeddings\")\n",
    "else:\n",
    "    print(\"No cached MuQ L9-12 embeddings found. Will extract from audio.\")\n",
    "\n",
    "# Check what we have\n",
    "cached_keys = {p.stem for p in MUQ_L9_12_DIR.glob('*.pt')}\n",
    "missing_keys = [k for k in ALL_KEYS if k not in cached_keys]\n",
    "print(f\"MuQ L9-12 Cached: {len(cached_keys)}, Missing: {len(missing_keys)}\")\n",
    "\n",
    "# Extract missing embeddings\n",
    "if missing_keys:\n",
    "    print(f\"\\nExtracting {len(missing_keys)} MuQ L9-12 embeddings...\")\n",
    "    extract_muq_embeddings(AUDIO_DIR, MUQ_L9_12_DIR, missing_keys, layer_start=9, layer_end=13)\n",
    "    \n",
    "    # Upload newly extracted embeddings\n",
    "    print(\"\\nUploading MuQ L9-12 embeddings to GDrive...\")\n",
    "    run_rclone(['rclone', 'copy', str(MUQ_L9_12_DIR), GDRIVE_MUQ_L9_12],\n",
    "               \"Uploading MuQ L9-12 embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kipoivwu3t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Define seeds and model configuration for multi-seed stability\n",
    "STABILITY_SEEDS = [42, 123, 456, 789, 1337]\n",
    "\n",
    "# M1c_muq_L9-12 configuration (best performing model)\n",
    "M1C_CONFIG = {\n",
    "    **BASE_CONFIG,\n",
    "    'input_dim': 1024,\n",
    "    'hidden_dim': 512,\n",
    "    'dropout': 0.2,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'pooling_stats': 'mean_std',\n",
    "}\n",
    "\n",
    "def make_muq_stats_model(cfg):\n",
    "    \"\"\"Factory function for MuQ stats model.\"\"\"\n",
    "    return MuQStatsModel(\n",
    "        input_dim=cfg['input_dim'],\n",
    "        hidden_dim=cfg['hidden_dim'],\n",
    "        dropout=cfg['dropout'],\n",
    "        learning_rate=cfg['learning_rate'],\n",
    "        weight_decay=cfg['weight_decay'],\n",
    "        pooling_stats=cfg['pooling_stats'],\n",
    "        max_epochs=cfg['max_epochs'],\n",
    "    )\n",
    "\n",
    "print(f\"Seeds to test: {STABILITY_SEEDS}\")\n",
    "print(f\"Configuration: hidden_dim={M1C_CONFIG['hidden_dim']}, lr={M1C_CONFIG['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r76rzamzom8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1a: Piece-based fold baseline (current approach)\n",
    "exp_id = 'A1a_piece_fold'\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    exp_checkpoint_dir = CHECKPOINT_ROOT / exp_id\n",
    "    exp_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pl.seed_everything(42, workers=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: MuQ L9-12 with piece-based folds (current methodology)\")\n",
    "    print(f\"Audio: Salamander | Folds: Piece-based | Samples: {len(ALL_KEYS)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    existing_folds = [f for f in range(4) if (exp_checkpoint_dir / f\"fold{f}_best.ckpt\").exists()]\n",
    "    if existing_folds:\n",
    "        print(f\"Found existing checkpoints for folds: {existing_folds}\")\n",
    "    \n",
    "    existing_results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "    fold_results = {}\n",
    "    if existing_results_file.exists():\n",
    "        with open(existing_results_file) as f:\n",
    "            existing_data = json.load(f)\n",
    "            if 'fold_results' in existing_data:\n",
    "                fold_results = {int(k): v for k, v in existing_data['fold_results'].items()}\n",
    "    \n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for fold in range(4):\n",
    "        ckpt_path = exp_checkpoint_dir / f\"fold{fold}_best.ckpt\"\n",
    "        \n",
    "        if ckpt_path.exists():\n",
    "            print(f\"Fold {fold}: Loading existing checkpoint\")\n",
    "            model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "        else:\n",
    "            train_ds = MERTDataset(MUQ_L9_12_DIR, LABELS, FOLD_ASSIGNMENTS, fold, \"train\", M1C_CONFIG[\"max_frames\"])\n",
    "            val_ds = MERTDataset(MUQ_L9_12_DIR, LABELS, FOLD_ASSIGNMENTS, fold, \"val\", M1C_CONFIG[\"max_frames\"])\n",
    "            \n",
    "            print(f\"Fold {fold}: Training ({len(train_ds)} train, {len(val_ds)} val)\")\n",
    "            \n",
    "            train_dl = DataLoader(train_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=True,\n",
    "                                  collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "            val_dl = DataLoader(val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                                collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "            \n",
    "            model = make_muq_stats_model(M1C_CONFIG)\n",
    "            callbacks = [\n",
    "                ModelCheckpoint(dirpath=exp_checkpoint_dir, filename=f\"fold{fold}_best\",\n",
    "                               monitor=\"val_r2\", mode=\"max\", save_top_k=1),\n",
    "                EarlyStopping(monitor=\"val_r2\", mode=\"max\", patience=M1C_CONFIG[\"patience\"], verbose=False),\n",
    "            ]\n",
    "            \n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=M1C_CONFIG[\"max_epochs\"], callbacks=callbacks,\n",
    "                logger=CSVLogger(save_dir=LOG_DIR, name=exp_id, version=f\"fold{fold}\"),\n",
    "                accelerator=\"auto\", devices=1, gradient_clip_val=M1C_CONFIG[\"gradient_clip_val\"],\n",
    "                enable_progress_bar=True, deterministic=True, log_every_n_steps=10,\n",
    "            )\n",
    "            \n",
    "            trainer.fit(model, train_dl, val_dl)\n",
    "            fold_results[fold] = float(callbacks[0].best_model_score or 0)\n",
    "            print(f\"Fold {fold} complete: val_r2 = {fold_results[fold]:.4f}\")\n",
    "            model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "        \n",
    "        run_rclone(['rclone', 'copy', str(exp_checkpoint_dir), f'{GDRIVE_RESULTS}/checkpoints/{exp_id}'],\n",
    "                   f\"Uploading fold {fold} checkpoint\")\n",
    "        \n",
    "        # Evaluate\n",
    "        val_ds = MERTDataset(MUQ_L9_12_DIR, LABELS, FOLD_ASSIGNMENTS, fold, \"val\", M1C_CONFIG[\"max_frames\"])\n",
    "        val_dl = DataLoader(val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                            collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "        \n",
    "        model.eval().to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                pred = model(batch[\"embeddings\"].cuda(), batch[\"attention_mask\"].cuda(), batch.get(\"lengths\"))\n",
    "                all_preds.append(pred.cpu().numpy())\n",
    "                all_labels.append(batch[\"labels\"].numpy())\n",
    "        \n",
    "        if fold not in fold_results:\n",
    "            fold_results[fold] = float(r2_score(np.vstack(all_labels[-len(val_ds):]), np.vstack(all_preds[-len(val_ds):])))\n",
    "        \n",
    "        # Save intermediate\n",
    "        with open(existing_results_file, 'w') as f:\n",
    "            json.dump({'experiment_id': exp_id, 'status': 'in_progress', 'fold_results': fold_results,\n",
    "                      'audio_source': 'salamander', 'fold_method': 'piece_based'}, f, indent=2, default=numpy_serializer)\n",
    "        run_rclone(['rclone', 'copyto', str(existing_results_file), f'{GDRIVE_RESULTS}/{exp_id}.json'],\n",
    "                   f\"Uploading intermediate results (fold {fold})\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    overall_r2 = r2_score(np.vstack(all_labels), np.vstack(all_preds))\n",
    "    A1A_R2 = overall_r2\n",
    "    \n",
    "    print(f\"\\n{exp_id} Overall R2: {overall_r2:.4f}\")\n",
    "    print(f\"Per-fold R2: {[f'{fold_results[f]:.4f}' for f in range(4)]}\")\n",
    "    print(f\"Fold std: {np.std([fold_results[f] for f in range(4)]):.4f}\")\n",
    "    \n",
    "    final_results = {'experiment_id': exp_id, 'status': 'complete', 'overall_r2': overall_r2,\n",
    "                    'fold_results': fold_results, 'fold_std': float(np.std([fold_results[f] for f in range(4)])),\n",
    "                    'audio_source': 'salamander', 'fold_method': 'piece_based', 'n_samples': len(ALL_KEYS)}\n",
    "    with open(existing_results_file, 'w') as f:\n",
    "        json.dump(final_results, f, indent=2, default=numpy_serializer)\n",
    "    \n",
    "    sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)\n",
    "    ALL_RESULTS[exp_id] = final_results\n",
    "else:\n",
    "    results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "    if results_file.exists():\n",
    "        with open(results_file) as f:\n",
    "            ALL_RESULTS[exp_id] = json.load(f)\n",
    "        A1A_R2 = ALL_RESULTS[exp_id].get('overall_r2', 0)\n",
    "    else:\n",
    "        A1A_R2 = COMPLETED_CACHE.get(exp_id, 0)\n",
    "    print(f\"\\nSKIP {exp_id}: already complete (R2={A1A_R2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4hzm3idtum9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1b: Create performer-based fold assignments\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def get_performer_id(key: str) -> str:\n",
    "    \"\"\"Extract performer ID from sample key.\n",
    "    \n",
    "    Key format: 'Performer_Piece_Segment' or similar.\n",
    "    We want to group by performer so same performer is never in both train and test.\n",
    "    \"\"\"\n",
    "    parts = key.split('_')\n",
    "    # First part is typically the performer ID (e.g., 'P01', 'performer1', etc.)\n",
    "    return parts[-1] if parts else key\n",
    "\n",
    "# Get unique performers and their samples\n",
    "performers = defaultdict(list)\n",
    "for key in ALL_KEYS:\n",
    "    performer_id = get_performer_id(key)\n",
    "    performers[performer_id].append(key)\n",
    "\n",
    "print(f\"Found {len(performers)} unique performers\")\n",
    "print(f\"Samples per performer: min={min(len(v) for v in performers.values())}, \"\n",
    "      f\"max={max(len(v) for v in performers.values())}, \"\n",
    "      f\"mean={np.mean([len(v) for v in performers.values()]):.1f}\")\n",
    "\n",
    "# Create performer-based 4-fold split\n",
    "performer_list = list(performers.keys())\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "PERFORMER_FOLD_ASSIGNMENTS = {}\n",
    "PERFORMER_FOLD_BY_KEY = {}\n",
    "\n",
    "for fold_idx, (train_perf_idx, val_perf_idx) in enumerate(kf.split(performer_list)):\n",
    "    val_performers = [performer_list[i] for i in val_perf_idx]\n",
    "    fold_keys = []\n",
    "    for perf in val_performers:\n",
    "        fold_keys.extend(performers[perf])\n",
    "    PERFORMER_FOLD_ASSIGNMENTS[f\"fold_{fold_idx}\"] = fold_keys\n",
    "    for key in fold_keys:\n",
    "        PERFORMER_FOLD_BY_KEY[key] = fold_idx\n",
    "\n",
    "print(f\"\\nPerformer-based fold sizes: {[len(PERFORMER_FOLD_ASSIGNMENTS[f'fold_{i}']) for i in range(4)]}\")\n",
    "\n",
    "# Save performer fold assignments\n",
    "performer_folds_file = RESULTS_DIR / 'performer_fold_assignments.json'\n",
    "with open(performer_folds_file, 'w') as f:\n",
    "    json.dump(PERFORMER_FOLD_ASSIGNMENTS, f, indent=2)\n",
    "run_rclone(['rclone', 'copyto', str(performer_folds_file), \n",
    "           f'{GDRIVE_RESULTS}/performer_fold_assignments.json'],\n",
    "           \"Uploading performer fold assignments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "am7hzbkvq55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1b: Performer-based fold baseline\n",
    "exp_id = 'A1b_performer_fold'\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    exp_checkpoint_dir = CHECKPOINT_ROOT / exp_id\n",
    "    exp_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pl.seed_everything(42, workers=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: MuQ L9-12 with PERFORMER-based folds\")\n",
    "    print(f\"Audio: Salamander | Folds: Performer-based | Samples: {len(ALL_KEYS)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    existing_folds = [f for f in range(4) if (exp_checkpoint_dir / f\"fold{f}_best.ckpt\").exists()]\n",
    "    if existing_folds:\n",
    "        print(f\"Found existing checkpoints for folds: {existing_folds}\")\n",
    "    \n",
    "    existing_results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "    fold_results = {}\n",
    "    if existing_results_file.exists():\n",
    "        with open(existing_results_file) as f:\n",
    "            existing_data = json.load(f)\n",
    "            if 'fold_results' in existing_data:\n",
    "                fold_results = {int(k): v for k, v in existing_data['fold_results'].items()}\n",
    "    \n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for fold in range(4):\n",
    "        ckpt_path = exp_checkpoint_dir / f\"fold{fold}_best.ckpt\"\n",
    "        \n",
    "        if ckpt_path.exists():\n",
    "            print(f\"Fold {fold}: Loading existing checkpoint\")\n",
    "            model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "        else:\n",
    "            train_ds = MERTDataset(MUQ_L9_12_DIR, LABELS, PERFORMER_FOLD_ASSIGNMENTS, fold, \"train\", M1C_CONFIG[\"max_frames\"])\n",
    "            val_ds = MERTDataset(MUQ_L9_12_DIR, LABELS, PERFORMER_FOLD_ASSIGNMENTS, fold, \"val\", M1C_CONFIG[\"max_frames\"])\n",
    "            \n",
    "            print(f\"Fold {fold}: Training ({len(train_ds)} train, {len(val_ds)} val)\")\n",
    "            \n",
    "            train_dl = DataLoader(train_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=True,\n",
    "                                  collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "            val_dl = DataLoader(val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                                collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "            \n",
    "            model = make_muq_stats_model(M1C_CONFIG)\n",
    "            callbacks = [\n",
    "                ModelCheckpoint(dirpath=exp_checkpoint_dir, filename=f\"fold{fold}_best\",\n",
    "                               monitor=\"val_r2\", mode=\"max\", save_top_k=1),\n",
    "                EarlyStopping(monitor=\"val_r2\", mode=\"max\", patience=M1C_CONFIG[\"patience\"], verbose=False),\n",
    "            ]\n",
    "            \n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=M1C_CONFIG[\"max_epochs\"], callbacks=callbacks,\n",
    "                logger=CSVLogger(save_dir=LOG_DIR, name=exp_id, version=f\"fold{fold}\"),\n",
    "                accelerator=\"auto\", devices=1, gradient_clip_val=M1C_CONFIG[\"gradient_clip_val\"],\n",
    "                enable_progress_bar=True, deterministic=True, log_every_n_steps=10,\n",
    "            )\n",
    "            \n",
    "            trainer.fit(model, train_dl, val_dl)\n",
    "            fold_results[fold] = float(callbacks[0].best_model_score or 0)\n",
    "            print(f\"Fold {fold} complete: val_r2 = {fold_results[fold]:.4f}\")\n",
    "            model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "        \n",
    "        run_rclone(['rclone', 'copy', str(exp_checkpoint_dir), f'{GDRIVE_RESULTS}/checkpoints/{exp_id}'],\n",
    "                   f\"Uploading fold {fold} checkpoint\")\n",
    "        \n",
    "        val_ds = MERTDataset(MUQ_L9_12_DIR, LABELS, PERFORMER_FOLD_ASSIGNMENTS, fold, \"val\", M1C_CONFIG[\"max_frames\"])\n",
    "        val_dl = DataLoader(val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                            collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "        \n",
    "        model.eval().to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                pred = model(batch[\"embeddings\"].cuda(), batch[\"attention_mask\"].cuda(), batch.get(\"lengths\"))\n",
    "                all_preds.append(pred.cpu().numpy())\n",
    "                all_labels.append(batch[\"labels\"].numpy())\n",
    "        \n",
    "        if fold not in fold_results:\n",
    "            fold_results[fold] = float(r2_score(np.vstack(all_labels[-len(val_ds):]), np.vstack(all_preds[-len(val_ds):])))\n",
    "        \n",
    "        with open(existing_results_file, 'w') as f:\n",
    "            json.dump({'experiment_id': exp_id, 'status': 'in_progress', 'fold_results': fold_results,\n",
    "                      'audio_source': 'salamander', 'fold_method': 'performer_based'}, f, indent=2, default=numpy_serializer)\n",
    "        run_rclone(['rclone', 'copyto', str(existing_results_file), f'{GDRIVE_RESULTS}/{exp_id}.json'],\n",
    "                   f\"Uploading intermediate results (fold {fold})\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    overall_r2 = r2_score(np.vstack(all_labels), np.vstack(all_preds))\n",
    "    A1B_R2 = overall_r2\n",
    "    \n",
    "    print(f\"\\n{exp_id} Overall R2: {overall_r2:.4f}\")\n",
    "    print(f\"Per-fold R2: {[f'{fold_results[f]:.4f}' for f in range(4)]}\")\n",
    "    print(f\"Fold std: {np.std([fold_results[f] for f in range(4)]):.4f}\")\n",
    "    \n",
    "    final_results = {'experiment_id': exp_id, 'status': 'complete', 'overall_r2': overall_r2,\n",
    "                    'fold_results': fold_results, 'fold_std': float(np.std([fold_results[f] for f in range(4)])),\n",
    "                    'audio_source': 'salamander', 'fold_method': 'performer_based', 'n_samples': len(ALL_KEYS)}\n",
    "    with open(existing_results_file, 'w') as f:\n",
    "        json.dump(final_results, f, indent=2, default=numpy_serializer)\n",
    "    \n",
    "    sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)\n",
    "    ALL_RESULTS[exp_id] = final_results\n",
    "else:\n",
    "    results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "    if results_file.exists():\n",
    "        with open(results_file) as f:\n",
    "            ALL_RESULTS[exp_id] = json.load(f)\n",
    "        A1B_R2 = ALL_RESULTS[exp_id].get('overall_r2', 0)\n",
    "    else:\n",
    "        A1B_R2 = COMPLETED_CACHE.get(exp_id, 0)\n",
    "    print(f\"\\nSKIP {exp_id}: already complete (R2={A1B_R2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dx1luo78pk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Analyze current fold distribution\n",
    "def get_composer(key: str) -> str:\n",
    "    \"\"\"Extract composer from key (first part before underscore).\"\"\"\n",
    "    return key.split('_')[0]\n",
    "\n",
    "def get_composition_name(key: str) -> str:\n",
    "    \"\"\"Extract composition group name (without performer ID).\"\"\"\n",
    "    parts = key.split(\"_\")\n",
    "    prefix = \"_\".join(parts[:-2])  # Everything except last 2 parts\n",
    "    suffix = \"_\".join(parts[-1:])  # Last part (segment ID)\n",
    "    return prefix + \"_\" + suffix\n",
    "\n",
    "# Analyze current distribution\n",
    "print(\"=\"*60)\n",
    "print(\"CURRENT FOLD DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for fold_id in range(4):\n",
    "    fold_keys = FOLD_ASSIGNMENTS.get(f'fold_{fold_id}', [])\n",
    "    \n",
    "    # Composer distribution\n",
    "    composers = defaultdict(int)\n",
    "    for key in fold_keys:\n",
    "        composers[get_composer(key)] += 1\n",
    "    \n",
    "    # Unique compositions\n",
    "    compositions = set(get_composition_name(key) for key in fold_keys)\n",
    "    \n",
    "    print(f\"\\nFold {fold_id}: {len(fold_keys)} samples, {len(compositions)} compositions\")\n",
    "    print(f\"  Composers: {dict(composers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7su0lh071sf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Implement stratified fold assignment function\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def create_stratified_folds(labels: Dict, n_folds: int = 4, seed: int = 42) -> Dict:\n",
    "    \"\"\"Create stratified fold assignments balancing composer and difficulty.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Group samples by composition (piece+segment without performer)\n",
    "    2. Compute average label per composition as difficulty proxy\n",
    "    3. Bin compositions into difficulty quartiles\n",
    "    4. Stratify by (composer, difficulty_bin) to balance folds\n",
    "    \n",
    "    Returns:\n",
    "        Dict with fold_0, fold_1, fold_2, fold_3 keys\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    all_keys = list(labels.keys())\n",
    "    \n",
    "    # Group by composition\n",
    "    composition_groups = defaultdict(list)\n",
    "    for key in all_keys:\n",
    "        comp_name = get_composition_name(key)\n",
    "        composition_groups[comp_name].append(key)\n",
    "    \n",
    "    # Compute composition-level features for stratification\n",
    "    comp_features = []\n",
    "    comp_names = list(composition_groups.keys())\n",
    "    \n",
    "    for comp_name in comp_names:\n",
    "        keys = composition_groups[comp_name]\n",
    "        composer = get_composer(keys[0])\n",
    "        \n",
    "        # Average label across all dimensions as difficulty proxy\n",
    "        avg_labels = []\n",
    "        for key in keys:\n",
    "            avg_labels.append(np.mean(labels[key]))\n",
    "        avg_difficulty = np.mean(avg_labels)\n",
    "        \n",
    "        comp_features.append({\n",
    "            'comp_name': comp_name,\n",
    "            'composer': composer,\n",
    "            'avg_difficulty': avg_difficulty,\n",
    "            'n_samples': len(keys),\n",
    "        })\n",
    "    \n",
    "    # Bin difficulty into quartiles\n",
    "    difficulties = [f['avg_difficulty'] for f in comp_features]\n",
    "    quartiles = np.percentile(difficulties, [25, 50, 75])\n",
    "    \n",
    "    def get_difficulty_bin(d):\n",
    "        if d < quartiles[0]:\n",
    "            return 'Q1'\n",
    "        elif d < quartiles[1]:\n",
    "            return 'Q2'\n",
    "        elif d < quartiles[2]:\n",
    "            return 'Q3'\n",
    "        return 'Q4'\n",
    "    \n",
    "    # Create stratification labels: (composer, difficulty_bin)\n",
    "    strat_labels = []\n",
    "    for f in comp_features:\n",
    "        diff_bin = get_difficulty_bin(f['avg_difficulty'])\n",
    "        strat_labels.append(f\"{f['composer']}_{diff_bin}\")\n",
    "    \n",
    "    # Use StratifiedKFold on compositions\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    fold_assignments = {f'fold_{i}': [] for i in range(n_folds)}\n",
    "    \n",
    "    for fold_idx, (_, val_indices) in enumerate(skf.split(comp_names, strat_labels)):\n",
    "        for idx in val_indices:\n",
    "            comp_name = comp_names[idx]\n",
    "            fold_assignments[f'fold_{fold_idx}'].extend(composition_groups[comp_name])\n",
    "    \n",
    "    return fold_assignments\n",
    "\n",
    "print(\"Stratified fold assignment function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1oir6lhzfk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Generate new stratified fold assignments\n",
    "STRATIFIED_FOLD_ASSIGNMENTS = create_stratified_folds(LABELS, n_folds=4, seed=42)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NEW STRATIFIED FOLD DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for fold_id in range(4):\n",
    "    fold_keys = STRATIFIED_FOLD_ASSIGNMENTS.get(f'fold_{fold_id}', [])\n",
    "    \n",
    "    # Composer distribution\n",
    "    composers = defaultdict(int)\n",
    "    for key in fold_keys:\n",
    "        composers[get_composer(key)] += 1\n",
    "    \n",
    "    # Unique compositions\n",
    "    compositions = set(get_composition_name(key) for key in fold_keys)\n",
    "    \n",
    "    print(f\"\\nFold {fold_id}: {len(fold_keys)} samples, {len(compositions)} compositions\")\n",
    "    print(f\"  Composers: {dict(composers)}\")\n",
    "\n",
    "# Compare sample counts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Sample counts per fold\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original:   {[len(FOLD_ASSIGNMENTS.get(f'fold_{i}', [])) for i in range(4)]}\")\n",
    "print(f\"Stratified: {[len(STRATIFIED_FOLD_ASSIGNMENTS.get(f'fold_{i}', [])) for i in range(4)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ak2tzx50wdk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1c: Stratified fold baseline (balanced by composer/difficulty)\n",
    "exp_id = 'A1c_stratified_fold'\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    exp_checkpoint_dir = CHECKPOINT_ROOT / exp_id\n",
    "    exp_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pl.seed_everything(42, workers=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: MuQ L9-12 with STRATIFIED folds (composer/difficulty balanced)\")\n",
    "    print(f\"Audio: Salamander | Folds: Stratified | Samples: {len(ALL_KEYS)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    existing_folds = [f for f in range(4) if (exp_checkpoint_dir / f\"fold{f}_best.ckpt\").exists()]\n",
    "    if existing_folds:\n",
    "        print(f\"Found existing checkpoints for folds: {existing_folds}\")\n",
    "    \n",
    "    existing_results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "    fold_results = {}\n",
    "    if existing_results_file.exists():\n",
    "        with open(existing_results_file) as f:\n",
    "            existing_data = json.load(f)\n",
    "            if 'fold_results' in existing_data:\n",
    "                fold_results = {int(k): v for k, v in existing_data['fold_results'].items()}\n",
    "    \n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for fold in range(4):\n",
    "        ckpt_path = exp_checkpoint_dir / f\"fold{fold}_best.ckpt\"\n",
    "        \n",
    "        if ckpt_path.exists():\n",
    "            print(f\"Fold {fold}: Loading existing checkpoint\")\n",
    "            model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "        else:\n",
    "            train_ds = MERTDataset(MUQ_L9_12_DIR, LABELS, STRATIFIED_FOLD_ASSIGNMENTS, fold, \"train\", M1C_CONFIG[\"max_frames\"])\n",
    "            val_ds = MERTDataset(MUQ_L9_12_DIR, LABELS, STRATIFIED_FOLD_ASSIGNMENTS, fold, \"val\", M1C_CONFIG[\"max_frames\"])\n",
    "            \n",
    "            print(f\"Fold {fold}: Training ({len(train_ds)} train, {len(val_ds)} val)\")\n",
    "            \n",
    "            train_dl = DataLoader(train_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=True,\n",
    "                                  collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "            val_dl = DataLoader(val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                                collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "            \n",
    "            model = make_muq_stats_model(M1C_CONFIG)\n",
    "            callbacks = [\n",
    "                ModelCheckpoint(dirpath=exp_checkpoint_dir, filename=f\"fold{fold}_best\",\n",
    "                               monitor=\"val_r2\", mode=\"max\", save_top_k=1),\n",
    "                EarlyStopping(monitor=\"val_r2\", mode=\"max\", patience=M1C_CONFIG[\"patience\"], verbose=False),\n",
    "            ]\n",
    "            \n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=M1C_CONFIG[\"max_epochs\"], callbacks=callbacks,\n",
    "                logger=CSVLogger(save_dir=LOG_DIR, name=exp_id, version=f\"fold{fold}\"),\n",
    "                accelerator=\"auto\", devices=1, gradient_clip_val=M1C_CONFIG[\"gradient_clip_val\"],\n",
    "                enable_progress_bar=True, deterministic=True, log_every_n_steps=10,\n",
    "            )\n",
    "            \n",
    "            trainer.fit(model, train_dl, val_dl)\n",
    "            fold_results[fold] = float(callbacks[0].best_model_score or 0)\n",
    "            print(f\"Fold {fold} complete: val_r2 = {fold_results[fold]:.4f}\")\n",
    "            model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "        \n",
    "        run_rclone(['rclone', 'copy', str(exp_checkpoint_dir), f'{GDRIVE_RESULTS}/checkpoints/{exp_id}'],\n",
    "                   f\"Uploading fold {fold} checkpoint\")\n",
    "        \n",
    "        val_ds = MERTDataset(MUQ_L9_12_DIR, LABELS, STRATIFIED_FOLD_ASSIGNMENTS, fold, \"val\", M1C_CONFIG[\"max_frames\"])\n",
    "        val_dl = DataLoader(val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                            collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "        \n",
    "        model.eval().to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                pred = model(batch[\"embeddings\"].cuda(), batch[\"attention_mask\"].cuda(), batch.get(\"lengths\"))\n",
    "                all_preds.append(pred.cpu().numpy())\n",
    "                all_labels.append(batch[\"labels\"].numpy())\n",
    "        \n",
    "        if fold not in fold_results:\n",
    "            fold_results[fold] = float(r2_score(np.vstack(all_labels[-len(val_ds):]), np.vstack(all_preds[-len(val_ds):])))\n",
    "        \n",
    "        with open(existing_results_file, 'w') as f:\n",
    "            json.dump({'experiment_id': exp_id, 'status': 'in_progress', 'fold_results': fold_results,\n",
    "                      'audio_source': 'salamander', 'fold_method': 'stratified'}, f, indent=2, default=numpy_serializer)\n",
    "        run_rclone(['rclone', 'copyto', str(existing_results_file), f'{GDRIVE_RESULTS}/{exp_id}.json'],\n",
    "                   f\"Uploading intermediate results (fold {fold})\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    overall_r2 = r2_score(np.vstack(all_labels), np.vstack(all_preds))\n",
    "    A1C_R2 = overall_r2\n",
    "    A1C_STD = np.std([fold_results[f] for f in range(4)])\n",
    "    \n",
    "    print(f\"\\n{exp_id} Overall R2: {overall_r2:.4f}\")\n",
    "    print(f\"Per-fold R2: {[f'{fold_results[f]:.4f}' for f in range(4)]}\")\n",
    "    print(f\"Fold std: {A1C_STD:.4f}\")\n",
    "    \n",
    "    final_results = {'experiment_id': exp_id, 'status': 'complete', 'overall_r2': overall_r2,\n",
    "                    'fold_results': fold_results, 'fold_std': float(A1C_STD),\n",
    "                    'audio_source': 'salamander', 'fold_method': 'stratified', 'n_samples': len(ALL_KEYS)}\n",
    "    with open(existing_results_file, 'w') as f:\n",
    "        json.dump(final_results, f, indent=2, default=numpy_serializer)\n",
    "    \n",
    "    sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)\n",
    "    ALL_RESULTS[exp_id] = final_results\n",
    "else:\n",
    "    results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "    if results_file.exists():\n",
    "        with open(results_file) as f:\n",
    "            ALL_RESULTS[exp_id] = json.load(f)\n",
    "        A1C_R2 = ALL_RESULTS[exp_id].get('overall_r2', 0)\n",
    "        A1C_STD = ALL_RESULTS[exp_id].get('fold_std', 0)\n",
    "    else:\n",
    "        A1C_R2 = COMPLETED_CACHE.get(exp_id, 0)\n",
    "        A1C_STD = 0\n",
    "    print(f\"\\nSKIP {exp_id}: already complete (R2={A1C_R2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i8wnlh0hcpf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1 Results Comparison: Three-way fold method comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"A1 COMPARISON: FOLD METHOD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get std values\n",
    "A1A_STD = ALL_RESULTS.get('A1a_piece_fold', {}).get('fold_std', 0) if 'A1a_piece_fold' in ALL_RESULTS else 0\n",
    "A1B_STD = ALL_RESULTS.get('A1b_performer_fold', {}).get('fold_std', 0) if 'A1b_performer_fold' in ALL_RESULTS else 0\n",
    "\n",
    "print(f\"\\n{'Fold Method':<20} {'R2':>10} {'Fold Std':>12} {'Notes':<30}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"{'Piece-based':<20} {A1A_R2:>10.4f} {A1A_STD:>12.4f} {'Current approach':<30}\")\n",
    "print(f\"{'Performer-based':<20} {A1B_R2:>10.4f} {A1B_STD:>12.4f} {'Tests unseen performer gen.':<30}\")\n",
    "print(f\"{'Stratified':<20} {A1C_R2:>10.4f} {A1C_STD:>12.4f} {'Balanced composer/difficulty':<30}\")\n",
    "\n",
    "# Determine best fold method\n",
    "# Criteria: Highest R2, but if performer-based is close and has lower std, prefer it for rigor\n",
    "fold_methods = {\n",
    "    'piece_based': {'r2': A1A_R2, 'std': A1A_STD, 'assignments': FOLD_ASSIGNMENTS},\n",
    "    'performer_based': {'r2': A1B_R2, 'std': A1B_STD, 'assignments': PERFORMER_FOLD_ASSIGNMENTS},\n",
    "    'stratified': {'r2': A1C_R2, 'std': A1C_STD, 'assignments': STRATIFIED_FOLD_ASSIGNMENTS},\n",
    "}\n",
    "\n",
    "# Pick method with highest R2\n",
    "best_method = max(fold_methods.keys(), key=lambda k: fold_methods[k]['r2'])\n",
    "BEST_A1_R2 = fold_methods[best_method]['r2']\n",
    "BEST_FOLD_METHOD = best_method\n",
    "BEST_FOLD_ASSIGNMENTS = fold_methods[best_method]['assignments']\n",
    "\n",
    "# But add commentary\n",
    "print(f\"\\nBest R2: {best_method} ({BEST_A1_R2:.4f})\")\n",
    "\n",
    "if A1B_R2 < A1A_R2:\n",
    "    drop_pct = ((A1A_R2 - A1B_R2) / A1A_R2) * 100\n",
    "    print(f\"Performer-based shows {drop_pct:.1f}% lower R2 than piece-based.\")\n",
    "    print(\"This indicates some degree of performer-specific learning in piece-based folds.\")\n",
    "\n",
    "if A1C_STD < A1A_STD:\n",
    "    print(f\"Stratified folds reduce variance (std: {A1C_STD:.4f} vs {A1A_STD:.4f})\")\n",
    "\n",
    "print(f\"\\n>>> DECISION: Use {BEST_FOLD_METHOD.upper()} folds for A2 and Phase B+ <<<\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78gotmznjxq",
   "metadata": {},
   "source": [
    "### A2: Audio Source Comparison - Salamander vs Pianoteq Ensemble\n",
    "\n",
    "**Hypothesis**: Training on multiple soundfonts (same MIDI, different audio renders) improves robustness by forcing the model to learn performance characteristics rather than soundfont-specific timbral features.\n",
    "\n",
    "- **A2_salamander_baseline**: Uses A1 results (already computed)\n",
    "- **A2_pianoteq_ensemble**: Train on ALL 6 Pianoteq soundfonts (7212 samples)\n",
    "\n",
    "The Pianoteq ensemble effectively provides 6x data augmentation through timbre variation while keeping the performance labels constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3nx4xq9mg5x",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2a: Set up Pianoteq ensemble data (all 6 soundfonts)\n",
    "# Download audio and extract/cache MuQ embeddings for each soundfont\n",
    "\n",
    "PIANOTEQ_MUQ_DIRS = {}\n",
    "\n",
    "for soundfont in PIANOTEQ_SOUNDFONTS:\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Setting up {soundfont}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Audio directory\n",
    "    sf_audio_dir = PIANOTEQ_ROOT / soundfont\n",
    "    sf_audio_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download audio if not present\n",
    "    existing_files = list(sf_audio_dir.glob('*.wav'))\n",
    "    if len(existing_files) < len(ALL_KEYS):\n",
    "        print(f\"Downloading {soundfont} audio...\")\n",
    "        run_rclone(['rclone', 'copy', f'{GDRIVE_PIANOTEQ}/{soundfont}', str(sf_audio_dir), '--progress'],\n",
    "                   f\"Downloading {soundfont}\")\n",
    "    else:\n",
    "        print(f\"Audio already present: {len(existing_files)} files\")\n",
    "\n",
    "    # MuQ embedding directory\n",
    "    sf_muq_dir = MUQ_CACHE_ROOT / f'pianoteq_{soundfont}_L9-12'\n",
    "    sf_muq_dir.mkdir(parents=True, exist_ok=True)\n",
    "    PIANOTEQ_MUQ_DIRS[soundfont] = sf_muq_dir\n",
    "\n",
    "    # Check for cached embeddings on GDrive\n",
    "    gdrive_muq_path = f'gdrive:crescendai_data/audio_baseline/muq_embeddings/pianoteq_{soundfont}_L9-12'\n",
    "    result = subprocess.run(['rclone', 'lsf', gdrive_muq_path], capture_output=True, text=True)\n",
    "\n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        cached_count = len(result.stdout.strip().split(''))\n",
    "        if cached_count >= len(ALL_KEYS):\n",
    "            print(f\"Downloading cached MuQ embeddings ({cached_count} files)...\")\n",
    "            run_rclone(['rclone', 'copy', gdrive_muq_path, str(sf_muq_dir), '--progress'],\n",
    "                       f\"Downloading {soundfont} MuQ embeddings\")\n",
    "            continue\n",
    "\n",
    "    # Extract embeddings if not cached\n",
    "    existing_emb = list(sf_muq_dir.glob('*.pt'))\n",
    "    if len(existing_emb) < len(ALL_KEYS):\n",
    "        print(f\"Extracting MuQ L9-12 embeddings for {soundfont}...\")\n",
    "        # MuQExtractor takes layer_start/layer_end in constructor (layer_end is exclusive)\n",
    "        extractor = MuQExtractor(layer_start=9, layer_end=13, cache_dir=sf_muq_dir)\n",
    "\n",
    "        audio_files = sorted(sf_audio_dir.glob('*.wav'))\n",
    "        for audio_file in tqdm(audio_files, desc=f\"Extracting {soundfont}\"):\n",
    "            emb_file = sf_muq_dir / f\"{audio_file.stem}.pt\"\n",
    "            if emb_file.exists():\n",
    "                continue\n",
    "\n",
    "            # extract_from_file returns averaged embedding directly and caches it\n",
    "            extractor.extract_from_file(audio_file)\n",
    "\n",
    "        del extractor\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Upload to GDrive cache\n",
    "        print(f\"Uploading {soundfont} embeddings to GDrive cache...\")\n",
    "        run_rclone(['rclone', 'copy', str(sf_muq_dir), gdrive_muq_path],\n",
    "                   f\"Caching {soundfont} embeddings\")\n",
    "    else:\n",
    "        print(f\"Embeddings already extracted: {len(existing_emb)} files\")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Pianoteq ensemble setup complete: {len(PIANOTEQ_SOUNDFONTS)} soundfonts\")\n",
    "print(f\"Total samples available: {len(ALL_KEYS) * len(PIANOTEQ_SOUNDFONTS)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yg1h49n4na",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2b: Multi-soundfont dataset for Pianoteq ensemble training\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiSoundfontMERTDataset(Dataset):\n",
    "    \"\"\"Dataset that loads embeddings from multiple soundfont directories.\n",
    "    Each sample appears multiple times (once per soundfont) with the same label.\n",
    "    This effectively augments the data through timbre variation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        soundfont_dirs: Dict[str, Path],  # soundfont_name -> embedding_dir\n",
    "        labels: Dict[str, List[float]],\n",
    "        fold_assignments: Dict[str, List[str]],\n",
    "        fold: int,\n",
    "        split: str,  # \"train\" or \"val\"\n",
    "        max_frames: int = 300,\n",
    "    ):\n",
    "        self.soundfont_dirs = soundfont_dirs\n",
    "        self.labels = labels\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "        # Get keys for this split\n",
    "        if split == \"train\":\n",
    "            split_keys = []\n",
    "            for f in range(4):\n",
    "                if f != fold:\n",
    "                    split_keys.extend(fold_assignments.get(f\"fold_{f}\", []))\n",
    "        else:\n",
    "            split_keys = fold_assignments.get(f\"fold_{fold}\", [])\n",
    "        \n",
    "        # Filter to keys that have labels and exist in all soundfonts\n",
    "        self.samples = []  # List of (soundfont, key) tuples\n",
    "        soundfont_names = list(soundfont_dirs.keys())\n",
    "        \n",
    "        for key in split_keys:\n",
    "            if key not in labels:\n",
    "                continue\n",
    "            # Check if embedding exists in all soundfonts\n",
    "            all_exist = all(\n",
    "                (soundfont_dirs[sf] / f\"{key}.pt\").exists()\n",
    "                for sf in soundfont_names\n",
    "            )\n",
    "            if all_exist:\n",
    "                # Add one sample per soundfont\n",
    "                for sf in soundfont_names:\n",
    "                    self.samples.append((sf, key))\n",
    "        \n",
    "        self.samples = sorted(self.samples)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        soundfont, key = self.samples[idx]\n",
    "        emb_path = self.soundfont_dirs[soundfont] / f\"{key}.pt\"\n",
    "        \n",
    "        emb = torch.load(emb_path, weights_only=True)\n",
    "        \n",
    "        # Truncate/pad to max_frames\n",
    "        if emb.shape[0] > self.max_frames:\n",
    "            emb = emb[:self.max_frames]\n",
    "        \n",
    "        label = torch.tensor(self.labels[key], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            \"key\": f\"{soundfont}_{key}\",\n",
    "            \"embeddings\": emb,\n",
    "            \"labels\": label,\n",
    "            \"length\": emb.shape[0],\n",
    "        }\n",
    "\n",
    "print(\"MultiSoundfontMERTDataset defined\")\n",
    "print(f\"Available soundfonts: {list(PIANOTEQ_MUQ_DIRS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sl7f0annf0m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2c: Train on Pianoteq ensemble (all 6 soundfonts)\n",
    "exp_id = 'A2_pianoteq_ensemble'\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    exp_checkpoint_dir = CHECKPOINT_ROOT / exp_id\n",
    "    exp_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pl.seed_everything(42, workers=True)\n",
    "    \n",
    "    # Use the best fold method from A1\n",
    "    fold_assignments_to_use = BEST_FOLD_ASSIGNMENTS\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: MuQ L9-12 trained on ALL 6 Pianoteq soundfonts\")\n",
    "    print(f\"Audio: Pianoteq ensemble | Folds: {BEST_FOLD_METHOD} | Soundfonts: {len(PIANOTEQ_SOUNDFONTS)}\")\n",
    "    print(f\"Total training samples per fold: ~{len(ALL_KEYS) * len(PIANOTEQ_SOUNDFONTS) * 3 // 4}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    existing_folds = [f for f in range(4) if (exp_checkpoint_dir / f\"fold{f}_best.ckpt\").exists()]\n",
    "    if existing_folds:\n",
    "        print(f\"Found existing checkpoints for folds: {existing_folds}\")\n",
    "    \n",
    "    existing_results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "    fold_results = {}\n",
    "    if existing_results_file.exists():\n",
    "        with open(existing_results_file) as f:\n",
    "            existing_data = json.load(f)\n",
    "            if 'fold_results' in existing_data:\n",
    "                fold_results = {int(k): v for k, v in existing_data['fold_results'].items()}\n",
    "    \n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for fold in range(4):\n",
    "        ckpt_path = exp_checkpoint_dir / f\"fold{fold}_best.ckpt\"\n",
    "        \n",
    "        if ckpt_path.exists():\n",
    "            print(f\"Fold {fold}: Loading existing checkpoint\")\n",
    "            model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "        else:\n",
    "            # Create multi-soundfont datasets\n",
    "            train_ds = MultiSoundfontMERTDataset(\n",
    "                PIANOTEQ_MUQ_DIRS, LABELS, fold_assignments_to_use, fold, \"train\", M1C_CONFIG[\"max_frames\"]\n",
    "            )\n",
    "            val_ds = MultiSoundfontMERTDataset(\n",
    "                PIANOTEQ_MUQ_DIRS, LABELS, fold_assignments_to_use, fold, \"val\", M1C_CONFIG[\"max_frames\"]\n",
    "            )\n",
    "            \n",
    "            print(f\"Fold {fold}: Training ({len(train_ds)} train, {len(val_ds)} val)\")\n",
    "            \n",
    "            train_dl = DataLoader(\n",
    "                train_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=True,\n",
    "                collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True,\n",
    "            )\n",
    "            val_dl = DataLoader(\n",
    "                val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True,\n",
    "            )\n",
    "            \n",
    "            model = make_muq_stats_model(M1C_CONFIG)\n",
    "            \n",
    "            callbacks = [\n",
    "                ModelCheckpoint(\n",
    "                    dirpath=exp_checkpoint_dir, filename=f\"fold{fold}_best\",\n",
    "                    monitor=\"val_r2\", mode=\"max\", save_top_k=1,\n",
    "                ),\n",
    "                EarlyStopping(\n",
    "                    monitor=\"val_r2\", mode=\"max\", patience=M1C_CONFIG[\"patience\"], verbose=False\n",
    "                ),\n",
    "            ]\n",
    "            \n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=M1C_CONFIG[\"max_epochs\"],\n",
    "                callbacks=callbacks,\n",
    "                logger=CSVLogger(save_dir=LOG_DIR, name=exp_id, version=f\"fold{fold}\"),\n",
    "                accelerator=\"auto\", devices=1,\n",
    "                gradient_clip_val=M1C_CONFIG[\"gradient_clip_val\"],\n",
    "                enable_progress_bar=True, deterministic=True, log_every_n_steps=10,\n",
    "            )\n",
    "            \n",
    "            trainer.fit(model, train_dl, val_dl)\n",
    "            fold_results[fold] = float(callbacks[0].best_model_score or 0)\n",
    "            print(f\"Fold {fold} complete: val_r2 = {fold_results[fold]:.4f}\")\n",
    "            \n",
    "            model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "        \n",
    "        # Sync checkpoint to GDrive after each fold\n",
    "        run_rclone(['rclone', 'copy', str(exp_checkpoint_dir), \n",
    "                   f'{GDRIVE_RESULTS}/checkpoints/{exp_id}'],\n",
    "                   f\"Uploading fold {fold} checkpoint\")\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_ds = MultiSoundfontMERTDataset(\n",
    "            PIANOTEQ_MUQ_DIRS, LABELS, fold_assignments_to_use, fold, \"val\", M1C_CONFIG[\"max_frames\"]\n",
    "        )\n",
    "        val_dl = DataLoader(\n",
    "            val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "            collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True,\n",
    "        )\n",
    "        \n",
    "        model.eval().to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                pred = model(\n",
    "                    batch[\"embeddings\"].cuda(),\n",
    "                    batch[\"attention_mask\"].cuda(),\n",
    "                    batch.get(\"lengths\"),\n",
    "                )\n",
    "                all_preds.append(pred.cpu().numpy())\n",
    "                all_labels.append(batch[\"labels\"].numpy())\n",
    "        \n",
    "        if fold not in fold_results:\n",
    "            fold_preds = np.vstack(all_preds[-len(val_ds):])\n",
    "            fold_labels = np.vstack(all_labels[-len(val_ds):])\n",
    "            fold_results[fold] = float(r2_score(fold_labels, fold_preds))\n",
    "        \n",
    "        # Save intermediate results\n",
    "        intermediate_results = {\n",
    "            'experiment_id': exp_id,\n",
    "            'status': 'in_progress',\n",
    "            'fold_results': fold_results,\n",
    "            'audio_source': 'pianoteq_ensemble',\n",
    "            'soundfonts': PIANOTEQ_SOUNDFONTS,\n",
    "            'fold_method': BEST_FOLD_METHOD,\n",
    "        }\n",
    "        with open(existing_results_file, 'w') as f:\n",
    "            json.dump(intermediate_results, f, indent=2, default=numpy_serializer)\n",
    "        \n",
    "        run_rclone(['rclone', 'copyto', str(existing_results_file), \n",
    "                   f'{GDRIVE_RESULTS}/{exp_id}.json'],\n",
    "                   f\"Uploading intermediate results (fold {fold})\")\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Final results\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    overall_r2 = r2_score(all_labels, all_preds)\n",
    "    \n",
    "    A2_PIANOTEQ_R2 = overall_r2\n",
    "    print(f\"\\n{exp_id} Overall R2: {overall_r2:.4f}\")\n",
    "    print(f\"Per-fold R2: {[f'{fold_results[f]:.4f}' for f in range(4)]}\")\n",
    "    \n",
    "    final_results = {\n",
    "        'experiment_id': exp_id,\n",
    "        'status': 'complete',\n",
    "        'overall_r2': overall_r2,\n",
    "        'fold_results': fold_results,\n",
    "        'audio_source': 'pianoteq_ensemble',\n",
    "        'soundfonts': PIANOTEQ_SOUNDFONTS,\n",
    "        'n_soundfonts': len(PIANOTEQ_SOUNDFONTS),\n",
    "        'fold_method': BEST_FOLD_METHOD,\n",
    "        'total_samples': len(ALL_KEYS) * len(PIANOTEQ_SOUNDFONTS),\n",
    "    }\n",
    "    with open(existing_results_file, 'w') as f:\n",
    "        json.dump(final_results, f, indent=2, default=numpy_serializer)\n",
    "    \n",
    "    sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)\n",
    "    ALL_RESULTS[exp_id] = final_results\n",
    "else:\n",
    "    results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "    if results_file.exists():\n",
    "        with open(results_file) as f:\n",
    "            ALL_RESULTS[exp_id] = json.load(f)\n",
    "        A2_PIANOTEQ_R2 = ALL_RESULTS[exp_id].get('overall_r2', 0)\n",
    "    else:\n",
    "        A2_PIANOTEQ_R2 = COMPLETED_CACHE.get(exp_id, 0)\n",
    "    print(f\"\\nSKIP {exp_id}: already complete (R2={A2_PIANOTEQ_R2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tjdit6f4sto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2 Results Comparison: Salamander vs Pianoteq Ensemble\n",
    "# Note: A2_salamander_baseline is effectively BEST_A1_R2 (using the best fold method)\n",
    "\n",
    "A2_SALAMANDER_R2 = BEST_A1_R2  # From A1 comparison\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"A2 COMPARISON: SALAMANDER vs PIANOTEQ ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Audio Source':<30} {'Samples':>10} {'R2':>10} {'Delta':>10}\")\n",
    "print(\"-\"*65)\n",
    "print(f\"{'Salamander (single)':<30} {len(ALL_KEYS):>10} {A2_SALAMANDER_R2:>10.4f} {'baseline':>10}\")\n",
    "print(f\"{'Pianoteq ensemble (6x)':<30} {len(ALL_KEYS)*6:>10} {A2_PIANOTEQ_R2:>10.4f} {A2_PIANOTEQ_R2 - A2_SALAMANDER_R2:>+10.4f}\")\n",
    "\n",
    "if A2_PIANOTEQ_R2 > A2_SALAMANDER_R2:\n",
    "    improvement = ((A2_PIANOTEQ_R2 - A2_SALAMANDER_R2) / A2_SALAMANDER_R2) * 100\n",
    "    print(f\"\\nPianoteq ensemble shows {improvement:.1f}% improvement!\")\n",
    "    print(\"Multi-soundfont training improves robustness through timbre augmentation.\")\n",
    "    BEST_AUDIO_SOURCE = 'pianoteq_ensemble'\n",
    "    BEST_A2_R2 = A2_PIANOTEQ_R2\n",
    "else:\n",
    "    print(f\"\\nSalamander baseline is comparable or better.\")\n",
    "    print(\"Single soundfont training is sufficient for this task.\")\n",
    "    BEST_AUDIO_SOURCE = 'salamander'\n",
    "    BEST_A2_R2 = A2_SALAMANDER_R2\n",
    "\n",
    "print(f\"\\n>>> DECISION: Use {BEST_AUDIO_SOURCE.upper()} for subsequent experiments <<<\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuvbv2x4o9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase A Summary: Optimal Configuration for Phase B\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE A COMPLETE: OPTIMAL CONFIGURATION DETERMINED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n  Fold Method: {BEST_FOLD_METHOD.upper()}\")\n",
    "print(f\"  Audio Source: {BEST_AUDIO_SOURCE.upper()}\")\n",
    "print(f\"  Best R2 achieved: {max(BEST_A1_R2, A2_PIANOTEQ_R2):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Phase B experiments will use this configuration:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "if BEST_AUDIO_SOURCE == 'pianoteq_ensemble':\n",
    "    print(\"  - Training data: Pianoteq ensemble (all 6 soundfonts)\")\n",
    "    print(f\"  - Total samples: {len(ALL_KEYS) * len(PIANOTEQ_SOUNDFONTS)}\")\n",
    "    PHASE_B_MUQ_DIR = PIANOTEQ_MUQ_DIRS  # Dictionary for multi-soundfont\n",
    "    PHASE_B_DATASET_CLASS = MultiSoundfontMERTDataset\n",
    "else:\n",
    "    print(\"  - Training data: Salamander (single soundfont)\")\n",
    "    print(f\"  - Total samples: {len(ALL_KEYS)}\")\n",
    "    PHASE_B_MUQ_DIR = MUQ_L9_12_DIR\n",
    "    PHASE_B_DATASET_CLASS = MERTDataset\n",
    "\n",
    "if BEST_FOLD_METHOD == 'performer_based':\n",
    "    print(\"  - Fold method: Performer-based (stricter generalization test)\")\n",
    "    PHASE_B_FOLDS = PERFORMER_FOLD_ASSIGNMENTS\n",
    "else:\n",
    "    print(\"  - Fold method: Piece-based (standard approach)\")\n",
    "    PHASE_B_FOLDS = FOLD_ASSIGNMENTS\n",
    "\n",
    "# Save Phase A configuration\n",
    "phase_a_config = {\n",
    "    'best_fold_method': BEST_FOLD_METHOD,\n",
    "    'best_audio_source': BEST_AUDIO_SOURCE,\n",
    "    'A1_piece_fold_r2': A1_PIECE_FOLD_R2,\n",
    "    'A1_performer_fold_r2': A1_PERFORMER_FOLD_R2,\n",
    "    'A2_salamander_r2': A2_SALAMANDER_R2,\n",
    "    'A2_pianoteq_r2': A2_PIANOTEQ_R2,\n",
    "}\n",
    "config_file = RESULTS_DIR / 'phase_a_config.json'\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(phase_a_config, f, indent=2)\n",
    "\n",
    "run_rclone(['rclone', 'copyto', str(config_file), \n",
    "           f'{GDRIVE_RESULTS}/phase_a_config.json'],\n",
    "           \"Uploading Phase A configuration\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Ready for Phase B experiments!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92emiu939y",
   "metadata": {},
   "source": [
    "---\n",
    "## PHASE B: Core Validation Experiments\n",
    "\n",
    "Using the optimal configuration from Phase A, we now run the core validation experiments:\n",
    "\n",
    "- **B1: Multi-Seed Stability** - Validate R2 is stable across 5 random seeds\n",
    "- **B2: Stratified Folds** - Test if stratifying by composer/difficulty improves variance\n",
    "- **B3: D7 Completion** - Complete any missing experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beug3tu3e1r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1: Multi-Seed Stability Analysis\n",
    "# Tests whether the best configuration is stable across different random seeds\n",
    "\n",
    "exp_id = 'B1_multi_seed_stability'\n",
    "STABILITY_SEEDS = [42, 123, 456, 789, 1337]\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    exp_checkpoint_dir = CHECKPOINT_ROOT / exp_id\n",
    "    exp_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: Multi-seed stability using optimal Phase A config\")\n",
    "    print(f\"Audio: {BEST_AUDIO_SOURCE} | Folds: {BEST_FOLD_METHOD}\")\n",
    "    print(f\"Seeds: {STABILITY_SEEDS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Load existing results if resuming\n",
    "    existing_results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "    seed_results = {}\n",
    "    if existing_results_file.exists():\n",
    "        with open(existing_results_file) as f:\n",
    "            existing_data = json.load(f)\n",
    "            if 'seed_results' in existing_data:\n",
    "                seed_results = {int(k): v for k, v in existing_data['seed_results'].items()}\n",
    "    \n",
    "    for seed in STABILITY_SEEDS:\n",
    "        if seed in seed_results and seed_results[seed].get('overall_r2') is not None:\n",
    "            print(f\"\\nSKIP seed {seed}: already complete (R2={seed_results[seed]['overall_r2']:.4f})\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        pl.seed_everything(seed, workers=True)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        seed_dir = exp_checkpoint_dir / f\"seed_{seed}\"\n",
    "        seed_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        fold_r2_scores = {}\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        for fold in range(4):\n",
    "            ckpt_path = seed_dir / f\"fold{fold}_best.ckpt\"\n",
    "            \n",
    "            if ckpt_path.exists():\n",
    "                print(f\"  Fold {fold}: Loading existing checkpoint\")\n",
    "                model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "            else:\n",
    "                # Use optimal configuration from Phase A\n",
    "                if BEST_AUDIO_SOURCE == 'pianoteq_ensemble':\n",
    "                    train_ds = MultiSoundfontMERTDataset(\n",
    "                        PIANOTEQ_MUQ_DIRS, LABELS, PHASE_B_FOLDS, fold, \"train\", M1C_CONFIG[\"max_frames\"]\n",
    "                    )\n",
    "                    val_ds = MultiSoundfontMERTDataset(\n",
    "                        PIANOTEQ_MUQ_DIRS, LABELS, PHASE_B_FOLDS, fold, \"val\", M1C_CONFIG[\"max_frames\"]\n",
    "                    )\n",
    "                else:\n",
    "                    train_ds = MERTDataset(\n",
    "                        MUQ_L9_12_DIR, LABELS, PHASE_B_FOLDS, fold, \"train\", M1C_CONFIG[\"max_frames\"]\n",
    "                    )\n",
    "                    val_ds = MERTDataset(\n",
    "                        MUQ_L9_12_DIR, LABELS, PHASE_B_FOLDS, fold, \"val\", M1C_CONFIG[\"max_frames\"]\n",
    "                    )\n",
    "                \n",
    "                print(f\"  Fold {fold}: Training ({len(train_ds)} train, {len(val_ds)} val)\")\n",
    "                \n",
    "                train_dl = DataLoader(\n",
    "                    train_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=True,\n",
    "                    collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True,\n",
    "                )\n",
    "                val_dl = DataLoader(\n",
    "                    val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                    collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True,\n",
    "                )\n",
    "                \n",
    "                model = make_muq_stats_model(M1C_CONFIG)\n",
    "                \n",
    "                callbacks = [\n",
    "                    ModelCheckpoint(\n",
    "                        dirpath=seed_dir, filename=f\"fold{fold}_best\",\n",
    "                        monitor=\"val_r2\", mode=\"max\", save_top_k=1,\n",
    "                    ),\n",
    "                    EarlyStopping(\n",
    "                        monitor=\"val_r2\", mode=\"max\", patience=M1C_CONFIG[\"patience\"], verbose=False\n",
    "                    ),\n",
    "                ]\n",
    "                \n",
    "                trainer = pl.Trainer(\n",
    "                    max_epochs=M1C_CONFIG[\"max_epochs\"],\n",
    "                    callbacks=callbacks,\n",
    "                    logger=CSVLogger(save_dir=LOG_DIR, name=f\"{exp_id}_seed{seed}\", version=f\"fold{fold}\"),\n",
    "                    accelerator=\"auto\", devices=1,\n",
    "                    gradient_clip_val=M1C_CONFIG[\"gradient_clip_val\"],\n",
    "                    enable_progress_bar=True, deterministic=True, log_every_n_steps=10,\n",
    "                )\n",
    "                \n",
    "                trainer.fit(model, train_dl, val_dl)\n",
    "                fold_r2_scores[fold] = float(callbacks[0].best_model_score or 0)\n",
    "                print(f\"    Fold {fold} complete: val_r2 = {fold_r2_scores[fold]:.4f}\")\n",
    "                \n",
    "                model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "            \n",
    "            # Sync after each fold\n",
    "            run_rclone(['rclone', 'copy', str(seed_dir), \n",
    "                       f'{GDRIVE_RESULTS}/checkpoints/{exp_id}/seed_{seed}'],\n",
    "                       f\"Uploading seed {seed} fold {fold} checkpoint\")\n",
    "            \n",
    "            # Evaluate\n",
    "            if BEST_AUDIO_SOURCE == 'pianoteq_ensemble':\n",
    "                val_ds = MultiSoundfontMERTDataset(\n",
    "                    PIANOTEQ_MUQ_DIRS, LABELS, PHASE_B_FOLDS, fold, \"val\", M1C_CONFIG[\"max_frames\"]\n",
    "                )\n",
    "            else:\n",
    "                val_ds = MERTDataset(\n",
    "                    MUQ_L9_12_DIR, LABELS, PHASE_B_FOLDS, fold, \"val\", M1C_CONFIG[\"max_frames\"]\n",
    "                )\n",
    "            val_dl = DataLoader(\n",
    "                val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True,\n",
    "            )\n",
    "            \n",
    "            model.eval().to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dl:\n",
    "                    pred = model(\n",
    "                        batch[\"embeddings\"].cuda(),\n",
    "                        batch[\"attention_mask\"].cuda(),\n",
    "                        batch.get(\"lengths\"),\n",
    "                    )\n",
    "                    all_preds.append(pred.cpu().numpy())\n",
    "                    all_labels.append(batch[\"labels\"].numpy())\n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Compute seed-level metrics\n",
    "        all_preds_arr = np.vstack(all_preds)\n",
    "        all_labels_arr = np.vstack(all_labels)\n",
    "        seed_r2 = r2_score(all_labels_arr, all_preds_arr)\n",
    "        \n",
    "        seed_results[seed] = {\n",
    "            'overall_r2': float(seed_r2),\n",
    "            'fold_r2': fold_r2_scores if fold_r2_scores else None,\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nSeed {seed} Overall R2: {seed_r2:.4f}\")\n",
    "        \n",
    "        # Save intermediate results\n",
    "        intermediate_results = {\n",
    "            'experiment_id': exp_id,\n",
    "            'status': 'in_progress',\n",
    "            'seed_results': seed_results,\n",
    "            'audio_source': BEST_AUDIO_SOURCE,\n",
    "            'fold_method': BEST_FOLD_METHOD,\n",
    "        }\n",
    "        with open(existing_results_file, 'w') as f:\n",
    "            json.dump(intermediate_results, f, indent=2, default=numpy_serializer)\n",
    "        \n",
    "        run_rclone(['rclone', 'copyto', str(existing_results_file), \n",
    "                   f'{GDRIVE_RESULTS}/{exp_id}.json'],\n",
    "                   f\"Uploading intermediate results (seed {seed})\")\n",
    "    \n",
    "    # Compute stability statistics\n",
    "    r2_values = [seed_results[s]['overall_r2'] for s in STABILITY_SEEDS]\n",
    "    mean_r2 = np.mean(r2_values)\n",
    "    std_r2 = np.std(r2_values)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MULTI-SEED STABILITY RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Mean R2: {mean_r2:.4f} +/- {std_r2:.4f}\")\n",
    "    print(f\"Range: [{min(r2_values):.4f}, {max(r2_values):.4f}]\")\n",
    "    print(f\"CV: {(std_r2/mean_r2)*100:.2f}%\")\n",
    "    \n",
    "    final_results = {\n",
    "        'experiment_id': exp_id,\n",
    "        'status': 'complete',\n",
    "        'seed_results': seed_results,\n",
    "        'mean_r2': float(mean_r2),\n",
    "        'std_r2': float(std_r2),\n",
    "        'r2_range': [float(min(r2_values)), float(max(r2_values))],\n",
    "        'cv_percent': float((std_r2/mean_r2)*100),\n",
    "        'audio_source': BEST_AUDIO_SOURCE,\n",
    "        'fold_method': BEST_FOLD_METHOD,\n",
    "    }\n",
    "    with open(existing_results_file, 'w') as f:\n",
    "        json.dump(final_results, f, indent=2, default=numpy_serializer)\n",
    "    \n",
    "    sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)\n",
    "    ALL_RESULTS[exp_id] = final_results\n",
    "else:\n",
    "    print(f\"\\nSKIP {exp_id}: already complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpxm4eg9rbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2: Cross-Soundfont Generalization (Leave-One-Out)\n",
    "# Train on 5 soundfonts, test on held-out 6th - tests timbre invariance\n",
    "exp_id = 'B2_cross_soundfont'\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    exp_checkpoint_dir = CHECKPOINT_ROOT / exp_id\n",
    "    exp_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: Leave-one-out soundfont generalization\")\n",
    "    print(f\"Train on 5 soundfonts, test on held-out 6th\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    existing_results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "    loo_results = {}\n",
    "    \n",
    "    if existing_results_file.exists():\n",
    "        with open(existing_results_file) as f:\n",
    "            existing_data = json.load(f)\n",
    "            if 'loo_results' in existing_data:\n",
    "                loo_results = existing_data['loo_results']\n",
    "    \n",
    "    for held_out_sf in PIANOTEQ_SOUNDFONTS:\n",
    "        if held_out_sf in loo_results:\n",
    "            print(f\"\\nSKIP {held_out_sf}: already complete (R2={loo_results[held_out_sf]['test_r2']:.4f})\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- Held-out soundfont: {held_out_sf} ---\")\n",
    "        \n",
    "        # Train soundfonts (all except held-out)\n",
    "        train_soundfonts = {sf: PIANOTEQ_MUQ_DIRS[sf] for sf in PIANOTEQ_SOUNDFONTS if sf != held_out_sf}\n",
    "        test_soundfont_dir = PIANOTEQ_MUQ_DIRS[held_out_sf]\n",
    "        \n",
    "        pl.seed_everything(42, workers=True)\n",
    "        sf_checkpoint_dir = exp_checkpoint_dir / held_out_sf\n",
    "        sf_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Train on 5 soundfonts, 4-fold CV\n",
    "        fold_results = {}\n",
    "        all_test_preds, all_test_labels = [], []\n",
    "        \n",
    "        for fold in range(4):\n",
    "            ckpt_path = sf_checkpoint_dir / f\"fold{fold}_best.ckpt\"\n",
    "            \n",
    "            if ckpt_path.exists():\n",
    "                print(f\"  Fold {fold}: Loading existing checkpoint\")\n",
    "                model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "            else:\n",
    "                train_ds = MultiSoundfontMERTDataset(train_soundfonts, LABELS, BEST_FOLD_ASSIGNMENTS, \n",
    "                                                     fold, \"train\", M1C_CONFIG[\"max_frames\"])\n",
    "                val_ds = MultiSoundfontMERTDataset(train_soundfonts, LABELS, BEST_FOLD_ASSIGNMENTS,\n",
    "                                                    fold, \"val\", M1C_CONFIG[\"max_frames\"])\n",
    "                \n",
    "                print(f\"  Fold {fold}: Training on {len(train_soundfonts)} soundfonts ({len(train_ds)} samples)\")\n",
    "                \n",
    "                train_dl = DataLoader(train_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=True,\n",
    "                                      collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "                val_dl = DataLoader(val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                                    collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "                \n",
    "                model = make_muq_stats_model(M1C_CONFIG)\n",
    "                callbacks = [\n",
    "                    ModelCheckpoint(dirpath=sf_checkpoint_dir, filename=f\"fold{fold}_best\",\n",
    "                                   monitor=\"val_r2\", mode=\"max\", save_top_k=1),\n",
    "                    EarlyStopping(monitor=\"val_r2\", mode=\"max\", patience=M1C_CONFIG[\"patience\"], verbose=False),\n",
    "                ]\n",
    "                \n",
    "                trainer = pl.Trainer(\n",
    "                    max_epochs=M1C_CONFIG[\"max_epochs\"], callbacks=callbacks,\n",
    "                    logger=CSVLogger(save_dir=LOG_DIR, name=f\"{exp_id}_{held_out_sf}\", version=f\"fold{fold}\"),\n",
    "                    accelerator=\"auto\", devices=1, gradient_clip_val=M1C_CONFIG[\"gradient_clip_val\"],\n",
    "                    enable_progress_bar=True, deterministic=True, log_every_n_steps=10,\n",
    "                )\n",
    "                \n",
    "                trainer.fit(model, train_dl, val_dl)\n",
    "                fold_results[fold] = float(callbacks[0].best_model_score or 0)\n",
    "                model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "            \n",
    "            run_rclone(['rclone', 'copy', str(sf_checkpoint_dir), f'{GDRIVE_RESULTS}/checkpoints/{exp_id}/{held_out_sf}'],\n",
    "                       f\"Uploading {held_out_sf} fold {fold}\")\n",
    "            \n",
    "            # Test on held-out soundfont\n",
    "            test_ds = MERTDataset(test_soundfont_dir, LABELS, BEST_FOLD_ASSIGNMENTS, fold, \"val\", M1C_CONFIG[\"max_frames\"])\n",
    "            test_dl = DataLoader(test_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                                 collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "            \n",
    "            model.eval().to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                for batch in test_dl:\n",
    "                    pred = model(batch[\"embeddings\"].cuda(), batch[\"attention_mask\"].cuda(), batch.get(\"lengths\"))\n",
    "                    all_test_preds.append(pred.cpu().numpy())\n",
    "                    all_test_labels.append(batch[\"labels\"].numpy())\n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Compute held-out soundfont R2\n",
    "        test_r2 = r2_score(np.vstack(all_test_labels), np.vstack(all_test_preds))\n",
    "        \n",
    "        loo_results[held_out_sf] = {\n",
    "            'train_soundfonts': [sf for sf in PIANOTEQ_SOUNDFONTS if sf != held_out_sf],\n",
    "            'fold_results': fold_results,\n",
    "            'test_r2': float(test_r2),\n",
    "        }\n",
    "        \n",
    "        print(f\"  {held_out_sf} held-out test R2: {test_r2:.4f}\")\n",
    "        \n",
    "        # Save intermediate\n",
    "        with open(existing_results_file, 'w') as f:\n",
    "            json.dump({'experiment_id': exp_id, 'status': 'in_progress', 'loo_results': loo_results}, \n",
    "                     f, indent=2, default=numpy_serializer)\n",
    "        run_rclone(['rclone', 'copyto', str(existing_results_file), f'{GDRIVE_RESULTS}/{exp_id}.json'],\n",
    "                   f\"Uploading intermediate results ({held_out_sf})\")\n",
    "    \n",
    "    # Summary\n",
    "    test_r2s = [loo_results[sf]['test_r2'] for sf in PIANOTEQ_SOUNDFONTS]\n",
    "    mean_test_r2 = np.mean(test_r2s)\n",
    "    std_test_r2 = np.std(test_r2s)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"CROSS-SOUNDFONT GENERALIZATION RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n{'Held-out Soundfont':<30} {'Test R2':>10}\")\n",
    "    print(\"-\"*45)\n",
    "    for sf in PIANOTEQ_SOUNDFONTS:\n",
    "        print(f\"{sf:<30} {loo_results[sf]['test_r2']:>10.4f}\")\n",
    "    print(\"-\"*45)\n",
    "    print(f\"{'Mean'::<30} {mean_test_r2:>10.4f}\")\n",
    "    print(f\"{'Std'::<30} {std_test_r2:>10.4f}\")\n",
    "    \n",
    "    final_results = {\n",
    "        'experiment_id': exp_id, 'status': 'complete',\n",
    "        'loo_results': loo_results,\n",
    "        'mean_test_r2': float(mean_test_r2),\n",
    "        'std_test_r2': float(std_test_r2),\n",
    "        'fold_method': BEST_FOLD_METHOD,\n",
    "    }\n",
    "    with open(existing_results_file, 'w') as f:\n",
    "        json.dump(final_results, f, indent=2, default=numpy_serializer)\n",
    "    \n",
    "    sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)\n",
    "    ALL_RESULTS[exp_id] = final_results\n",
    "else:\n",
    "    print(f\"\\nSKIP {exp_id}: already complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swnsqrjlwmm",
   "metadata": {},
   "source": [
    "### B3: Statistical Rigor - Bootstrap CIs and Significance Tests\n",
    "\n",
    "**Goal**: Compute bootstrap confidence intervals and statistical significance for all Phase A comparisons.\n",
    "\n",
    "This ensures our claims about fold methods and audio sources are statistically sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatl86qfes6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B3: Bootstrap CIs and Significance Tests\n",
    "exp_id = 'B3_bootstrap_significance'\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: Bootstrap CIs and paired t-tests for all comparisons\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    N_BOOTSTRAP = 1000\n",
    "    \n",
    "    def bootstrap_ci(values, n_bootstrap=N_BOOTSTRAP, ci=0.95):\n",
    "        \"\"\"Compute bootstrap confidence interval.\"\"\"\n",
    "        bootstrapped = []\n",
    "        for _ in range(n_bootstrap):\n",
    "            sample = np.random.choice(values, size=len(values), replace=True)\n",
    "            bootstrapped.append(np.mean(sample))\n",
    "        lower = np.percentile(bootstrapped, (1-ci)/2 * 100)\n",
    "        upper = np.percentile(bootstrapped, (1+ci)/2 * 100)\n",
    "        return lower, upper\n",
    "    \n",
    "    results = {'comparisons': {}}\n",
    "    \n",
    "    # A1: Fold method comparison\n",
    "    print(\"\\n--- A1: Fold Method CIs ---\")\n",
    "    fold_method_r2s = {\n",
    "        'piece_based': A1A_R2,\n",
    "        'performer_based': A1B_R2,\n",
    "        'stratified': A1C_R2,\n",
    "    }\n",
    "    \n",
    "    for method, r2 in fold_method_r2s.items():\n",
    "        # Get per-fold R2s for bootstrap\n",
    "        exp_results = ALL_RESULTS.get(f'A1{\"a\" if method==\"piece_based\" else \"b\" if method==\"performer_based\" else \"c\"}_{method.replace(\"_based\", \"\") if \"based\" in method else method}_fold', {})\n",
    "        fold_r2s = list(exp_results.get('fold_results', {}).values())\n",
    "        if fold_r2s:\n",
    "            ci_low, ci_high = bootstrap_ci(fold_r2s)\n",
    "            print(f\"  {method}: R2={r2:.4f} [95% CI: {ci_low:.4f}, {ci_high:.4f}]\")\n",
    "            results['comparisons'][f'A1_{method}'] = {\n",
    "                'r2': r2, 'ci_95': [float(ci_low), float(ci_high)], 'fold_r2s': fold_r2s\n",
    "            }\n",
    "    \n",
    "    # Paired t-test: piece vs performer\n",
    "    if 'A1a_piece_fold' in ALL_RESULTS and 'A1b_performer_fold' in ALL_RESULTS:\n",
    "        piece_folds = list(ALL_RESULTS['A1a_piece_fold'].get('fold_results', {}).values())\n",
    "        perf_folds = list(ALL_RESULTS['A1b_performer_fold'].get('fold_results', {}).values())\n",
    "        if len(piece_folds) == len(perf_folds) == 4:\n",
    "            t_stat, p_value = stats.ttest_rel(piece_folds, perf_folds)\n",
    "            print(f\"\\n  Paired t-test (piece vs performer): t={t_stat:.3f}, p={p_value:.4f}\")\n",
    "            results['comparisons']['piece_vs_performer'] = {'t_stat': float(t_stat), 'p_value': float(p_value)}\n",
    "    \n",
    "    # A2: Audio source comparison  \n",
    "    print(\"\\n--- A2: Audio Source CIs ---\")\n",
    "    if 'A2_pianoteq_ensemble' in ALL_RESULTS:\n",
    "        ensemble_folds = list(ALL_RESULTS['A2_pianoteq_ensemble'].get('fold_results', {}).values())\n",
    "        if ensemble_folds:\n",
    "            ci_low, ci_high = bootstrap_ci(ensemble_folds)\n",
    "            print(f\"  Pianoteq ensemble: R2={A2_PIANOTEQ_R2:.4f} [95% CI: {ci_low:.4f}, {ci_high:.4f}]\")\n",
    "            results['comparisons']['A2_pianoteq'] = {\n",
    "                'r2': A2_PIANOTEQ_R2, 'ci_95': [float(ci_low), float(ci_high)]\n",
    "            }\n",
    "    \n",
    "    # B1: Multi-seed stability\n",
    "    if 'B1_multi_seed_stability' in ALL_RESULTS:\n",
    "        seed_r2s = [ALL_RESULTS['B1_multi_seed_stability']['seed_results'][str(s)]['overall_r2'] \n",
    "                    for s in [42, 123, 456, 789, 1337] \n",
    "                    if str(s) in ALL_RESULTS['B1_multi_seed_stability'].get('seed_results', {})]\n",
    "        if seed_r2s:\n",
    "            ci_low, ci_high = bootstrap_ci(seed_r2s)\n",
    "            print(f\"\\n--- B1: Multi-Seed Stability ---\")\n",
    "            print(f\"  Mean R2={np.mean(seed_r2s):.4f} +/- {np.std(seed_r2s):.4f}\")\n",
    "            print(f\"  [95% CI: {ci_low:.4f}, {ci_high:.4f}]\")\n",
    "            results['comparisons']['B1_stability'] = {\n",
    "                'mean_r2': float(np.mean(seed_r2s)), 'std_r2': float(np.std(seed_r2s)),\n",
    "                'ci_95': [float(ci_low), float(ci_high)]\n",
    "            }\n",
    "    \n",
    "    # B2: Cross-soundfont\n",
    "    if 'B2_cross_soundfont' in ALL_RESULTS:\n",
    "        loo_r2s = [ALL_RESULTS['B2_cross_soundfont']['loo_results'][sf]['test_r2'] \n",
    "                   for sf in PIANOTEQ_SOUNDFONTS \n",
    "                   if sf in ALL_RESULTS['B2_cross_soundfont'].get('loo_results', {})]\n",
    "        if loo_r2s:\n",
    "            ci_low, ci_high = bootstrap_ci(loo_r2s)\n",
    "            print(f\"\\n--- B2: Cross-Soundfont Generalization ---\")\n",
    "            print(f\"  Mean held-out R2={np.mean(loo_r2s):.4f} +/- {np.std(loo_r2s):.4f}\")\n",
    "            print(f\"  [95% CI: {ci_low:.4f}, {ci_high:.4f}]\")\n",
    "            results['comparisons']['B2_cross_soundfont'] = {\n",
    "                'mean_r2': float(np.mean(loo_r2s)), 'std_r2': float(np.std(loo_r2s)),\n",
    "                'ci_95': [float(ci_low), float(ci_high)]\n",
    "            }\n",
    "    \n",
    "    # Save results\n",
    "    existing_results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "    final_results = {\n",
    "        'experiment_id': exp_id, 'status': 'complete',\n",
    "        'n_bootstrap': N_BOOTSTRAP,\n",
    "        'comparisons': results['comparisons'],\n",
    "    }\n",
    "    with open(existing_results_file, 'w') as f:\n",
    "        json.dump(final_results, f, indent=2, default=numpy_serializer)\n",
    "    \n",
    "    sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)\n",
    "    ALL_RESULTS[exp_id] = final_results\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "else:\n",
    "    print(f\"\\nSKIP {exp_id}: already complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6p6np73ji16",
   "metadata": {},
   "source": [
    "---\n",
    "## PHASE C: Cross-Dataset Transfer\n",
    "\n",
    "Test generalization to external datasets. Uses the optimal configuration from Phase A.\n",
    "\n",
    "### Storage Requirements\n",
    "- **MAESTRO v2.0.0**: ~300GB (115GB ZIP + ~200GB extracted)\n",
    "- **ASAP**: ~1GB (metadata only, uses MAESTRO audio)\n",
    "- **PSyllabus**: ~20-50GB (YouTube audio via yt-dlp)\n",
    "- **Total**: ~320-350GB\n",
    "\n",
    "### Setup Order\n",
    "1. Run **MAESTRO + ASAP Setup** cell (downloads ~115GB, extracts ~200GB)\n",
    "2. Run **PSyllabus Setup** cell (downloads ~550 YouTube videos)\n",
    "3. Run experiments C1, C2, C3\n",
    "\n",
    "### Experiments\n",
    "\n",
    "**C1: PSyllabus Difficulty Correlation**\n",
    "- Correlate model predictions with external piano difficulty ratings (1-11 Henle scale)\n",
    "- Tests whether model captures skill-level-related features\n",
    "\n",
    "**C2: ASAP Multi-Performer Analysis**\n",
    "- Analyze prediction variance across different performers playing the same pieces\n",
    "- High variance = model captures performer-specific interpretation\n",
    "- Low variance = model captures piece-invariant features\n",
    "\n",
    "**C3: MAESTRO Zero-Shot Transfer**\n",
    "- Zero-shot evaluation on MAESTRO professional piano recordings\n",
    "- No ground truth labels - analyze prediction distribution\n",
    "- Expect higher mean predictions (professional competition recordings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maestro_asap_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAESTRO + ASAP Dataset Setup\n",
    "# Downloads MAESTRO v2.0.0 (~115GB ZIP, ~200GB extracted) and clones ASAP repository\n",
    "# ASAP links multiple performances of same pieces to MAESTRO audio\n",
    "# Storage required: ~300GB\n",
    "\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAESTRO_VERSION = 'v2.0.0'\n",
    "MAESTRO_URL = f'https://storage.googleapis.com/magentadata/datasets/maestro/{MAESTRO_VERSION}/maestro-{MAESTRO_VERSION}.zip'\n",
    "MAESTRO_ROOT = DATA_ROOT / 'maestro'\n",
    "MAESTRO_ZIP = MAESTRO_ROOT / f'maestro-{MAESTRO_VERSION}.zip'\n",
    "MAESTRO_EXTRACTED = MAESTRO_ROOT / f'maestro-{MAESTRO_VERSION}'\n",
    "MAESTRO_AUDIO_DIR = MAESTRO_EXTRACTED / '2018'  # MAESTRO organizes by year\n",
    "\n",
    "ASAP_DIR = DATA_ROOT / 'asap'\n",
    "ASAP_REPO = ASAP_DIR / 'asap-dataset'\n",
    "\n",
    "MAESTRO_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "ASAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def download_with_progress(url: str, output_path: Path, desc: str):\n",
    "    \"\"\"Download file with progress bar.\"\"\"\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=desc) as t:\n",
    "        urllib.request.urlretrieve(url, output_path, reporthook=t.update_to)\n",
    "\n",
    "# Step 1: Download MAESTRO if not exists\n",
    "if not MAESTRO_EXTRACTED.exists():\n",
    "    if not MAESTRO_ZIP.exists():\n",
    "        print(f\"Downloading MAESTRO {MAESTRO_VERSION} (~115GB)...\")\n",
    "        print(\"This will take a while depending on network speed.\")\n",
    "        download_with_progress(MAESTRO_URL, MAESTRO_ZIP, f\"MAESTRO {MAESTRO_VERSION}\")\n",
    "    else:\n",
    "        print(f\"MAESTRO ZIP exists: {MAESTRO_ZIP}\")\n",
    "    \n",
    "    # Extract MAESTRO\n",
    "    print(f\"Extracting MAESTRO (~200GB uncompressed)...\")\n",
    "    with zipfile.ZipFile(MAESTRO_ZIP, 'r') as zf:\n",
    "        zf.extractall(MAESTRO_ROOT)\n",
    "    print(f\"Extracted to: {MAESTRO_EXTRACTED}\")\n",
    "    \n",
    "    # Optionally delete ZIP to save space\n",
    "    # MAESTRO_ZIP.unlink()\n",
    "else:\n",
    "    print(f\"MAESTRO exists: {MAESTRO_EXTRACTED}\")\n",
    "\n",
    "# Step 2: Clone ASAP repository\n",
    "if not ASAP_REPO.exists():\n",
    "    print(\"Cloning ASAP repository...\")\n",
    "    result = subprocess.run(\n",
    "        ['git', 'clone', 'https://github.com/fosfrancesco/asap-dataset.git', str(ASAP_REPO)],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Failed to clone ASAP: {result.stderr}\")\n",
    "    print(f\"Cloned to: {ASAP_REPO}\")\n",
    "else:\n",
    "    print(f\"ASAP repo exists: {ASAP_REPO}\")\n",
    "\n",
    "# Step 3: Load MAESTRO metadata and create mapping\n",
    "maestro_csv = MAESTRO_EXTRACTED / f'maestro-{MAESTRO_VERSION}.csv'\n",
    "if maestro_csv.exists():\n",
    "    import csv\n",
    "    MAESTRO_METADATA = []\n",
    "    with open(maestro_csv) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            MAESTRO_METADATA.append(row)\n",
    "    print(f\"Loaded {len(MAESTRO_METADATA)} MAESTRO entries\")\n",
    "    \n",
    "    # Create canonical_title -> entries mapping\n",
    "    MAESTRO_BY_TITLE = {}\n",
    "    for entry in MAESTRO_METADATA:\n",
    "        title = entry['canonical_title']\n",
    "        if title not in MAESTRO_BY_TITLE:\n",
    "            MAESTRO_BY_TITLE[title] = []\n",
    "        MAESTRO_BY_TITLE[title].append(entry)\n",
    "    \n",
    "    # Find pieces with multiple performers (for ASAP-style analysis)\n",
    "    MAESTRO_MULTI_PERFORMER = {k: v for k, v in MAESTRO_BY_TITLE.items() if len(v) >= 2}\n",
    "    print(f\"Pieces with 2+ performances: {len(MAESTRO_MULTI_PERFORMER)}\")\n",
    "else:\n",
    "    print(f\"MAESTRO CSV not found: {maestro_csv}\")\n",
    "    MAESTRO_METADATA = []\n",
    "    MAESTRO_BY_TITLE = {}\n",
    "    MAESTRO_MULTI_PERFORMER = {}\n",
    "\n",
    "# Step 4: Load ASAP metadata and link to MAESTRO\n",
    "asap_metadata_dir = ASAP_REPO / 'metadata'\n",
    "if asap_metadata_dir.exists():\n",
    "    import yaml\n",
    "    \n",
    "    ASAP_ENTRIES = []\n",
    "    for yaml_file in asap_metadata_dir.glob('*.yml'):\n",
    "        with open(yaml_file) as f:\n",
    "            try:\n",
    "                data = yaml.safe_load(f)\n",
    "                if data:\n",
    "                    data['_file'] = yaml_file.stem\n",
    "                    ASAP_ENTRIES.append(data)\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    print(f\"Loaded {len(ASAP_ENTRIES)} ASAP entries\")\n",
    "    \n",
    "    # Find ASAP entries that link to MAESTRO\n",
    "    ASAP_WITH_MAESTRO = [e for e in ASAP_ENTRIES if 'maestro_audio_path' in str(e)]\n",
    "    print(f\"ASAP entries with MAESTRO links: {len(ASAP_WITH_MAESTRO)}\")\n",
    "else:\n",
    "    print(\"ASAP metadata directory not found\")\n",
    "    ASAP_ENTRIES = []\n",
    "    ASAP_WITH_MAESTRO = []\n",
    "\n",
    "print(f\"\\nMAESTRO + ASAP setup complete!\")\n",
    "print(f\"  MAESTRO audio: {MAESTRO_EXTRACTED}\")\n",
    "print(f\"  ASAP repo: {ASAP_REPO}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psyllabus_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSyllabus Dataset Setup\n",
    "# Downloads metadata from Zenodo and audio from YouTube via yt-dlp\n",
    "# Storage required: ~20-50GB depending on sampling\n",
    "# Note: YouTube downloads can be slow due to rate limiting\n",
    "\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "\n",
    "# Install yt-dlp if not available\n",
    "try:\n",
    "    import yt_dlp\n",
    "except ImportError:\n",
    "    print(\"Installing yt-dlp...\")\n",
    "    subprocess.run(['pip', 'install', '-q', 'yt-dlp'], check=True)\n",
    "    import yt_dlp\n",
    "\n",
    "PSYLLABUS_DIR = DATA_ROOT / 'psyllabus'\n",
    "PSYLLABUS_METADATA_URL = 'https://zenodo.org/records/14794592/files/new_clean_data.json?download=1'\n",
    "PSYLLABUS_METADATA_FILE = PSYLLABUS_DIR / 'new_clean_data.json'\n",
    "PSYLLABUS_AUDIO_DIR = PSYLLABUS_DIR / 'audio'\n",
    "PSYLLABUS_CHECKPOINT_FILE = PSYLLABUS_DIR / 'download_checkpoint.json'\n",
    "PSYLLABUS_MUQ_DIR = PSYLLABUS_DIR / 'muq_cache'\n",
    "\n",
    "PSYLLABUS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PSYLLABUS_AUDIO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PSYLLABUS_MUQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Target samples per difficulty level (1-11)\n",
    "SAMPLES_PER_DIFFICULTY = 50\n",
    "\n",
    "def download_with_requests(url: str, dest: Path, desc: str):\n",
    "    \"\"\"Download file with progress bar using requests.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    total = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(dest, 'wb') as f:\n",
    "        with tqdm(total=total, unit='B', unit_scale=True, desc=desc) as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "\n",
    "# Step 1: Download metadata from Zenodo\n",
    "if not PSYLLABUS_METADATA_FILE.exists():\n",
    "    print(\"Downloading PSyllabus metadata from Zenodo...\")\n",
    "    download_with_requests(PSYLLABUS_METADATA_URL, PSYLLABUS_METADATA_FILE, \"PSyllabus metadata\")\n",
    "else:\n",
    "    print(f\"PSyllabus metadata exists: {PSYLLABUS_METADATA_FILE}\")\n",
    "\n",
    "# Step 2: Load and parse metadata\n",
    "with open(PSYLLABUS_METADATA_FILE) as f:\n",
    "    psyllabus_raw = json.load(f)\n",
    "\n",
    "# Parse entries - structure may vary\n",
    "PSYLLABUS_ENTRIES = []\n",
    "if isinstance(psyllabus_raw, list):\n",
    "    PSYLLABUS_ENTRIES = psyllabus_raw\n",
    "elif isinstance(psyllabus_raw, dict):\n",
    "    if 'data' in psyllabus_raw:\n",
    "        PSYLLABUS_ENTRIES = psyllabus_raw['data']\n",
    "    else:\n",
    "        PSYLLABUS_ENTRIES = list(psyllabus_raw.values())\n",
    "\n",
    "# Filter entries with difficulty and YouTube URL\n",
    "PSYLLABUS_VALID = []\n",
    "for entry in PSYLLABUS_ENTRIES:\n",
    "    if isinstance(entry, dict):\n",
    "        difficulty = entry.get('difficulty') or entry.get('henle_level') or entry.get('level')\n",
    "        youtube_url = entry.get('youtube_url') or entry.get('url') or entry.get('video_url')\n",
    "        if difficulty is not None and youtube_url:\n",
    "            try:\n",
    "                diff_int = int(difficulty)\n",
    "                if 1 <= diff_int <= 11:\n",
    "                    PSYLLABUS_VALID.append({\n",
    "                        'id': entry.get('id', len(PSYLLABUS_VALID)),\n",
    "                        'title': entry.get('title', entry.get('piece_name', 'Unknown')),\n",
    "                        'composer': entry.get('composer', 'Unknown'),\n",
    "                        'difficulty': diff_int,\n",
    "                        'youtube_url': youtube_url,\n",
    "                    })\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "\n",
    "print(f\"Found {len(PSYLLABUS_VALID)} PSyllabus entries with difficulty and YouTube URL\")\n",
    "\n",
    "# Step 3: Sample evenly across difficulty levels\n",
    "from collections import defaultdict\n",
    "by_difficulty = defaultdict(list)\n",
    "for entry in PSYLLABUS_VALID:\n",
    "    by_difficulty[entry['difficulty']].append(entry)\n",
    "\n",
    "print(\"\\nEntries by difficulty level:\")\n",
    "for diff in sorted(by_difficulty.keys()):\n",
    "    print(f\"  Level {diff}: {len(by_difficulty[diff])} entries\")\n",
    "\n",
    "# Sample up to SAMPLES_PER_DIFFICULTY from each level\n",
    "random.seed(42)  # Reproducible sampling\n",
    "PSYLLABUS_SAMPLED = []\n",
    "for diff in sorted(by_difficulty.keys()):\n",
    "    entries = by_difficulty[diff]\n",
    "    n_sample = min(len(entries), SAMPLES_PER_DIFFICULTY)\n",
    "    sampled = random.sample(entries, n_sample)\n",
    "    PSYLLABUS_SAMPLED.extend(sampled)\n",
    "\n",
    "print(f\"\\nSampled {len(PSYLLABUS_SAMPLED)} entries for download\")\n",
    "\n",
    "# Step 4: Download YouTube audio with checkpointing\n",
    "# Load checkpoint if exists\n",
    "if PSYLLABUS_CHECKPOINT_FILE.exists():\n",
    "    with open(PSYLLABUS_CHECKPOINT_FILE) as f:\n",
    "        checkpoint = json.load(f)\n",
    "    downloaded_ids = set(checkpoint.get('downloaded', []))\n",
    "    failed_ids = set(checkpoint.get('failed', []))\n",
    "else:\n",
    "    downloaded_ids = set()\n",
    "    failed_ids = set()\n",
    "\n",
    "# Check already downloaded files\n",
    "existing_files = {p.stem for p in PSYLLABUS_AUDIO_DIR.glob('*.wav')}\n",
    "downloaded_ids.update(str(eid) for eid in existing_files if eid.isdigit())\n",
    "\n",
    "to_download = [e for e in PSYLLABUS_SAMPLED \n",
    "               if str(e['id']) not in downloaded_ids and str(e['id']) not in failed_ids]\n",
    "\n",
    "print(f\"Already downloaded: {len(downloaded_ids)}\")\n",
    "print(f\"Previously failed: {len(failed_ids)}\")\n",
    "print(f\"Remaining to download: {len(to_download)}\")\n",
    "\n",
    "def save_checkpoint():\n",
    "    with open(PSYLLABUS_CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump({'downloaded': list(downloaded_ids), 'failed': list(failed_ids)}, f)\n",
    "\n",
    "# Download with yt-dlp\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'wav',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'quiet': True,\n",
    "    'no_warnings': True,\n",
    "}\n",
    "\n",
    "for i, entry in enumerate(tqdm(to_download, desc=\"Downloading PSyllabus audio\")):\n",
    "    entry_id = str(entry['id'])\n",
    "    output_path = PSYLLABUS_AUDIO_DIR / f\"{entry_id}.wav\"\n",
    "    \n",
    "    try:\n",
    "        ydl_opts['outtmpl'] = str(PSYLLABUS_AUDIO_DIR / f\"{entry_id}.%(ext)s\")\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([entry['youtube_url']])\n",
    "        \n",
    "        if output_path.exists():\n",
    "            downloaded_ids.add(entry_id)\n",
    "        else:\n",
    "            failed_ids.add(entry_id)\n",
    "    except Exception as e:\n",
    "        failed_ids.add(entry_id)\n",
    "        print(f\"\\nFailed to download {entry_id}: {e}\")\n",
    "    \n",
    "    # Checkpoint every 10 downloads\n",
    "    if (i + 1) % 10 == 0:\n",
    "        save_checkpoint()\n",
    "    \n",
    "    # Rate limiting - wait 1-3 seconds between downloads\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "save_checkpoint()\n",
    "\n",
    "# Final status\n",
    "final_downloaded = list(PSYLLABUS_AUDIO_DIR.glob('*.wav'))\n",
    "print(f\"\\nPSyllabus setup complete!\")\n",
    "print(f\"  Downloaded audio files: {len(final_downloaded)}\")\n",
    "print(f\"  Failed downloads: {len(failed_ids)}\")\n",
    "\n",
    "# Create mapping of downloaded files with difficulty\n",
    "PSYLLABUS_DOWNLOADED = []\n",
    "for entry in PSYLLABUS_SAMPLED:\n",
    "    audio_path = PSYLLABUS_AUDIO_DIR / f\"{entry['id']}.wav\"\n",
    "    if audio_path.exists():\n",
    "        entry['audio_path'] = str(audio_path)\n",
    "        PSYLLABUS_DOWNLOADED.append(entry)\n",
    "\n",
    "print(f\"  Ready for analysis: {len(PSYLLABUS_DOWNLOADED)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1_psyllabus_exp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1: PSyllabus Difficulty Correlation\n",
    "# Correlate PercePiano predictions with external difficulty ratings\n",
    "# Requires: PSYLLABUS_DOWNLOADED from setup cell\n",
    "\n",
    "exp_id = 'C1_psyllabus_difficulty'\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: Correlate predictions with PSyllabus difficulty ratings\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Check if PSyllabus data is available\n",
    "    if 'PSYLLABUS_DOWNLOADED' not in dir() or len(PSYLLABUS_DOWNLOADED) == 0:\n",
    "        raise RuntimeError(\"Run PSyllabus setup cell first!\")\n",
    "    \n",
    "    print(f\"PSyllabus samples available: {len(PSYLLABUS_DOWNLOADED)}\")\n",
    "    \n",
    "    # Step 1: Extract MuQ embeddings for PSyllabus audio\n",
    "    psyllabus_keys = [str(e['id']) for e in PSYLLABUS_DOWNLOADED]\n",
    "    cached_psyllabus = {p.stem for p in PSYLLABUS_MUQ_DIR.glob('*.pt')}\n",
    "    missing_psyllabus = [k for k in psyllabus_keys if k not in cached_psyllabus]\n",
    "    \n",
    "    if missing_psyllabus:\n",
    "        print(f\"Extracting {len(missing_psyllabus)} MuQ embeddings...\")\n",
    "        extract_muq_embeddings(\n",
    "            PSYLLABUS_AUDIO_DIR, PSYLLABUS_MUQ_DIR, missing_psyllabus,\n",
    "            layer_start=9, layer_end=13  # MuQ L9-12\n",
    "        )\n",
    "    \n",
    "    # Step 2: Load best model from Phase A/B\n",
    "    best_ckpt = None\n",
    "    # Try several possible checkpoint locations\n",
    "    ckpt_candidates = [\n",
    "        CHECKPOINT_ROOT / 'A1c_stratified_fold' / 'fold0_best.ckpt',\n",
    "        CHECKPOINT_ROOT / 'A1a_piece_fold' / 'fold0_best.ckpt',\n",
    "        CHECKPOINT_ROOT / 'B1_multi_seed_stability' / 'seed_42' / 'fold0_best.ckpt',\n",
    "    ]\n",
    "    for ckpt in ckpt_candidates:\n",
    "        if ckpt.exists():\n",
    "            best_ckpt = ckpt\n",
    "            break\n",
    "    \n",
    "    if best_ckpt is None:\n",
    "        raise RuntimeError(\"No trained model found! Run Phase A experiments first.\")\n",
    "    \n",
    "    print(f\"Loading model from {best_ckpt.name}...\")\n",
    "    model = MuQStatsModel.load_from_checkpoint(best_ckpt)\n",
    "    model = model.to('cuda').eval()\n",
    "    \n",
    "    # Step 3: Run inference on PSyllabus\n",
    "    predictions = []\n",
    "    difficulties = []\n",
    "    \n",
    "    for entry in tqdm(PSYLLABUS_DOWNLOADED, desc=\"PSyllabus inference\"):\n",
    "        emb_path = PSYLLABUS_MUQ_DIR / f\"{entry['id']}.pt\"\n",
    "        if not emb_path.exists():\n",
    "            continue\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            emb = torch.load(emb_path, weights_only=True).unsqueeze(0).cuda()\n",
    "            if emb.shape[1] > 300:  # max_frames\n",
    "                emb = emb[:, :300, :]\n",
    "            mask = torch.ones(1, emb.shape[1], dtype=torch.bool).cuda()\n",
    "            pred = model(emb, mask).cpu().numpy()[0]\n",
    "            predictions.append(pred)\n",
    "            difficulties.append(entry['difficulty'])\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    difficulties = np.array(difficulties)\n",
    "    \n",
    "    print(f\"\\nAnalyzed {len(predictions)} PSyllabus samples\")\n",
    "    \n",
    "    # Step 4: Compute correlations between predictions and difficulty\n",
    "    from scipy.stats import spearmanr, pearsonr\n",
    "    \n",
    "    correlations = {}\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"CORRELATION WITH DIFFICULTY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"{'Dimension':<25} {'Spearman':>10} {'p-value':>10}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    for i, dim in enumerate(PERCEPIANO_DIMENSIONS):\n",
    "        rho, p = spearmanr(predictions[:, i], difficulties)\n",
    "        correlations[dim] = {'spearman_rho': float(rho), 'p_value': float(p)}\n",
    "        sig = '*' if p < 0.05 else ''\n",
    "        print(f\"{dim:<25} {rho:>10.3f} {p:>10.4f} {sig}\")\n",
    "    \n",
    "    # Overall quality correlation\n",
    "    overall_pred = predictions.mean(axis=1)\n",
    "    overall_rho, overall_p = spearmanr(overall_pred, difficulties)\n",
    "    print(\"-\"*50)\n",
    "    print(f\"{'Overall (mean)':<25} {overall_rho:>10.3f} {overall_p:>10.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    final_results = {\n",
    "        'experiment_id': exp_id,\n",
    "        'n_samples': len(predictions),\n",
    "        'difficulty_range': [int(difficulties.min()), int(difficulties.max())],\n",
    "        'correlations': correlations,\n",
    "        'overall_correlation': {'spearman_rho': float(overall_rho), 'p_value': float(overall_p)},\n",
    "        'by_difficulty': {\n",
    "            int(d): int((difficulties == d).sum())\n",
    "            for d in sorted(set(difficulties))\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    with open(RESULTS_DIR / f'{exp_id}.json', 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS)\n",
    "    ALL_RESULTS[exp_id] = final_results\n",
    "else:\n",
    "    print(f\"\\nSKIP {exp_id}: already complete\")\n",
    "    existing = RESULTS_DIR / f'{exp_id}.json'\n",
    "    if existing.exists():\n",
    "        with open(existing) as f:\n",
    "            ALL_RESULTS[exp_id] = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2_asap_exp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2: ASAP Multi-Performer Analysis\n",
    "# Analyze prediction variance for same pieces played by different performers\n",
    "# Uses MAESTRO audio via ASAP mappings\n",
    "# Requires: MAESTRO_MULTI_PERFORMER from setup cell\n",
    "\n",
    "exp_id = 'C2_asap_multiperformer'\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: Multi-performer variance analysis on ASAP/MAESTRO\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Check if MAESTRO data is available\n",
    "    if 'MAESTRO_MULTI_PERFORMER' not in dir() or len(MAESTRO_MULTI_PERFORMER) == 0:\n",
    "        raise RuntimeError(\"Run MAESTRO+ASAP setup cell first!\")\n",
    "    \n",
    "    print(f\"Multi-performer pieces available: {len(MAESTRO_MULTI_PERFORMER)}\")\n",
    "    \n",
    "    # Setup MuQ cache for MAESTRO\n",
    "    MAESTRO_MUQ_DIR = MAESTRO_ROOT / 'muq_cache'\n",
    "    MAESTRO_MUQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get list of audio files to process\n",
    "    maestro_audio_files = []\n",
    "    for title, entries in MAESTRO_MULTI_PERFORMER.items():\n",
    "        for entry in entries:\n",
    "            audio_path = MAESTRO_EXTRACTED / entry['audio_filename']\n",
    "            if audio_path.exists():\n",
    "                maestro_audio_files.append({\n",
    "                    'path': audio_path,\n",
    "                    'key': entry['audio_filename'].replace('.wav', '').replace('/', '_'),\n",
    "                    'title': title,\n",
    "                    'split': entry.get('split', 'unknown'),\n",
    "                })\n",
    "    \n",
    "    print(f\"Audio files for multi-performer pieces: {len(maestro_audio_files)}\")\n",
    "    \n",
    "    # Extract MuQ embeddings\n",
    "    cached_keys = {p.stem for p in MAESTRO_MUQ_DIR.glob('*.pt')}\n",
    "    missing_keys = [f for f in maestro_audio_files if f['key'] not in cached_keys]\n",
    "    \n",
    "    if missing_keys:\n",
    "        print(f\"Extracting {len(missing_keys)} MuQ embeddings...\")\n",
    "        # Create temp symlinks for extraction\n",
    "        temp_audio_dir = MAESTRO_ROOT / 'temp_audio'\n",
    "        temp_audio_dir.mkdir(exist_ok=True)\n",
    "        for f in missing_keys:\n",
    "            link_path = temp_audio_dir / f\"{f['key']}.wav\"\n",
    "            if not link_path.exists():\n",
    "                link_path.symlink_to(f['path'])\n",
    "        \n",
    "        extract_muq_embeddings(\n",
    "            temp_audio_dir, MAESTRO_MUQ_DIR, [f['key'] for f in missing_keys],\n",
    "            layer_start=9, layer_end=13\n",
    "        )\n",
    "    \n",
    "    # Load best model\n",
    "    best_ckpt = None\n",
    "    ckpt_candidates = [\n",
    "        CHECKPOINT_ROOT / 'A1c_stratified_fold' / 'fold0_best.ckpt',\n",
    "        CHECKPOINT_ROOT / 'A1a_piece_fold' / 'fold0_best.ckpt',\n",
    "    ]\n",
    "    for ckpt in ckpt_candidates:\n",
    "        if ckpt.exists():\n",
    "            best_ckpt = ckpt\n",
    "            break\n",
    "    \n",
    "    if best_ckpt is None:\n",
    "        raise RuntimeError(\"No trained model found!\")\n",
    "    \n",
    "    print(f\"Loading model from {best_ckpt.name}...\")\n",
    "    model = MuQStatsModel.load_from_checkpoint(best_ckpt)\n",
    "    model = model.to('cuda').eval()\n",
    "    \n",
    "    # Run inference and group by piece\n",
    "    piece_predictions = {}\n",
    "    for f in tqdm(maestro_audio_files, desc=\"MAESTRO inference\"):\n",
    "        emb_path = MAESTRO_MUQ_DIR / f\"{f['key']}.pt\"\n",
    "        if not emb_path.exists():\n",
    "            continue\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            emb = torch.load(emb_path, weights_only=True).unsqueeze(0).cuda()\n",
    "            if emb.shape[1] > 300:\n",
    "                emb = emb[:, :300, :]\n",
    "            mask = torch.ones(1, emb.shape[1], dtype=torch.bool).cuda()\n",
    "            pred = model(emb, mask).cpu().numpy()[0]\n",
    "            \n",
    "            title = f['title']\n",
    "            if title not in piece_predictions:\n",
    "                piece_predictions[title] = []\n",
    "            piece_predictions[title].append(pred)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Analyze variance across performers for each piece\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"MULTI-PERFORMER VARIANCE ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    intra_piece_stds = []\n",
    "    piece_stats = {}\n",
    "    \n",
    "    for title, preds in piece_predictions.items():\n",
    "        if len(preds) >= 2:\n",
    "            preds_array = np.array(preds)\n",
    "            mean_pred = preds_array.mean(axis=0)\n",
    "            std_pred = preds_array.std(axis=0)\n",
    "            \n",
    "            piece_stats[title] = {\n",
    "                'n_performances': len(preds),\n",
    "                'mean_std': float(std_pred.mean()),\n",
    "                'max_std': float(std_pred.max()),\n",
    "            }\n",
    "            intra_piece_stds.append(std_pred.mean())\n",
    "    \n",
    "    # Summary statistics\n",
    "    mean_intra_std = np.mean(intra_piece_stds) if intra_piece_stds else 0\n",
    "    \n",
    "    print(f\"Pieces with 2+ performances: {len(piece_stats)}\")\n",
    "    print(f\"Mean intra-piece std: {mean_intra_std:.4f}\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    if mean_intra_std > 0.1:\n",
    "        print(\"  High variance -> Model captures performer-specific interpretation\")\n",
    "    else:\n",
    "        print(\"  Low variance -> Model captures piece-invariant features\")\n",
    "    \n",
    "    # Save results\n",
    "    final_results = {\n",
    "        'experiment_id': exp_id,\n",
    "        'n_pieces': len(piece_stats),\n",
    "        'n_performances_total': sum(p['n_performances'] for p in piece_stats.values()),\n",
    "        'mean_intra_piece_std': float(mean_intra_std),\n",
    "        'piece_stats': piece_stats,\n",
    "    }\n",
    "    \n",
    "    with open(RESULTS_DIR / f'{exp_id}.json', 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS)\n",
    "    ALL_RESULTS[exp_id] = final_results\n",
    "else:\n",
    "    print(f\"\\nSKIP {exp_id}: already complete\")\n",
    "    existing = RESULTS_DIR / f'{exp_id}.json'\n",
    "    if existing.exists():\n",
    "        with open(existing) as f:\n",
    "            ALL_RESULTS[exp_id] = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3_maestro_exp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C3: MAESTRO Zero-Shot Transfer\n",
    "# Evaluate model on MAESTRO dataset (no ground truth, analyze prediction distribution)\n",
    "# Requires: MAESTRO_METADATA from setup cell\n",
    "\n",
    "exp_id = 'C3_maestro_transfer'\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: Zero-shot evaluation on MAESTRO dataset\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Check if MAESTRO data is available\n",
    "    if 'MAESTRO_METADATA' not in dir() or len(MAESTRO_METADATA) == 0:\n",
    "        raise RuntimeError(\"Run MAESTRO+ASAP setup cell first!\")\n",
    "    \n",
    "    print(f\"MAESTRO entries available: {len(MAESTRO_METADATA)}\")\n",
    "    \n",
    "    # Setup MuQ cache\n",
    "    MAESTRO_MUQ_DIR = MAESTRO_ROOT / 'muq_cache'\n",
    "    MAESTRO_MUQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Sample subset for analysis (full dataset is large)\n",
    "    MAX_SAMPLES = 500\n",
    "    random.seed(42)\n",
    "    maestro_sample = random.sample(MAESTRO_METADATA, min(MAX_SAMPLES, len(MAESTRO_METADATA)))\n",
    "    \n",
    "    # Get audio files\n",
    "    maestro_files = []\n",
    "    for entry in maestro_sample:\n",
    "        audio_path = MAESTRO_EXTRACTED / entry['audio_filename']\n",
    "        if audio_path.exists():\n",
    "            maestro_files.append({\n",
    "                'path': audio_path,\n",
    "                'key': entry['audio_filename'].replace('.wav', '').replace('/', '_'),\n",
    "                'canonical_title': entry['canonical_title'],\n",
    "                'year': entry.get('year', 'unknown'),\n",
    "                'split': entry.get('split', 'unknown'),\n",
    "            })\n",
    "    \n",
    "    print(f\"Sampled {len(maestro_files)} MAESTRO files for analysis\")\n",
    "    \n",
    "    # Extract MuQ embeddings\n",
    "    cached_keys = {p.stem for p in MAESTRO_MUQ_DIR.glob('*.pt')}\n",
    "    missing_keys = [f for f in maestro_files if f['key'] not in cached_keys]\n",
    "    \n",
    "    if missing_keys:\n",
    "        print(f\"Extracting {len(missing_keys)} MuQ embeddings...\")\n",
    "        temp_audio_dir = MAESTRO_ROOT / 'temp_audio'\n",
    "        temp_audio_dir.mkdir(exist_ok=True)\n",
    "        for f in missing_keys:\n",
    "            link_path = temp_audio_dir / f\"{f['key']}.wav\"\n",
    "            if not link_path.exists():\n",
    "                link_path.symlink_to(f['path'])\n",
    "        \n",
    "        extract_muq_embeddings(\n",
    "            temp_audio_dir, MAESTRO_MUQ_DIR, [f['key'] for f in missing_keys],\n",
    "            layer_start=9, layer_end=13\n",
    "        )\n",
    "    \n",
    "    # Load best model\n",
    "    best_ckpt = None\n",
    "    ckpt_candidates = [\n",
    "        CHECKPOINT_ROOT / 'A1c_stratified_fold' / 'fold0_best.ckpt',\n",
    "        CHECKPOINT_ROOT / 'A1a_piece_fold' / 'fold0_best.ckpt',\n",
    "    ]\n",
    "    for ckpt in ckpt_candidates:\n",
    "        if ckpt.exists():\n",
    "            best_ckpt = ckpt\n",
    "            break\n",
    "    \n",
    "    if best_ckpt is None:\n",
    "        raise RuntimeError(\"No trained model found!\")\n",
    "    \n",
    "    print(f\"Loading model from {best_ckpt.name}...\")\n",
    "    model = MuQStatsModel.load_from_checkpoint(best_ckpt)\n",
    "    model = model.to('cuda').eval()\n",
    "    \n",
    "    # Run inference\n",
    "    predictions = []\n",
    "    for f in tqdm(maestro_files, desc=\"MAESTRO inference\"):\n",
    "        emb_path = MAESTRO_MUQ_DIR / f\"{f['key']}.pt\"\n",
    "        if not emb_path.exists():\n",
    "            continue\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            emb = torch.load(emb_path, weights_only=True).unsqueeze(0).cuda()\n",
    "            if emb.shape[1] > 300:\n",
    "                emb = emb[:, :300, :]\n",
    "            mask = torch.ones(1, emb.shape[1], dtype=torch.bool).cuda()\n",
    "            pred = model(emb, mask).cpu().numpy()[0]\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Analyze prediction distributions\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"MAESTRO PREDICTION DISTRIBUTION\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Samples: {len(predictions)}\")\n",
    "    print(f\"\\n{'Dimension':<25} {'Mean':>8} {'Std':>8} {'Min':>8} {'Max':>8}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    prediction_stats = {}\n",
    "    for i, dim in enumerate(PERCEPIANO_DIMENSIONS):\n",
    "        dim_preds = predictions[:, i]\n",
    "        stats = {\n",
    "            'mean': float(dim_preds.mean()),\n",
    "            'std': float(dim_preds.std()),\n",
    "            'min': float(dim_preds.min()),\n",
    "            'max': float(dim_preds.max()),\n",
    "        }\n",
    "        prediction_stats[dim] = stats\n",
    "        print(f\"{dim:<25} {stats['mean']:>8.3f} {stats['std']:>8.3f} {stats['min']:>8.3f} {stats['max']:>8.3f}\")\n",
    "    \n",
    "    # Compare to PercePiano distribution\n",
    "    print(f\"\\nNote: MAESTRO is professional competition recordings,\")\n",
    "    print(f\"      so we expect higher mean predictions than PercePiano training data.\")\n",
    "    \n",
    "    # Save results\n",
    "    final_results = {\n",
    "        'experiment_id': exp_id,\n",
    "        'n_samples': len(predictions),\n",
    "        'prediction_stats': prediction_stats,\n",
    "        'note': 'Zero-shot transfer to MAESTRO (no ground truth labels)',\n",
    "    }\n",
    "    \n",
    "    with open(RESULTS_DIR / f'{exp_id}.json', 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS)\n",
    "    ALL_RESULTS[exp_id] = final_results\n",
    "else:\n",
    "    print(f\"\\nSKIP {exp_id}: already complete\")\n",
    "    existing = RESULTS_DIR / f'{exp_id}.json'\n",
    "    if existing.exists():\n",
    "        with open(existing) as f:\n",
    "            ALL_RESULTS[exp_id] = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ikkpvvot0f",
   "metadata": {},
   "source": [
    "---\n",
    "## PHASE D: Advanced Analysis\n",
    "\n",
    "Detailed analysis of model behavior and per-dimension performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y11h25kkoxq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D1: Per-Dimension R2 Breakdown\n",
    "# Analyze which of the 19 PercePiano dimensions are best/worst predicted\n",
    "exp_id = 'D1_per_dimension'\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: Per-dimension R2 breakdown for 19 PercePiano dimensions\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Use the best model from Phase A\n",
    "    best_exp_id = f\"A1{'a' if BEST_FOLD_METHOD == 'piece_based' else 'b' if BEST_FOLD_METHOD == 'performer_based' else 'c'}_{BEST_FOLD_METHOD.replace('_based', '') if 'based' in BEST_FOLD_METHOD else BEST_FOLD_METHOD}_fold\"\n",
    "    best_checkpoint_dir = CHECKPOINT_ROOT / best_exp_id\n",
    "    \n",
    "    if not best_checkpoint_dir.exists():\n",
    "        print(f\"Best model checkpoint not found: {best_checkpoint_dir}\")\n",
    "    else:\n",
    "        # Collect all predictions and labels across folds\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        for fold in range(4):\n",
    "            ckpt_path = best_checkpoint_dir / f\"fold{fold}_best.ckpt\"\n",
    "            if not ckpt_path.exists():\n",
    "                continue\n",
    "            \n",
    "            model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "            model.eval().to(\"cuda\")\n",
    "            \n",
    "            val_ds = MERTDataset(MUQ_L9_12_DIR, LABELS, BEST_FOLD_ASSIGNMENTS, fold, \"val\", M1C_CONFIG[\"max_frames\"])\n",
    "            val_dl = DataLoader(val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                                collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_dl:\n",
    "                    pred = model(batch[\"embeddings\"].cuda(), batch[\"attention_mask\"].cuda(), batch.get(\"lengths\"))\n",
    "                    all_preds.append(pred.cpu().numpy())\n",
    "                    all_labels.append(batch[\"labels\"].numpy())\n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "        \n",
    "        # Compute per-dimension R2\n",
    "        per_dim_r2 = {}\n",
    "        for i, dim_name in enumerate(PERCEPIANO_DIMENSIONS):\n",
    "            dim_r2 = r2_score(all_labels[:, i], all_preds[:, i])\n",
    "            per_dim_r2[dim_name] = float(dim_r2)\n",
    "        \n",
    "        # Sort by R2\n",
    "        sorted_dims = sorted(per_dim_r2.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n{'Dimension':<30} {'R2':>10} {'Category':<20}\")\n",
    "        print(\"-\"*65)\n",
    "        for dim_name, r2 in sorted_dims:\n",
    "            category = DIMENSION_CATEGORIES.get(dim_name, 'Unknown')\n",
    "            print(f\"{dim_name:<30} {r2:>10.4f} {category:<20}\")\n",
    "        \n",
    "        # Category-level aggregation\n",
    "        category_r2s = defaultdict(list)\n",
    "        for dim_name, r2 in per_dim_r2.items():\n",
    "            cat = DIMENSION_CATEGORIES.get(dim_name, 'Unknown')\n",
    "            category_r2s[cat].append(r2)\n",
    "        \n",
    "        print(f\"\\n{'Category':<20} {'Mean R2':>10} {'Count':>8}\")\n",
    "        print(\"-\"*40)\n",
    "        for cat, r2s in sorted(category_r2s.items(), key=lambda x: np.mean(x[1]), reverse=True):\n",
    "            print(f\"{cat:<20} {np.mean(r2s):>10.4f} {len(r2s):>8}\")\n",
    "        \n",
    "        # Save results\n",
    "        existing_results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "        final_results = {\n",
    "            'experiment_id': exp_id, 'status': 'complete',\n",
    "            'per_dimension_r2': per_dim_r2,\n",
    "            'sorted_dimensions': sorted_dims,\n",
    "            'category_mean_r2': {cat: float(np.mean(r2s)) for cat, r2s in category_r2s.items()},\n",
    "            'best_dimensions': sorted_dims[:5],\n",
    "            'worst_dimensions': sorted_dims[-5:],\n",
    "        }\n",
    "        with open(existing_results_file, 'w') as f:\n",
    "            json.dump(final_results, f, indent=2, default=numpy_serializer)\n",
    "        \n",
    "        sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)\n",
    "        ALL_RESULTS[exp_id] = final_results\n",
    "else:\n",
    "    print(f\"\\nSKIP {exp_id}: already complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lxeywwcsmwp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D2: Error Analysis by Composer, Difficulty, and Segment Position\n",
    "exp_id = 'D2_error_analysis'\n",
    "\n",
    "if should_run_experiment(exp_id, CHECKPOINT_ROOT, RESULTS_DIR, GDRIVE_RESULTS, COMPLETED_CACHE):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {exp_id}\")\n",
    "    print(f\"Description: Error analysis by composer, difficulty, segment position\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    def get_composer(key: str) -> str:\n",
    "        \"\"\"Extract composer from sample key.\"\"\"\n",
    "        parts = key.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            return parts[1] if parts[0].startswith('P') else parts[0]\n",
    "        return 'Unknown'\n",
    "    \n",
    "    def get_segment_position(key: str) -> str:\n",
    "        \"\"\"Extract segment position from key (early/middle/late).\"\"\"\n",
    "        parts = key.split('_')\n",
    "        for p in parts:\n",
    "            if p.startswith('seg'):\n",
    "                seg_num = int(p.replace('seg', ''))\n",
    "                if seg_num <= 2:\n",
    "                    return 'early'\n",
    "                elif seg_num <= 5:\n",
    "                    return 'middle'\n",
    "                else:\n",
    "                    return 'late'\n",
    "        return 'unknown'\n",
    "    \n",
    "    # Use best model predictions (reuse from D1 if available)\n",
    "    best_exp_id = f\"A1{'a' if BEST_FOLD_METHOD == 'piece_based' else 'b' if BEST_FOLD_METHOD == 'performer_based' else 'c'}_{BEST_FOLD_METHOD.replace('_based', '') if 'based' in BEST_FOLD_METHOD else BEST_FOLD_METHOD}_fold\"\n",
    "    best_checkpoint_dir = CHECKPOINT_ROOT / best_exp_id\n",
    "    \n",
    "    if not best_checkpoint_dir.exists():\n",
    "        print(f\"Best model checkpoint not found: {best_checkpoint_dir}\")\n",
    "    else:\n",
    "        # Collect predictions with keys\n",
    "        all_preds, all_labels, all_keys = [], [], []\n",
    "        \n",
    "        for fold in range(4):\n",
    "            ckpt_path = best_checkpoint_dir / f\"fold{fold}_best.ckpt\"\n",
    "            if not ckpt_path.exists():\n",
    "                continue\n",
    "            \n",
    "            model = MuQStatsModel.load_from_checkpoint(ckpt_path)\n",
    "            model.eval().to(\"cuda\")\n",
    "            \n",
    "            val_ds = MERTDataset(MUQ_L9_12_DIR, LABELS, BEST_FOLD_ASSIGNMENTS, fold, \"val\", M1C_CONFIG[\"max_frames\"])\n",
    "            val_dl = DataLoader(val_ds, batch_size=M1C_CONFIG[\"batch_size\"], shuffle=False,\n",
    "                                collate_fn=mert_collate_fn, num_workers=M1C_CONFIG[\"num_workers\"], pin_memory=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_dl:\n",
    "                    pred = model(batch[\"embeddings\"].cuda(), batch[\"attention_mask\"].cuda(), batch.get(\"lengths\"))\n",
    "                    all_preds.append(pred.cpu().numpy())\n",
    "                    all_labels.append(batch[\"labels\"].numpy())\n",
    "                    all_keys.extend(batch[\"keys\"])\n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "        \n",
    "        # Compute per-sample errors\n",
    "        errors = np.abs(all_preds - all_labels).mean(axis=1)  # Mean absolute error per sample\n",
    "        \n",
    "        # Analysis by composer\n",
    "        composer_errors = defaultdict(list)\n",
    "        for key, err in zip(all_keys, errors):\n",
    "            composer = get_composer(key)\n",
    "            composer_errors[composer].append(err)\n",
    "        \n",
    "        print(\"\\n--- Error by Composer ---\")\n",
    "        print(f\"{'Composer':<20} {'Mean MAE':>10} {'Std':>10} {'N':>8}\")\n",
    "        print(\"-\"*50)\n",
    "        for composer, errs in sorted(composer_errors.items(), key=lambda x: np.mean(x[1])):\n",
    "            if len(errs) >= 5:  # Only show composers with enough samples\n",
    "                print(f\"{composer[:20]:<20} {np.mean(errs):>10.4f} {np.std(errs):>10.4f} {len(errs):>8}\")\n",
    "        \n",
    "        # Analysis by segment position\n",
    "        position_errors = defaultdict(list)\n",
    "        for key, err in zip(all_keys, errors):\n",
    "            pos = get_segment_position(key)\n",
    "            position_errors[pos].append(err)\n",
    "        \n",
    "        print(\"\\n--- Error by Segment Position ---\")\n",
    "        print(f\"{'Position':<15} {'Mean MAE':>10} {'Std':>10} {'N':>8}\")\n",
    "        print(\"-\"*45)\n",
    "        for pos in ['early', 'middle', 'late', 'unknown']:\n",
    "            if pos in position_errors:\n",
    "                errs = position_errors[pos]\n",
    "                print(f\"{pos:<15} {np.mean(errs):>10.4f} {np.std(errs):>10.4f} {len(errs):>8}\")\n",
    "        \n",
    "        # Analysis by label magnitude (difficulty proxy)\n",
    "        label_means = all_labels.mean(axis=1)\n",
    "        low_label_mask = label_means < np.percentile(label_means, 33)\n",
    "        high_label_mask = label_means > np.percentile(label_means, 66)\n",
    "        \n",
    "        print(\"\\n--- Error by Label Magnitude ---\")\n",
    "        print(f\"{'Label Range':<20} {'Mean MAE':>10} {'N':>8}\")\n",
    "        print(\"-\"*40)\n",
    "        print(f\"{'Low (bottom 33%)':<20} {np.mean(errors[low_label_mask]):>10.4f} {sum(low_label_mask):>8}\")\n",
    "        print(f\"{'Medium (middle 33%)':<20} {np.mean(errors[~low_label_mask & ~high_label_mask]):>10.4f} {sum(~low_label_mask & ~high_label_mask):>8}\")\n",
    "        print(f\"{'High (top 33%)':<20} {np.mean(errors[high_label_mask]):>10.4f} {sum(high_label_mask):>8}\")\n",
    "        \n",
    "        # Save results\n",
    "        existing_results_file = RESULTS_DIR / f'{exp_id}.json'\n",
    "        final_results = {\n",
    "            'experiment_id': exp_id, 'status': 'complete',\n",
    "            'composer_errors': {k: {'mean': float(np.mean(v)), 'std': float(np.std(v)), 'n': len(v)} \n",
    "                               for k, v in composer_errors.items() if len(v) >= 5},\n",
    "            'position_errors': {k: {'mean': float(np.mean(v)), 'std': float(np.std(v)), 'n': len(v)}\n",
    "                               for k, v in position_errors.items()},\n",
    "            'label_magnitude_errors': {\n",
    "                'low': float(np.mean(errors[low_label_mask])),\n",
    "                'medium': float(np.mean(errors[~low_label_mask & ~high_label_mask])),\n",
    "                'high': float(np.mean(errors[high_label_mask])),\n",
    "            },\n",
    "        }\n",
    "        with open(existing_results_file, 'w') as f:\n",
    "            json.dump(final_results, f, indent=2, default=numpy_serializer)\n",
    "        \n",
    "        sync_experiment_to_gdrive(exp_id, final_results, RESULTS_DIR, CHECKPOINT_ROOT, GDRIVE_RESULTS, ALL_RESULTS)\n",
    "        ALL_RESULTS[exp_id] = final_results\n",
    "else:\n",
    "    print(f\"\\nSKIP {exp_id}: already complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9of2mmm2ffs",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Summary\n",
    "\n",
    "Generate paper-ready summary of all experiments and upload to GDrive.\n",
    "\n",
    "This section compiles all Phase A, B, and C results into tables suitable for the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "me7yqvyvp7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 60: Generate paper-ready summary table\n",
    "print(\"=\"*70)\n",
    "print(\"STRONGEST PAPER EXPERIMENTS: FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TIER 1 RESULTS (Critical for acceptance)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Multi-seed stability\n",
    "if 'multi_seed_stability' in ALL_RESULTS:\n",
    "    ms = ALL_RESULTS['multi_seed_stability']\n",
    "    print(f\"\\n1. Multi-Seed Stability:\")\n",
    "    print(f\"   Mean R2: {ms['summary']['mean_r2']:.4f}\")\n",
    "    print(f\"   Std R2:  {ms['summary']['std_r2']:.4f}\")\n",
    "    print(f\"   Status:  {ms['summary']['stability_status']}\")\n",
    "\n",
    "# Stratified folds\n",
    "if 'stratified_folds' in ALL_RESULTS:\n",
    "    sf = ALL_RESULTS['stratified_folds']\n",
    "    print(f\"\\n2. Stratified Fold Redistribution:\")\n",
    "    print(f\"   Avg R2: {sf['summary']['avg_r2']:.4f}\")\n",
    "    print(f\"   Std R2: {sf['summary']['std_r2']:.4f}\")\n",
    "    if 'comparison' in sf:\n",
    "        print(f\"   Variance reduction: {sf['comparison']['variance_reduction_pct']:.1f}%\")\n",
    "\n",
    "# D7 completion\n",
    "if 'D7_muq_baseline' in ALL_RESULTS:\n",
    "    d7 = ALL_RESULTS['D7_muq_baseline']\n",
    "    print(f\"\\n3. D7_muq_baseline (Complete):\")\n",
    "    print(f\"   Avg R2: {d7['summary']['avg_r2']:.4f}\")\n",
    "    print(f\"   CI: [{d7['summary']['r2_ci_95'][0]:.4f}, {d7['summary']['r2_ci_95'][1]:.4f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TIER 2 RESULTS (Strengthen significantly)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Pianoteq augmentation\n",
    "if 'pianoteq_augmentation' in ALL_RESULTS:\n",
    "    pa = ALL_RESULTS['pianoteq_augmentation']\n",
    "    print(f\"\\n4. Pianoteq Soundfont Augmentation:\")\n",
    "    print(f\"   Status: {pa['status']}\")\n",
    "    if 'soundfonts' in pa:\n",
    "        for sf, data in pa['soundfonts'].items():\n",
    "            print(f\"     {sf}: {data['count']} files\")\n",
    "\n",
    "# MAESTRO analysis\n",
    "if 'maestro_analysis' in ALL_RESULTS:\n",
    "    ma = ALL_RESULTS['maestro_analysis']\n",
    "    print(f\"\\n5. MAESTRO Cross-Dataset Analysis:\")\n",
    "    print(f\"   Samples: {ma['n_samples']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TIER 3 RESULTS (Polish and depth)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Error analysis\n",
    "if 'error_analysis' in ALL_RESULTS:\n",
    "    ea = ALL_RESULTS['error_analysis']\n",
    "    print(f\"\\n6. Error Analysis: Complete\")\n",
    "\n",
    "# Ablations\n",
    "if 'ablation_studies' in ALL_RESULTS:\n",
    "    ab = ALL_RESULTS['ablation_studies']\n",
    "    print(f\"\\n7. Ablation Studies: {ab['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ajduvcd45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 61: Save aggregate results\n",
    "aggregate_results = {\n",
    "    'notebook': 'train_strongest_paper.ipynb',\n",
    "    'description': 'Strongest paper experiments for ISMIR 2026',\n",
    "    'experiments': ALL_RESULTS,\n",
    "    'verification_checklist': {\n",
    "        'multi_seed_stability': 'multi_seed_stability' in ALL_RESULTS,\n",
    "        'stratified_folds': 'stratified_folds' in ALL_RESULTS,\n",
    "        'd7_complete': 'D7_muq_baseline' in ALL_RESULTS,\n",
    "        'pianoteq_augmentation': 'pianoteq_augmentation' in ALL_RESULTS,\n",
    "        'maestro_analysis': 'maestro_analysis' in ALL_RESULTS,\n",
    "        'error_analysis': 'error_analysis' in ALL_RESULTS,\n",
    "        'ablation_studies': 'ablation_studies' in ALL_RESULTS,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save locally\n",
    "with open(RESULTS_DIR / 'strongest_paper_all_results.json', 'w') as f:\n",
    "    json.dump(aggregate_results, f, indent=2, default=numpy_serializer)\n",
    "\n",
    "print(f\"Saved aggregate results to {RESULTS_DIR / 'strongest_paper_all_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eqrheiwkb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 62: Upload all results to GDrive\n",
    "print(\"\\nUploading all results to GDrive...\")\n",
    "\n",
    "# Upload aggregate results\n",
    "run_rclone(['rclone', 'copyto',\n",
    "            str(RESULTS_DIR / 'strongest_paper_all_results.json'),\n",
    "            f'{GDRIVE_RESULTS}/strongest_paper_all_results.json'],\n",
    "           \"Uploading aggregate results\")\n",
    "\n",
    "# Upload all individual result files\n",
    "for json_file in RESULTS_DIR.glob('*.json'):\n",
    "    run_rclone(['rclone', 'copyto',\n",
    "                str(json_file),\n",
    "                f'{GDRIVE_RESULTS}/{json_file.name}'],\n",
    "               f\"Uploading {json_file.name}\")\n",
    "\n",
    "# Upload all checkpoints\n",
    "if CHECKPOINT_ROOT.exists():\n",
    "    run_rclone(['rclone', 'copy',\n",
    "                str(CHECKPOINT_ROOT),\n",
    "                f'{GDRIVE_RESULTS}/checkpoints',\n",
    "                '--progress'],\n",
    "               \"Uploading all checkpoints\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nResults uploaded to: {GDRIVE_RESULTS}\")\n",
    "print(\"\\nVerification commands:\")\n",
    "print(f\"  rclone cat {GDRIVE_RESULTS}/multi_seed_stability.json | python3 -c \\\"import json,sys; d=json.load(sys.stdin); print(f'Mean R2: {{d[\\\\\\\"summary\\\\\\\"][\\\\\\\"mean_r2\\\\\\\"]:.4f}}')\\\"\")\n",
    "print(f\"  rclone cat {GDRIVE_RESULTS}/stratified_folds.json | python3 -c \\\"import json,sys; d=json.load(sys.stdin); print(f'Avg R2: {{d[\\\\\\\"summary\\\\\\\"][\\\\\\\"avg_r2\\\\\\\"]:.4f}}')\\\"\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
