CrescendAI Research & Product Timeline v2
==========================================
Restructured Feb 2025: Organized around the core question —
"How well is the student playing what the score asks for?"

Target user: Sarah — 3 years playing, no teacher, records on her phone,
wants direction on what to work on next.

North star: Give Sarah one piece of useful feedback on one passage she's
working on. Not perfect. Not comprehensive. One thing a teacher would
actually say after hearing her play.


Current Results (Jan 2025)
--------------------------
Audio Foundation Model (MuQ):
  - R² = 0.537 on PercePiano (19 quality dimensions)
  - 55% improvement over symbolic baseline
  - Validated: cross-soundfont, PSyllabus difficulty, MAESTRO zero-shot
  - Key limitation: captures piece characteristics more than performer quality

Contrastive Pairwise Ranking (Phase 1 — complete):
  - Model E2a: 84% overall pairwise accuracy (Kendall's tau = 0.681)
  - MuQ baseline: 70.3% → disentanglement adds +13.7 percentage points
  - Best dimensions: drama (90.6%), articulation_length (87.7%)
  - Trained on ASAP/MAESTRO same-piece, multi-performer pairs
  - Successfully disentangles piece characteristics from performer quality

Score Alignment Experiments (failed — lessons learned):
  - Experiment A: Standard DTW on raw MuQ cosine embeddings
    - Subsampled 5x, Sakoe-Chiba radius=500
    - ~18s weighted mean onset error, only 30% within 30ms threshold
    - 214 ASAP validation performances
  - Experiment B: Learned MLP projection (1024→512→256) with soft-DTW loss
    - 22 epochs → representation collapse to ~12s uniform error, 0% within 30ms
    - Failure mode: soft-DTW divergence optimizes global sequence similarity,
      not temporal correspondence → MLP collapsed embeddings to narrow region,
      flattening the cost matrix
  - Core insight: MuQ layers 9-12 encode slow-changing semantic content
    ("this sounds like Chopin"), not sharp temporal/harmonic features.
    These embeddings are fundamentally wrong for frame-level alignment.
  - Experiment C (measure-level pooling): not run — pooling frames to measures
    only loses resolution; the bottleneck is the feature representation itself.

Conclusion from alignment work:
  Don't make one model do everything. Use the right representation for each
  sub-problem. MuQ embeddings for quality assessment. Spectral/symbolic
  features for alignment. Combine at the feedback layer.


==========================================================================
REVISED PHASE STRUCTURE
==========================================================================

Phase 1: Musical Intelligence Layer [NEW PRIORITY]
---------------------------------------------------
PROMOTES: Old Phase 3 (Pedagogy). Now comes first because it's the thing
that makes Sarah trust the product and it solves the rubato problem.

SOLVES: Without this, the system flags every expressive choice as an error.
Sarah plays rubato, system says "timing is off," Sarah never comes back.
The core challenge isn't detecting deviations — it's distinguishing
intentional expression from error and deciding what's worth mentioning.

Core question: "How well is the student playing what the score asks for?"
NOT: "How much does this deviate from the score?"

Approach: Option B — Priority Signal (selected from three candidates)
  - Not "what's wrong" but "what's WORTH MENTIONING"
  - A teacher hears ten things, chooses to stop for one
  - Learning the filtering/attention function
  - This is exactly what prevents rubato from being flagged as error:
    the system *notices* tempo deviation, *recognizes* it as musically
    coherent, and *chooses not to mention it*

Why Option B over A and C:
  - Option A (Dimension Labels): clean ML formulation but imposes artificial
    structure on fluid teaching. Outputs classification, not prioritization.
  - Option C (Transformation Directions): elegant but requires paired examples
    of same passage with different qualities. Data-hungry and hard to validate.
  - Option B matches how teachers actually work: selective, not exhaustive.
    The filtering function IS the product.

Musical context encoding (Jai's domain expertise as technical moat):
  - Score-aware interpretation: low dynamics in a pianissimo passage = fine;
    low dynamics in a fortissimo passage = problem worth mentioning
  - Phrase structure awareness: tempo deviation at phrase boundaries =
    likely intentional; mid-phrase = likely error
  - Compensatory patterns: rubato has structure (slow down → catch up);
    uncontrolled fluctuation doesn't
  - Stylistic norms: Chopin permits different freedoms than Bach

Data source: Masterclass recordings (YouTube, Tonebase, conservatory archives)
  - Stopping points = implicit "this matters enough to halt momentum"
  - Verbal feedback = dimension vocabulary grounded in real teaching
  - Sequence of feedback = prioritization signal
  - Student level + repertoire = context

Deliverable: A working priority model that, given a quality assessment of a
passage and its musical context, outputs 1-3 things worth saying — and
correctly stays silent on things that aren't worth mentioning.

Success metric: Present system feedback alongside teacher feedback on same
performances. Do teachers agree the system mentioned the right thing?
Not necessarily the same words — but the same moment, the same issue.

Open questions:
  - Is teacher mental reference consistent across student levels?
  - Does masterclass feedback (advanced students) transfer to Sarah's level?
  - What's the minimum viable masterclass dataset to test this?
  - How to handle "noticed but didn't mention" — negative examples are
    critical for training priority signal but hard to collect
  - Can this be bootstrapped with rules/heuristics from Jai's expertise
    before needing a learned model?


Phase 2: Coarse Alignment [DRAMATICALLY DOWNSCOPED]
----------------------------------------------------
DEMOTES: Old Phase 2 (Score Alignment). No longer a blocker. No longer
requires sub-30ms onset accuracy.

SOLVES: Need to know roughly where in the piece the student is playing
so feedback can be localized to "the transition section" or "measures 20-24."

Key reframe: We do NOT need frame-level audio alignment. We need to answer
"what part of the piece is this?" at measure or phrase resolution. This is
a solved problem that doesn't require novel research.

Approach (tiered, start simple):
  1. User-assisted (MVP): Sarah selects the piece and passage she's working on.
     "I'm practicing measures 20-32 of Clair de Lune." Zero alignment needed.
  2. Coarse automatic: Chroma/CQT features via librosa → standard DTW against
     score. Sub-second accuracy is trivially achievable and more than sufficient
     for measure-level localization.
  3. Refined (later): If needed, improve with beat-tracking + symbolic alignment.

What we're NOT doing:
  - Sub-30ms onset alignment (unnecessary for feedback)
  - Audio-to-audio alignment in MuQ space (proven not to work)
  - Any alignment that blocks the MVP

The insight: Teachers think in musical landmarks ("the transition into the
development"), not timestamps. Measure-level is more than enough. Sarah
doesn't need "onset at 4.237s was 23ms late." She needs "the chromatic
run before the recapitulation is rushing."

Deliverable: Given a student recording and a piece/passage identifier,
return a mapping from audio time regions to score locations at measure
or phrase granularity.

Success metric: >90% of audio segments correctly mapped to the right
measure group. Validated on MAESTRO (ground truth exists).

Open questions:
  - For MVP, is user-assisted selection sufficient? (Probably yes.)
  - How to handle when Sarah skips a section or restarts mid-passage?
  - At what point does automatic alignment become necessary vs. nice-to-have?


Phase 3: Real Audio & Phone Recording Bridge [PULLED FORWARD]
--------------------------------------------------------------
PROMOTES: Old Phase 5. Now comes before MVP because it's an existential
risk. If the system doesn't work on phone recordings, nothing else matters.

SOLVES: All training is on synthetic Pianoteq audio. Sarah records on her
iPhone in an apartment with street noise. This gap will break everything
if not addressed early.

Known risks:
  - Room acoustics: reverb, resonance, early reflections
  - Microphone quality: phone mic frequency response, compression artifacts
  - Background noise: traffic, HVAC, conversation
  - Instrument variation: Sarah's upright piano ≠ concert Steinway
  - Recording distance/angle: phone on music stand vs. across the room

Approach:
  - Noise augmentation during training (room impulse responses, additive noise)
  - Evaluate existing MuQ model on phone recordings to measure the gap
  - Collect 50-100 phone recordings of piano performances (can start now,
    before any model changes — the data itself is valuable)
  - Domain adaptation if gap is significant

Critical test: Record the same performance simultaneously with Pianoteq
rendering AND phone recording. Does quality assessment agree? If not,
how much does it diverge?

Deliverable: Validated evidence that the quality model produces meaningful
results on phone-quality audio, or a clear plan to bridge the gap.

Success metric: Pairwise ranking accuracy on phone recordings within 10%
of synthetic audio performance (i.e., >75% if synthetic is 84%).

Open questions:
  - How much of the MuQ R²=0.537 reflects recording quality vs playing quality?
    Phone recordings might actually force better disentanglement.
  - Can aggressive augmentation during fine-tuning close the gap without
    catastrophic forgetting of quality signal?
  - Is there a simpler feature representation (e.g., mel spectrogram) that's
    more robust to recording conditions for quality assessment?
  - Should we collect phone recordings now, even before we have a model
    that works on them? (Yes. Start immediately.)


Phase 4: MVP — One Useful Piece of Feedback
--------------------------------------------
PROMOTES: Old Phase 6, now achievable because blockers are removed.

SOLVES: Sarah can upload a recording and get feedback a teacher would give.

Core experience:
  1. Sarah selects piece and passage (user-assisted alignment for v1)
  2. Uploads phone recording
  3. System evaluates quality across dimensions
  4. Musical intelligence layer filters and prioritizes
  5. Sarah receives 1-3 pieces of actionable feedback with musical context

Example output:
  "In measures 20-24, your dynamics are quite flat — the score marks a
  crescendo building toward the sforzando in measure 25. Try exaggerating
  the build more than feels natural; recordings tend to compress dynamics."

NOT:
  "Timing deviation: +23ms at onset 4.237s. Dynamics: 0.34 (below threshold).
  Articulation: 0.67. Tempo stability: 0.82..."

Technical requirements:
  - Works on phone recordings (Phase 3 validated)
  - Inference < 10 seconds per passage
  - Feedback is musically contextual (Phase 1)
  - User selects piece/passage manually (Phase 2, tier 1)

Deliverable: Working prototype that Sarah can use. Collect feedback from
5-10 real piano students to validate that the feedback is useful.

Success metric: >60% of feedback items rated "useful" or "very useful" by
students. At least one student says something like "I wouldn't have noticed
that on my own."

Open questions:
  - What's the right output format? Natural language? Annotated score?
  - How much score context (dynamics markings, tempo markings, phrase
    structure) needs to be fed into the system vs. inferred?
  - Should the first version support a curated set of pieces (where we
    pre-encode the score context) or be open-ended?
  - How to handle pieces not in our database?


Phase 5: Temporal Refinement & Automated Alignment
----------------------------------------------------
SOLVES: Moving beyond user-assisted passage selection to automatic
localization. Enabling feedback at finer temporal resolution.

Gate: Only pursue this AFTER validating that Phase 4 feedback is useful.
If the feedback itself isn't valuable, better alignment doesn't help.

Approach:
  - Automatic piece identification (audio fingerprinting or embedding matching)
  - Beat-level alignment using symbolic methods (chroma + DTW, well-established)
  - Multi-scale feedback: passage-level → phrase-level → measure-level
  - Attention-based localization from MuQ as interpretability signal

Deliverable: System that automatically identifies what piece/passage is
being played and provides measure-level localized feedback without user input.

Success metric: Correct piece identification >95%. Measure-level alignment
accuracy >90%. Feedback quality maintained or improved from Phase 4.


Phase 6: Student Data & Longitudinal Tracking
-----------------------------------------------
SOLVES: Tracking Sarah's progress over time. Personalizing feedback based
on her level, goals, and history.

- Partner with 1-2 music schools for systematic data collection
- Student recordings with paired teacher annotations
- Privacy/consent framework
- Progress tracking: "Your dynamics in Chopin have improved since last month"
- Personalization: system learns what Sarah already knows and focuses on
  what she's ready to learn next

Open questions:
  - Does masterclass-trained feedback transfer to intermediate students
    with basic errors? This phase tests that assumption.
  - What does useful progress tracking look like for a self-taught student?
  - How to avoid "teaching to the test" where student optimizes for the
    model's metrics rather than actual musical improvement?


Phase 7: Real-Time Practice Companion [UNCHANGED — LONG TERM]
--------------------------------------------------------------
COMPLEXITY: Very High (unsolved research + product/UX challenges)
DEPENDENCIES: Everything above plus new research

Vision: Seamless open recording session with AI filling gaps between playing
with guidance. Like a masterclass where AI can wait for natural pauses and
guide student through a lesson.

NOT: Eager AI that interrupts constantly or lists everything wrong
IS: Selective, prioritized, well-timed guidance that earns trust

Key challenges:
  - Interruption timing (wrong moment breaks flow, damages confidence)
  - Building trust (teacher earns right to interrupt through relationship)
  - Limited context (AI has seconds/minutes of audio, not years of history)
  - "When to speak" is harder than "what to say"
  - Model efficiency for streaming inference
  - Conversational timing in educational contexts (novel research area)


==========================================================================
OPEN RESEARCH QUESTIONS (kept for ongoing thinking)
==========================================================================

Architecture & Representation
  - Is there a representation that serves BOTH alignment and quality?
    (Current evidence says no, but worth revisiting as models evolve.)
  - Could a fine-tuned audio model learn alignment without catastrophic
    forgetting of quality signal? What's the actual cost to test this?
  - How much musical nuance do we actually lose in symbolic space for
    piano specifically? Pedaling, timbral intent, dynamic envelope — do
    these matter for Sarah's level?
  - If we go symbolic for alignment, why not evaluate in symbolic space
    entirely? What does the audio encoder buy us over a strong symbolic
    model for Sarah's use case?
  - Could we build a piano MIDI encoder that rivals MuQ quality results?
    What would that architecture look like?

The Rubato Problem (expression vs. error)
  - Rubato has compensatory timing structure. Can this be detected
    automatically in symbolic space via IOI curves?
  - Is the distinction between "intentional rubato" and "uncontrolled
    fluctuation" learnable, or does it require score context + stylistic
    knowledge?
  - Different periods/composers permit different expressive freedoms.
    How much stylistic knowledge needs to be encoded explicitly vs learned?
  - What about agogic accents, tenuto, fermata interpretation — other
    forms of intentional timing deviation?

Pedagogy & Priority
  - Is teacher mental reference consistent across student levels?
  - Does masterclass feedback (to advanced students) transfer to
    intermediate students with basic errors?
  - What's the minimum viable masterclass dataset to test priority signal?
  - Which is more learnable: "what to say" or "when to stay silent"?
  - Can Jai's own teaching intuitions be encoded as rules/heuristics
    to bootstrap the priority model before training on data?

Quality Assessment
  - How much of MuQ R²=0.537 reflects recording quality vs playing quality?
  - Can contrastive disentanglement be validated on pieces outside
    ASAP/MAESTRO? Sarah isn't playing competition repertoire.
  - What quality dimensions matter most for intermediate students?
    Drama and articulation_length are most disentangleable — are they
    also most pedagogically useful?
  - Annotation noise in PercePiano: what's the performance ceiling?

Score Alignment (archived but not forgotten)
  - Original goal: avoid audio→MIDI conversion loss. But if modern piano
    transcription gets 95%+ note F1, is the loss actually meaningful
    for Sarah's use case?
  - The synthesized-audio-comparison approach fails because a metronomically
    flat rendering makes every expressive choice look like an error.
    Could an expressive rendering (style transfer) solve this? Whose
    interpretation? Is this even tractable?
  - Chroma/CQT DTW for coarse alignment: what's the actual accuracy on
    student recordings with wrong notes and restarts?

Product & UX
  - What does Sarah actually want to hear? "Your dynamics are flat" vs
    "try exaggerating the crescendo" vs "listen to how Horowitz shapes
    this phrase" — which framing drives practice behavior?
  - Curated piece library vs open-ended? Tradeoff between quality of
    feedback and breadth of repertoire.
  - How to handle pieces with no score in the database?
  - What's the right feedback cadence? Every recording? Weekly summaries?


==========================================================================
DATA NEEDS SUMMARY (updated)
==========================================================================

Have:
  - PercePiano: 1,202 segments with perceptual ratings (19 dimensions)
  - ASAP/MAESTRO: multi-performer data, enables contrastive training
  - PSyllabus: difficulty ratings
  - Contrastive model E2a: trained, 84% pairwise accuracy

Need (ordered by priority):
  1. Phone recordings of piano performances (start collecting NOW, Phase 3)
     - Even 50 recordings helps characterize the synthetic→real gap
     - Recruit from local piano teachers, online communities, friends
  2. Masterclass corpus with timestamps + transcriptions (Phase 1)
     - Stopping points, verbal feedback, student level, repertoire
     - Start with 10-20 masterclasses to test feasibility
  3. Teacher validation set (Phase 1 & 4)
     - Give teachers student recordings, collect their top 1-3 feedback items
     - Compare against system output
  4. Student recordings with paired teacher feedback (Phase 6)
     - Systematic collection, privacy/consent framework
     - Partner with music schools